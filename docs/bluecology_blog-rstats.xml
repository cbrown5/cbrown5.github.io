<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Seascapemodels</title>
<link>https://www.seascapemodels.org/bluecology_blog.html#category=rstats</link>
<atom:link href="https://www.seascapemodels.org/bluecology_blog-rstats.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Tue, 04 Nov 2025 13:00:00 GMT</lastBuildDate>
<item>
  <title>Keeping enthusiasm about learning R</title>
  <link>https://www.seascapemodels.org/posts/2025-11-04-LLMs-coding-carbon-emissions/</link>
  <description><![CDATA[ 





<p>Staying enthused about learning R is a challenge. You need progress from the initial motivation driven by ‘I need to learn this skill for my research career’ through a phase of tedium on your way to a competent R programmer.</p>
<p>Here’s my story.</p>
<section id="fitting-a-multinomial-peg-into-an-anova-hole" class="level2">
<h2 class="anchored" data-anchor-id="fitting-a-multinomial-peg-into-an-anova-hole">Fitting a multinomial peg into an ANOVA hole</h2>
<p>I first attempted to learn R while wintering at a remote field station in the Southern Ocean. I had one book, 2004 satellite internet and a lot of time to kill.</p>
<p>Learning R was hard. Then you also have to learn statistics.</p>
<p>I had chosen a hard statistical question to start with. My colleague asked for help analysing data. The aim was to predict which of three behaviours flocks of birds would show under different situations. I was trying to wrangle ANOVA to do this analysis, because that is the method my book described.</p>
<p>I went around and around in circles. Were the behaviours factors or the response variable? If they were a response variable, how could the response have just three types of unordered outcomes? In hindsight I now see why I went in circles. The appropriate analysis would have been a multinomial model. Easy with <code>brms</code> (which didn’t exist then) or <code>mgcv</code>, but GLMs were not covered in standard text books in 2004, let alone multinomial GLMs.</p>
<p>I did however manage to learn how to code an ANOVA, as well as learning a bit about R code.</p>
<p>Coding requires us to be frustratingly precise in ways that human languages are not. This is hard for new programmers. And the learning curve for R is reknowned for being steeper than other coding languages.</p>
<p>The first step to competence is comprehending the exactness of programming.</p>
</section>
<section id="like-a-spoken-language-but-pedantic" class="level2">
<h2 class="anchored" data-anchor-id="like-a-spoken-language-but-pedantic">Like a spoken language, but pedantic</h2>
<p>After my misadventure in the Southern Ocean I studied evolutionary modelling for my Honours thesis. I used Matlab, a commercial product developed by a single company. Matlab has the major benefit of its consistency. Its functions are used in consistent ways, the help files have a consistent style.</p>
<p>R is open-source. ‘Learning R’ is an experience in trying to comprehend the thinking of scores of different open-source developers.</p>
<p>The <code>R is an ecosystem</code> cliche comes to mind. R is like a real ecosystem. The software ecosystem’s solutions to problems are diverse.</p>
<p>More often than someone has coded a solution to your problem in an R package. If only your could grasp how to use that solution the way its creator intended.</p>
</section>
<section id="data-structures" class="level2">
<h2 class="anchored" data-anchor-id="data-structures">Data structures</h2>
<p>Undersanding data structures was a key insight…</p>
<p>Curiosity helps.</p>
<p>R has been my main science tool for over 20 years now.</p>
<p>As I gained more comprehension of R, stats in R became more like a puzzle. My own curiosity about what was possible kept me going. There is always something new to discover in a diverse software ecosystem.</p>
<p>At times the imperitive quickly became procrastination. I’ve spent too many late nights trying a new spatial or Bayesian stats package, just out of curiosity.</p>
<p>Twenty years on I often find it easier to code up with my own solutions to my research questions, rather than use someone else’s packaged solution. Its easier to comprehend a problem if you work through it yourself. Its easier to understand your own R style than someone else’s.</p>
<p>Many people learn R because they have to. It is hard to work in many fields of science without some programming skills.</p>
<p>Impact of genAI</p>
<p>Other people’s innovations add</p>
<p>Most data problems you come up with development statistical programmers</p>
<p>Always someting new to discover</p>
<p>There’s been turn-over too. Pipes and <code>tidyverse</code> changed everything. These packages made data wrangling and plotting so much easier.</p>
<p>Teaching R got easier. I could see 95% of my students checking out whenever I had to explain a cracker nested operation like <code>mean(log(subset(dat, grp == 'a')$x))</code></p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-11-04-LLMs-coding-carbon-emissions/</guid>
  <pubDate>Tue, 04 Nov 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Upcoming R workshops in Brisbane Feebruary 2026</title>
  <link>https://www.seascapemodels.org/posts/2025-11-03-upcoming-R-workshop-brisbane/</link>
  <description><![CDATA[ 





<p>R workshops in Brisbane Australia, February 2026.</p>
<p>For many years I had the pleasure of teaching with Ant Richardson and the team for these R workshops. I’m not there anymore, but recommend attending these if you can make the dates:</p>
<p>Transform your research questions into robust statistical insights with our 2026 Summer R Workshops in an immersive exploration of R programming and statistical modelling. These workshops in February 2026 are designed for beginners and researchers who want to build a solid foundation in R programming and progress through to advanced statistical techniques. Whether you’re completely new to R or looking to strengthen your statistical modelling skills, join us to develop the analytical expertise essential for modern research!</p>
<p>Registration is now open. For more information, see our website https://mathmarecol.github.io/RWorkshop/summer.html and the attached flyer. For any questions, email cbcs-workshops@uq.edu.au.</p>
<p>Testimonials: “I truly enjoyed the whole workshop. Great teachers, really nice real data examples and good notes and background” “I am feeling a lot better about using stats now than I was previously” “I think it should be a compulsory course for all PhDs”</p>



 ]]></description>
  <category>rstats</category>
  <guid>https://www.seascapemodels.org/posts/2025-11-03-upcoming-R-workshop-brisbane/</guid>
  <pubDate>Sun, 02 Nov 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Should we still teach R coding in this age of genAI?</title>
  <link>https://www.seascapemodels.org/posts/2025-11-02-should-we-teach-coding/</link>
  <description><![CDATA[ 





<p>I often get asked if we should still be teaching coding skills to students and researchers now that we have generative AI tools that can write code for us.</p>
<p>The general consensus is yes, we should still teach coding. For an ecology specific <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14325">argument see here</a>.</p>
<p>Teaching coding is teaching people how to think logically and structure problems. These skills are essential not only in coding, but also for good science and problem solving in general.</p>
<p>So you need to learn to code to learn to think logically and structure problems.</p>
<p>A more practical standpoint is that LLMs are good at generating code that they have seen a lot of examples of in their data. But they are not so good at creative work on novel topics or niche areas.</p>
<p>For example, AI coding assistants struggle to make a simple <code>tmap</code> (the R mapping package) work well. <code>tmap</code> was updated recently, meaning the examples in the LLM’s training are out of date.</p>
<p>The coding assistants also seem to default to code patterns they are most familiar with, like <code>ggplot2</code> syntax. Not all of this works with <code>tmap</code>.</p>
<p>In general, I find they perform much better at statistical modelling in R than they do with complex geospatial analyses.</p>
<p>This practical issue may increasingly become less relevant as LLMs get better and we create better resources to inform their actions (like a tmap specific guide for LLMs to read before advising you, or web searches of the tmap page).</p>
<p>But it is still likely that frontier modelling will require deep human engagement with code. So learning to code is important for researchers.</p>
<p>I’ll leave the final say to Andrej Karpathy, former head of AI at Tesla and founding member of OpenAI’s research group.</p>
<p>In a <a href="https://www.dwarkesh.com/p/andrej-karpathy">recent podcast interview</a> he talks about a repo he made to help teach people how to work with LLMs. His advice to learners is literally to re-write his repo by hand, not even to cut and paste.</p>
<p>He goes on to explain that AI agents are good for very standard code or code that is common on the internet. But they perform poorly for creating new code, or code that contradicts common patterns.</p>
<p>For any research code he most commonly uses the code auto-complete features that AI assistants have, rather than AI agents.</p>
<p>He suggests that to really learn something you should code it yourself, using examples only as references.</p>
<p>Karpathy came up with the term ‘vibe-coding’ which means creating code by prompting, without reviewing the code. Usually an AI agent is used. So its saying a lot if he thinks that learning means doing it the old fashioned way of writing code yourself.</p>



 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-11-02-should-we-teach-coding/</guid>
  <pubDate>Sat, 01 Nov 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Carbon emissions…</title>
  <link>https://www.seascapemodels.org/posts/2025-11-05-R-collaborator-for-20-years/</link>
  <description><![CDATA[ 





<p>Estimate carbon emissions from LLMs coding assitants</p>



 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-11-05-R-collaborator-for-20-years/</guid>
  <pubDate>Sat, 01 Nov 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Environmental impacts of large language models</title>
  <link>https://www.seascapemodels.org/posts/2025-10-22-environmental-impacts-LLMs/</link>
  <description><![CDATA[ 





<p>Just want to quickly highlight a few investigative blogs on the environmental impacts of LLMs.</p>
<p>There are two main issues: water use (for cooling servers) and carbon emissions generated by creating the hardware, training the models and then using the models to get responses (‘inference’).</p>
<p>This blog does a deep dive on the <a href="https://andymasley.substack.com/p/the-ai-water-issue-is-fake">water issues</a>. The key finding is that the water issues seem to be overstated (he says ‘fake’ but I don’t agree with that word choice, water issues are real, just overstated).</p>
<p>Then academic Hannah Ritchies and author of ‘Sustainability by Numbers’ page sums up the <a href="https://www.sustainabilitybynumbers.com/p/carbon-footprint-chatgpt">carbon cost of chatGPT</a>. She draws heavily on <a href="https://andymasley.substack.com/p/a-few-meta-points-on-my-posts-on">Andy Masley</a> detailed work.</p>
<p>In particular, blog author (<a href="https://www.andymasley.com/">Andy Masley</a>) argues that the public has limited attention span for environmental issues and AI isn’t the biggest priority. He sums it up nicely:</p>
<p>“AI water use is mostly fake, personal prompt costs are mostly fake, but AI as a whole is going to put a huge strain on our electrical grid going forward that’s going to matter a lot for the green energy transition, local electricity prices, and air pollution”</p>
<p>By fake, he means your personal usage is trivially small. He highlights golf courses and streaming videos as much larger uses of water and emissions respectively.</p>
<p>What also matters is the counterfactual - ie what would you be doing if you weren’t using chatGPT? If you would be online shopping, using a microwave or streaming videos instead, then your personal emissions are probably slightly lower if you switch your time to chatGPT.</p>
<p>If you are concerned about the impacts of your AI use you can easily do a personal offset by commiting to some simple behaviour changes. <a href="https://www.it.ox.ac.uk/article/reducing-energy-consumption-pcs">e.g.&nbsp;turn you computer off at night and turn it off at the power plug</a>.</p>
<section id="what-about-coders" class="level2">
<h2 class="anchored" data-anchor-id="what-about-coders">What about coders?</h2>
<p>One unaddressed question in these posts is the personal impacts of genAI power users (like me). By that I mean people using it for coding. The blogs above deal with people who are doing the occaisonal short chatGPT prompt. Coders might be leaning on AI hundreds of times a day for prompts that range from short to very long (and more energy intensive).</p>
<p>That’s something I’d like to do the numbers for later on.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-10-22-environmental-impacts-LLMs/</guid>
  <pubDate>Tue, 21 Oct 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>modelbased package for plotting interactions</title>
  <link>https://www.seascapemodels.org/posts/2025-10-21-modelbased/</link>
  <description><![CDATA[ 





<p>I’ve been a big fan of the <code>visreg</code> package for a while, it makes it convenient to plot predictions from models. But recently I was struggling with a model that had many two-way interactions terms.</p>
<p>I checked out the key alternative to <code>visreg</code>, <code>ggeffects</code>, which is very popular (the paper has over 2000 citations). But the <code>ggeffects</code> page states it will be superseded by the <a href="(https://easystats.github.io/modelbased/)"><code>modelbased</code> package</a>. So here’s a quick demo of how to use <code>modelbased</code> to plot interaction effects from a GLM.</p>
<p><code>modelbased</code> is part of an ecosystem of packages for handling statistical models: <a href="https://easystats.github.io/modelbased/reference/estimate_slopes.html#references"><code>easystats</code></a>.</p>
<p>First I’m going to load data on coral cover and fish abundance from my <a href="https://github.com/cbrown5/example-ecological-data">Example ecological data</a> repo:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ggplot2)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(dplyr)</span></code></pre></div></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(easystats)</span></code></pre></div></div>
<div class="cell-output cell-output-stderr">
<pre><code># Attaching packages: easystats 0.7.5 (red = needs update)
✔ bayestestR  0.17.0   ✔ correlation 0.8.8 
✖ datawizard  1.2.0    ✔ effectsize  1.0.1 
✔ insight     1.4.2    ✔ modelbased  0.13.0
✖ performance 0.15.1   ✔ parameters  0.28.2
✔ report      0.6.1    ✔ see         0.12.0

Restart the R-Session and update packages with `easystats::easystats_update()`.</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(readr)</span>
<span id="cb7-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load data from https://github.com/cbrown5/example-ecological-data</span></span>
<span id="cb7-3"></span>
<span id="cb7-4">dat_url <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/fish-coral-cover-sites.csv"</span></span>
<span id="cb7-5"></span>
<span id="cb7-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Convert counts of coral to proportions</span></span>
<span id="cb7-7">dat <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">read_csv</span>(dat_url) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb7-8">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb7-9">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">cb_cover =</span> cb_cover <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_pts, </span>
<span id="cb7-10">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">soft_cover =</span> soft_cover <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_pts</span>
<span id="cb7-11">    ) </span></code></pre></div></div>
<div class="cell-output cell-output-stderr">
<pre><code>Rows: 49 Columns: 13</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr  (2): flow, logged
dbl (11): site, reef.id, pres.topa, pres.habili, secchi, coordx, coordy, cb_...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
</div>
</div>
<p>Now we’ll fit a simple poisson GLM. <code>pres.topa</code> is count of topa fish, a fish that likes to live in branching corals (<code>cb_cover</code>), and <code>logged</code> is a binary variable indicating whether the site was near to logging or not (which causes pollution that harms fish and corals).</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">m1 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glm</span>(pres.topa <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> cb_cover <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>  logged,</span>
<span id="cb10-2">          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> dat,</span>
<span id="cb10-3">          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> poisson)</span>
<span id="cb10-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summary</span>(m1)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = pres.topa ~ cb_cover * logged, family = poisson, 
    data = dat)

Coefficients:
                          Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -3.839      1.124  -3.415 0.000639 ***
cb_cover                     8.691      1.979   4.392 1.12e-05 ***
loggedNot logged             5.313      1.136   4.675 2.93e-06 ***
cb_cover:loggedNot logged   -7.830      2.017  -3.882 0.000104 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 426.61  on 48  degrees of freedom
Residual deviance: 265.33  on 45  degrees of freedom
AIC: 353.22

Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
<p>It simple to then plot the interaction effects. I’m going to do so on <code>link</code> scale (log scale for poisson models):</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">pr <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">estimate_means</span>(m1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cb_cover"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logged"</span>),</span>
<span id="cb12-2">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">estimate =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical"</span>,</span>
<span id="cb12-3">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">type =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link"</span>)</span>
<span id="cb12-4"></span>
<span id="cb12-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot</span>(pr) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-6">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(</span>
<span id="cb12-7">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hard coral cover"</span>,</span>
<span id="cb12-8">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted Topa abundance (log scale)"</span>,</span>
<span id="cb12-9">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Soft coral cover"</span>, </span>
<span id="cb12-10">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fill =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Soft coral cover"</span></span>
<span id="cb12-11">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>  </span>
<span id="cb12-12">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_classic</span>()</span></code></pre></div></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://www.seascapemodels.org/posts/2025-10-21-modelbased/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>One neat thing that <code>modelbased</code> does is give you different options for how to estimate marginal means. The default is “typical”, which estimates marginal means at the average value of numerical predictors and averages over frequencies of categorical predicotrs. But you can also do “average” which averages over sample distributions and “population” which attemps to create a counterfactual for all possible combinations of variables.</p>
<p>These answer different questions, see the <a href="https://easystats.github.io/modelbased/reference/estimate_means.html">documentation for more details</a>.</p>
<p>I’m going to make a dataframe of all three methods so we can compare them:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">pr_typical <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">estimate_means</span>(m1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cb_cover"</span>,</span>
<span id="cb13-2">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">estimate =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical"</span>,</span>
<span id="cb13-3">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">type =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link"</span>)</span>
<span id="cb13-4"></span>
<span id="cb13-5">pr_average <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">estimate_means</span>(m1, </span>
<span id="cb13-6">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">cb_cover =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">unique</span>(dat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>cb_cover)),</span>
<span id="cb13-7">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">estimate =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"average"</span>,</span>
<span id="cb13-8">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">type =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link"</span>)</span>
<span id="cb13-9"></span>
<span id="cb13-10">pr_population <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">estimate_means</span>(m1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cb_cover"</span>),</span>
<span id="cb13-11">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">estimate =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"population"</span>,</span>
<span id="cb13-12">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">type =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link"</span>)</span>
<span id="cb13-13"></span>
<span id="cb13-14">pr_all <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(</span>
<span id="cb13-15">    pr <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">method =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical by logging"</span>),</span>
<span id="cb13-16">    pr_typical <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">method =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical"</span>),</span>
<span id="cb13-17">    pr_average <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">method =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"average"</span>),</span>
<span id="cb13-18">    pr_population <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">method =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"population"</span>)</span>
<span id="cb13-19">)</span></code></pre></div></div>
</div>
<p>Now we plot them:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(pr_all, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> cb_cover, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> Mean, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> logged)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb14-2">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb14-3">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_linerange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymin =</span> CI_low, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymax =</span> CI_high)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb14-4">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>method) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb14-5">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(</span>
<span id="cb14-6">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hard coral cover"</span>,</span>
<span id="cb14-7">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted Topa abundance (log scale)"</span>,</span>
<span id="cb14-8">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Logged"</span></span>
<span id="cb14-9">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>  </span>
<span id="cb14-10">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_classic</span>()</span></code></pre></div></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://www.seascapemodels.org/posts/2025-10-21-modelbased/index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>And one more plot just of population and typical to more easily see the diffence:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1">pd <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">position_dodge</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">width =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>)</span>
<span id="cb15-2">pr_all <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb15-3">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(method <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%in%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"population"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical"</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb15-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(., <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> cb_cover, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> Mean, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> method)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb15-5">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">position =</span> pd) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb15-6">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_linerange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymin =</span> CI_low, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymax =</span> CI_high), </span>
<span id="cb15-7">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">position =</span> pd) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb15-8">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(</span>
<span id="cb15-9">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hard coral cover"</span>,</span>
<span id="cb15-10">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted Topa abundance (log scale)"</span>,</span>
<span id="cb15-11">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Logged"</span></span>
<span id="cb15-12">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>  </span>
<span id="cb15-13">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_classic</span>()</span></code></pre></div></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://www.seascapemodels.org/posts/2025-10-21-modelbased/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>You can see see some key differences. The typical method shows effects of hard coral cover for a typical observation. There are slightly more unlogged than logged sites in the data:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">table</span>(dat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>logged)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Logged Not logged 
        21         28 </code></pre>
</div>
</div>
<p>So the typical sample is different from a balanced population level average.</p>
<p>The population and typical predictions fall in between the predictions that are conditional on logging status, because both are kinds of averages over the two logging categories.</p>
<p>Predictions from the ‘average’ method show an inconsistent pattern, because they are averaging over samples. You can roughly see the trends for logged and unlogged sites.</p>
<p>Finally, let’s convince ourselves that the unbalanced data is causing these differences. I’m going to randomly remove some unlogged sites to create a balanced dataset, and refit the model:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#randomly remove 7 'Not logged' sites to create balance</span></span>
<span id="cb18-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">set.seed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">707</span>)</span>
<span id="cb18-3">irm <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">which</span>(dat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>logged <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Not logged"</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)</span>
<span id="cb18-4">dat2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> dat[<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>irm, ]</span>
<span id="cb18-5"></span>
<span id="cb18-6">m2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glm</span>(pres.topa <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> cb_cover <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>  logged,</span>
<span id="cb18-7">          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> dat2,</span>
<span id="cb18-8">          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> poisson)</span>
<span id="cb18-9"></span>
<span id="cb18-10">pr_typical2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">estimate_means</span>(m2, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cb_cover"</span>,</span>
<span id="cb18-11">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">estimate =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical"</span>,</span>
<span id="cb18-12">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">type =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link"</span>)</span>
<span id="cb18-13"></span>
<span id="cb18-14">pr_population2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">estimate_means</span>(m2, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cb_cover"</span>),</span>
<span id="cb18-15">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">estimate =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"population"</span>,</span>
<span id="cb18-16">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">type =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link"</span>)</span>
<span id="cb18-17"></span>
<span id="cb18-18">pr_all2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(</span>
<span id="cb18-19">    pr_typical2 <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">method =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"typical"</span>),</span>
<span id="cb18-20">    pr_population2 <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">method =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"population"</span>)</span>
<span id="cb18-21">)</span>
<span id="cb18-22"></span>
<span id="cb18-23">pd <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">position_dodge</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">width =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>)</span>
<span id="cb18-24"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(pr_all2, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> cb_cover, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> Mean, </span>
<span id="cb18-25">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> method)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-26">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">position =</span> pd) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-27">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_linerange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymin =</span> CI_low, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymax =</span> CI_high),</span>
<span id="cb18-28">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">position =</span> pd) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-29">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(</span>
<span id="cb18-30">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hard coral cover"</span>,</span>
<span id="cb18-31">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Predicted Topa abundance (log scale)"</span>,</span>
<span id="cb18-32">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Logged"</span></span>
<span id="cb18-33">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>  </span>
<span id="cb18-34">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_classic</span>()</span></code></pre></div></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://www.seascapemodels.org/posts/2025-10-21-modelbased/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>You can see the two types of predictions are identical with balanced data.</p>



 ]]></description>
  <category>rstats</category>
  <guid>https://www.seascapemodels.org/posts/2025-10-21-modelbased/</guid>
  <pubDate>Mon, 20 Oct 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Resource for using Large Language Model tools in R</title>
  <link>https://www.seascapemodels.org/posts/2025-10-06-llm-tools-for-r-book/</link>
  <description><![CDATA[ 





<p>If you’re interested in using Large Language Models (LLMs) with R, you should check out Luis D. Verde Arregoitia’s new online resource: <a href="https://luisdva.github.io/llmsr-book/index.html">Large Language Model tools for R</a>. Its available in English and Spanish.</p>
<p>The resouce provides a comprehensive guide to working with LLMs in R, covering packages, techniques, and practical applications.</p>
<p>This is a great complement to my <a href="https://www.seascapemodels.org/AI-assistants-for-scientific-coding/">workshop and book</a> that guides you through some of the options and prompt engineering for high quality R analyses.</p>
<p>If you’re looking to integrate AI capabilities into your R workflows, this book is definitely worth exploring.</p>
<p>Check it out: <a href="https://luisdva.github.io/llmsr-book/index.html">https://luisdva.github.io/llmsr-book/index.html</a></p>



 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-10-06-llm-tools-for-r-book/</guid>
  <pubDate>Sun, 05 Oct 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>New online book AI Assistants for Scientific Coding</title>
  <link>https://www.seascapemodels.org/posts/2025-10-05-AI-assistants-for-scientific-coding/</link>
  <description><![CDATA[ 





<p>I’ve released a new online book, <a href="https://www.seascapemodels.org/AI-assistants-for-scientific-coding/">AI Assistants for Scientific Coding</a>. It’s a practical guide to using language models to support scientific computing and analysis.</p>
<p>The book focuses on helping people who already use R or Python for data analysis. It’s not an introduction to programming; instead, it shows how to work with AI tools effectively and responsibly once you know the basics.</p>
<p>What you’ll find inside:</p>
<ul>
<li>Choosing and using AI coding assistants, from simple chat tools to agents that can run and test code</li>
<li>Prompting strategies that improve reliability for real analysis tasks</li>
<li>Examples from environmental science (GLMs, multivariate stats), with methods general to other fields</li>
<li>Notes on ethics, copyright, costs, and environmental impacts</li>
</ul>
<p>Read it here: https://www.seascapemodels.org/AI-assistants-for-scientific-coding/</p>
<p>The material also serves as reference notes for a one‑day workshop and will evolve as the field changes. If you’re interested in the prompting side of statistical workflows, there’s an accompanying preprint: https://doi.org/10.32942/X2CS80</p>



 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-10-05-AI-assistants-for-scientific-coding/</guid>
  <pubDate>Sat, 04 Oct 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Automating the Github Copilot Agent from the command line with Copilot CLI</title>
  <link>https://www.seascapemodels.org/posts/2025-10-04-running-github-copilot-CLI-agents-from-R/</link>
  <description><![CDATA[ 





<p><a href="https://docs.github.com/en/copilot/concepts/agents/about-copilot-cli">Github Copilot CLI (Command Line Interface)</a> recently became available. It is an AI agent, meaning it can use tools autonomously in a loop. It has two modes:</p>
<ol type="1">
<li><p>An interactive mode that is like a terminal version of the Copilot agent that runs in the chat window</p></li>
<li><p>A programmatic mode that can be run with shell scripts.</p></li>
</ol>
<p>The Copilot CLI is available with paid versions of github copilot.</p>
<p>The programmatic mode interested me, because it allows you to write scripts that call github copilot agents. This means you could run replicate agents on the same problem, then gather there results for analysis.</p>
<p>I, of course, want to do everything from the R program, because I’m not great with unix code or Python. <a href="https://docs.github.com/en/copilot/how-tos/set-up/install-copilot-cli">Once you’ve setup the copilot CLI</a>, its very easy to run it from R:</p>
<pre><code>
copilot_cmd &lt;- "copilot -p 'Set-up this project directory with a readme.md file, and directories for outputs, scripts, plots.' --allow-all-tools"

system(copilot_cmd)
</code></pre>
<p>This will run the agent autonomously in the current working directory of your R session.</p>
<p>Now I wouldn’t recommend using <code>--allow-all-tools</code> like this however. <a href="https://docs.github.com/en/copilot/concepts/agents/about-copilot-cli#security-considerations">There are important security considerations</a>. Like prompt injection attacks where an Agent goes on the web and gets tricked into doing something bad to your computer when it reads some malicious content. Likewise, agents can just stuff up and just delete or overwrite a bunch of files you wanted to keep.</p>
<p>You do need to allow some tools however, otherwise you might as well run the agent in interactive mode (because you’ll have to manually approve every tool use). This defeats the time-saving goal of running agents in a loop.</p>
<p>Here’s my current set-up:</p>
<p><code>copilot -p 'A prompt here' --allow-all-tools --deny-tool 'shell(cd)' --deny-tool 'shell(git)' --deny-tool 'shell(pwd)' --deny-tool 'fetch' --deny-tool 'extensions' --deny-tool 'websearch' --deny-tool 'githubRepo'"</code></p>
<p>I allow all tools, then prevent tools related to changing working directory, accessing the web or viewing directory context.</p>
<p>There is still some risk here. e.g., if you want it to run Rscripts you might want to include in your prompt something like ‘Use <code>Rscript my-script.R</code> to run R scripts from the terminal.’ The R scripts could include anything (including connecting to the web).</p>
<p>Here’s an example in action. I use sprintf to format the terminal command with the prompt, tools and sub-directory path.</p>
<pre><code>copilot_prompt &lt;- "Set-up this project directory with a readme.md file, and directories for outputs, scripts, plots. Then create example data to illustrate a poisson GLM. Make prediction plots. Use `Rscript 'my-script.R'` to run R files. "

copilot_tools &lt;- "--allow-all-tools --deny-tool 'shell(cd)' --deny-tool 'shell(git)' --deny-tool 'shell(pwd)' --deny-tool 'fetch' --deny-tool 'extensions' --deny-tool 'websearch' --deny-tool 'githubRepo'"

subdir_path &lt;- "dir1"

 copilot_cmd &lt;- sprintf(
    "cd '%s' &amp;&amp; copilot -p '%s' %s",
    subdir_path,
    copilot_prompt,
    copilot_tools
  )

system(copilot_cmd)
</code></pre>
<p>From here it would be easy to create a loop over different subdirectory paths, and run a separate agent in each one. The advantage of <code>cd</code>ing into each path before the agent opens is that the agent can’t then see context from other sub-directories. So you get independent agent runs.</p>
<p>You could use this to see how different prompts perform, or do complex prompt strategies like ‘tree of thought’. In tree of thought you ask an agent to create the same thing many times, it will do it slightly differently each time. Then you aggregate the results and pick the most common or most accurate one.</p>
<p>My one gripe is that Github have provided us with woeful documentation of what the tool names are. I’m guessing the tools are what you can see if you open the chat window, click ‘Agent’ mode, then click the tool symbol.</p>
<p>I’d love to have some default tool sets that simplify the tool permissions. For instance a tool set that locks down all web access.</p>



 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-10-04-running-github-copilot-CLI-agents-from-R/</guid>
  <pubDate>Fri, 03 Oct 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Generative AI and R workshops in Hobart Australia</title>
  <link>https://www.seascapemodels.org/posts/2025-09-18-ai-r-workshops/</link>
  <description><![CDATA[ 





<p>In person Hobart Australia 11th and 12th November 2025</p>
<p><a href="https://www.seascapemodels.org/CMS-R-workshop/">Course page and further details</a></p>
<section id="registration" class="level2">
<h2 class="anchored" data-anchor-id="registration"><a href="https://payments.utas.edu.au/Register/booking?UDS_ACTION=S1I&amp;UDS_ACTION_DATA=bFAgA0dNWAFOKFdyNEJDBShXQg1KKE0bKUcyXAE1VDR7NVtc">Registration</a></h2>
<p>The R program is a powerful tool for data analysis, but has a steep learning curve. Join us for a 2-day workshop that will accelerate your R skills. Our modern approach to teaching leverages AI to help you create analyses with natural language, and code with fewer errors, but higher accuracy.</p>
<p>Presenter: A/Prof Chris J Brown c.j.brown@utas.edu.au Guest presenter: Professor Anthony Richardson, The University of Queensland and CSIRO Environment</p>
<p>Cost per day: $200 students, $300 professionals. Waiver available for indigenous scholars. Register for 1 or 2 days.</p>
</section>
<section id="day-1-11th-nov-quality-data-analysis-with-ai-coding-assistants" class="level2">
<h2 class="anchored" data-anchor-id="day-1-11th-nov-quality-data-analysis-with-ai-coding-assistants">Day 1 (11th Nov): Quality data analysis with AI coding assistants</h2>
<p>Suitable for R or Python users**. We’ll show you how to use large language models (like chatGPT) directly in your R or Python coding environment. You can accelerate your coding and get personalised help on your research. But there are risks, AI makes mistakes. We’ve developed guidelines to help you get the most out of AI for coding. We’ll also discuss the ethical implications of using AI to assist with research and whether ‘vibe coding’ is appropriate for scientists. This workshop is suitable for people with limited coding experience (e.g.&nbsp;can read data into Rstudio and make plots) as well as experts who want to know more about AI assistants.</p>
</section>
<section id="day-2-12th-nov-generalized-linear-models-in-r" class="level2">
<h2 class="anchored" data-anchor-id="day-2-12th-nov-generalized-linear-models-in-r">Day 2 (12th Nov): Generalized linear models (in R**)</h2>
<p>GLMs are a powerful tool for statistical analysis and prediction. We’ll teach the basics of the linear model and how it can be extended to all sorts of data in the generalized linear model. We’ll cover model verification, estimating effect size and significance and making predictions. The beauty of GLMs is that you only need some basic principles and you are ready to tackle many types of data, not like the old days when you had to know a different test for each new type of data.</p>
<ul>
<li>There may be additional subscription costs for AI software of up to USD20. Students can get free licenses. I will email instructions for getting software licenses prior to the workshops.</li>
</ul>
<p>**Note these workshops are not suitable for coding beginners. If you’ve never used R or Python before we suggest you take an introductory workshop. IMAS runs one, feel free to contact us for more info on that. As a rule of thumb, if you have Rstudio, know how to read data into R and make some plots you are ready for these workshops.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-09-18-ai-r-workshops/</guid>
  <pubDate>Wed, 17 Sep 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Paper in a prompt creating a first draft in one prompt</title>
  <link>https://www.seascapemodels.org/posts/2025-09-12-creating-a-first-draft-in-one-prompt/</link>
  <description><![CDATA[ 





<p>*It was a rather long prompt</p>
<p>I wanted to see if I could get an AI agent to complete a first draft of a paper, including data analysis, searching for references on the web and writing a complete draft.</p>
<p>I used the Roo Code agent software with the Claude Sonnet 4.0 large language model.</p>
<p><a href="https://www.seascapemodels.org/data/ai-generated-manuscript">I’ve shared the results from replicate 1 here</a>.</p>
<p>I’ve also pasted the abstracts from the two replicates below. Both used identical prompts and settings in Roo Code. Its interesting how they are different. For instance the first one frames the study in terms of estimating the scale of human impacts. The second frames the study as being about studying an under-studied mechanism.</p>
<p>The experiment started with a project folder with data and detailed instructions for the research question, the data and the analyses I wanted. The instructions included details of R packages and specific statistics, for instance I told the model to use Bray-Curtis distances for the multivariate analysis.</p>
<p>I also provided basic instructions for how to write the paper. The agent had web search capabilities, allowing it to access , and read real references, and then incorporate insights from those into the paper.</p>
<p>It is important to note that this draft has not been edited by a human. Parts of it are not scientifically accurate. It should not be read as anything other than an experiment. It contains errors and some of the references are false.</p>
<p>For instance, it made up the following reference, but oddly linked it to a real reference (that is different) using a real DOI:</p>
<p>Hamilton, Richard J et al.&nbsp;2025. “Modeling Sediment Impacts on Juvenile Reef Fish.” Frontiers in Marine Science. https://doi.org/10.3389/fmars.2025.1513498.</p>
<p>It didn’t make much of an effort to create good instructions for writing a paper. I’m sure you could do a lot better than this with more careful prompting.</p>
<p>AI generated papers should never replace human oversight. If you rely solely on AI, you miss the chance to truly understand your subject. Science is about learning, not just producing content.</p>
<p>What this sort of thing could be useful for is as a way to iterate and quickly explore alternative options for how to write a paper, or explore alternative interpretations.</p>
<p>It can also help you get over ‘the fear of the white page’ (writers block), by giving you something to start editing.</p>
<section id="abstract-from-replicate-1" class="level2">
<h2 class="anchored" data-anchor-id="abstract-from-replicate-1">Abstract from replicate 1</h2>
<p>Terrestrial logging threatens coral reef ecosystems through increased sedimentation, yet the spatial patterns of impact remain poorly understood. We examined benthic community composition at 49 coral reef sites across a gradient of logging impacts in Kia Province, Solomon Islands, spanning distances from 0.20 to 22.4 km from logging operations. Using multidimensional scaling ordination and permutational multivariate analysis of variance, we tested for differences in community structure between logged and not logged sites and across distance categories. Benthic communities differed significantly between logged and not logged sites (PERMANOVA p = 0.027), with logged sites showing 14.2% lower coral cover (48.0% vs 62.2%). Distance-dependent effects were highly significant (p = 0.002), explaining 11.9% of community variance. Sites within 1 km of logging showed severely degraded conditions (31.2% coral cover, 33.3% sand cover), while sites beyond 10 km maintained high coral cover (62.9%). Recovery followed a clear gradient: moderate distance sites (1-3 km) had 55.3% coral cover, far sites (3-10 km) had 60.4% coral cover. Categorical distance analysis proved more informative than continuous distance relationships, indicating threshold effects rather than linear gradients. These findings demonstrate that logging impacts extend further than previously documented, with measurable effects at sites 1-3 km from operations. Our results provide quantitative evidence for establishing buffer zones around coral reefs and highlight the need for watershed-scale conservation planning that integrates terrestrial forest management with marine protected area design.</p>
</section>
<section id="abstract-from-replicate-2" class="level2">
<h2 class="anchored" data-anchor-id="abstract-from-replicate-2">Abstract from replicate 2</h2>
<p>Coral reef ecosystems face increasing threats from terrestrial activities, with logging-induced sedimentation representing a significant but understudied impact mechanism. This study examined benthic community structure across 49 coral reef sites in the Solomon Islands using multidimensional scaling (MDS) ordination to test the hypothesis that logging activities influence reef community composition through sedimentation. We applied non-metric multidimensional scaling to benthic cover data using both Euclidean and Bray-Curtis distance metrics, with environmental vector fitting to identify ecological gradients. The analysis revealed strong community gradients primarily associated with coral branching cover (r² = 0.82) and soft coral cover (r² = 0.62), while water clarity showed moderate correlation (r² = 0.24) with community structure. Contrary to expectations, distance to logging showed weak correlation with community patterns (r² = 0.056, p = 0.101), suggesting that logging impacts operate through pathways not captured by simple proximity measures. The results demonstrate MDS ordination effectiveness for revealing coral community structure patterns while highlighting the complexity of logging-reef relationships in tropical marine systems.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-09-12-creating-a-first-draft-in-one-prompt/</guid>
  <pubDate>Thu, 11 Sep 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Create reports and tutorials with generative AI from R</title>
  <link>https://www.seascapemodels.org/posts/2025-09-02-AI-reports-with-openrouter-and-r/</link>
  <description><![CDATA[ 





<p>Several AI model providers have integrated web search capabilities into their large language models. I was attempting to use these features via the R <code>ellmer</code> package. However, changes to LLMs are so frequent that <code>ellmer</code> isn’t keeping up.</p>
<p>I did get <code>ellmer</code> to run Perplexity’s Sonar web search model, however it didn’t provide me with the references, which are essential.</p>
<p>If you haven’t seen these AI generated reports, check out the example at the end. Its a useful way to get a quick literature summary or create bespoke R tutorials.</p>
<p>Here <a href="https://github.com/cbrown5/web-search-ai/blob/main/README.md">I’m just announcing a couple of simple R scripts that you can use to do web search powered AI reports with the Openrouter service.</a></p>
<p>I just created two functions (with the help of copilot of course), one that makes a call to the Openrouter API to send a question to a model. The second function processes the output (which is in JSON format) to make a nice qmd, with the references hyper-linked (be sure to check the url before clicking them, who knows what the AI will turn up!). From there you can render the qmd to get a pdf/word/html report.</p>
<p>The functions are easy to use. First <a href="https://github.com/cbrown5/web-search-ai/blob/main/perplexity-search-functions.R">download or copy these functions from my github</a>.</p>
<p>Note never trust any code from someone else that sends requests to LLMs! It could potentially include harmful prompts. I recommend reading all code that is sending prompts to LLMs just to be sure you know what it does.</p>
<section id="using-the-code-to-do-a-search-and-create-a-report" class="level2">
<h2 class="anchored" data-anchor-id="using-the-code-to-do-a-search-and-create-a-report">Using the code to do a search and create a report</h2>
<p>Once you have my two functions, you will need to <a href="https://www.seascapemodels.org/AI-assistants-for-scientific-coding/03-set-up.html#sec-apikeys">set-up your Openrouter API key</a> and save the key somewhere (e.g.&nbsp;you can use <code>usethis::edit_r_environ()</code> and save it in there like OPENROUTER_API_KEY=“MY-KEY-HERE”)</p>
<p>Here’s an example of using the function:</p>
<pre><code>library(httr)
library(jsonlite)

source("perplexity-search-functions.R")

openrouter_api_key &lt;- Sys.getenv("OPENROUTER_API_KEY")

user_message &lt;- "I want to learn how to use the NIMBLE package to fit autoregressive time-series models"

system_message &lt;- "You are a helpful AI agent who creates statistical analysis tutorials in R. 
        Rules: 
        1. Include text and examples of code in your responses. 
        2. Produce reports that are less than 10000 words."

#Send response to openrouter 
response &lt;- call_openrouter_api(
  openrouter_api_key,
  model = "perplexity/sonar-deep-research",
  system_message = system_message,
  user_message,
  search_context_size = "medium"
  #Options "low"  "medium", "high"
)

#Save the response as a qmd
save_response_as_qmd(response, "results/AR-models-in-NIMBLE.qmd")</code></pre>
</section>
<section id="inputs-for-the-llm" class="level2">
<h2 class="anchored" data-anchor-id="inputs-for-the-llm">Inputs for the LLM</h2>
<p>The user message is your prompt for searching. The system message sets the scope of how the report is created. Note that anything for the web search goes in the user message, not the system message <a href="https://docs.perplexity.ai/guides/prompt-guide">see the Perplexity guidelines for more info on prompting advice, its different than for regular LLMs</a>.</p>
<p>For example, another idea for a system prompt could be:</p>
<pre><code>system_message &lt;- "You are a helpful AI agent who creates summary reports of the scientific literature. 
        Rules: 
        1. Produce reports that are less than 2000 words.
        2. Include a Summary section that summarizes key research trends. "

user_message &lt;- "What are the impacts of climate warming on fish physiology documented in the peer-reviewed academic literature"
</code></pre>
<p><code>search_context_size</code> is meant to control how much effort it puts in, its hard to tell if that impacts results or not, see <a href="https://openrouter.ai/docs/features/web-search">openrouter’s docs for more information.</a>.</p>
</section>
<section id="model-choices" class="level2">
<h2 class="anchored" data-anchor-id="model-choices">Model choices</h2>
<p>Other models to try are:</p>
<ul>
<li><p><code>perplexity/sonar</code> for a simpler, cheaper searches, including citations.</p></li>
<li><p><code>perplexity/sonar-deep-research</code> For deeper, more expensive searches with citations and reasoning.</p></li>
<li><p><code>openai/o4-mini</code> Is another option, but doesn’t return citations.</p></li>
</ul>
<p>Explore the openrouter site for other web search enabled LLMs.</p>
</section>
<section id="customization" class="level2">
<h2 class="anchored" data-anchor-id="customization">Customization</h2>
<p>The <code>call_openrouter_api.R</code> function is a template for customizing, not a comprehensive framework for using the openrouter API. Its actually very easy to connect to LLMs from R (even though most examples online are in python or typscript). Here’s a basic template:</p>
<pre><code>library(httr)
library(jsonlite)
response &lt;- POST(
    url = "https://openrouter.ai/api/v1/chat/completions",
    add_headers(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", openrouter_api_key)
    ),
    body = toJSON(list(
      model = model,
      messages = list(
        list(
          role = "system",
          content = system_message
        ),  
        list(
          role = "user",
          content = user_message
        )
      )
    ), auto_unbox = TRUE),
    encode = "raw"
  )
</code></pre>
<p>The trick then is formatting the output, which is what my other function does (focussing on perplexities models).</p>
</section>
<section id="report-example" class="level2">
<h2 class="anchored" data-anchor-id="report-example">Report example</h2>
<p>Below is an example, I asked used the system prompt above and asked about fitting surplus production models (for fisheries science) with the NIMBLE package. Note one minor bug with my function converting qmd references to hyperlinks, it potential does R sub-scripts as well. Something to work on later. Here’s the AI report:</p>
<p>You can fit a <strong>surplus production model (SPM)</strong> with the NIMBLE package in R by writing the model in BUGS language, which NIMBLE extends, specifying your likelihood (e.g., production dynamics with catch data) and priors for the parameters, and then using NIMBLE’s MCMC capabilities to estimate the posterior distributions of parameters. Here is a stepwise approach with example code snippets:</p>
<ol type="1">
<li><strong>Specify the surplus production model in BUGS code</strong>:<br>
This typically models biomass ( B_t ) evolving over time according to a logistic growth or Gompertz model minus catches ( C_t ). For example, the Schaefer model can be expressed as:<br>
[ B_{t+1} = B_t + r B_t (1 - ) - C_t + _t ]<br>
where (r) is intrinsic growth rate, (K) is carrying capacity, and (_t) models process noise.</li>
</ol>
<pre><code>library(nimble)

# Define the model in BUGS syntax
spm_code &lt;- nimbleCode({
  # Priors for parameters
  r ~ dunif(0, 2)           # growth rate
  K ~ dunif(maxCatch, 10 * maxCatch)  # carrying capacity (expand as appropriate)
  sigma ~ dunif(0, 5)       # process noise SD

  B&lt;a href="https://oliviergimenez.github.io/banana-book/intronimble.html" target="_blank"&gt;[1]&lt;/a&gt; ~ dunif(0, K)       # initial biomass
  
  for(t in 1:(nYears - 1)) {
    mu[t] &lt;- B[t] + r * B[t] * (1 - B[t]/K) - catch[t]
    B[t + 1] ~ dnorm(mu[t], sd = sigma)
  }
})</code></pre>
<ol start="2" type="1">
<li><strong>Prepare the data and constants</strong> (e.g., catch time series, number of years):</li>
</ol>
<pre><code>data &lt;- list(catch = catch_vector)  # your observed catches
constants &lt;- list(nYears = length(catch_vector), maxCatch = max(catch_vector))
inits &lt;- list(r = 0.5, K = max(catch_vector)*5, sigma = 0.1, B = rep(NA, length(catch_vector)))</code></pre>
<ol start="3" type="1">
<li><strong>Build, compile, and run the model with NIMBLE</strong>:</li>
</ol>
<pre><code># Create model object
spm_model &lt;- nimbleModel(spm_code, data = data, inits = inits, constants = constants)

# Compile model
c_spm_model &lt;- compileNimble(spm_model)

# Configure MCMC; default samplers are usually fine
mcmc_conf &lt;- configureMCMC(spm_model)
mcmc &lt;- buildMCMC(mcmc_conf)
c_mcmc &lt;- compileNimble(mcmc, project = spm_model)

# Run MCMC sampling
samples &lt;- runMCMC(c_mcmc, niter = 10000, nburnin = 2000, thin = 5)

# Check results
summary(samples)</code></pre>
<p>This workflow follows NIMBLE’s general approach of specifying hierarchical Bayesian models in an extended BUGS language, compiling fast C++ versions of the model and samplers, and running MCMC algorithms<a href="https://oliviergimenez.github.io/banana-book/intronimble.html" target="_blank">[1]</a><a href="https://www.rdocumentation.org/packages/nimble/versions/1.3.0/topics/nimble-package" target="_blank">[2]</a><a href="https://github.com/nimble-dev/nimble" target="_blank">[4]</a><a href="https://r-nimble.org" target="_blank">[7]</a>. The key is coding the surplus production dynamics explicitly as state-transition equations with observation or process error as needed.</p>
<p>For clarity, here is a <strong>minimal example with dummy catch data</strong>:</p>
<pre><code>library(nimble)

catch_vector &lt;- c(100, 120, 90, 80, 110) # example catches

spm_code &lt;- nimbleCode({
  r ~ dunif(0, 2)
  K ~ dunif(maxCatch, 10 * maxCatch)
  sigma ~ dunif(0, 5)

  B&lt;a href="https://oliviergimenez.github.io/banana-book/intronimble.html" target="_blank"&gt;[1]&lt;/a&gt; ~ dunif(0, K)
  for(t in 1:(nYears - 1)) {
    mu[t] &lt;- B[t] + r * B[t] * (1 - B[t]/K) - catch[t]
    B[t+1] ~ dnorm(mu[t], sd = sigma)
  }
})

data &lt;- list(catch = catch_vector)
constants &lt;- list(nYears = length(catch_vector), maxCatch = max(catch_vector))
inits &lt;- list(r = 0.5, K = max(catch_vector)*5, sigma = 0.1, B = rep(NA, length(catch_vector)))

spm_model &lt;- nimbleModel(spm_code, data = data, inits = inits, constants = constants)
c_spm_model &lt;- compileNimble(spm_model)

mcmc_conf &lt;- configureMCMC(spm_model)
mcmc &lt;- buildMCMC(mcmc_conf)
c_mcmc &lt;- compileNimble(mcmc, project = spm_model)

samples &lt;- runMCMC(c_mcmc, niter = 5000, nburnin = 1000, thin = 2)
print(summary(samples))</code></pre>
<p>This example models biomass dynamics probabilistically and fits parameters (r), (K), and noise level () given known catches using Bayesian inference.</p>
<p>If you want to implement maximum likelihood fitting instead, NIMBLE lets you compile the model and evaluate likelihoods for optimization, as demonstrated in tutorials<a href="https://oliviergimenez.github.io/banana-book/intronimble.html" target="_blank">[1]</a>.</p>
<p>In summary: - <strong>Write your surplus production model in NIMBLE’s BUGS language</strong>, defining biomass dynamics, catch, and priors for parameters<br>
- <strong>Compile and run MCMC</strong> sampling in NIMBLE to fit the model<br>
- <strong>Extract posterior samples</strong> for growth rate, carrying capacity, biomass, and uncertainty<br>
- Optionally, use <strong>likelihood-based methods</strong> by compiling the model and defining your own optimizer function in R<a href="https://oliviergimenez.github.io/banana-book/intronimble.html" target="_blank">[1]</a><a href="https://r-nimble.org" target="_blank">[7]</a>.</p>
<p>This approach leverages NIMBLE’s power for hierarchical, Bayesian time-series stock assessment models including surplus production models.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="https://oliviergimenez.github.io/banana-book/intronimble.html" target="_blank">oliviergimenez.github.io/banana-book/intronimble.html</a></li>
<li><a href="https://www.rdocumentation.org/packages/nimble/versions/1.3.0/topics/nimble-package" target="_blank">www.rdocumentation.org/packages/nimble/versions/1.3.0/topics/nimble-package</a></li>
<li><a href="https://www.youtube.com/watch?v=DE4Q82QgSBs" target="_blank">www.youtube.com/watch</a></li>
<li><a href="https://github.com/nimble-dev/nimble" target="_blank">github.com/nimble-dev/nimble</a></li>
<li><a href="https://r-nimble.org/documentation-2" target="_blank">r-nimble.org/documentation-2</a></li>
<li><a href="https://cran.r-project.org/web/packages/nimbleCarbon/vignettes/nimble_carbon_vignette.html" target="_blank">cran.r-project.org/web/packages/nimbleCarbon/vignettes/nimble_carbon_vignette.html</a></li>
<li><a href="https://r-nimble.org" target="_blank">r-nimble.org/</a></li>
<li><a href="https://www.rdocumentation.org/packages/nimble/versions/1.3.0" target="_blank">www.rdocumentation.org/packages/nimble/versions/1.3.0</a></li>
</ol>


</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <category>research-skills</category>
  <guid>https://www.seascapemodels.org/posts/2025-09-02-AI-reports-with-openrouter-and-r/</guid>
  <pubDate>Mon, 01 Sep 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Writing scientific papers with quarto and AI assistants</title>
  <link>https://www.seascapemodels.org/posts/2025-07-07-writing-papers-with-quarto/</link>
  <description><![CDATA[ 





<p>Below is my suggested workflow for using quarto to write scientific papers. I’m currently switching to doing as many projects as possible with quarto rather than word for a few reasons:</p>
<ol type="1">
<li>Easier to manage document style</li>
<li>Easier to manage references</li>
<li>Workflows that auto-update figures/tables when R code is re-run</li>
<li>Generative AI integration that is customizable.</li>
</ol>
<p>Point 3 is great, no more cut and pasting figures into word documents!</p>
<p>Point 4 is the big one. I’m developing my own ‘writing mentor’ scripts for large language models. Using quarto lets me implement writing advice specific to science direclty into my manuscripts.</p>
<p>Quarto is ‘What You See is What you Make’, meaning that you write special syntax for formatting. Once you are used to it, this is way easier way to manage styles than word.</p>
<p>The downside is getting your (non-coding) collaborators to edit files in quarto. This is the biggest bottleneck to my use of quarto/markdown. Currently I send them word documents then have to manually integrate the feedback. Or I work in quarto until the near final stages, accepting comments only, then get them to edit the final manuscript.</p>
<p>For instance, <a href="https://conbio.onlinelibrary.wiley.com/doi/full/10.1111/cobi.13079?casa_token=zF8vihnFfcMAAAAA%3A9WlbXPCghdwS2WvyRqGjRqYPrng7q4_xPwZvu9K52p6gd_8lWs2qcgrehfg4ehAThC7ni32Ybr02iA">I wrote most of this paper in markdown</a> but had to go to word editing towards the end so I could get edits from my collaborator. Once you’ve progressed it in word, its hard to go back to markdown.</p>
<p>Instructions below are high level. There are quite a few pieces of software you need to do this, so I’ve linked to tutorials for each below.</p>
<section id="download-and-install-an-ide" class="level3">
<h3 class="anchored" data-anchor-id="download-and-install-an-ide">1. Download and install an IDE</h3>
<p>Download and install VScode.</p>
<p><a href="https://www.seascapemodels.org/rstats/2025/06/12/setting-up-vscode-r-genAI.html">Instructions online e.g.&nbsp;here</a></p>
<p>I’m using VScode because of its AI assistant integration. But you could also use positron if you have issues with VScode or want to use a Posit product rather than a Microsoft product.</p>
</section>
<section id="get-git-and-github" class="level3">
<h3 class="anchored" data-anchor-id="get-git-and-github">2. Get git and github</h3>
<p>Install git on your computer. Optionally, get a github account and connect to that. Git does version control. Github lets you share that online. If your collaborators are github users then you can also share edits on documents this way.</p>
<p>Git is also essential if you are using AI assistants. Sometimes they majorly stuff up your documents. So keeping back ups with git is essential.</p>
</section>
<section id="vscode-extensions" class="level3">
<h3 class="anchored" data-anchor-id="vscode-extensions">3. VScode extensions</h3>
<p>Install these VScode extentions (or equivalents if you are using positron, note that many vscode extensions are also compatable with Positron)</p>
<ul>
<li>Quarto extension.</li>
</ul>
<p>Open VSCode and click the four boxes ‘extension’ icon on the LHS then search and install the Quarto extension.</p>
<p>Optional extensions: - R language extension (<a href="https://www.seascapemodels.org/rstats/2025/06/12/setting-up-vscode-r-genAI.html">can be tricky to connect VScode to R, suggest you web search for advice</a>)</p>
</section>
<section id="optional-steps-for-ai-integration" class="level3">
<h3 class="anchored" data-anchor-id="optional-steps-for-ai-integration">4. Optional steps for AI integration</h3>
<p>This next step is optional. If you are using quarto or markdown its possible to get large-language models to help with many paper writing tasks (including the writing). This is a specialized area though and I’ve only given basic technical instructions here. Actually getting it to work well is another topic altogether and something I’m still developing…</p>
<p>Get an API key with an LLM provider (e.g.&nbsp;<a href="https://openrouter.ai/sign-up">OpenRouter</a>, <a href="https://platform.openai.com/api-keys">OpenAI</a>, <a href="https://console.anthropic.com/login?returnTo=%2F%3F">Anthropic</a>). You’ll need a developer account to get an API key, rather than a regular account that you may already have (see links in last sentence). Buy some credits so you can use your API.</p>
<p>Make sure you save your API key somewhere safe! You can usually only view them once on creation. You’ll need it for the workshop.</p>
<p>Get the Roo Code extension for vscode/positron.</p>
<p><a href="https://docs.roocode.com/">Read the documents/watch the tutorials and learn how to use Roo Code</a></p>
<p>You can now <a href="https://docs.roocode.com/features/custom-modes">create a custom mode</a>, e.g.&nbsp;a ‘scientific writing mode’ in Roo code. As of writing this requires clicking the mode selection button at the bottom of the Roo Code Pane, then click the Cog, then the <code>+</code> button to make a new mode. Then you need to write a ‘Role Definition’ and ‘Custom instructions’. For tools I just use ‘Read Files’, ‘Edit Files’ and unclick the others (will save you money and tokens).</p>
<p>This is the hard part that needs a lot of thought:</p>
<p>In the custom instructions you should write detailed instructions on how to help an author with scientific writing. For instance, you might want to put some very strong instructions about not making up references. You might also put instructions about your particular writing style preferences. I’m working on a template, but am not yet ready to share it.</p>
<p>See <a href="https://docs.roocode.com/features/custom-modes">Roo code documentation</a> for more advice on custom modes.</p>
</section>
<section id="using-quarto" class="level3">
<h3 class="anchored" data-anchor-id="using-quarto">5. Using quarto</h3>
<p>Take a tutorial and <a href="https://quarto.org/docs/get-started/hello/rstudio.html">learn how to use Quarto</a>.</p>
<p>For academic paper writing the key things to understand from the Quarto tutorial are:</p>
<ul>
<li>How to knit as word or pdf (pdf requires extra software installations)</li>
<li>Formatting, headings, bold etc…</li>
<li>YAML frontmatter for styles, linking a bibliography and bibliography style</li>
<li>How to insert images and/or code.</li>
</ul>
<p><strong>Note on AI integration</strong> once you are using quarto and Roo Code you can simply ask Roo Code to do things in your document (like outline a paper template) by referencing the file (e.g. <span class="citation" data-cites="myfile.qmd">@myfile.qmd</span>) in the prompt box.</p>
<p>Whether this works well for you is another questions. Prompting well requires a lot of thought and practice. Its not simply going to write a paper for you. You have to give the AI assistant detailed, specific, instructions and lots of context.</p>
</section>
<section id="yaml-front-matter" class="level3">
<h3 class="anchored" data-anchor-id="yaml-front-matter">6. YAML front matter</h3>
<p>The <code>YAML</code> controls how your qmd document is rendered. Here’s an example of mine:</p>
<pre><code>---
title: "The paper's title"
format: docx
editor: visual
bibliography: mybib.bib
csl: myjournal.csl
execute: 
  echo: false
  message: false
  warning: false
---</code></pre>
<p>This goes at the top of your document. A few key points.</p>
<p><code>format</code> controls document type to render this as, here a word doc.</p>
<p><code>editor</code> controls how it is viewed in vscode. Options are <code>editor: visual</code> and <code>editor: source</code>. Visual looks more like a word doc, source looks more like markdown. You’ll have to save and re-open the document for this to change.</p>
<p><code>bibliography</code> links to a bibtex file where your references are stored.</p>
<p><code>csl</code> links to a style guide for the bibliography.</p>
<p>More on styles and references below.</p>
<p><code>execute</code> is controlling how R code is run and if the R code appears in the document.</p>
</section>
<section id="rendering-as-a-document" class="level3">
<h3 class="anchored" data-anchor-id="rendering-as-a-document">7. Rendering as a document</h3>
<p>Use the short-cut key ‘cmd-shift-K’/‘cntrl-shft-k’ (mac/windows) to preview your document. It will also create a rendered version in your current directory.</p>
<p>Its helpful to set: <code>format: html</code> when you are writing the document, then you get a live preview in vscode. Use <code>format: docx</code> when you want a word document.</p>
<p>Its worth also learning the short-cut `cmd-shft-p’/‘cntrl-shft-p’, this brings up searchable actions for all extensions in vscode. The one you want is ‘Quarto: preview’ which does the same as the shortcut above.</p>
<p>I tend to have minimal R code in my quarto manuscript. Or none at all (just reference .png files for figures). This keeps rendering quick. Also your document can get unweildy if there is a lot of text mixed in with R code.</p>
</section>
<section id="word-counts" class="level3">
<h3 class="anchored" data-anchor-id="word-counts">8. Word counts</h3>
<p>There are various word count extensions for vscode qmd and md documents.</p>
</section>
<section id="document-styles" class="level3">
<h3 class="anchored" data-anchor-id="document-styles">9. Document styles</h3>
<p>Getting a word document to follow a particular style is a bit fiddly. You need to set-up a template word document with styles the include that as a reference in your YAML.</p>
<p><a href="https://quarto.org/docs/output-formats/ms-word-templates.html">See instructions here.</a></p>
</section>
<section id="reference-manager-integration" class="level3">
<h3 class="anchored" data-anchor-id="reference-manager-integration">10. Reference manager integration</h3>
<p>Quarto integrates with many different reference managers. <a href="https://quarto.org/docs/authoring/citations.html">There’s a good guide here</a>.</p>
<p>In brief you create a <code>.bib</code> file that has your references in it. This is then linked in the YAML. The manual way to manage this is just to create a <code>.bib</code> file and paste bibtext entries directly into it (available on most journal’s pages as a citation format, as well as google scholar).</p>
<p>e.g.&nbsp;the bibtext for R looks like this:</p>
<pre><code>@Manual{Rlanguage,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }</code></pre>
<p>Then in quarto you just type <code>@</code> and a dropdown of all your references will appear. <code>@Manual{Rlanguage,</code> the Rlanguage bit is the CiteKey that will appear in the dropdown. So <code>@Rlanguage</code> will insert that reference into the bibliography and the citation at that place in the document.</p>
<p>You can streamline the process of gathering and managing references with a reference manager.</p>
<p>My workflow in Zotero is as follows:</p>
<ul>
<li>Open Zotero on my computer</li>
<li>Go to journal webpage for paper</li>
<li>Use zotero plugin to my browser to grab the citation and save it to a library</li>
<li>Go to my quarto document in VScode</li>
<li>type <code>@</code> and a drop down of all references in all libraries on zotero appears. Pick the one I want.</li>
<li>Click the <code>OK</code> button which saves that reference into my local <code>.bib</code> file.</li>
</ul>
<p>For some reason (that does not seem to be documented in any quarto tutorials anywhere!) it will find any reference I have anywhere in zotero and then save that bibtex entry to my local <code>.bib</code> file, so it is now accessible for use in my quarto doc. This only works if I have zotero open and use <code>editor: visual</code> in the YAML.</p>
<p>There are many other options however.</p>
</section>
<section id="optional-ai-integration-for-reference-management" class="level3">
<h3 class="anchored" data-anchor-id="optional-ai-integration-for-reference-management">11. Optional AI integration for reference management</h3>
<p>You can get AI assistants to help with referencing if you keep your notes on papers linked to your references. For instance, you could keep your notes on references in the bibtex field for <code>notes</code>. Alternatively you could create another quarto/markdown document that has a header for each citation tag along with its notes in a structured way:</p>
<pre><code>## Rlanguage 

### What it is

The R software for scientific computing. 

### Usage

Citation for the R software. Use this at least once in every paper where i've used R for statistics

## edgar2023continent

### What is it

Key paper that shows Australia is losing its marine biodiversity. 

### Usage

Cite this as evidence that Australia is losing coastal marine biodiversity and as evidence that climate change is causing marine biodiversity loss
</code></pre>
<p>It doesn’t matter how you do this, so long as you follow a consistent structure. I’ve used the CiteKey as the main header for each reference entry. Then I’ve put in markdown sections about each paper and why I might wnat to cite it. Then you can get Roo Code to help with inserting references.</p>
<p>Note that if you are using the <code>.bib</code> directly just be careful not to plagiarise! Roo Code might insert excerpts from the abstracts/titles directly into your written document, which is a no-no for publishing.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>research-skills</category>
  <guid>https://www.seascapemodels.org/posts/2025-07-07-writing-papers-with-quarto/</guid>
  <pubDate>Sun, 06 Jul 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Prompting large language models for quality ecological statistics</title>
  <link>https://www.seascapemodels.org/posts/2025-06-27-prompting-llms-quality-ecological-statistics/</link>
  <description><![CDATA[ 





<p>Announcing a new <a href="https://ecoevorxiv.org/repository/view/9493/">pre-print on prompt engineering for ecological statistics.</a>. https://doi.org/10.32942/X2CS80</p>
<p>The pre-print describes the challenges and opportunities of using large language models and agents to do ecological statistics. It then goes onto describe some prompt engineering strategies that can help you make the most of these tools, but still get reliable statistical advice and code.</p>
<p>The pre-print accompanies <a href="https://www.seascapemodels.org/R-llm-workshop/index.html#summary">a one day course</a> I’ve published that teaches how to use LLM tools for R and statistics, as well as discussing the environmental and ethical challenges.</p>



 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-06-27-prompting-llms-quality-ecological-statistics/</guid>
  <pubDate>Thu, 26 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Why we are all naturally Bayesians not frequentists</title>
  <link>https://www.seascapemodels.org/posts/2025-06-25-why-frequentist-statistics-makes-no-sense/</link>
  <description><![CDATA[ 





<p>I don’t see why its a thing to say “I’m a Bayesian”. Being a Bayesian is normal for any rational person as we will prove in a couple of sentences. Being a frequentist is what is what is strange. It would make more sense to assume we are all Bayesians, and have frequentists have to declare “I’m a frequentist”.</p>
<p>The statistical quantity of interest for frequentist is the likelihood, which is defined as the probability of your observations given your hypothesis. It can be written:</p>
<p><code>Pr(D | H)</code></p>
<p>(Equivalently probability of data given we assume a particular model). For a Bayesian the statistical quantity is the probability of the hypothesis given the observations or:</p>
<p><code>Pr(H | D)</code></p>
<p>(Equivalently probability of a model given the data).</p>
<p>Say we’re out on a boat and we catch a glance of a human sized animal dive underwater with a flap of its tail. Was it a mermaid or a dugong? (A dugong is type of seagrass eating mammal).</p>
<p>A frequentist would think in likelihoods and conclude, correctly, that both mermaid and dugong are equally likely: Both creatures, if assumed to exist, could result in a similar observation of us seeing a body and a tail diving underwater, i.e.</p>
<p><code>Pr(seeing a tail | dugongs exist) ≈ Pr(seeing a tail | mermaid exist)</code></p>
<p>A rational person would guess the observed tail was a dugong. That is because a rational person is thinking in terms of probabilities. The probability we observed a mermaid is vanishingly small, because all prior evidence leads us to believe that mermaids don’t exist (though I do know some less than rational people who believe in ‘merfolk’).</p>
<p>In logic, Bayes theorem says we can calculate the probability of the hypothesis</p>
<p><code>Pr(H | D) = Pr(D | H)*Pr(H) / Pr(D)</code></p>
<p>So for the hypothesis “it’s a dugong”:</p>
<p><code>Pr(dugong | seeing tail) = Pr(seeing tail | dugong)*Pr(dugong) / Pr(seeing tail)</code></p>
<p>For our two models, dugong versus mermaid, we have similar values for the likelihoods and <code>pr(D)</code>, but the ‘prior probabilities’, the <code>Pr(H)</code>, are very different. <code>Pr(dugong)</code> is plausible (even though is rare to see them), whereas <code>Pr(mermaid)</code> is zero, because merfolk don’t exist.</p>
<p>We multiply the likelihood by the prior in the Bayes equation, so the <code>Pr(mermaid | seeing tail)</code> becomes zero.</p>
<p>(If you are new to Bayes theory, you might be wondering what does <code>Pr(D)</code> mean? How do we calculate the probability of the data? For the above example it doesn’t matter as <code>Pr(seeing a tail)</code> is the same for both hypotheses. One of the challenges of Bayes stats in practice is calculating this <code>Pr(D)</code>. A lot of statistical computing is devoted to figuring out how to do that. In brief though <code>Pr(D)</code> can be calculated as the sum of <code>Pr(seeing tail | dugong)*Pr(H)</code> for all discrete hypotheses or the integral if our hypotheses are continuous (like parameter values)).</p>
<p>Does this philosophical distinction it matter in the practice of statistics?</p>
<p>The Bayesian version of an analysis, such as a GLM, will often given similar or the same predictions as the frequentist version. Yet ecology, like many other disciplines, has a problem with repeatability and p-hacking.</p>
<p>Part of the problem stems from the methods we use. For large sample sizes frequentist and Bayesian results often do converge.</p>
<p>But small sample sizes are common in many fields, including ecology. A small sample size study that finds an extreme and significant result is attractive to publish. This issue has been particularly problematic in <a href="https://www.nature.com/articles/s41586-019-1903-y">behavioural ecology</a>. These false positives come to dominate the literature.</p>
<p>Appropriate use of Bayesian priors would penalize these extreme effect sizes. This is equivalent to setting a low probability on mermaid sightings above. Some have <a href="https://xcelab.net/rm/">called for stronger priors to be the default choice</a></p>
<p>So in every day life we think of probabilities Bayesians, not like frequentists. There are numerous Bayesian R packages now and they are increasingly easy to use. So it makes sense that we should apply the same logic to our statistical analyses as well.</p>
<p>Thanks to Sober’s ‘Philosophy of Biology’ for this example (he uses gremlins in the roof rather than mermaids) and Coralina for telling me about the book!</p>



 ]]></description>
  <category>rstats</category>
  <category>research</category>
  <guid>https://www.seascapemodels.org/posts/2025-06-25-why-frequentist-statistics-makes-no-sense/</guid>
  <pubDate>Tue, 24 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Setting up VScode for R and generative AI tools</title>
  <link>https://www.seascapemodels.org/posts/2025-06-12-setting-up-vscode-r-genAI/</link>
  <description><![CDATA[ 





<p>VScode has many extensions that let you create and run entire workflows via using prompts to a large language model. Its not widely used in the R community yet, but I expect it will be soon. You can create your entire R project, interpret the results and write a draft of your findings without writing any R code.</p>
<p>Most of these tools are not available (as of writing) in RStudio, or have only limited functionality. So you need to use a different IDE (Integrated Development Environment) to run your R code. Here I’ll explain how to set-up VSCode (a popular IDE) so you can use Cline.</p>
<section id="software-requirements" class="level2">
<h2 class="anchored" data-anchor-id="software-requirements">Software requirements</h2>
<p>To set up VScode for R and Cline, you’ll need:</p>
<ul>
<li>R programming language</li>
<li>VScode text editor</li>
<li>R extension for VScode</li>
<li>Cline AI assistant extension for VScode</li>
</ul>
<p>Note that if you computer is controlled centrally by an IT department, you may need to request admin access to install software, or email IT and ask for them to come and help you.</p>
</section>
<section id="install-r" class="level2">
<h2 class="anchored" data-anchor-id="install-r">Install R</h2>
<ol type="1">
<li>Go to the official R project website: https://www.r-project.org/</li>
<li>Click the “download R” link in the Getting Started section</li>
<li>Choose a CRAN mirror close to your location</li>
<li>Download the appropriate R installer for your operating system</li>
<li>Run the installer and follow the prompts to complete installation</li>
</ol>
</section>
<section id="r-packages" class="level2">
<h2 class="anchored" data-anchor-id="r-packages">R packages</h2>
<ol type="1">
<li>Open R or RStudio</li>
<li>Install language server <code>install.packages("languageserver")</code></li>
<li>Install httpgd <code>install.packages("httpgd")</code> (this helps improve plots in VScode). NOTE that httpgd seems to often be removed from CRAN, then come back again, I’m not sure why… If you are having trouble you can try install from a different repo, see instructions here: https://community.r-multiverse.org/httpgd</li>
</ol>
</section>
<section id="install-vscode" class="level2">
<h2 class="anchored" data-anchor-id="install-vscode">Install VScode</h2>
<ol type="1">
<li>Go to the official VScode website: https://code.visualstudio.com/</li>
<li>Click the big blue “Download” button</li>
<li>Download the appropriate VScode installer for your operating system</li>
<li>Run the installer and follow the prompts</li>
<li>Launch VScode once installation is complete</li>
</ol>
</section>
<section id="install-r-extension" class="level2">
<h2 class="anchored" data-anchor-id="install-r-extension">Install R extension</h2>
<ol type="1">
<li>Open VScode</li>
<li>Open the Extensions view in VScode (click the boxes on left hand side)</li>
<li>Search for “R” in the extensions marketplace</li>
<li>Select the “R” extension published by REditorSupport</li>
<li>Click the “Install” button</li>
<li>Restart VScode after installation if prompted</li>
</ol>
<p><a href="https://code.visualstudio.com/docs/languages/r">More info on vscode and R here</a></p>
</section>
<section id="connect-r-and-vscode" class="level2">
<h2 class="anchored" data-anchor-id="connect-r-and-vscode">Connect R and VScode</h2>
<ol type="1">
<li>Open a new terminal in VScode (Terminal &gt; New Terminal)</li>
<li>Check that R is installed by running: <code>R --version</code></li>
<li>Type <code>R</code> to open the R console in the terminal</li>
<li>Now open any R script in VS code (File &gt; Open)</li>
<li>Run some R code to check that VS code can connect to R in the terminal. Use the shortcut Ctrl+Enter/Cmd+Enter or press the play button in the top right of the script editor.</li>
</ol>
<p>If R is not found then open extensions (left hand side, boxes icon), filter by ‘enabled’ then click the R extension. Now click the cog icon in the R extension and select ‘settings’ from the dropdown. Search for ‘rpath’. Check that it has the correct path to R on your computer. You can find the path by opening a terminal and typing <code>which R</code> (on mac) or in a windows terminal <code>where R</code>.</p>
<p>While you have the extension settings open search for ‘httgp’ and make sure <code>Plot: Use Httpgd</code> is enabled.</p>
</section>
<section id="install-gen-ai-extensions" class="level2">
<h2 class="anchored" data-anchor-id="install-gen-ai-extensions">Install gen AI extensions</h2>
<ol type="1">
<li>Open the Extensions view in VScode (Ctrl+Shift+X)</li>
<li>Search for the genAI assistant of your choice. I’m use Roo Code currently. Cline is another popular choice.</li>
<li>Select the extension</li>
<li>Click the “Install” button</li>
<li>The extension icon (e.g.&nbsp;a Roo if using Roo Code) should appear in the VScode sidebar</li>
</ol>
</section>
<section id="set-up-extension-in-vscode" class="level2">
<h2 class="anchored" data-anchor-id="set-up-extension-in-vscode">Set up extension in VScode</h2>
<ol type="1">
<li>Click on the extension icon (e.g.&nbsp;a roo for roo code or robot for cline) on the left hand side</li>
<li>Click the cog (if the settings don’t open automatically)</li>
<li>Select your API provider and cut and paste the API key into the box (see next step if you don’t have one).</li>
</ol>
</section>
<section id="get-an-api-key" class="level2">
<h2 class="anchored" data-anchor-id="get-an-api-key">Get an API key</h2>
<p>An API key is like a password that allows the AI assistant (e.g.&nbsp;roo code) to send your prompt to a large language model. Your key should be kept private. Usually you’ll have to buy some credits. These allow you to send prompts to the LLM. You’ll be paying per prompt.</p>
<p>Now you need to choose your large language model provider. I’m currently using OpenRouter and Anthropic, which have a diversity of models for generating text, code and reading images. Do some web searching to find out the latest info on providers and models.</p>
<p>You choose depends on what you want to do and your budget. Some providers offer a free tier. You’ll need to web search for the latest info on this.</p>
<p>You can get the links to any supported provider from your extension settings, just select the provider and then a link will appear to sign-up for that provider.</p>
<p>Once you’ve chosen a provider, create an account and follow their instructions for creating an API key. You will probably also need to buy some credit to use the model.</p>
<p>Note that if you already have a subscription account, e.g.&nbsp;for chatGPT, you will get need an API key to use Cline. This might be in a different section of your account settings. If you are using a subscription service you are probably paying monthly to use the provider’s web app. We need the API key to be able to use the model directly via code.</p>
</section>
<section id="youre-all-set" class="level2">
<h2 class="anchored" data-anchor-id="youre-all-set">You’re all set</h2>
<p>Now you’re set and ready to start using Cline to code and write text.</p>
</section>
<section id="issues-and-tips" class="level2">
<h2 class="anchored" data-anchor-id="issues-and-tips">Issues and tips</h2>
<p>This is just a list of issues I’ve had and how I’ve solved them.</p>
<p><em>Plotting</em> If your R plots look weird (like tiny font), make sure httpgp is enabled. Go back to steps above and see how to do that.</p>
<p><em>Viewing data</em> There are various extensions for viewing csv and excel files. It is worth looking into these so that when you do <code>View(dat)</code> in R you get a nice table. Some also allow editing.</p>
<p><em>Getting help to install software</em> My computer is somewhat locked down by IT, so getting this set-up was a bit fiddly and required a few requests to IT to install software.</p>
<p><em>R markdown</em> There are options in the R extension settings for how to knit markdown. You may need to configure these if you want to knit markdown docs from VScode. If you are having trouble knitting markdown it may mean that the path to pandoc is not set correctly. <a href="https://stackoverflow.com/questions/60766646/need-help-assigning-global-settings-for-rstudios-pandoc-in-vscode-to-knit-pdf-d">There is some helpful instructions here</a></p>
<p><em>R terminal crashes</em> If I run too much R code at once (like selecting a big block then running) the terminal tends to crash. Initially I see a little highlighted box saying ‘PTY HOST’. Then I need to close all the terminals (with the bin icon) and start again. Try radian if this is a problem. You can also code run line-by-line or source whole scripts from the terminal (which works fine). I tried debugging this by increasing the buffer but to on avail.</p>
<p><em>Shortcut keys</em> (on osx) cmd-/ to comment uncomment lines. cmd-shift-p to open the command palette, cmd-b to open the file explorer, cmd-enter to run lines or selection of R code, cmd-shift-c to open terminal in new window, cntrl-shift-` to open a new terminal in vs code.</p>
</section>
<section id="installing-radian-optional" class="level2">
<h2 class="anchored" data-anchor-id="installing-radian-optional">Installing radian (optional)</h2>
<p>Radian is a terminal editor that is a bit nicer than the base R one. It does autocomplete in the terminal (like Rstudio does in the console), colours code/brackets etc… and allows multi-line editing in the terminal.</p>
<p>To set this up, install radian (you need python to do this). More <a href="https://github.com/randy3k/radian?tab=readme-ov-file">instructions here</a>.</p>
<p>Then go to the terminal and find the path where radian is installed (e.g.&nbsp;<code>which radian</code> on mac or <code>where radian</code> on windows).</p>
<p>Now open your settings in VScode (cmd-,) and search for ‘rterm’ (stands for ‘R Terminal’, don’t change the rpath which we set just before). Add the path to radian to the rterm setting. Also search for the setting ‘R: Bracketed Paste’ and make sure it is enabled.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>research-skills</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-06-12-setting-up-vscode-r-genAI/</guid>
  <pubDate>Wed, 11 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Open repo of ecological data for use in teaching</title>
  <link>https://www.seascapemodels.org/posts/2025-06-10-open-access-data-for-teaching/</link>
  <description><![CDATA[ 





<p>Just a quick announcement. I’ve organized and posted two ecological datasets that I find helpful for creating examples for teaching. The repo and instructions for use and attribution are on github:</p>
<p>https://github.com/cbrown5/example-ecological-data</p>
<p>There are two sets of data posted, each with multiple flat csv files.</p>
<p>One is field survey data of benthic habitats and fish. Useful for data wrangling of site survey data, multivariate community analysis and GLMs with count data</p>
<p>The second is experimental data of algal growth under multiple stressors. Useful for teaching missing data, plotting trends over time and analysing interacting effects with GLMs.</p>
<p>All data can be downloaded directly into R like this:</p>
<pre><code>benthic_cover_url &lt;- "https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/benthic_cover.csv"

library(readr)
dat &lt;- read_csv(benthic_cover_url)
write_csv(dat, "benthic_cover.csv")</code></pre>



 ]]></description>
  <category>rstats</category>
  <guid>https://www.seascapemodels.org/posts/2025-06-10-open-access-data-for-teaching/</guid>
  <pubDate>Mon, 09 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>Vibe coding with AI agents is not for scientists</title>
  <link>https://www.seascapemodels.org/posts/2025-06-08-vibe-coding-not-for-scientists/</link>
  <description><![CDATA[ 





<p>Vibe coding is the idea of creating software with large language models (e.g.&nbsp;chat GPT, Claude), using only prompting and no, or very little <a href="https://en.wikipedia.org/wiki/Vibe_coding">human written code</a>.</p>
<p>Co-founder of openAI says of vibe coding: <a href="https://www.businessinsider.com/vibe-coding-ai-silicon-valley-andrej-karpathy-2025-2">“It’s not really coding - I just see things, say things, run things, and copy-paste things, and it mostly works.”</a>.</p>
<p>The problem with vibe coding for scientists is modelling requires precision in coding and statistical logic. Small differences in data wrangling or model formulation can result in logically flawed results.</p>
<p>My experience of vibe coding is that it tends to make code that runs, but has mistakes in the critical places. This is very problematic if you do not manually check all the logic.</p>
<section id="what-is-vibe-coding" class="level2">
<h2 class="anchored" data-anchor-id="what-is-vibe-coding">What is vibe coding</h2>
<p>Vibe coding has gained popularity with agentic AI systems, such as Github Copilot, Cline, Roo Code and Claude Code. These are systems that take your initial instructions and then iterate it through creating it and debugging it with minimal or no human intervention.</p>
<p>Anthropic claims that their latest model can work for hours on software development without need for human intervention. The way it works is that the LLM returns the results of its code to itself, checks those results, then decides on the next action to take.</p>
<p>As an ecological modeller I thought these tools could be useful for developing routine R code. Common tasks include data wrangling, building and running various versions of a regression model and then generating plots.</p>
<p>So I’ve been experimenting with agentic AI, including with Cline, Roo Code and Github Copilot for ecological statistics.</p>
</section>
<section id="flawed-logic" class="level2">
<h2 class="anchored" data-anchor-id="flawed-logic">Flawed logic</h2>
<p>The clincher from the quote above is <strong>“…and it mostly works.”</strong></p>
<p>I’ve found for statistics the bits that don’t work are the critical science parts. So vibe coding tends to produce code that runs and produces believable plots, but results that are logically flawed.</p>
<p><strong>Here’s an example of how vibe coding can produce flawed science:</strong></p>
<p>I’m analysing some turtle monitoring data to calculate mean number of nests per night. The problem is that zeros, nights with no nests, are implicit. Monitoring was done every night for three months, and the field team records nesting events the observe.</p>
<p>I tried vibe coding to calculate mean number of nests per night. It just calculated the mean number out of the total number of unique dates in the dataset. What we actually need to do first is add zeros, nights without nesting, back into the data first. Otherwise, we get a mean that isn’t comparable across years or sites</p>
</section>
<section id="over-engineering" class="level2">
<h2 class="anchored" data-anchor-id="over-engineering">Over-engineering</h2>
<p>You might say - so just go back and fix the error. That’s exactly what I did. However, herein lies a problem.</p>
<p>AI agents tend to totally over-engineer R code, producing scripts that are very robust to programmatic errors, but also very lengthy (see example below). There’s an overwhelming amount of code to go through to find these small but significant mistakes.</p>
<p>AI has a great breadth of knowledge, so a lengthy vibe coding session will produce a diversity functions and coding logic. You need to be experienced to interpet a lot of what it produces.</p>
<p>Your typical R user knows enough to analyse their data and make some plots. They are going to be overwhelmed trying to interpret 100s of lines of R code.</p>
</section>
<section id="how-to-use-ai-agents-safely" class="level2">
<h2 class="anchored" data-anchor-id="how-to-use-ai-agents-safely">How to use AI agents safely</h2>
<p>AI agents are still very useful. But for scientific coding they need careful oversight. You can either:</p>
<ol type="1">
<li>Let it run autonomously, then carefully go back through what its created and review everything</li>
<li>Check and review it step-by-step, and redirect it</li>
</ol>
<p>If you are getting out of your depth, then I suggest stopping and reviewing the code until you are sure what it does. You can also prompt the agent to add checks in (like checking number of samples matches your expectations).</p>
<p>Remember that the commercial agents, like Claude code or github copilot, are designed for software engineers. So they write robust R code, but they are also not very good at statistics.</p>
<section id="example-of-over-engineering" class="level3">
<h3 class="anchored" data-anchor-id="example-of-over-engineering">Example of over engineering</h3>
<p>AI Agents love to cover all the bases. This can be nice, and I’ve learnt some new tricks this way. However, its total overkill if you just want to create a few plots. Here’s one example where I was getting copilot to do an multivariate analysis. It started by creating this whole script, complete with custom functions, just to load a few packages:</p>
<pre><code>
# List of required packages
required_packages &lt;- c(
  "tidyverse",  # For data manipulation and visualization
  "vegan",      # For community analysis
  "ggplot2",    # For data visualization
  "ggrepel",    # For non-overlapping text labels
  "cluster",    # For cluster analysis
  "knitr",      # For report generation
  "kableExtra"  # For table formatting in reports
)

# Function to check and install packages
install_if_missing &lt;- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    cat(paste0("Installing package: ", pkg, "\n"))
    install.packages(pkg, dependencies = TRUE)
  } else {
    cat(paste0("Package already installed: ", pkg, "\n"))
  }
}

# Install missing packages
cat("Checking and installing required packages...\n")
for (pkg in required_packages) {
  install_if_missing(pkg)
}

# Load the packages to verify installation
cat("\nLoading packages to verify installation...\n")
for (pkg in required_packages) {
  cat(paste0("Loading package: ", pkg, "\n"))
  library(pkg, character.only = TRUE)
}

cat("\nAll required packages are installed and loaded.\n")
cat("You can now proceed with running the analysis scripts.\n")
</code></pre>


</section>
</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-06-08-vibe-coding-not-for-scientists/</guid>
  <pubDate>Sat, 07 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
  <title>How to get your LLM model to run and interpret R code</title>
  <link>https://www.seascapemodels.org/posts/2025-03-17-LLMs-in-R-tool-use/</link>
  <description><![CDATA[ 





<p>Tools let AI models run code and access other applications. This can be a tool on your computer (like an R function), or access and API.</p>
<p>By creating tools you can give your LLM access to databases online (via their APIs) or let them run code on your computer, interpret the output and use it in their responses.</p>
<p>There are many ways to use tools. So far many tool examples are for computer programmers. There are some generally useful tools out there (like creating files or searching the web). However, I wanted to see if I could make a tool that was specifically useful in my research field.</p>
<p>In this tutorial I’ll look at using them to get ocean data from a public database that has an API.</p>
<section id="how-tools-work" class="level2">
<h2 class="anchored" data-anchor-id="how-tools-work">How tools work</h2>
<p>The basic example of a tool is getting the current date and time. Then when you ask for the time, instead of hallucinating a time and date, the model can use a tool to get an accurate current time and date.</p>
<p>Tools are defined using a Model Context Protocol, which is a standardized structure for connecting natural language models to programmatic tools (which have all the normal programming rules like being case-sensitive, syntax appropriate to the language etc).</p>
<p>Its hard to find clear explanations of how they work, but here’s what I understand:</p>
<p>The tool is ‘registered’ with the model. This registering is just a prompt to the model, but it is done with a specific structure that defines the tool, its parameters, outputs and use cases. This way the model knows how to write code to run the tool.</p>
<p>Then when you chat with the model it might interpret that tool use would be helpful. Like if you ask the time and it ‘knows’ about a time-date tool, it might use it. It writes the appropriate code to use the tool, sends that back to your computer (or to the API), where the code is evaluated. The output is then sent back to the model. The model interprets this into the final response it will give you.</p>
<p><a href="https://ellmer.tidyverse.org/articles/tool-calling.html">There’s a good example of how tools don’t work and do work in the ellmer documentation.</a></p>
<p>Let’s try a more interesting tool than the usual example of getting date and time.</p>
</section>
<section id="use-case" class="level2">
<h2 class="anchored" data-anchor-id="use-case">Use case</h2>
<p><a href="https://imos-animaltracking.github.io/remora/index.html">The <code>remora</code> package lets you extract ocean data from the Integrated Marine Observing System (IMOS) and Bluelink Reanalysis (BRAN) model.</a>. We can give an LLM access to this data by creating a tool that uses <code>remora</code> to extract the data.</p>
</section>
<section id="remora" class="level2">
<h2 class="anchored" data-anchor-id="remora">remora</h2>
<p>We’ll start by making some data and testing out <code>remora</code>. <a href="https://imos-animaltracking.github.io/remora/index.html">Note that <code>remora</code> isn’t on cran, so you will need to install it from github.</a></p>
<p>What we do below is download ocean temperatures for some made up data. We just need coordinates and dates.</p>
<p><a href="https://imos-animaltracking.github.io/remora/articles/extractBlue.html">For a more detailed example see the remora documentation</a></p>
<pre><code>library(remora)
library(tidyverse)
library(raster)

# Create a dataframe of coordinates near Tasmania with different dates in 2021
tas_dat &lt;- data.frame(
  datetime = ymd_hms(paste(
        c("2021-02-15", "2021-02-15", "2021-02-15", 
                   "2021-02-15", "2021-02-15"),
                   "10:00:00")),
  X = c(147.2, 148.5, 146.8, 145.3, 144.7),  # Longitudes around Tasmania
  Y = c(-42.5, -41.8, -43.2, -42.9, -41.5)
)
write.csv(tas_dat, "tas_dat.csv", row.names = FALSE)</code></pre>
<p>Extracting the SST data at a given depth is simple:</p>
<pre><code>tas_temp &lt;- 
    extractBlue(df = tas_dat,
                X = "X", 
                Y = "Y", 
                datetime = "datetime", 
                env_var = "BRAN_temp",
                extract_depth = 30,
                verbose = TRUE,
                full_timeperiod = FALSE)</code></pre>
</section>
<section id="defining-the-tool" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-tool">Defining the tool</h2>
<p>Now we know how to use <code>remora</code> to get the data, we can define a tool so the LLM can do it.</p>
<p>We’ll use the <code>ellmer</code> package to define the tool. First step is to make a function.</p>
<p>The <code>ellmer</code> documentation recommends you keep the function inputs and outputs simple. First of all, LLMs can’t handle complex R data structures (like complex lists). Second, the LLM will be consuming the inputs and outputs. This means it uses tokens, which cost you $ and use up the model’s context window (capacity for prompts).</p>
<p>So you don’t want to have gigabytes of data going into or out of the tool.</p>
<p>To get around this I’ve defined a tool that takes a path and write a file. That way the LLM doesn’t ‘see’ the data, it only sees the input and output paths and my depth request.</p>
<pre><code>#' Extracts SST data from the IMOS BlueLink database
#'
#' @param data_path A path to a csv dataframe
#' @param depth The depth to extract the SST data from
#' @return The current time in the given time zone.
get_sst &lt;- function(data_path, depth) {
  dat &lt;- read.csv(data_path)
  datout &lt;- extractBlue(df = dat,
                X = "X", 
                Y = "Y", 
                datetime = "datetime", 
                env_var = "BRAN_temp",
                extract_depth = depth,
                verbose = FALSE,
                full_timeperiod = FALSE)
  write.csv(datout, "temp_data.csv", row.names = FALSE)
  return("File written to sst_data.csv")
}</code></pre>
<p>Now just test the function works:</p>
<pre><code>get_sst("tas_dat.csv", 30)</code></pre>
<p>The <code>ellmer</code> package has a <code>create_tool_def</code> that semi-automates the next step based on the roxygen documentation I wrote above the function. However, this currently seems only to work if you have an openAI API key.</p>
</section>
<section id="register-the-tool-with-a-chat" class="level2">
<h2 class="anchored" data-anchor-id="register-the-tool-with-a-chat">Register the tool with a chat</h2>
<p>Start a new chat:</p>
<pre><code>library(ellmer)
chat &lt;- chat_claude(
  model = "claude-3-5-haiku-20241022", 
  max_tokens = 1000
)</code></pre>
<p>(You should probably use a strong logic model, like sonnet or GPT4.0 for tool use, but haiku worked fine for this simple one).</p>
<p>Now we register the tool with the chat:</p>
<pre><code>mytool &lt;- tool(
  get_sst,
  "Gets SST data from the IMOS BlueLink database",
  data_path = type_string(
    "Path to a csv dataframe that has columns datetime, X, Y",
    required = TRUE
  ),
  depth = type_number(
    "The depth to extract the SST data from",
    required = TRUE
  )
)

chat$register_tool(mytool)</code></pre>
<p>Note the careful documentation of the tool. This is going to become prompts for the model, so use all your normal prompt strategies to help it figure out when to use the tool .</p>
<p>We also use the <code>type_xxx</code> functions from <code>ellmer</code> to define the types of the inputs. This is important to ensure the model knows how to properly write the code for the tool use.</p>
</section>
<section id="using-the-tool" class="level2">
<h2 class="anchored" data-anchor-id="using-the-tool">Using the tool</h2>
<p>This is the easy bit:</p>
<pre><code>chat$chat("Can you get me SST data at 21m depth for the coordinates in tas_dat.csv?")</code></pre>
<p>If that works for you (as it did for me) you should have a new csv file in your working directory with the SST data.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>With a little bit of code you can give your LLM prompts and responses new types of capabilities.</p>
<p>My simple example could be improved by adding more parameters into the function and adding error checking and reports.</p>
<p>If we wanted to keep going we could create an analysis tool that then analyses the data from file, makes the plots, and so on.</p>
<p>Tool definitions are based on function documentation. This means you can take any existing R function and easily turn it into a tool.</p>
<p>You do want to be careful about how much and the types of data that go into tools. There are obviously security concerns (what if the model created malicious code?) as well as privacy concerns if you are working with sensitive data.</p>
<p>API costs could also blow up. <code>ellmer</code> provides a function <code>token_usage()</code>, to help you keep track of spending.</p>
<p>Overall, I was excited at how easy it was to make a tool with interesting applications in my field of ocean sciences.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-03-17-LLMs-in-R-tool-use/</guid>
  <pubDate>Sun, 16 Mar 2025 13:00:00 GMT</pubDate>
</item>
<item>
  <title>The ellmer package for using LLMs with R is a game changer for scientists</title>
  <link>https://www.seascapemodels.org/posts/2025-03-15-LMs-in-R-with-ellmer/</link>
  <description><![CDATA[ 





<section id="why-is-ellmer-a-game-changer-for-scientists" class="level2">
<h2 class="anchored" data-anchor-id="why-is-ellmer-a-game-changer-for-scientists">Why is ellmer a game changer for scientists?</h2>
<p>In this tutorial we’ll look at how we can access LLM agents through API calls. We’ll use this skill for created structued data from documents.</p>
<p>We’ll use the R <a href="https://ellmer.tidyverse.org/index.html"><code>ellmer</code> package</a> (launched 25th Feb 2025).</p>
<p>There are a few package options <a href="https://www.seascapemodels.org/rstats/2025/02/12/LLMs-for-literature-reviews.html">(I was also using <code>tidychatmodels</code> before)</a>.</p>
<p><code>ellmer</code> is a game changer for scientists using R: It supports tool use and has functions for structured data.</p>
<p>Before <code>ellmer</code> you had to know other languages and data-structures, like JSON. Ellmer means that many powerful LLM uses are now easily accessible to R users.</p>
<p>Tool use means the LLM can run commands on your computer to retrieve information. You know how LLMs can be bad at simple math like 2+2 or today’s date? Well, with a tool, the LLM would know to use R to calculate this to return the correct answer. Tools can also connect to web APIs, means they can also be used to retrieve information or databases from the web.</p>
<p>The functions that assist you in creating structured data from text are also important.</p>
<p>For instance, by combining tool use with structured data extraction, <code>ellmer</code> could be used to synthesize literature for a quantitative review. We’ll cover that here.</p>
<p>Another application would be to interpret the meta-data from an online database, download the data, write and run the analysis and then write up the findings.</p>
<p>Because you are running the LLM from R it means you can batch process prompts, such as asking it to summarize many documents.</p>
<p>They are a couple of obvious uses. I’m sure people will soon come up with many more.</p>
<p>Let’s see how to use ellmer to extract structured data from peer reviewed studies.</p>
<section id="setup-authorisation" class="level3">
<h3 class="anchored" data-anchor-id="setup-authorisation">Setup authorisation</h3>
<p>First, you need to get an API key from the provider. Login to the provider’s website and follow the instructions.</p>
<p>Then, you need to add the key to your <code>.Renviron</code> file:</p>
<p><code>usethis::edit_r_environ()</code></p>
<p>Then type in your key like this:</p>
<p><code>ANTHROPIC_API_KEY="xxxxxx"</code></p>
<p>Then restart R. <code>ellmer</code> will automatically find your key so long as you use the recommended envirment variable names. See <code>?ellmer::chat_claude</code> (or <code>chat_xxx</code> where xxx is whatever provider you are using).</p>
</section>
<section id="interactive-chat" class="level3">
<h3 class="anchored" data-anchor-id="interactive-chat">Interactive chat</h3>
<p>Posit recommends this is useful for experimenting with new prompts. It can also be better than the web interfaces you might be used to. This is because, unlike on the web interfaces, you can control the system message.</p>
<p>Most models have three types of messages:</p>
<p><strong>system</strong>, which sets the context, role and is given higher priority than the user message</p>
<p><strong>user</strong>, which is your conversation or request</p>
<p><strong>assistant</strong>, which is the model’s response</p>
<p>Many prompt engineer pros have found better results if you put as much of your request in the system as possible. So I recommend you play around with this yourself.</p>
<pre><code>library(ellmer)

chat &lt;- chat_claude(
  system_prompt = "You are a friendly but terse assistant.",
  model = "claude-3-5-haiku-20241022", 
  max_tokens = 1000
)

live_console(chat)
live_browser(chat)</code></pre>
<p>Try this but experiment with a new system prompts. Here’s an example, try your own.</p>
<pre><code>chat &lt;- chat_claude(
  system_prompt = "You are a mystic with expertise in R programming. You answer questions indirectly in riddles and rhymes.",
  model = "claude-3-5-haiku-20241022", 
  max_tokens = 1000
)

live_console(chat)</code></pre>
<blockquote class="blockquote">
<p>User: how can I do a glm in R?</p>
</blockquote>
<pre><code>*Swirling mystical orb glows with R code*

Through statistics' mist, a model takes flight,
glm() is the spell to cast in the night,
With family bound, be it gaussian or more,
And formula written as y ~ x4.
...</code></pre>
</section>
<section id="programmatically-generate-content" class="level3">
<h3 class="anchored" data-anchor-id="programmatically-generate-content">Programmatically generate content</h3>
<p>So once you’ve tested your prompt you can run it like this:</p>
<pre><code>chat &lt;- chat_claude(
  system_prompt = "You are a surfy dude who likes waves, tubes and statistics.",
  model = "claude-3-5-haiku-20241022", 
  max_tokens = 1000
)

chat$chat("How can I do a glm in R?")</code></pre>
<p>‘… Hang ten with your statistical surfboard! 🏄‍♂️📊’</p>
<p>That was cheesy…</p>
</section>
</section>
<section id="example-clean-pdfs-text-and-summarize" class="level2">
<h2 class="anchored" data-anchor-id="example-clean-pdfs-text-and-summarize">Example: Clean pdfs text and summarize</h2>
<p>Now let’s see if we can use <code>ellmer</code> to clean up some text from a pdf and summarize it. <code>ellmer</code> has some handy functions for processing pdfs to text, so they can then be fed into prompts.</p>
<p>I’m going to attempt to summarize my <a href="https://conbio.onlinelibrary.wiley.com/doi/10.1111/conl.13056">recent paper on turtle fishing</a>.</p>
<pre><code>x &lt;- content_pdf_url("https://conbio.onlinelibrary.wiley.com/doi/epdf/10.1111/conl.13056")</code></pre>
<p>This fails with a 403 error. This means the server is blocking the request, it probably guesses (correctly) that I’m calling the pdf programmatically: it thinks I’m a bot (which this tutorial kind of is creating).</p>
<p>We can also try with a file on our hard drive, we just have to manually download the pdf.</p>
<pre><code>mypdf &lt;- content_pdf_file("pdf-examples/Brown_etal2024 national scale turtle mortality.pdf")</code></pre>
<p>That works, now let’s use it within a chat. First set-up our chat:</p>
<pre><code>chat &lt;- chat_claude(
  system_prompt = "You are a research assistant who specializes in extracting structured data from scientific papers.",
  model = "claude-3-5-haiku-20241022", 
  max_tokens = 1000
)
</code></pre>
<p>Now, we can use ellmer’s functions for specifying structured data. Many LLMs can be used to generate data in the JSON format (they were specifically trained with that in mind).</p>
<p><code>ellmer</code> handles the conversion from JSON to R objects that are easier for us R users to understand.</p>
<p>You use the <code>type_object</code> then <code>type_number</code>, <code>type_string</code> etc.. to specify the types of data. <a href="https://ellmer.tidyverse.org/articles/structured-data.html">Read more in the ellmer package vignettes</a></p>
<pre><code>paper_stats &lt;- type_object(
  sample_size = type_number("Sample size of the study"),
  year_of_study = type_number("Year data was collected"),
  method = type_string("Summary of statistical method, one paragraph max")
)
</code></pre>
<p>Finally, we send the request for a summary to the provider:</p>
<pre><code>turtle_study &lt;- chat$extract_data(mypdf, type = paper_stats)</code></pre>
<p>The <code>turtle_study</code> object will contain the structured data from the pdf. I <em>think</em> (the ellmer documentation is a bit sparse on implementation details) ellmer is converting a JSON that comes from the LLM to a friendly R list.</p>
<pre><code>class(turtle_study)
#list</code></pre>
<p>And:</p>
<pre><code>turtle_study$sample_size
#11935
turtle_study$year_of_study
#2018
turtle_study$method
#The study estimated national-scale turtle catches for two fisheries in the Solomon Islands 
#- a small-scale reef fishery and a tuna longline fishery - using community surveys and 
#electronic monitoring. The researchers used nonparametric bootstrapping to scale up 
#catch data and calculate national-level estimates with confidence intervals.</code></pre>
<p>It works, but like any structured lit review you need to be careful what questions you ask. Even more so with an LLM as you are not reading the paper and understanding the context.</p>
<p>In this case the sample size its given us is the estimated number of turtles caught. This was a model output, not a sample size. In fact this paper has several methods with different sample sizes. So some work would be needed to fine-tune the prompt, especially if you are batch processing many papers.</p>
<p>You should also experiment with models, I used Claude haiku because its cheap, but Claude sonnet would probably be more accurate.</p>
</section>
<section id="batch-processing-prompts" class="level2">
<h2 class="anchored" data-anchor-id="batch-processing-prompts">Batch processing prompts</h2>
<p>Let’s try this with a batch of papers (here I’ll just use two). For this example I’ll just use two abstracts, which I’ve obtained as plain text. The first is from <a href="https://zslpublications.onlinelibrary.wiley.com/doi/10.1111/j.1469-1795.2010.00413.x">another study on turtle catch in Madagascar</a>. The second is from my study above.</p>
<p>What we’ll do is create a function that reads in the text, then passes it to the LLM, using the request for structured data from above.</p>
<pre><code>  process_abstract &lt;- function(file_path, chat) {
  # Read in the text file
  abstract_text &lt;- readLines(file_path, warn = FALSE)
  
  # Extract data from the abstract
  result &lt;- chat$extract_data(abstract_text, type = paper_stats)
  
  return(result)
}</code></pre>
<p>Now set-up our chat and data request</p>
<pre><code># Create chat object if not already created
chat &lt;- chat_claude(
      system_prompt = "You are a research assistant who specializes in extracting structured data from scientific papers.",
      model = "claude-3-5-haiku-20241022", 
      max_tokens = 1000
)</code></pre>
<p>There’s a risk that the LLM will hallucinate data if it can’t find an answer. To try to prevent this we can set an option , required = FALSE. Then the LLM should return ‘NULL’ if it can’t find the data.</p>
<pre><code># Define the structured data format
paper_stats &lt;- type_object(
    sample_size = type_number("Number of surveys conducted to estimate turtle catch", required = FALSE),
    turtles_caught = type_number("Estimate for number of turtles caught", required = FALSE),
    year_of_study = type_number("Year data was collected", required = FALSE),
    region = type_string("Country or geographic region of the study", required = FALSE)
  )</code></pre>
<p>Now we can batch process the abstracts and get the structured data</p>
<pre><code>
abstract_files &lt;- list.files(path = "pdf-examples", pattern = "\\.txt$", full.names = TRUE)
results &lt;- lapply(abstract_files, function(file) process_abstract(file, chat))
names(results) &lt;- basename(abstract_files)

# Display results
print(results)</code></pre>
<p>In my first take without the required = FALSE I got some fake results. It hallucinated that the Humber study was conducted in 2023 (it was published in 2010!) and that there were 2 villages surveyed in my study. The problem was that you can’t get that data from the abstracts. So the model is hallucinating a response.</p>
<p>Unfortunately, with required = FALSE it still hallucinated answers. I then tried Claude sonnet (a more powerful reasoning model) and it correctly put NULL for my study’s sample size, but still got the year wrong for the Humber study.</p>
<p>I think this could work, but some work on the prompts would be needed.</p>
</section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<p>The ellmer package solves some of the challenges <a href="https://www.seascapemodels.org/rstats/2025/02/12/LLMs-for-literature-reviews.html">I outlined in my last blog on LLM access</a> from R. But others are deeper conceptual challengs and remain. I’ll repeat those here</p>
<section id="cost-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="cost-uncertainty">Cost uncertainty</h3>
<p>This should be cheap. It cost &lt;1c to make this post with all the testing. So in theory you could do 100s of methods sections for &lt;100USD. However, if you are testing back and forwards a lot or using full papers the cost could add up. It will be hard to estimate this until people get more experience.</p>
</section>
<section id="obtaining-the-papers-and-dealing-with-unstructued-text-in-pdfs-or-html" class="level3">
<h3 class="anchored" data-anchor-id="obtaining-the-papers-and-dealing-with-unstructued-text-in-pdfs-or-html">Obtaining the papers and dealing with unstructued text in PDFs or HTML</h3>
<p>A big challenge will be getting the text into a format that the LLM can use. Then there are issues like obtaining the text. Downloading pdfs is time consuming and data intensive. Trying to read text data from webpages can also be hard, due to paywalls and rate limits (you might get blocked for making reqeat requests).</p>
<p>For instance, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/brv.12344?casa_token=LVnFzoFBBU8AAAAA%3AcsyopYDWDBRZN7y2JL7eHYxzqayxu2GvKB-7gdEYkdeZSi5p5o1oXTwj49FqwBJz-IpPS6wxJ_SX0h2f">in a past study we did where we did simple ‘bag of words analysis’</a> we either downloaded the pdfs manually, or set timers to delay web hits and avoid getting blocked.</p>
<p>HTML format would be ideal, because the tags mean the sections of the paper, and the figures already semi-structured.</p>
<p>The ellmer pdf utility function seems to work ok for getting text from pdfs. I’m guessing it could be improved though, e.g.&nbsp;to remove wastefull (=$) text like page headers.</p>
</section>
<section id="prompting" class="level3">
<h3 class="anchored" data-anchor-id="prompting">Prompting</h3>
<p>Need to experiment with this to get it right. It might also be good to repeat prompt the same text to triangulate accurate results.</p>
</section>
<section id="validation" class="level3">
<h3 class="anchored" data-anchor-id="validation">Validation</h3>
<p>You’ll definitely want to manually check the output and report accuracy statistics in your study. So maybe your review has 1000 papers, you’ll want to manually check 100 of them to see how accurate the LLM was.</p>
</section>
<section id="youll-still-need-to-read-a-lot-of-papers-to-write-a-good-lit-review" class="level3">
<h3 class="anchored" data-anchor-id="youll-still-need-to-read-a-lot-of-papers-to-write-a-good-lit-review">You’ll still need to read a lot of papers to write a good lit review</h3>
<p>A lit review is more than the systematic data. I still believe you need to read a lot of papers in order to understand the literature and make a useful synthesis. If you just use AI you’re vulnerable to the <a href="https://www.nature.com/articles/s41586-024-07146-0">‘illusion of understanding’</a>.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>This tool will be best for well defined tasks and consistently written papers. For instance, an ideal use case would be reviewing 500 ocean acidification papers that all used similar experimental designs and terminology. You’ll then be able to get consistent answers to prompts about sample size etc…</p>
<p>Another good use case would be to extract model types from species distribution model papers.</p>
<p>Harder tasks will be where the papers are from diverse disciplines, or use inconsistent terminology, or methods. My study was a good example of that, there were about 5 different sample sizes reported. So in this example we’d need first to think clearly about what sample size you wanted to extract before writing the prompt.</p>


</section>
</section>

 ]]></description>
  <category>rstats</category>
  <category>genAI</category>
  <guid>https://www.seascapemodels.org/posts/2025-03-15-LMs-in-R-with-ellmer/</guid>
  <pubDate>Fri, 14 Mar 2025 13:00:00 GMT</pubDate>
</item>
</channel>
</rss>
