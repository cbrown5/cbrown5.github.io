[
  {
    "objectID": "bluecology_blog.html",
    "href": "bluecology_blog.html",
    "title": "Seascape Models",
    "section": "",
    "text": "Chris Brown’s blog on ocean science, quantitative ecology and R programming. Find our R blogs on R-Bloggers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIts easy to make your own AI agent in R\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nNov 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpcoming R workshops in Brisbane February 2026\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nShould we still teach R coding in this age of genAI?\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nNov 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDoes data from protected areas help to improve ecolgoical forecasts? New pre-print\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating harvest levels to reduce feral deer populations in Tasmania\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnvironmental impacts of large language models\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nOct 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nmodelbased package for plotting interactions\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nForage fish fisheries and marine predators\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nResource for using Large Language Model tools in R\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew online book AI Assistants for Scientific Coding\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nOct 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating the Github Copilot Agent from the command line with Copilot CLI\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nOct 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPlanning an effective workshop\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI and R workshops in Hobart Australia\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nSep 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAI generated scicomm games\n\n\n\ngenAI\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProjecting Uncertainty in Ecosystem Persistence Under Climate Change\n\n\n\nresearch\n\nmodelling\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nSep 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPaper in a prompt creating a first draft in one prompt\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAI aided note taking\n\n\n\nresearch-skills\n\ngenAI\n\n\n\n\n\n\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMaking the jump from PhD to Postdoc\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreate reports and tutorials with generative AI from R\n\n\n\nrstats\n\ngenAI\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoints to consider when framing a regional study for the international literature\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nAug 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick tips for facilitating a good meeting\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nAug 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s getting harder to predict ecological change\n\n\n\nresearch\n\nmodelling\n\n\n\n\n\n\n\n\n\nAug 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWriting scientific papers with quarto and AI assistants\n\n\n\nrstats\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPrompting large language models for quality ecological statistics\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nJun 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we are all naturally Bayesians not frequentists\n\n\n\nrstats\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up VScode for R and generative AI tools\n\n\n\nrstats\n\nresearch-skills\n\ngenAI\n\n\n\n\n\n\n\n\n\nJun 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOpen repo of ecological data for use in teaching\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nVibe coding with AI agents is not for scientists\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nJun 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWhy every peer review you write should be nice\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTranslating data and models into benefits for coastal ecosystems\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get your LLM model to run and interpret R code\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe ellmer package for using LLMs with R is a game changer for scientists\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use large language models to assist in systematic literature reviews\n\n\n\nrstats\n\ngenAI\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nFeb 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up VScode for R and Cline\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow your research paper can have impact on environmental management\n\n\n\nresearch\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpeeding up the recovery and restoration of coastal habitats\n\n\n\nresearch\n\nmodelling\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPhD opportunity Integrated assessment of marine ecosystems\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStock assessment models overstate sustainability of the world’s fisheries\n\n\n\nresearch\n\nmodelling\n\nfisheries\n\n\n\n\n\n\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nChris’ secret recipe to writing papers quickly\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMaking an ocean science video game\n\n\n\nresearch-skills\n\ngenAI\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProbability intervals with GAMs\n\n\n\nrstats\n\nmodelling\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTips for providing written feedback to students\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNew PhD opportunities in fisheries\n\n\n\nresearch\n\nphd-projects\n\n\n\n\n\n\n\n\n\nAug 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to structural causal modelling\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does generative AI mean for literature reviews?\n\n\n\nresearch-skills\n\ngenAI\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do Australian marine scientists like about their jobs?\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPhD projects on marine conservation and ecosystem modelling available\n\n\n\nresearch\n\nphd-projects\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian structural equation model tutorial\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity education as social media\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhy GLMs should be a priority when teaching statistics\n\n\n\nrstats\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTips for using chatGPT to learn R\n\n\n\nrstats\n\ngenAI\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaking predictions from GAMs in R explained by an 18th century author\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOrganising R scripts\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMoving to University of Tasmania\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCoaching your supervisor\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nR workshops Brisbane February 2023\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNew vlogs on conservation mapping\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNew vlog Conservation Hackers\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to work with non-governmental organisations\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPhD projects on marine conservation and ecosystem modelling available\n\n\n\nresearch\n\nphd-projects\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nScience Meets Parliament\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMay 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to ask more questions and be a better scientist\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMay 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nClimate smart mangrove restoration\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReading for new quantitative ecology PhD students updated\n\n\n\nresearch\n\nphd-projects\n\n\n\n\n\n\n\n\n\nMar 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAmbitious conservation action sees a brighter future for mangroves and seagrass\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nFeb 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTips for the design of ecological experiments\n\n\n\nrstats\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nDec 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAsking and answering questions at seminars\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nDec 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nElectronic monitoring offers insights into catch of western Pacific tuna longline fisheries\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nOct 15, 2021\n\n\nMax Campbell\n\n\n\n\n\n\n\n\n\n\n\n\nTime for grants: grant writing strategies\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAre lengthy grant applications fair?\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow much time does it take to write a grant?\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRunning R code for all combinations of some parameters with lapply karate\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent approaches to Species Distribution Modelling in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the deal with global biodiversity maps\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nNew fellowship on predicting coastal ecological futures in an era of unprecedented change\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nAug 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting global emissions from mangrove deforestation\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCombining local knowledge and scientific surveys to estimate declines in threatened species\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPhD projects on marine conservation and ecosystem modelling available\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting interactive effects of multiple stressors on marine ecosystems\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA comparison of terra and stars packages\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA comparison of terra and raster packages\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMay 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFilling gaps in global assessments of ecosystems to benefit coastal wetland conservation\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nMay 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSeaspiracy talking points for discussions with general public\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow will heatwaves and coral loss affect reef fisheries?\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow to solve common problems with GAMs\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWhen interpolation doesn’t work with GAMs\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow do you forecast whether your next research project will be successful?\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nFeb 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBasic math biologist should know by heart\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nFeb 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFast functions with pipes\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nDec 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nEncouragement not praise\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nDec 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR workshops in Brisbane 2021\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNew course on preparation and graphing of biological data in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nCorrecting for confounded variables with GLMs\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUniversal inference in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nResearch update, addressing trends emerging in 2020\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPhD projects on marine conservation and ecosystem modelling available\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTime sinks for data wrangling\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow much has the data informed your isotope mixing model\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAre trees greener near salmon spawning streams?\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nApr 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nA case against pipes in R and what to do instead\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHome isolation work success plan\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMar 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAdding desktop based work to your PhD? Here’s some things to think about\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMar 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFacing uncertainty with a growth mindset\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMar 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFacing uncertainty\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMar 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial predictions with GAMs and rasters\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nReading for new quantitative ecology PhD students\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nFeb 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThree ways to calculate distances in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R and power analysis to inform experimental design\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSeagrass habitats lack protection at a global scale\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nOct 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nMake a map of your study site with tmap\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nInforming priors for a time-series models to detect declines in tiger sharks\n\n\n\nrstats\n\nfisheries\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nThree myths youve probably heard about scicomm\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 21, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nGLM tip getting non-linear with splines\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 21, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR courses February 2020\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nAug 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nShould I do more study or start a PhD?\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re hiring a quantitative ecologist\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWorking within your lab group to create a more equitable and diverse scientific workforce\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJun 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDodging negativity bias in a critical work culture\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nApr 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDiary of becoming a Bayesian\n\n\n\nrstats\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMar 18, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR course in data wrangling and spatial analysis notes are online\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHiring research assistant in spatial ecology\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get leadership experience in your PhD\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJan 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWhat type of leaders should a PhD program produce\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJan 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nFinding confidence in your PhD and overcoming imposter syndrome\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nDec 17, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDecline in large shark species on Queensland’s coastline\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nDec 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models understanding the link function\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow I became a science communicator without realizing it\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBeginner Intermediate and Advanced R courses February 2019\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nEnforcing marine protected areas to reach ecological targets for recovery\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nSep 24, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nImproving fisheries means saving their habitats\n\n\n\nresearch\n\nfisheries\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nSep 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nFive key skills to learn in your PhD and demonstrate in job interviews\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nAug 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGuide to posting pre-prints for ecologists\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nAug 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNew PhD opportunities to work on conserving the health of coastal wetlands\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJul 24, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey your audience and visualise the results with R and Google forms\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nA brief guide to data visuals in R in 2018\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPostdoctoral job in Aquatic Ecology\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJul 2, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nKeeping up your publication track record when you change jobs\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJun 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nEcologists are misusing isotope mixing models\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nApr 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I interpret the AIC\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGrowing your international scientific career as a parent\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nApr 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nFour new postdoc and science communicator positions to work on the global health of wetlands\n\n\n\nresearch\n\ncoastal-wetlands\n\n\n\n\n\n\n\n\n\nMar 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifying conservation priorities for ridge to reef management in tropical catchments\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMar 24, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nShould I learn sf or sp for spatial R programming\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 23, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nThe fallacy of the 90 hour work week\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I calculate the R squared metric for a Bayesian model\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJan 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Linear Models The Basics\n\n\n\nrstats\n\nmodelling\n\n\n\n\n\n\n\n\n\nJan 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNew postdoc and PhD positions on multiple stressors of ecosystems\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nDec 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nIn rememberance of the ecosystems I knew as a child\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nNov 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\ne-media tools for the creation, publication and promotion of academic papers\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nNov 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nAiming for balance between recreational and commercial fisheries\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nNov 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nNew grant to study impacts of interacting stressors on marine ecosystems\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nNov 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nChecking residual distributions for non-normal GLMs\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nManaging for uncertain interactions among multiple stressors of marine populations\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nOct 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nFrustration and success when trying to make a difference\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nOct 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nQueenland The Coal State\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nOct 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nStay motivated with R programming by knowing your emotions\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSome thoughts about Bayesian Kriging in INLA\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 7, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSocially equitable recovery of fisheries\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nAug 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nseascapemodels.org version 2\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nAug 8, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of the conservation optimism hashtag\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nAug 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nWhat analysis programs drive conservation science?\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nAug 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMemorable dataviz with the R program, talk awarded people’s choice prize\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are people saying about ICCB2017 on Twitter?\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nWhat analysis program do conservation scientists use?\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nData visuals notes for my talks in 2017\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nData visuals notes for my talks in 2017\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does under or over-dispersion look like?\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSmoothing a time-series with a Bayesian model\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying the magnitude of a population decline with Bayesian time-series modelling\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing your research network with social media\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJun 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nNetworks of habitats increase the vulnerability of migratory species to climate change\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMay 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHacks for making time-effective and lower stress decisions\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nMay 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising uncertainty in time-series using animations\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMay 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing R packages for mixed effects modelling based on the car you drive\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nA fast method to add annotations to a plot\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 29, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMean trophic levels of a genera from FishBase\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMake your R simulation models 20 times faster\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nPart 3: Spatial analysis of geotagged data\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 22, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGood research papers build coherent stories, but what is a coherent story\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nFeb 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nThe next advance in spatial R\n\n\n\n\n\n\n\n\nJan 7, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHighlights of 2016\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nDec 26, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nProtecting forests in Fiji to give coral reefs a future\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nDec 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nCreate an interactive web map with geotagged photos\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 23, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle controls who you cite\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nNov 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nFrom geotagged photos to species distribution models\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 14, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting exif data from photos using R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 14, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nA ‘tidyverse’ approach to simulation models\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 28, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nCreate an inset plot\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Spatial Analysis in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 6, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nLearning from failures\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 26, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nA fast start to R scripts\n\n\n\n\n\n\n\n\nAug 7, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpurious doomsday scenarios?\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nAug 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWrite like a runner trains\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nJul 25, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nR programming tools for conservation scientists\n\n\n\nresearch-skills\n\nrstats\n\n\n\n\n\n\n\n\n\nJul 8, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nMore info on my talk at SCBO 2016\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJul 7, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nMaking continuous colour palettes with RColorBrewer\n\n\n\n\n\n\n\n\nJul 7, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter stats from ICRS2016\n\n\n\nresearch\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nR programming tools for coral reef scientists\n\n\n\nresearch\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 22, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the future of coral reefs? - ICRS summary\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nInternational Coral Reef Symposium Day 2\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nInternational Coral Reef Symposium Day 1\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 20, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWind predicts connectivity for coral reef fish\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 8, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nPhD position on fisheries dependence on marine habitats\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nJun 1, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity analysis in R the easy way\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMay 19, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nCosting the impact of cyclone Winston on Fiji’s ecosystem services\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMay 16, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of recreational fishing areas\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nMar 28, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create a culture of using R programming at your institution\n\n\n\nresearch-skills\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 15, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nTall Young Poppy Award\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMar 8, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my new webpage\n\n\n\n\n\n\n\n\nMar 7, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHow long does it take to write major national grant?\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nMar 6, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nAutomate adding labels to figures in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 26, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies on the Move - Day 3 synopsis\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies on the Move - Day 2 synopsis\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies on the Move -some things I learned\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies on the Move - Day 1 synopsis\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring range shifts\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nFeb 9, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nAre more marine protected areas needed to protect ocean biodiversity?\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nDec 3, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nCommandments for entering data to use in R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 13, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nNew fellowship on fisheries and their habitats\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nNov 3, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly load many files into R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 23, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nHow statistics help us get causation right\n\n\n\nresearch-skills\n\n\n\n\n\n\n\n\n\nSep 23, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nDigitising graphs with R\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 17, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nHow to think like an economist\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 16, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nMarine reserves without the short-term losses in harvest\n\n\n\nresearch\n\nfisheries\n\n\n\n\n\n\n\n\n\nJul 6, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are conservation priorities?\n\n\n\n\n\n\n\n\nMay 8, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling using R\n\n\n\n\n\n\n\n\nMay 8, 2015\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data/leafletmap_files/leaflet-providers-1.0.27/license.html",
    "href": "data/leafletmap_files/leaflet-providers-1.0.27/license.html",
    "title": "Seascapemodels",
    "section": "",
    "text": "Copyright (c) 2013 Leaflet Providers contributors All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "data/tassie-scicomm-games/catch-allocation-race/game-idea.html",
    "href": "data/tassie-scicomm-games/catch-allocation-race/game-idea.html",
    "title": "Seascapemodels",
    "section": "",
    "text": "I want to make a game about sharing fish catches between commercial and recreational fishing sectors.\nThe goal of the game is to keep everyone happy. The game play is continuous.\nIn the game, the player sees three boxes.\n\nA pond full of fish. Fish numbers change according to the logistic growth model with catches from both sectors.\nA management panel where the player has two slider bars. The first slider bar is for total recreational catch. The second slider is for total commercial catch.\nA final panel that shows performance on outcomes for the environment, commercial fishing and recreational fishing. Use emojis for each sector to illustrate how happy or unhappy they are. Environmental happiness is based on the number of fish. Commercial and recreational happiness is based on their catches. Be creative here.\n\nGame play is continuous for 1 minute. At the end the player is aiming to make a good balance and keep everyone happy."
  },
  {
    "objectID": "people/alumni/Alyssa-Giffin.html",
    "href": "people/alumni/Alyssa-Giffin.html",
    "title": "Dr Alyssa Giffin",
    "section": "",
    "text": "Alyssa joined us for her PhD studies after completing her MSc in Tropical Marine Biology and Ecology at James Cook University in Townsville. She completed her PhD in 2023.\nAlyssa’s PhD thesis “Marine and coastal ecosystem-based adaptation” looked at how coastal ecosystems can be used to help communities adapt to climate change. Her chapters included systematic literature review, spatial planning and modelling. Alyssa’s research was part of the Global Wetlands Project, and she was supervised by Prof. Rod Connolly and Dr Chris Brown.\nHer key thesis papers include:\n\nIncorporating cyclone risk in the design of marine protected and conserved areas as an ecosystem-based adaptation approach.\nMarine and coastal ecosystem-based adaptation in Asia and Oceania: review of approaches and integration with marine spatial planning"
  },
  {
    "objectID": "people/alumni/Jackson-Stockbridge.html",
    "href": "people/alumni/Jackson-Stockbridge.html",
    "title": "Jackson Stockbridge",
    "section": "",
    "text": "Jackson completed his PhD at The University of Adelaide in December 2022 and joined the team in early 2023. His PhD research evaluated the methods used to estimate the effect of multiple stressors on seagrass ecosystems.\nHe worked with Dr Caitie Kuempel’s Conservation Action Team and Dr Chris Brown’s Seascapemodels Lab. Jackson uses spatial data to map the cumulative effects of multiple stressors on industries and the environment to support marine spatial planning.\nHe has shown for which countries we can significantly reduce the footprint of offshore industries if we can colocate wind energy and aquaculture infrastructure."
  },
  {
    "objectID": "people/alumni/Olivia-King.html",
    "href": "people/alumni/Olivia-King.html",
    "title": "Olivia King",
    "section": "",
    "text": "Olivia was awarded her PhD in 2023. She has a background in ecotoxicology, water quality, the derivation of Water Quality Guidelines for pesticides in aquatic ecosystems and environmental pollution incidents. Olivia has completed both, her Bachelor of Science (Marine and Wildlife Biology) and Honours (Ecotoxicology) degrees at Griffith University on the Gold Coast.\nOlivia’s PhD research focused on the combined and interactive effects of three water quality stressors common to the Great Barrier Reef. Specifically, Olivia aimed to understand if ecotoxicological models can predict the effects of photosystem II-inhibiting herbicide exposure, dissolved inorganic nitrogen (DIN) enrichment and reduced light (due to excess suspended sediment) on marine microalgae. Olivia’s PhD was supported by an Australian Government Research Training Program Stipend Scholarship and an ARC Discovery Project grant.\nOlivia was supervised by Dr Christopher Brown and Dr Jason van de Merwe."
  },
  {
    "objectID": "people/alumni/Dale-Bryan-Brown.html",
    "href": "people/alumni/Dale-Bryan-Brown.html",
    "title": "Dale Bryan-Brown",
    "section": "",
    "text": "Dale graduated his PhD in 2020. He studied the role of connectivity in the conservation of marine organisms, particularly mangroves. During his PhD he led work on global trends in the fragmentation of mangrove forests.\nAfter his PhD Dale retrained as a teacher and he is now working in a teaching career.\nRead Dale’s review of research on marine species connectivity: Patterns and trends in marine population connectivity research"
  },
  {
    "objectID": "people/alumni/Andrew-Broadley.html",
    "href": "people/alumni/Andrew-Broadley.html",
    "title": "Andrew Broadley",
    "section": "",
    "text": "Andrew graduated from his PhD in 2021. He worked on the linkages between marine fisheries production and river flow in Northern Australia. He was part funded by the National Environmental Science Program for Northern Australia.\nHis key thesis papers include:\n\nImpact of water development on river flows and the catch of a commercial marine fishery.\nA global review of the critical link between river flows and productivity in marine fisheries"
  },
  {
    "objectID": "people/alumni/Kristin-Jinks.html",
    "href": "people/alumni/Kristin-Jinks.html",
    "title": "Kristin Jinks",
    "section": "",
    "text": "A PhD student working primarily with the Connolly Lab. She is working on promoting ecosystem resilience through connectivity, valuing deep-water seagrass ecosystems - analysis of their role in food webs, Carbon connectivity."
  },
  {
    "objectID": "people/alumni/Tessie-Liddell.html",
    "href": "people/alumni/Tessie-Liddell.html",
    "title": "Tessie Liddell",
    "section": "",
    "text": "A PhD candidate at Griffith University’s College of Art, Tessie is an animation artist. She is studying how animation can be used to communicate the impacts of land-use change on riverine and marine ecosystems. She is particularly interested in turning science into a narrative that will resonate with people outside of a scientific audience. You can view some of her works here."
  },
  {
    "objectID": "people/alumni/Christina-Buelow.html",
    "href": "people/alumni/Christina-Buelow.html",
    "title": "Dr Christina Buelow",
    "section": "",
    "text": "Christina Buelow\nChristina joined the team in 2019 as a postdoctoral fellow on the Global Wetlands Project.\nShe worked on informing conservation actions for coastal wetlands at the global scale. In particular, she is working with the Global Mangrove Alliance to inform on how conservation actions can be scaled up from local to global scales.\nShe graduated with a PhD in Environmental Science from James Cook University in 2018. She is currently based at the University of Sydney.\nKey research includes:\n\nAmbitious global targets for mangrove and seagrass recovery\nEnabling conservation theories of change\nProjecting Uncertainty in Ecosystem Persistence Under Climate Change"
  },
  {
    "objectID": "people/alumni/Maree-Hume.html",
    "href": "people/alumni/Maree-Hume.html",
    "title": "Maree Hume",
    "section": "",
    "text": "Maree was a research assistant working part-time during 2019 to help on a range of projects. She has a background in environmental law and is currently studying for her honours degree at Griffith Uni."
  },
  {
    "objectID": "people/alumni/Jodie-Haig.html",
    "href": "people/alumni/Jodie-Haig.html",
    "title": "Dr Jodie Haig",
    "section": "",
    "text": "Jodie joined the Australian Rivers Institute and the Global Wetlands Project (GLOW) as a Research Fellow in 2018. She applied her ecological and environmental management expertise to test the application of environmental economic accounting principles to wetlands and river systems at regional, national and international scales.\nJodie left the team in 2019 and was last seen working for State Government on threatened species conservation. She is an adjunct to the Australian Rivers Institute.\nFind out more about Jodie’s research via her:\n\nORCID ID\nScopus Author ID: 25825134100\nResearch Gate page"
  },
  {
    "objectID": "people/alumni/Laura-Griffiths.html",
    "href": "people/alumni/Laura-Griffiths.html",
    "title": "Laura Griffiths",
    "section": "",
    "text": "Laura graduated her PhD in 2022. Laura was studying the effects of multiple stressors on coastal marine ecosystems.\nA key paper from her PhD found critical gaps in seagrass protection across 20 case-studies globally. In particular she showed a need to address multiple pressures and cumulative impacts."
  },
  {
    "objectID": "people/alumni/Will-Taylor.html",
    "href": "people/alumni/Will-Taylor.html",
    "title": "Will Taylor",
    "section": "",
    "text": "Will was working as a research assistant reviewing associations between fished species and their habitats as part of Chris’ ARC project. He has been successful in furthering his career in the application of environmental science to management and was last seen working as an environmental health officer."
  },
  {
    "objectID": "people/students/Jitesh-Ramesh.html",
    "href": "people/students/Jitesh-Ramesh.html",
    "title": "Jitesh Ramesh",
    "section": "",
    "text": "Jitesh started his Honours with UTas mid-2025, supervised by Dr Brown and Dr Scott Bennett. He is studying productivity shifts in Australia’s south-east marine ecosystems.\nAustralia’s south-east marine ecosystems have undergone dramatic changes over the past three decades. This project will integrate multiple lines of evidence to test for a system-wide productivity change that spans shelf and coastal reef ecosystems. The project’s findings have implications for the management of fisheries and conservation of south-east marine ecosystems. Several studies independently point to productivity declines in different south-east marine ecosystems. This project will integrate data from trawl fisheries, reef surveys and macroalgal productivity estimates to ask if these disparate trends share a common driver. The project will use ecosystem modelling tools to ask if fishing and climate are causing system wide change in productivity."
  },
  {
    "objectID": "people/students/Jordan-Holdorf.html",
    "href": "people/students/Jordan-Holdorf.html",
    "title": "Jordan Holdorf",
    "section": "",
    "text": "Jordan is completing her PhD after completing both her Bachelor of Marine Science and Honors at Griffith University and has a strong background in mathematics and modelling. She is cosupervised by Dr Chris Brown, Dr Melanie Roberts and Dr Ivan Diaz-Rainey.\nJordan’s PhD research focuses on investment modelling for the natural capital and carbon markets using the common language of money to better communicate between investors and conservation. She is investigating the optimisation of finances and how different types of investments can be utilised and optimised to fulfil a specific target."
  },
  {
    "objectID": "people/students/Alexandre-Tisseaux.html",
    "href": "people/students/Alexandre-Tisseaux.html",
    "title": "Alexandre Tisseaux",
    "section": "",
    "text": "Alexandre started his PhD with UTas mid 2025. He is studying “Integrated assessment of marine systems” supervised by Dr Brown with cosupervision from Dr Beth Fulton and Dr Katie Cresswell.\nIntegrated Ecosystem Assessments (IEAs) are used to evaluate the state of ecosystems to inform management, policy and monitoring. In contrast to existing frameworks for marine ecosystem management, they synthesize and analyse natural, social, and economic factors. Their primary aim is to inform ecosystem management by integrating scientific data to support decision-making processes. With increasing activities in Australian marine territory, there is a growing concern about potential threats to marine ecosystems. A multi-sectoral approach is essential for identifying and prioritizing risks to the environmental and social outcomes associated with emerging ocean industries, such as aquaculture and renewable energy. This integrated approach can help assess how human activities impact ecosystems and guide measures to achieve triple-bottom-line outcomes."
  },
  {
    "objectID": "people/collaborators/Global-Wetlands-Project.html",
    "href": "people/collaborators/Global-Wetlands-Project.html",
    "title": "Global Wetlands Project",
    "section": "",
    "text": "This international collaboration lead by Dr Chris Brown, Professor Rod Connolly and Dr Fernanda Adame. It includes a team of more than 15 researchers and studesnts\nThe Global Wetlands Project (GLOW) elevates scientific understanding of critical issues facing coastal wetlands and builds online tools to help address these issues.\nThe world needs coastal wetlands. Seagrass meadows, mangrove forests and saltmarshes are high-value habitats providing many benefits to humanity.\nUnfortunately, we’re losing these important places. There’s an urgent need to limit and reverse the loss of coastal wetlands in order to arrest biodiversity decline, protect communities and tackle climate change."
  },
  {
    "objectID": "prospective-students.html",
    "href": "prospective-students.html",
    "title": "Prospective students",
    "section": "",
    "text": "We regularly welcome new PhD students to our lab. While Chris has no projects currently advertised you are welcome to contact him if you would like to discuss opportunities\nProspective students with a background in quantitative skills are encouraged, but not essential. Training quantitative skills is something we specialise in.\nRead more about Chris’ lab, research and his team on this site. Learn more about UTas research degrees and search for active projects here.\nRead below for eligibility requirements. I receive many emails about PhD positions, in general I won’t respond if you don’t meet these minimum requirements.\n\n\nIn Australia PhD applicants are awarded Australian or University funded PhD scholarships on a competitive basis. There may also be opportunities for PhD studies on scholarships that are funded directly by a grant. This system means PhD application process in Australia is different to other countries.\nThe main round of PhD applications that are ranked for University and government scholarships are due for submission late September each year. Please contact Chris at least 4 weeks before hand to discuss opportunities.\nThere are also other application opportunities during there year, but these primarily apply for domestic students or students applying to projects funded by grants.\nIn general, PhD applications go into a competitive process that is ranked across the entire university. The exception is for grant funded projects, where students are ranked only against applicants to that specific project.\nThe University and government funded positions are highly competitive, and the level of competition depends on whether you are an Australian citizen or resident or international.\nAustralian applicants should have at a minimum first class Honours from their degree, or equivalent research experience (e.g. 1 year or more in a research job).\nInternational applicants should have equivalent of first class honours (e.g. research focused masters with GPA 6+) and either of: substantial (1+ years) research relevant work experience and/or a first authored publication in an internationally recognised journal.\nThere are fewer international scholarship positions at than domestic scholarships, hence the scholarships are more competitive than for domestic applicants.\nScholarships funded by grants may be less competitive. See my blog where I will advertise opportunities."
  },
  {
    "objectID": "prospective-students.html#phd-research",
    "href": "prospective-students.html#phd-research",
    "title": "Prospective students",
    "section": "",
    "text": "We regularly welcome new PhD students to our lab. While Chris has no projects currently advertised you are welcome to contact him if you would like to discuss opportunities\nProspective students with a background in quantitative skills are encouraged, but not essential. Training quantitative skills is something we specialise in.\nRead more about Chris’ lab, research and his team on this site. Learn more about UTas research degrees and search for active projects here.\nRead below for eligibility requirements. I receive many emails about PhD positions, in general I won’t respond if you don’t meet these minimum requirements.\n\n\nIn Australia PhD applicants are awarded Australian or University funded PhD scholarships on a competitive basis. There may also be opportunities for PhD studies on scholarships that are funded directly by a grant. This system means PhD application process in Australia is different to other countries.\nThe main round of PhD applications that are ranked for University and government scholarships are due for submission late September each year. Please contact Chris at least 4 weeks before hand to discuss opportunities.\nThere are also other application opportunities during there year, but these primarily apply for domestic students or students applying to projects funded by grants.\nIn general, PhD applications go into a competitive process that is ranked across the entire university. The exception is for grant funded projects, where students are ranked only against applicants to that specific project.\nThe University and government funded positions are highly competitive, and the level of competition depends on whether you are an Australian citizen or resident or international.\nAustralian applicants should have at a minimum first class Honours from their degree, or equivalent research experience (e.g. 1 year or more in a research job).\nInternational applicants should have equivalent of first class honours (e.g. research focused masters with GPA 6+) and either of: substantial (1+ years) research relevant work experience and/or a first authored publication in an internationally recognised journal.\nThere are fewer international scholarship positions at than domestic scholarships, hence the scholarships are more competitive than for domestic applicants.\nScholarships funded by grants may be less competitive. See my blog where I will advertise opportunities."
  },
  {
    "objectID": "prospective-students.html#honours-and-masters-research-theses",
    "href": "prospective-students.html#honours-and-masters-research-theses",
    "title": "Prospective students",
    "section": "Honours and Masters research theses",
    "text": "Honours and Masters research theses\nWe regularly welcome students to do their honours or masters research thesis with us (coursework masters). Both are about 9 months duration and produce the equivalent of a research paper.\nEligibility requirements are broadly an undergraduate degree in a relevant field.\nLearn more about Honours degrees and our Master of Marine and Antarctic Science.\nAn up-to-date listing of projects is available here\nThe guidelines for Masters by Research are similar to the PhD guidelines above."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seascape Models Research Group",
    "section": "",
    "text": "Welcome to the lab webpage of Associate Professor Chris Brown from the Institute of Marine and Antarctic Studies, Tasmania.\n\nThe future of ocean and coastal biodiversity is uncertain. This uncertainty threatens humanity’s place on the blue planet. We urgently need to adapt human uses of the ocean to conserve its ecosystems and ensure it continues to provide us with food, energy, recreation, clean water and clean air.\nWe are marine scientists who use quantitative tools to inform environmental decisions in the oceans. We do this by bringing ecological complexity to the planning tools used to inform decision making. We work with decision makers to provide quantitative advice on ocean and coastal conservation and fishery management. Our tools of choice are computer programming, mathematical models and statistical analysis. These tools allow us to work with complex datasets to find solutions for ocean management.\nWe specialize in the R programming language and teach R to others.\nChris’ teaching resources are on this site and are open-access\nRead more about our research team or our research on our blog.\nCheck out opportunities for prospective research students."
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Seascape Models Research Group",
    "section": "Current projects",
    "text": "Current projects\nBelow are a few key funded current projects. We work on a diverse range of ocean science topics and the list here is not comprehensive.\nRead more about our research projects in the blog and our students’ research projects.\n\n\n\n\n\n\n\n\n\n\nFellowship on predicting coastal ecological futures in an era of unprecedented change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Wetlands Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSea turtle conservation in the Pacific\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html",
    "href": "posts/2021-10-01-lapply-karate/index.html",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "",
    "text": "Sometimes we want to run the same bit of code many times for different parameters. Then if we have multiple parameters, we may want to run the code for all combinations of those parameters. I use what I call ‘lapply karate’ to perform this task.\nApplications include in simulating data, simulating data and fitting a model to test model fit, conducting sensitivity analyses for models or even making lots of plots.\nWe’d also like our method to generalize easily to other parameters. So for instance, once we run the model for all combos of parameter A and B, we’d also like to run it for all combos of parameters A and C, without too much more coding."
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#running-r-code-for-all-combinations-of-some-parameters-with-lapply-karate",
    "href": "posts/2021-10-01-lapply-karate/index.html#running-r-code-for-all-combinations-of-some-parameters-with-lapply-karate",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "",
    "text": "Sometimes we want to run the same bit of code many times for different parameters. Then if we have multiple parameters, we may want to run the code for all combinations of those parameters. I use what I call ‘lapply karate’ to perform this task.\nApplications include in simulating data, simulating data and fitting a model to test model fit, conducting sensitivity analyses for models or even making lots of plots.\nWe’d also like our method to generalize easily to other parameters. So for instance, once we run the model for all combos of parameter A and B, we’d also like to run it for all combos of parameters A and C, without too much more coding."
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#options-for-looping-over-all-combinations-of-several-variables",
    "href": "posts/2021-10-01-lapply-karate/index.html#options-for-looping-over-all-combinations-of-several-variables",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "Options for looping over all combinations of several variables",
    "text": "Options for looping over all combinations of several variables\nThere are many options for doing this task. Here are some common ones:\nfor or while loops (but gets unweildy)\nVectorize function (base R) - maybe a bit slow?\nApply family (base R), e.g. lapply, tapply, apply, sapply etc…\npurrr package (map and pmap functions)\ndata.table Probably the fastest?\ndplyr Also very fast\nmclapply to do same as lapply on multiple threads!\nThere are different performance considerations with all of above. But for an introduction I find sapply and lapply are the most intuitive*.\n\nBut not that intuitive, so read one, but this code is definitely one to try yourself."
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#function-to-simulate-an-autoregressive-model.",
    "href": "posts/2021-10-01-lapply-karate/index.html#function-to-simulate-an-autoregressive-model.",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "Function to simulate an autoregressive model.",
    "text": "Function to simulate an autoregressive model.\nFirst, let’s setup a case-study simulation. Below code just simulates data from an autoregressive model. Try changing to ar parameter to see what happens:\nlibrary(ggplot2)\ntheme_set(theme_classic())\ntmax &lt;- 300\nset.seed(42)\nyt &lt;- arima.sim(list(order=c(1,0,0), ar=0.99), n=tmax)\nt &lt;- 1:tmax\ndat &lt;- data.frame(t = t, yt = yt)\nggplot(dat) +\n  aes(x = t, y = yt) +\n  geom_line()\n The higher the ar parameter the stronger the deviations in time."
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#fitting-a-model",
    "href": "posts/2021-10-01-lapply-karate/index.html#fitting-a-model",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "Fitting a model",
    "text": "Fitting a model\nWe’ll use a GAM to fit a spline to the data we just simulated. Part of the GAM output is an EDF (Effective Degrees of Freedom) statistic. Higher values = greater non-linearity. We should see higher values of the EDF when the AR is greater (because then we get more strong non-linear deviations from the mean):\nlibrary(mgcv)\n\n## Loading required package: nlme\n\n## This is mgcv 1.8-31. For overview type 'help(\"mgcv-package\")'.\n\nm1 &lt;- gam(yt ~ s(t, bs = \"ad\"),\n          data = dat)\ndat$pred &lt;- predict(m1)\nggplot(dat) +\n  aes(x = t, y = yt) +\n  geom_line() +\n  geom_line(aes(y = pred), col = \"red\")\n Black line shows the data, red line shows the GAM fit. You can see the edf number if you do summary(m1), or we can access the EDF programmatically like this:\nsum(m1$edf)-1\n\n## [1] 36.32412"
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#lapply-karate",
    "href": "posts/2021-10-01-lapply-karate/index.html#lapply-karate",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "lapply karate",
    "text": "lapply karate\nNote Pablo García wrote into me after this blog was published. He pointed out that the base R functions mapply and Map are setup to be multivariate equivalents of sapply and lapply. So you may want to look at those as a slightly more intuitive way of performing the below computations.\n\nStep 1: Setup a function\nWe need to turn code above into a function, so we can run it many times for whatever paramter inputs. Here I will vary the random seed, correlation coef (rho) and timeseries length (tmax).\narima_vary &lt;- function(rho, tmax, iseed){\n  set.seed(iseed)\n  yt &lt;- arima.sim(list(order=c(1,0,0), ar=rho), n=tmax)\n  t &lt;- 1:tmax\n  dat &lt;- data.frame(t = t, yt = yt)\n  m1 &lt;- gam(yt ~ s(t, bs = \"ad\"),\n            data = dat)\n  return(sum(m1$edf)-1)\n}\n\narima_vary(0.8, 100, 1)\n\n## [1] 30.21985\nThe function returns the EDF from the fitted GAM\n\n\nStep 2: Loop over different values of rho\nWith sapply we can run our model for series of rho values:\nrhos &lt;- seq(0.01, 0.99, by = 0.2)\nedfvals &lt;- sapply(rhos, arima_vary, tmax = 100, iseed = 1)\nplot(rhos, edfvals)\n\n\n\nStep 3: Loop over all combinations of rho and iseed and tmax\nWhat about getting the edf for different combinations of the three parameters?\nFirst set-up a dataframe of all combos:\nrhos &lt;- seq(0.01, 0.99, by = 0.2)\nseeds &lt;- 1:5\ntmax_vals &lt;- c(100, 200, 400)\ndatin &lt;- expand.grid(rho = rhos, iseed = seeds,\n                     tmax = tmax_vals)\nnrow(datin)\n\n## [1] 75\nSo we are going to run our simulations 75 times (this can easily get very large, the larger the df, the longer our simulations will take).\ni &lt;- 1:nrow(datin)\n\n#one way using sapply\nsystem.time(\ndatin$edfvals &lt;- with(datin,\n                sapply(i, function(j){arima_vary(rho[j], tmax[j], iseed[j])})))\n\n##    user  system elapsed\n##   6.929   0.163   7.460\nIf it works, don’t forget the Kiai\nHYAH!\nIts a good idea to run it for a small number of combos first and check how long it will take.\nAnother way of doing the same thing using apply:\ndatin$edfvals &lt;-\n                apply(datin,1, function(j){\n                  arima_vary(j[\"rho\"], j[\"tmax\"], j[\"iseed\"])\n                  })\nThe above is the same as doing:\n#Manual way\narima_vary(datin$rho[1], datin$tmax[1], datin$iseed[1])\narima_vary(datin$rho[2], datin$tmax[2], datin$iseed[2])\narima_vary(datin$rho[3], datin$tmax[3], datin$iseed[3])\n#....\narima_vary(datin$rho[nrow(datin)], datin$tmax[nrow(datin)], datin$iseed[nrow(datin)])\nNow we’ve done all that, and saved it as a dataframe, the easiest way to plot the combinations of parameters is with ggplot:\nggplot(datin) +\n  aes(x = rho, y = edfvals, color = iseed) +\n  facet_wrap(~tmax) +\n  geom_point() +\n  stat_smooth()\n\n## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n We see greater EDF for greater auto-correlation, as expected, also longer timeseries length also increases the EDF."
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#lapply-karate-1",
    "href": "posts/2021-10-01-lapply-karate/index.html#lapply-karate-1",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "lapply karate",
    "text": "lapply karate\nNow get to lapply. lapply (for list apply) let’s us have more complex output. Here I output a dataframe of predictions, rather than just a single value.\nI call it lapply karate, because the trick is how you store the inputs and outputs to the function. If you’re smart, then you can get everything to output as dataframes and then its easy to plot all those simulations with ggplot.\nI’ll make a new function first, that outputs the dataframe (and input parameters, which enables me to save them for use later in ggplot)\nlarima_vary &lt;- function(rho, tmax, iseed){\n  set.seed(iseed)\n  yt &lt;- arima.sim(list(order=c(1,0,0), ar=rho), n=tmax)\n  t &lt;- 1:tmax\n  dat &lt;- data.frame(t = t, yt = yt)\n  m1 &lt;- gam(yt ~ s(t, bs = \"ad\"),\n            data = dat)\n  dat$pred &lt;- predict(m1)\n  dat$tmax &lt;- tmax\n  dat$rho &lt;- rho\n  dat$iseed &lt;- iseed\n  return(dat)\n}\n\n\ndatout &lt;- with(datin,\n                lapply(i, function(j){larima_vary(rho[j], tmax[j], iseed[j])}))\nNow bind all those dataframes (nested in a list), so we can plot them with ggplot. Below plot shows every GAM fit for each timeseries length (columns) and each correlation coef (rows).\nlibrary(dplyr)\n\n##\n## Attaching package: 'dplyr'\n\n## The following object is masked from 'package:nlme':\n##\n##     collapse\n\n## The following objects are masked from 'package:stats':\n##\n##     filter, lag\n\n## The following objects are masked from 'package:base':\n##\n##     intersect, setdiff, setequal, union\n\ndatplot &lt;- bind_rows(datout)\n\ndim(datplot)\n\n## [1] 17500     6\n\nnames(datplot)\n\n## [1] \"t\"     \"yt\"    \"pred\"  \"tmax\"  \"rho\"   \"iseed\"\n\nggplot(datplot) +\n  aes(x = t, y = pred, group = iseed) +\n  geom_line(alpha = 0.8) +\n  facet_grid(rho~tmax, scales = \"free\")\n HYAH!"
  },
  {
    "objectID": "posts/2021-10-01-lapply-karate/index.html#other-advanced-uses",
    "href": "posts/2021-10-01-lapply-karate/index.html#other-advanced-uses",
    "title": "Running R code for all combinations of some parameters with lapply karate",
    "section": "Other advanced uses",
    "text": "Other advanced uses\nYou can have your function output a list, so with lapply you end up with a list of lists. Then use lapply to extract or plot specific parts of the list for each object. purrr also has some handy functions for wrangling lists.\nYou can have more complex inputs to you function, like model formulas, then the function will run models with different formulas for all combinations of other inputs.\nCheck out various packages for parralel computing, to speed things up."
  },
  {
    "objectID": "posts/2017-03-23-summarize-by-genera/index.html",
    "href": "posts/2017-03-23-summarize-by-genera/index.html",
    "title": "Mean trophic levels of a genera from FishBase",
    "section": "",
    "text": "How would you selectively aggregate observations using R? For instance, say you have a table of trophic level estimates by fish species, but many species are missing values. For those species missing a value, you want to assign them the mean for their genus. I recently saw a post from Trevor Branch saying he had figured out exactly how to do this.\n\n\n\nThe well camoflaged Estuary Cod (Epinephelus malabaricus) doesn’t have a diet based trophic level estimate on fishbase. One way to estimate it could be to assign it the mean for all Epinephelus\n\n\nIt got me thinking, what is the shortest way I could think of making this selective summary.\nHere is my solution. I think it makes a nice lesson in using the dplyr package.\nFirst up we should load in the rfishbase package, which gives us access to the FishBase API (“Application Programming Interface”).\nlibrary(rfishbase)\nlibrary(dplyr)\nTo make this fast, we won’t do all fish species on Fish Base, but just the groupers (family Serranidae). Let’s find out their species names and make a new variable gensp that is the latin binomial (we will need this later):\ngroupers &lt;- fishbase %&gt;% filter(Family == \"Serranidae\") %&gt;%\n  mutate(gensp = paste(Genus, Species))\nnrow(groupers)\n\n## [1] 537\nIf you haven’t seen the %&gt;% ‘pipe’ symbol before, you had better look up the dplyr vignettes. Its a convenient way to string multiple commands together. So we now have a groupers data frame with a gensp variable. We can access the trophic information from fishbase using the ecology command:\ngrptroph &lt;- ecology(groupers$gensp, fields = c(\"DietTroph\"))\nnrow(grptroph)\n\n## [1] 233\n\nhead(grptroph)\n\n##                    sciname StockCode DietTroph SpecCode\n## 1 Acanthistius brasilianus        NA        NA      351\n## 2      Acanthistius fuscus        NA        NA    59850\n## 3      Acanthistius pictus        NA      4.23    57960\n## 4       Aethaloperca rogaa        NA        NA     6441\n## 5           Alphestes afer        NA      3.58     8726\n## 6    Alphestes immaculatus        NA        NA     8727\necology produces another dataframe (actually a tibble which is a similar but not the same to a data.frame but it works well with dplyr). Note that species with missing info are excluded from the result, so we only have 233 grouper species now.\nNote there is also a FoodTroph field, which is calculated slightly differently. Check out the fishbase manual for more info. Now just join our grptroph back go groupers so we get empty rows for species with missing diet info:\nd2 &lt;- left_join(groupers, grptroph)\nnrow(d2)\n\n## [1] 537\nGreat, back to all 537 species.\nNow the heart of my little program, produce a new dataframe d3 with a new variable trophall. trophall will contain the species trophic level if it has its own value and its genus mean trophic level if it doesn’t have its own value (for some genera don’t have any measurements so get NaN):\nd3 &lt;- d2 %&gt;% group_by(Genus) %&gt;%\n  mutate(mntroph = mean(DietTroph, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  mutate(trophall = ifelse(is.na(DietTroph), mntroph, DietTroph))\nTo step through this we:\n\nTake d2 and group it by the variable Genus\nCalculate the mean of DietTroph, removing missing values. The prior group_by command means we get the mean across species within each Genus.\nUngroup, so further calculations (using mutate) are not grouped by genus\nCalculate trophall by assigning the genera mean if a species had a missing value in DietTroph and keeping the value DietTroph if the species value wasn’t missing.\n\nLet’s check the result:\nd3 %&gt;% select(Genus, Species, DietTroph, trophall) %&gt;%\n    data.frame() %&gt;% head(20)\n\n##            Genus       Species DietTroph trophall\n## 1   Acanthistius   brasilianus        NA     4.23\n## 2   Acanthistius       cinctus        NA     4.23\n## 3   Acanthistius        fuscus        NA     4.23\n## 4   Acanthistius        joanae        NA     4.23\n## 5   Acanthistius     ocellatus        NA     4.23\n## 6   Acanthistius    pardalotus        NA     4.23\n## 7   Acanthistius  patachonicus        NA     4.23\n## 8   Acanthistius       paxtoni        NA     4.23\n## 9   Acanthistius        pictus      4.23     4.23\n## 10  Acanthistius   sebastoides        NA     4.23\n## 11  Acanthistius      serratus        NA     4.23\n## 12  Aethaloperca         rogaa        NA      NaN\n## 13     Alphestes          afer      3.58     3.58\n## 14     Alphestes   immaculatus        NA     3.58\n## 15     Alphestes multiguttatus        NA     3.58\n## 16 Anatolanthias    apiomycter        NA      NaN\n## 17       Anthias       anthias        NA      NaN\n## 18       Anthias asperilinguis        NA      NaN\n## 19       Anthias   cyprinoides        NA      NaN\n## 20       Anthias    helenensis        NA      NaN\nFinally, let’s find out what our Estuary cod gets:\nd3 %&gt;% filter(gensp == \"Epinephelus malabaricus\") %&gt;% select(trophall)\n\n## # A tibble: 1 × 1\n##   trophall\n##      &lt;dbl&gt;\n## 1     3.89\nThat’s how I would solve Trevor’s problem. Let me know if you have a more elegant way."
  },
  {
    "objectID": "posts/2017-03-23-summarize-by-genera/index.html#mean-trophic-levels-of-a-genera-from-fishbase",
    "href": "posts/2017-03-23-summarize-by-genera/index.html#mean-trophic-levels-of-a-genera-from-fishbase",
    "title": "Mean trophic levels of a genera from FishBase",
    "section": "",
    "text": "How would you selectively aggregate observations using R? For instance, say you have a table of trophic level estimates by fish species, but many species are missing values. For those species missing a value, you want to assign them the mean for their genus. I recently saw a post from Trevor Branch saying he had figured out exactly how to do this.\n\n\n\nThe well camoflaged Estuary Cod (Epinephelus malabaricus) doesn’t have a diet based trophic level estimate on fishbase. One way to estimate it could be to assign it the mean for all Epinephelus\n\n\nIt got me thinking, what is the shortest way I could think of making this selective summary.\nHere is my solution. I think it makes a nice lesson in using the dplyr package.\nFirst up we should load in the rfishbase package, which gives us access to the FishBase API (“Application Programming Interface”).\nlibrary(rfishbase)\nlibrary(dplyr)\nTo make this fast, we won’t do all fish species on Fish Base, but just the groupers (family Serranidae). Let’s find out their species names and make a new variable gensp that is the latin binomial (we will need this later):\ngroupers &lt;- fishbase %&gt;% filter(Family == \"Serranidae\") %&gt;%\n  mutate(gensp = paste(Genus, Species))\nnrow(groupers)\n\n## [1] 537\nIf you haven’t seen the %&gt;% ‘pipe’ symbol before, you had better look up the dplyr vignettes. Its a convenient way to string multiple commands together. So we now have a groupers data frame with a gensp variable. We can access the trophic information from fishbase using the ecology command:\ngrptroph &lt;- ecology(groupers$gensp, fields = c(\"DietTroph\"))\nnrow(grptroph)\n\n## [1] 233\n\nhead(grptroph)\n\n##                    sciname StockCode DietTroph SpecCode\n## 1 Acanthistius brasilianus        NA        NA      351\n## 2      Acanthistius fuscus        NA        NA    59850\n## 3      Acanthistius pictus        NA      4.23    57960\n## 4       Aethaloperca rogaa        NA        NA     6441\n## 5           Alphestes afer        NA      3.58     8726\n## 6    Alphestes immaculatus        NA        NA     8727\necology produces another dataframe (actually a tibble which is a similar but not the same to a data.frame but it works well with dplyr). Note that species with missing info are excluded from the result, so we only have 233 grouper species now.\nNote there is also a FoodTroph field, which is calculated slightly differently. Check out the fishbase manual for more info. Now just join our grptroph back go groupers so we get empty rows for species with missing diet info:\nd2 &lt;- left_join(groupers, grptroph)\nnrow(d2)\n\n## [1] 537\nGreat, back to all 537 species.\nNow the heart of my little program, produce a new dataframe d3 with a new variable trophall. trophall will contain the species trophic level if it has its own value and its genus mean trophic level if it doesn’t have its own value (for some genera don’t have any measurements so get NaN):\nd3 &lt;- d2 %&gt;% group_by(Genus) %&gt;%\n  mutate(mntroph = mean(DietTroph, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  mutate(trophall = ifelse(is.na(DietTroph), mntroph, DietTroph))\nTo step through this we:\n\nTake d2 and group it by the variable Genus\nCalculate the mean of DietTroph, removing missing values. The prior group_by command means we get the mean across species within each Genus.\nUngroup, so further calculations (using mutate) are not grouped by genus\nCalculate trophall by assigning the genera mean if a species had a missing value in DietTroph and keeping the value DietTroph if the species value wasn’t missing.\n\nLet’s check the result:\nd3 %&gt;% select(Genus, Species, DietTroph, trophall) %&gt;%\n    data.frame() %&gt;% head(20)\n\n##            Genus       Species DietTroph trophall\n## 1   Acanthistius   brasilianus        NA     4.23\n## 2   Acanthistius       cinctus        NA     4.23\n## 3   Acanthistius        fuscus        NA     4.23\n## 4   Acanthistius        joanae        NA     4.23\n## 5   Acanthistius     ocellatus        NA     4.23\n## 6   Acanthistius    pardalotus        NA     4.23\n## 7   Acanthistius  patachonicus        NA     4.23\n## 8   Acanthistius       paxtoni        NA     4.23\n## 9   Acanthistius        pictus      4.23     4.23\n## 10  Acanthistius   sebastoides        NA     4.23\n## 11  Acanthistius      serratus        NA     4.23\n## 12  Aethaloperca         rogaa        NA      NaN\n## 13     Alphestes          afer      3.58     3.58\n## 14     Alphestes   immaculatus        NA     3.58\n## 15     Alphestes multiguttatus        NA     3.58\n## 16 Anatolanthias    apiomycter        NA      NaN\n## 17       Anthias       anthias        NA      NaN\n## 18       Anthias asperilinguis        NA      NaN\n## 19       Anthias   cyprinoides        NA      NaN\n## 20       Anthias    helenensis        NA      NaN\nFinally, let’s find out what our Estuary cod gets:\nd3 %&gt;% filter(gensp == \"Epinephelus malabaricus\") %&gt;% select(trophall)\n\n## # A tibble: 1 × 1\n##   trophall\n##      &lt;dbl&gt;\n## 1     3.89\nThat’s how I would solve Trevor’s problem. Let me know if you have a more elegant way."
  },
  {
    "objectID": "posts/2015-09-23-correlation-heuristics/index.html",
    "href": "posts/2015-09-23-correlation-heuristics/index.html",
    "title": "How statistics help us get causation right",
    "section": "",
    "text": "How statistics help us get causation right\nStatistics are often maligned. People can play with numbers to bias the way we interpret reality. Or we can find reasons why two events happened at the same time without any real evidence. As the old adage goes, ‘correlation isn’t causation’. But proper use of statistics plays a crucial role in the advancement of knowledge.\n\n\n\nOur brains are prone to making up stories to explain chance events. I was lucky to spot this woodpecker on an exceptionally warm day. Next time it is warm, I will be prone to expecting more woodpeckers.\n\n\nOur human brains have an impressive but limited capacity to detect patterns in nature. We can be very good at finding patterns in nature, for instance, an experienced birder can easily look at a woodland and tell you what types of birds might be found there. But our ability to detect patterns is also very biased. We are very good at detecting some patterns, too good in fact, whereas other patterns we find very difficult to learn.\nI was recently reading Daniel Kahneman’s book ’Thinking, Fast and Slow. Kahneman was awarded a Nobel prize for his work on behavioural economics.\nKahneman describes how our brain uses ‘heuristics’ to detect patterns in nature, and make predictions. Sometimes these heuristics can rapidly generate good predictions, but they can also be strongly biased.\nOne skill we are particularly bad at is regression. Consider the figure. Maybe you have been going out and counting birds across different times. You think this bird likes warm days, so you will see more of them when it is hotter. In the first figure, there is a correlation between bird abundance and temperature (and by implication, warm days cause more birds to be out and about). However, without plotting this data and perhaps fitting a regression, we would have a hard time detecting this pattern.\n\n\n\nOur brains are prone to missing real correlations but then interpreting extreme chance events, like the red point, as causation\n\n\nIn the second figure there is no correlation, but one point stands out as being exceptionally warm and also having a great abundance of birds. We are likely to recall this exceptional event and make a up story about why that happened. Effectively, we would attribute causation without even correlation.\nKahneman goes so far as to suggest a method for ‘mental regression’ if we are trying to predict the outcome of one event, based on another.\nSay tomorrow is predicted to be very hot, and you want to estimate the number of birds you might see. We are likely to overestimate the number, because our normal mental heuristic will think ‘hot = more birds’, so ‘very hot = more birds than ever’. But, this heuristic doesn’t account for the large amount of variation in bird numbers not related to temperature (see the figure). For instance, some days a flock of birds might by chance be hanging around, inflating our counts.\nThe mental regression technique works like this:\nFirst estimate the average number of birds you might count on any day, say 20. Then determine your best guess based on intuition, the number you would expect based on it being very hot. Now guess the correlation between temperature and birds, say 40%. Finally, move 40% of the way from the average abundance toward your first best guess.\nWe can avoid the pitfalls of our mental heuristics if we use techniques like mental regression. When we can collect data, statistics provides us with the tools to find hidden patterns and avoid the pitfalls of intuitive thinking. Science, and the advancement of knowledge depends on robust statistics. To paraphrase the old adage: Correlation isn’t always causation, but chance events aren’t even correlation."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html",
    "title": "What type of leaders should a PhD program produce",
    "section": "",
    "text": "I read a curious quote today. It comes from a Careers article in Nature Magazine (22 Nov 2018), where they talked to several PhD students:\nI find this quote notable because it is well recognized that a PhD is about many things: learning science skills, building your network, and becoming a leader in a specialised discipline.\nBut the idea that a PhD is about developing as a leader more generally is a recent development.\nBut if ‘leadership’ is what people want, it begs the questions, why is leadership important for your career? and what does ‘leadership’ mean anyway?\nI’ll explore those questions in this post, in the next post I will look at how PhD students can seek out leadership experience."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html#modern-students-expert-a-greater-diversity-of-training-from-their-phds",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html#modern-students-expert-a-greater-diversity-of-training-from-their-phds",
    "title": "What type of leaders should a PhD program produce",
    "section": "Modern students expert a greater diversity of training from their PhDs",
    "text": "Modern students expert a greater diversity of training from their PhDs\nPhD Students are starting to ask for training in a greater diversity of skills from their PhD training. This is a good thing.\nUniversities are finally recognizing that most PhD graduates don’t go onto jobs in universities, and that a diverse skill-set is important for grads to succeed in the job market.\nLeadership skills are key, because they are in demand in a huge variety of different careers."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html#why-is-developing-as-a-leader-so-crucial-in-the-job-market",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html#why-is-developing-as-a-leader-so-crucial-in-the-job-market",
    "title": "What type of leaders should a PhD program produce",
    "section": "Why is developing as a leader so crucial in the job market?",
    "text": "Why is developing as a leader so crucial in the job market?\nPerhaps the most common question in job, scholarship and grant applications is some form of ‘demonstrate your experience in leadership’.\nThe first time I ran afoul of this tricky question was after undergrad, when I was interviewed for a Rhodes Scholarship.\nThe interview was in a huge dining room in Government House. The interviewers, who were six esteemed leaders in business, academia and sport, plus the Governor of Tasmania himself. I was very intimidated.\nIt was when the sports leader asked me what leadership I had shown in sport that my campaign for an Oxford scholarship came unstuck. I explained that I had been captain of the Tasmanian state junior underwater hockey team - second division. Yes a pretty niche position, having the honour of most experienced team-member in the lowest ranked state team for an obscure sport.\nIt didn’t help my case that one of the other applicants went on to win a gold medal in the Beijing Olympic games."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html#leaders-create-opportunities",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html#leaders-create-opportunities",
    "title": "What type of leaders should a PhD program produce",
    "section": "Leaders create opportunities",
    "text": "Leaders create opportunities\nSo what is ‘leadership’? I would say now:\nLeaders create opportunities that let them reach their goals.\nFor instance, some scientists can seemingly create funding out of thin air, through identifying the needs of research partners, like government, and then offering science solutions. They know how to create demand for the research they want to do.\nIn this way, the career of an experienced leader isn’t beholden to jobs that are advertised, they are creating the opportunities for employment to take their career where they want to go."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html#purpose-and-perspectives",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html#purpose-and-perspectives",
    "title": "What type of leaders should a PhD program produce",
    "section": "Purpose and perspectives",
    "text": "Purpose and perspectives\nKey traits that let leaders create opportunities are a sense of purpose and the ability to understand other people’s perspectives.\nLeaders will be motived by their own sense of purpose and work toward goals they have authored themselves. These goals might truly be leader’s own personal goals, or goals for their team or institution. The important part is that a leader has some ability to decide their own goals, they are not just taking the goals that someone else has given them.\nLeaders can also understand other people’s perspectives. This means they can see how their own goals fit within the bigger picture of what other people want."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html#how-having-purpose-and-perspective-helped-my-colleague-address-a-contentious-debate",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html#how-having-purpose-and-perspective-helped-my-colleague-address-a-contentious-debate",
    "title": "What type of leaders should a PhD program produce",
    "section": "How having ‘purpose and perspective’ helped my colleague address a contentious debate",
    "text": "How having ‘purpose and perspective’ helped my colleague address a contentious debate\nI want to give an example of these traits in action.\nI recently co-authored a paper about long-term declines in shark populations on the East Coast of Australia. The study’s lead, George Roff, anticipated a lot of media interest in the paper, because of concern in Australia about shark attacks.\nIn fact there was a shark attack the very week that our paper was published. Our concern was that we would be drawn into a contentious debate about whether shark culls were an effective means to prevent shark attacks on beaches. Many people believe that we should be trying to wipe out sharks.\nGeorge’s approach to interviews with the media demonstrated all the abilities in leadership I’ve described above.\nHe approached the media interviews with a strong sense of purpose - the media attention was the opportunity he had been seeking to raise awareness about how some of Australia’s iconic shark populations are threatened with extinction. So George directed the interviews towards discussing shark conservation in a reasoned and calm way.\nHe was also able to take the perspective of the general public and interviewers and understand that their would be considerable interest (and potentially anger) about shark attacks. George knew that many media outlets would be seeking a shark attack angle, so he had to work with this perspective in order to get his message out there.\nHowever, he nicely dealt with these contentious questions by sticking to his key message (the study was about shark conservation, not shark attacks). He was also ready for questions about specific shark attacks, and deflected them by talking generally about technologies for shark safety that don’t require killing sharks.\nYou can judge for yourself how he did. Needless to say, he was busy for 3 full days doing interviews everywhere from local radio to national TV."
  },
  {
    "objectID": "posts/2019-01-05-should-phd-teach-leadership/index.html#developing-as-a-leader-in-your-phd",
    "href": "posts/2019-01-05-should-phd-teach-leadership/index.html#developing-as-a-leader-in-your-phd",
    "title": "What type of leaders should a PhD program produce",
    "section": "Developing as a leader in your PhD",
    "text": "Developing as a leader in your PhD\nMuch of what I described above is a very summarized version of principles on ‘self-authorship’ from growth edge coaching.\nSo how can students, supervisors and universities support students to develop as leaders? That is the topic of my next post."
  },
  {
    "objectID": "posts/2020-12-29-pipe-functions/index.html",
    "href": "posts/2020-12-29-pipe-functions/index.html",
    "title": "Fast functions with pipes",
    "section": "",
    "text": "Pipes are becoming popular R syntax, because they help make code more readable and can speed up coding. You can even use pipes to create functions. This is handy if you want to repeat a piped process for several different datasets.\n\n\nLet’s start with a trivial example. You can access pipes through the tidyverse or the original magrittr package.\nWe’ll create a function that plots a data series in rank order:\nlibrary(magrittr)\nrank_plot &lt;- . %&gt;%\n  sort() %&gt;%\n  plot()\nSo to create a function out of a pipe we just start the pipe with the ..\nNow rank_plot has no data, but is rather what is called a ‘functional sequence’. Type rank_plot in your console to see this.\nLet’s use it to plot two randomly generated variables:\nset.seed(2020)\n\nx &lt;- rnorm(30)\ny &lt;- runif(30)\n\npar(mfrow = c(2,2))\nplot(x) #x data in original order\nplot(y) # y data in original order\nrank_plot(x) #rank ordered data\nrank_plot(y)\n\n\n\n\nLet’s true a slightly more sophisticated example, a map.\nWe’ll use tmap to make the maps.\nFirst access some data. Here I’ve picked a couple of random mushroom species that have data on the Queensland Government’s Wetland Info page. The data-tables record (rather out-of-date) observations of the species and the sites where they were recorded. This data is just for the area around Brisbane where I live:\namanita &lt;- read.csv(url(\"http://apps.des.qld.gov.au/species/?op=getsurveysbyspecies&taxonid=25531&f=csv\"))\npsilocybe &lt;- read.csv(url(\"http://apps.des.qld.gov.au/species/?op=getsurveysbyspecies&taxonid=28689&f=csv\"))\nnames(psilocybe)\n\n##  [1] \"ID\"                  \"Longitude\"           \"Latitude\"           \n##  [4] \"TaxonID\"             \"ScientificName\"      \"SiteVisitID\"        \n##  [7] \"StartDate\"           \"EndDate\"             \"SiteID\"             \n## [10] \"SiteCode\"            \"LocalityDetails\"     \"LocationPrecision\"  \n## [13] \"ProjectID\"           \"ProjectName\"         \"OrganisationID\"     \n## [16] \"OrganisationName\"    \"OrganisationAcronym\"\nI’ve wrapped the web addresses in the url() function as they are web datasources.\nNow to the maps. First, let’s make a function out of a tmap call:\nlibrary(tmap)\nlibrary(sf)\n\n## Linking to GEOS 3.8.1, GDAL 3.1.1, PROJ 6.3.1\n\nmymap &lt;- . %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")) %&gt;%\n  {tm_shape(.) +\n  tm_markers()}\nIf you want the map to be interactive, do this next:\ntmap_mode(\"view\")\ntmap_options(basemaps = c(Canvas = \"Esri.WorldTopoMap\"))\nThe function first converts the csv to a spatial points (st_points) object, then maps that data. Note I had to wrap the map in {} to include it at the end of the pipe.\nNow we can use it to map any of our dataframes. For instance:\nmymap(psilocybe)\n\n(This is a screen shot, but if you do this yourself it will be interactive)\n\n\n\nSo pipes are also handy for creating quick functions. This could also be very useful in situations where we pipe a series of dplyr functions to wrangle a dataset. They could be combined with loops, apply or pmap::map to repeat the call for different dataset.\nOne thing that I find challenging with %&gt;% is debugging is very hard. So a word of warning, keep your pipes short and simple so they are easy to debug (or use one of the specialist pipe debugging packages).\nPipes will likely be included in base R soon, and the R Core team are promising to make them more debuggable.\nEnjoy."
  },
  {
    "objectID": "posts/2020-12-29-pipe-functions/index.html#trivial-example",
    "href": "posts/2020-12-29-pipe-functions/index.html#trivial-example",
    "title": "Fast functions with pipes",
    "section": "",
    "text": "Let’s start with a trivial example. You can access pipes through the tidyverse or the original magrittr package.\nWe’ll create a function that plots a data series in rank order:\nlibrary(magrittr)\nrank_plot &lt;- . %&gt;%\n  sort() %&gt;%\n  plot()\nSo to create a function out of a pipe we just start the pipe with the ..\nNow rank_plot has no data, but is rather what is called a ‘functional sequence’. Type rank_plot in your console to see this.\nLet’s use it to plot two randomly generated variables:\nset.seed(2020)\n\nx &lt;- rnorm(30)\ny &lt;- runif(30)\n\npar(mfrow = c(2,2))\nplot(x) #x data in original order\nplot(y) # y data in original order\nrank_plot(x) #rank ordered data\nrank_plot(y)"
  },
  {
    "objectID": "posts/2020-12-29-pipe-functions/index.html#maps",
    "href": "posts/2020-12-29-pipe-functions/index.html#maps",
    "title": "Fast functions with pipes",
    "section": "",
    "text": "Let’s true a slightly more sophisticated example, a map.\nWe’ll use tmap to make the maps.\nFirst access some data. Here I’ve picked a couple of random mushroom species that have data on the Queensland Government’s Wetland Info page. The data-tables record (rather out-of-date) observations of the species and the sites where they were recorded. This data is just for the area around Brisbane where I live:\namanita &lt;- read.csv(url(\"http://apps.des.qld.gov.au/species/?op=getsurveysbyspecies&taxonid=25531&f=csv\"))\npsilocybe &lt;- read.csv(url(\"http://apps.des.qld.gov.au/species/?op=getsurveysbyspecies&taxonid=28689&f=csv\"))\nnames(psilocybe)\n\n##  [1] \"ID\"                  \"Longitude\"           \"Latitude\"           \n##  [4] \"TaxonID\"             \"ScientificName\"      \"SiteVisitID\"        \n##  [7] \"StartDate\"           \"EndDate\"             \"SiteID\"             \n## [10] \"SiteCode\"            \"LocalityDetails\"     \"LocationPrecision\"  \n## [13] \"ProjectID\"           \"ProjectName\"         \"OrganisationID\"     \n## [16] \"OrganisationName\"    \"OrganisationAcronym\"\nI’ve wrapped the web addresses in the url() function as they are web datasources.\nNow to the maps. First, let’s make a function out of a tmap call:\nlibrary(tmap)\nlibrary(sf)\n\n## Linking to GEOS 3.8.1, GDAL 3.1.1, PROJ 6.3.1\n\nmymap &lt;- . %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")) %&gt;%\n  {tm_shape(.) +\n  tm_markers()}\nIf you want the map to be interactive, do this next:\ntmap_mode(\"view\")\ntmap_options(basemaps = c(Canvas = \"Esri.WorldTopoMap\"))\nThe function first converts the csv to a spatial points (st_points) object, then maps that data. Note I had to wrap the map in {} to include it at the end of the pipe.\nNow we can use it to map any of our dataframes. For instance:\nmymap(psilocybe)\n\n(This is a screen shot, but if you do this yourself it will be interactive)"
  },
  {
    "objectID": "posts/2020-12-29-pipe-functions/index.html#conclusion",
    "href": "posts/2020-12-29-pipe-functions/index.html#conclusion",
    "title": "Fast functions with pipes",
    "section": "",
    "text": "So pipes are also handy for creating quick functions. This could also be very useful in situations where we pipe a series of dplyr functions to wrangle a dataset. They could be combined with loops, apply or pmap::map to repeat the call for different dataset.\nOne thing that I find challenging with %&gt;% is debugging is very hard. So a word of warning, keep your pipes short and simple so they are easy to debug (or use one of the specialist pipe debugging packages).\nPipes will likely be included in base R soon, and the R Core team are promising to make them more debuggable.\nEnjoy."
  },
  {
    "objectID": "posts/2023-06-17-PhD-projects-available/index.html",
    "href": "posts/2023-06-17-PhD-projects-available/index.html",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "",
    "text": "Current June 2023\nOur lab uses quantitative tools, including statistics, AI and numerical models, to help conservation managers address a range of issue facing marine ecosystems. Below are two current projects where we are recruiting students.\nChris can support students to pursue their own research questions related to the topics above, or email Chris to discuss project ideas."
  },
  {
    "objectID": "posts/2023-06-17-PhD-projects-available/index.html#project-suggestion-conserving-fish-nursery-habitats-to-support-sustainable-fisheries",
    "href": "posts/2023-06-17-PhD-projects-available/index.html#project-suggestion-conserving-fish-nursery-habitats-to-support-sustainable-fisheries",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Project suggestion: Conserving fish nursery habitats to support sustainable fisheries",
    "text": "Project suggestion: Conserving fish nursery habitats to support sustainable fisheries\nLittle is known about fish nursery habitats in Tasmania, especially seagrass, mudflats and soft-sediment ecosystems. Yet, these habitats support significant recreational and cultural fisheries. This project will involve a combination of field-work and modelling aimed at quantifying the value of nursery habitats and informing on their management. Email Chris (c.j.brown@utas.edu.au) for more information. PhD and honours projects available."
  },
  {
    "objectID": "posts/2023-06-17-PhD-projects-available/index.html#project-suggestion-conservation-and-restoration-of-coastal-habitats",
    "href": "posts/2023-06-17-PhD-projects-available/index.html#project-suggestion-conservation-and-restoration-of-coastal-habitats",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Project suggestion: Conservation and restoration of coastal habitats",
    "text": "Project suggestion: Conservation and restoration of coastal habitats\nWork with the team on Global Wetlands Project to help solve pressing challenges for conservation of coastal wetland habitats. PhD and Honours projects available to work on these habitats that are globally important for climate mitigation, biodiversity and fisheries. Email Chris (c.j.brown@utas.edu.au) for more information. PhD and honours projects available."
  },
  {
    "objectID": "posts/2023-06-17-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "href": "posts/2023-06-17-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Criteria for potential PhD applicants",
    "text": "Criteria for potential PhD applicants\nPlease consider the below criteria before contacting us. We will not be considering students for applications that do not meet these criteria, because they will not be competitive for funded positions.\nOur primary research methods are modelling tools. Potential applicants should have either experience in quantiative methods like R, or a very strong desire to learn more quantitative skills. Chris does not supervise projects that are primarily field or lab based, except as cosupervisor with other group leaders.\nOur students come with training in a diverse range of fields, including field ecology, maths and IT. Our lab group is outstanding at supporting quantitative training and current PhD students have ample opportunities to learn quantitative skills by working with their peers, postdoctoral researchers, Chris and through specialized training courses.\n\nAustralian students\nAustralian citizens and permanent residents should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters by research) in a relevant field.\n\n\nInternational students\nInternational applicants must have been awarded a 1st Class Honours degree or equivalent (e.g. Masters with a 40 credit point research thesis) in a relevant field. They must be lead author on at least one publication in an internationally recognized peer-reviewed journal. To be considered, journals should be internationally recognized, for instance they should have an Impact Factor rating from ISI or be listed by Scimago."
  },
  {
    "objectID": "posts/2021-10-15-Tuna-EM/index.html",
    "href": "posts/2021-10-15-Tuna-EM/index.html",
    "title": "Electronic monitoring offers insights into catch of western Pacific tuna longline fisheries",
    "section": "",
    "text": "By Max Campbell\nCollecting accurate data about catch and bycatch in fisheries is of great importance for managing fish stocks and preventing overfishing into the future.\nCurrently, most available catch data comes from reported catch from vessel logbooks. This data has obvious limitations including: ease of reporting, lack of expertise in identifying catch, and conflicts of interest. These limitations could lead to biased catch and bycatch records.\nTrained onboard human observers can address many of the potential biases in the catch recorded on fishing vessels, however, they are expensive and limited in supply. Instead, electronic monitoring (EM) offers a an opportunity to collect more accurate catch data on these fishing vessels.\nIn our paper (Brown et al. 2021 Marine Policy) we used a combination of logbook, human observers, and EM data from Palau, Federated States of Micronesia and the Republic of the Marshall Islands longline fishing vessels, to investigate differences among these methods. Further, we also describing complex patterns found catch and bycatch using advanced statistical methods.\nWe worked with The Nature Conservancy, who over a number of years have been helping these nations pilot electronic monitoring of their fisheries.\nDealing with catch data collected multiple methods in a multi-country setting has its challenges. This project required a great deal of care when processing the data, which was overcome by multiple researchers in our team performing detailed interrogations of the data.\nWe found that the retained catch reported in logbooks was up to three times lower than the retained catch reported by human observers and EM, and there was a lower species diversity reported in logbooks. Concerningly, we also found many discards were usually not reported in logbooks (including several threatened species such as several marine turtles).\nFurther, analysis of EM data revealed some previously undocumented correlations among several key catch and bycatch species in the fishery, that were likely induced by specific fishing practices (like use of certain types of bait).\nWe found that EM and Observer catch records were comparable, thus, EM may provide a suitable alternative to human observers. Where EM data seemed to provide a much more comprehensive representation of species composition and a more accurate catch data than the logbook records."
  },
  {
    "objectID": "posts/2016-11-21-google-scholar/index.html",
    "href": "posts/2016-11-21-google-scholar/index.html",
    "title": "Google controls who you cite",
    "section": "",
    "text": "Do you use Google scholar to search for academic papers? Then beware, the search engine is controlling, or at least strongly influencing, who you will cite.\nGoogle scholar has become increasingly useful and dominant in helping us find the papers we need. Among its most useful features are instant links to open access pdfs, where available, and ‘updates’ that recommend papers to you based on your own publications and the types of things you search for (see image). To get the updates you need to have a Google Scholar profile page, which most of us seem to these days. When searching you can also click directly to an authors profile page to see what other work they have published.\n\n\n\nGoogle scholar recommended these two papers to me, effectively predicting the paper I am working on before it has even been submitted (They are both great reads by the way)\n\n\nI recently wondered how searches on Google Scholar are ordered. If your paper comes out on top for popular key word searches, like ‘climate change’, you are much more likely to get read and therefore cited. I don’t have quantitative evidence for an effect of search rankings on cites, but there is plenty of circumstantial evidence for it.\nOrdering really matters, because even diligent researchers are unlikely to look through many search pages before trying another search. Numerous studies have shown we are most likely just to click the top result on web searches. That is why there is a whole industry based around search engine optimisation.\nIt is not totally transparent how Google Scholar searchers are ordered. On other platforms, like Web of Science, the search algorithm is transparent. The user gets to decide on which fields are searched (e.g. just the title, or the keywords, or whole topics) and ordering is also user controlled - you can order by date or citations. Therefore, if you use Web of Science, you may bias your reading (and cites) to newer or more highly cited papers. But at least you know that bias exists.\nGoogle Scholar’s algorithm is opaque, so we don’t know how it is biasing our citation practices. You can find some useful help on the search engine here. Of course they don’t tell you exactly how searches are ordered, otherwise people would game the system.\n[Authors note: after I published this blog Rich Grenyer sent me a link to this article from 2009 where some folk tried to reverse engineer how Google Scholar ranks searches. I recommend reading it if you use GS!]\n\n\n\nThe ordered of Google scholar search results is not that obvious, this one seems to be sorted by a mix of citations and keywords\n\n\nFor instance, see the image for the search ‘marine reserves and climate change’. The ordering is not consistently by citations, date or appearance of keywords in the title. The top four hits go from 2003 - 2007 - 2009 then back to 2006. Their citations vary from 2718, to 833 to 188 then back up to 1324. In fact, the ordering is determined by an algorithm that is trying to guess what you are most likely to click on.\nI doubt even Google’s coders could fully explain how searches are ordered. Machine learning algorithms, that try to guess what user wants are often so complex as to be opaque even to their developers. Standard Google searches rank pages based on ‘over 200 factors’. We can only assume Google Scholar does something similar. Presumably it ranks articles, in addition to relevance of your keywords, based on authors, citations, date, credibility of source, your location, articles you cite and so on.\nBack to academia, do we really want citations - one of our key measures of publication ‘quality’ - to be controlled by an algorithm run by a private company?\nAs an example, image the location of your IP address is used in determining the order of searches (it seems to play a role in the recommendations at least - the two articles recommended to me are both studies done in my home state). This would bias you toward local studies and you may miss cross-pollination of important ideas from other regions.\nI don’ think so. So while Google Scholar is convenient, I wouldn’t recommend you use it as your sole search engine. Ultimately, you want to have control over what you find and end up citing."
  },
  {
    "objectID": "posts/2016-11-21-google-scholar/index.html#google-controls-who-you-cite",
    "href": "posts/2016-11-21-google-scholar/index.html#google-controls-who-you-cite",
    "title": "Google controls who you cite",
    "section": "",
    "text": "Do you use Google scholar to search for academic papers? Then beware, the search engine is controlling, or at least strongly influencing, who you will cite.\nGoogle scholar has become increasingly useful and dominant in helping us find the papers we need. Among its most useful features are instant links to open access pdfs, where available, and ‘updates’ that recommend papers to you based on your own publications and the types of things you search for (see image). To get the updates you need to have a Google Scholar profile page, which most of us seem to these days. When searching you can also click directly to an authors profile page to see what other work they have published.\n\n\n\nGoogle scholar recommended these two papers to me, effectively predicting the paper I am working on before it has even been submitted (They are both great reads by the way)\n\n\nI recently wondered how searches on Google Scholar are ordered. If your paper comes out on top for popular key word searches, like ‘climate change’, you are much more likely to get read and therefore cited. I don’t have quantitative evidence for an effect of search rankings on cites, but there is plenty of circumstantial evidence for it.\nOrdering really matters, because even diligent researchers are unlikely to look through many search pages before trying another search. Numerous studies have shown we are most likely just to click the top result on web searches. That is why there is a whole industry based around search engine optimisation.\nIt is not totally transparent how Google Scholar searchers are ordered. On other platforms, like Web of Science, the search algorithm is transparent. The user gets to decide on which fields are searched (e.g. just the title, or the keywords, or whole topics) and ordering is also user controlled - you can order by date or citations. Therefore, if you use Web of Science, you may bias your reading (and cites) to newer or more highly cited papers. But at least you know that bias exists.\nGoogle Scholar’s algorithm is opaque, so we don’t know how it is biasing our citation practices. You can find some useful help on the search engine here. Of course they don’t tell you exactly how searches are ordered, otherwise people would game the system.\n[Authors note: after I published this blog Rich Grenyer sent me a link to this article from 2009 where some folk tried to reverse engineer how Google Scholar ranks searches. I recommend reading it if you use GS!]\n\n\n\nThe ordered of Google scholar search results is not that obvious, this one seems to be sorted by a mix of citations and keywords\n\n\nFor instance, see the image for the search ‘marine reserves and climate change’. The ordering is not consistently by citations, date or appearance of keywords in the title. The top four hits go from 2003 - 2007 - 2009 then back to 2006. Their citations vary from 2718, to 833 to 188 then back up to 1324. In fact, the ordering is determined by an algorithm that is trying to guess what you are most likely to click on.\nI doubt even Google’s coders could fully explain how searches are ordered. Machine learning algorithms, that try to guess what user wants are often so complex as to be opaque even to their developers. Standard Google searches rank pages based on ‘over 200 factors’. We can only assume Google Scholar does something similar. Presumably it ranks articles, in addition to relevance of your keywords, based on authors, citations, date, credibility of source, your location, articles you cite and so on.\nBack to academia, do we really want citations - one of our key measures of publication ‘quality’ - to be controlled by an algorithm run by a private company?\nAs an example, image the location of your IP address is used in determining the order of searches (it seems to play a role in the recommendations at least - the two articles recommended to me are both studies done in my home state). This would bias you toward local studies and you may miss cross-pollination of important ideas from other regions.\nI don’ think so. So while Google Scholar is convenient, I wouldn’t recommend you use it as your sole search engine. Ultimately, you want to have control over what you find and end up citing."
  },
  {
    "objectID": "posts/2016-12-26-highlights-2016/index.html",
    "href": "posts/2016-12-26-highlights-2016/index.html",
    "title": "Highlights of 2016",
    "section": "",
    "text": "It’s not every day that someone discovers something as fundamental as a new type of causality. In 2012, Sugihara and colleagues did, well almost. They describe a new method for detecting non-linear causality in time-series. The up-shot is, causality might have been hiding in our time-series for years and we have never noticed it.\nBasically causality in highly connected non-linear systems - read ecosystems - doesn’t look like a linear correlation\nThe impact of this work has probably been modulated somewhat by limits of available data - you need long and high resolution data-sets for their method.\nWell they have followed up this year with a solution - you can leverage multivariate time-series to detect causality. Multivariate data drastically reduces the number of samples you need.\nI recommend reading both these papers. They are pretty heavy, but it is worth the time, so give yourself a good few hours to really absorb the implications of their work.\n** Include image: this isn’t causation (correlation graph), THIS is causation (graph of simplex map)\n\n\n\nMultivariate statistics have leapt forward and it has never been been a more exciting time to do stats on community data….\nI was never that interested in community ecology, or at least the stats involved. You know all what I mean, all the stuff about 4th root transforms and dissimilarity indices. I found I never really knew how to interpret results once you abstracted a community to some sort of ‘distance’ measure.\nEnter model-based methods.\nNow we can infer species interactions, test for environmental associations and make predictions all without ever leaving the comfort of natural ecological units - counts of individuals.\nThere is a rapidly growing literature on this stuff, but I recommend any of the papers from David Warton’s group, which got me into model-based community analysis. Try his TREE review on joint modelling, Hui’s comparison of traditional vs model-based ordination and if you like trait analysis Brown’s (not me!) approach to trait*environment interactions."
  },
  {
    "objectID": "posts/2016-12-26-highlights-2016/index.html#papers-i-read-that-changed-the-way-i-thought-in-2016",
    "href": "posts/2016-12-26-highlights-2016/index.html#papers-i-read-that-changed-the-way-i-thought-in-2016",
    "title": "Highlights of 2016",
    "section": "",
    "text": "It’s not every day that someone discovers something as fundamental as a new type of causality. In 2012, Sugihara and colleagues did, well almost. They describe a new method for detecting non-linear causality in time-series. The up-shot is, causality might have been hiding in our time-series for years and we have never noticed it.\nBasically causality in highly connected non-linear systems - read ecosystems - doesn’t look like a linear correlation\nThe impact of this work has probably been modulated somewhat by limits of available data - you need long and high resolution data-sets for their method.\nWell they have followed up this year with a solution - you can leverage multivariate time-series to detect causality. Multivariate data drastically reduces the number of samples you need.\nI recommend reading both these papers. They are pretty heavy, but it is worth the time, so give yourself a good few hours to really absorb the implications of their work.\n** Include image: this isn’t causation (correlation graph), THIS is causation (graph of simplex map)\n\n\n\nMultivariate statistics have leapt forward and it has never been been a more exciting time to do stats on community data….\nI was never that interested in community ecology, or at least the stats involved. You know all what I mean, all the stuff about 4th root transforms and dissimilarity indices. I found I never really knew how to interpret results once you abstracted a community to some sort of ‘distance’ measure.\nEnter model-based methods.\nNow we can infer species interactions, test for environmental associations and make predictions all without ever leaving the comfort of natural ecological units - counts of individuals.\nThere is a rapidly growing literature on this stuff, but I recommend any of the papers from David Warton’s group, which got me into model-based community analysis. Try his TREE review on joint modelling, Hui’s comparison of traditional vs model-based ordination and if you like trait analysis Brown’s (not me!) approach to trait*environment interactions."
  },
  {
    "objectID": "posts/2016-12-26-highlights-2016/index.html#machines-with-intuition",
    "href": "posts/2016-12-26-highlights-2016/index.html#machines-with-intuition",
    "title": "Highlights of 2016",
    "section": "Machines with intuition",
    "text": "Machines with intuition\nGoogle research has built an algorithm that can beat the world’s top Go players.\nIf you don’t known Go it is a game a bit like checkers, but more complex. Go is widely considered a major challenge for machine learning, far harder than chess, because there are far to many possible moves for today’s computer simply to predict every possible outcome. A machine that can play Go well needs to have intuition. Somewhat scarily Google’s Alpha Go algorithm can be trained to develop intuition about how to win.\nIt makes me wonder how long before they automate the job of scientist… Several recent works (read this and this) have already automated (parts of) the job of statistician."
  },
  {
    "objectID": "posts/2016-12-26-highlights-2016/index.html#social-networks-rule-environmental-impacts",
    "href": "posts/2016-12-26-highlights-2016/index.html#social-networks-rule-environmental-impacts",
    "title": "Highlights of 2016",
    "section": "Social networks rule environmental impacts",
    "text": "Social networks rule environmental impacts\nBarnes - network analysis"
  },
  {
    "objectID": "posts/2016-12-26-highlights-2016/index.html#measuring-fish-habitats",
    "href": "posts/2016-12-26-highlights-2016/index.html#measuring-fish-habitats",
    "title": "Highlights of 2016",
    "section": "Measuring fish habitats",
    "text": "Measuring fish habitats\nFerrari habitat complexity paper"
  },
  {
    "objectID": "posts/2016-12-26-highlights-2016/index.html#do-another-post-on-talks-i-heard-that-changed-things",
    "href": "posts/2016-12-26-highlights-2016/index.html#do-another-post-on-talks-i-heard-that-changed-things",
    "title": "Highlights of 2016",
    "section": "Do another post on talks I heard that changed things:",
    "text": "Do another post on talks I heard that changed things:\nMegan’s SLR image of habitats near Sea level.\nResilience talk at ICRS - inspiring b/c no slides\nHere is what I wrote about Emma Lee’s talk:\n\n“A great talk from Emma Lee changed the way I think about science. I come from a very quantitative background, where the point of science is knowledge and improving the future for nature and humanity. But Lee convinced me that telling a story is a suitable goal in itself. People love to tell stories, so much so it is almost a basic human need. Science is the way to come closer to the truth in our thinking and in doing so provide an outlet for people to communicate authentic stories.”\n\nMaria Gardener\nTalk about Brazil fish. I was just blown away by the beaty and diversity"
  },
  {
    "objectID": "posts/2017-11-15-balancing-recreational-commercial-sectors/index.html",
    "href": "posts/2017-11-15-balancing-recreational-commercial-sectors/index.html",
    "title": "Aiming for balance between recreational and commercial fisheries",
    "section": "",
    "text": "The Queensland State Election is on 25th November. Past elections have seen promises on new fisheries policy. In a recent article for the Machinery of Government blog I discuss opportunities for the new government to create policy that will help achieve a sustainable balance between recreational and commercial sectors in Queensland.\nI won’t say any more here, but see the full blog if you are interested."
  },
  {
    "objectID": "posts/2025-10-05-AI-assistants-for-scientific-coding/index.html",
    "href": "posts/2025-10-05-AI-assistants-for-scientific-coding/index.html",
    "title": "New online book AI Assistants for Scientific Coding",
    "section": "",
    "text": "I’ve released a new online book, AI Assistants for Scientific Coding. It’s a practical guide to using language models to support scientific computing and analysis.\nThe book focuses on helping people who already use R or Python for data analysis. It’s not an introduction to programming; instead, it shows how to work with AI tools effectively and responsibly once you know the basics.\nWhat you’ll find inside:\n\nChoosing and using AI coding assistants, from simple chat tools to agents that can run and test code\nPrompting strategies that improve reliability for real analysis tasks\nExamples from environmental science (GLMs, multivariate stats), with methods general to other fields\nNotes on ethics, copyright, costs, and environmental impacts\n\nRead it here: https://www.seascapemodels.org/AI-assistants-for-scientific-coding/\nThe material also serves as reference notes for a one‑day workshop and will evolve as the field changes. If you’re interested in the prompting side of statistical workflows, there’s an accompanying preprint: https://doi.org/10.32942/X2CS80"
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html",
    "href": "posts/2022-05-12-ask-more-questions/index.html",
    "title": "How to ask more questions and be a better scientist",
    "section": "",
    "text": "As the world opens up we get to reconnect with our peers in person and make new connections. Much effort is put into training graduates to ‘make the pitch’ for their research when they first meet someone. But I find asking good questions is a more effective way to connect with others.\nAsking questions can take practice. Here are a few tips that have worked for me as I try to learn to be better at this skill.\nAsking questions in seminars is also an important skill to cultivate that I’ve covered previously"
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html#be-curious",
    "href": "posts/2022-05-12-ask-more-questions/index.html#be-curious",
    "title": "How to ask more questions and be a better scientist",
    "section": "Be curious",
    "text": "Be curious\nCultivate an honest curiosity in other people. Often at a conference or meeting there are people we want to meet because we have an agenda. When we meet them we might therefore follow a specific line of questioning\nBut you’ll gain a much deeper understanding of another person and their science if you ask open questions and see where it takes you. This means listening to the other person for what they have to say, not just asking questions to get what you want.\nLet them guide the topic conversation and see where that takes you."
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html#start-with-who-what-when-where-how",
    "href": "posts/2022-05-12-ask-more-questions/index.html#start-with-who-what-when-where-how",
    "title": "How to ask more questions and be a better scientist",
    "section": "Start with who, what when, where, how",
    "text": "Start with who, what when, where, how\nA good place to start is with ‘who, what, when, where, how’ questions. They invite thoughtful responses. Why questions are a bit less specific, and in some contexts can sound a bit whingy or confrontational."
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html#ask-about-the-journey",
    "href": "posts/2022-05-12-ask-more-questions/index.html#ask-about-the-journey",
    "title": "How to ask more questions and be a better scientist",
    "section": "Ask about the journey",
    "text": "Ask about the journey\nScience discussions don’t just have to be about the science. Some of the most interesting discussions I’ve had are about the journey the scientist took to get where they are in their career, or to get to their current research priorities.\nFor instance:\nYou can ask people:\n\nWhat motivates you to research topic xxxx\nWho were your mentors?\nWhen did you realize this was important to research?\nWhere where you when you discovered that?\nHow did do you translate your findings to managers/stakeholders?\nWhat challenges did you have?\nHow did you overcome those challenges?\n\nAnd so on. Notice the ‘who, what, when, where, how’"
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html#the-pause",
    "href": "posts/2022-05-12-ask-more-questions/index.html#the-pause",
    "title": "How to ask more questions and be a better scientist",
    "section": "The pause",
    "text": "The pause\nLearn to be comfortable with a pause in the flow of the conversation. If you can hold the pause without speaking often the other person will continue on the fill the space, sometimes with the most interesting and personal insights that you will hear from them.\nPeople of different cultures and background will be comfortable with different lengths of pauses. My friend who grew up in the country can always out-pause me, she’s comfortable sitting in silence for what feels like an uncomfortably long time to me. Whereas, people from big cities can speak much faster with little or no breaks, or even seem to speak over the top of you (and here’s a great podcast about the ‘pause’)."
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html#i-wonder.",
    "href": "posts/2022-05-12-ask-more-questions/index.html#i-wonder.",
    "title": "How to ask more questions and be a better scientist",
    "section": "I wonder….",
    "text": "I wonder….\nNot exactly a question, but another tool for getting people talking is to use a ‘I wonder’ sentence. Just put it out then then wait to see what people say. Like:\n“I wonder what making a discovery like that would feel like”\n“I wonder how you keep inspired in the face of those challenges”\n“I wonder if people can access this research and use it for….”"
  },
  {
    "objectID": "posts/2022-05-12-ask-more-questions/index.html#use-questions-to-facilitate-a-more-inclusive-discussion",
    "href": "posts/2022-05-12-ask-more-questions/index.html#use-questions-to-facilitate-a-more-inclusive-discussion",
    "title": "How to ask more questions and be a better scientist",
    "section": "Use questions to facilitate a more inclusive discussion",
    "text": "Use questions to facilitate a more inclusive discussion\nIn any group conversation you’ll notice some people naturally tend to dominate the discussion. This might mean you miss the perspectives of naturally quieter individuals.\nIt could be around coffee, or in a workshop dedicated to a topic.\nQuestions can be a great way to draw the quieter participants into the conversation.\nIf you know the people, then its pretty easy, you can just draw a particular person in with questions like “Dana, you have some experience in this, what have you noticed are the key challenges?”\nIts a bit harder if you don’t know the people or its a large group. One way to do it is to use the I wonder questions above. Another way is to say something along the lines of:\n“It would be great to hear from an expert on that for what they think about what Dana jus said”\nHope this has helped. Let me know on twitter (@bluecology) if you use any of the tips above and how they went."
  },
  {
    "objectID": "posts/2025-11-03-upcoming-R-workshop-brisbane/index.html",
    "href": "posts/2025-11-03-upcoming-R-workshop-brisbane/index.html",
    "title": "Upcoming R workshops in Brisbane February 2026",
    "section": "",
    "text": "R workshops in Brisbane Australia, February 2026.\nFor many years I had the pleasure of teaching with Ant Richardson and the team for these R workshops. I’m not there anymore, but recommend attending these if you can make the dates:\nTransform your research questions into robust statistical insights with our 2026 Summer R Workshops in an immersive exploration of R programming and statistical modelling. These workshops in February 2026 are designed for beginners and researchers who want to build a solid foundation in R programming and progress through to advanced statistical techniques. Whether you’re completely new to R or looking to strengthen your statistical modelling skills, join us to develop the analytical expertise essential for modern research!\nRegistration is now open. For more information, see our website https://mathmarecol.github.io/RWorkshop/summer.html and the attached flyer. For any questions, email cbcs-workshops@uq.edu.au.\nTestimonials: “I truly enjoyed the whole workshop. Great teachers, really nice real data examples and good notes and background” “I am feeling a lot better about using stats now than I was previously” “I think it should be a compulsory course for all PhDs”"
  },
  {
    "objectID": "posts/2016-06-01-phdposition/index.html",
    "href": "posts/2016-06-01-phdposition/index.html",
    "title": "PhD position on fisheries dependence on marine habitats",
    "section": "",
    "text": "PhD positions\nSustaining fisheries is important for economies, livelihoods and human health, but globally numerous fisheries are threatened by rapid loss of aquatic habitat. The role of habitat in fishery declines is poorly quantified, and we need new models that integrate existing datasets to attribute change in a fishery to change in its habitats.\n\n\n\nI am seeking motivated candidates to work with me on one of a range of projects relating to marine ecology and fisheries.\nThe PhD projects are flexible but may include analysis of coral reef datasets to understand how fishery species relate to their habitats and/or field work to quantify fishery-habitat relationships.\nI am looking for someone who is motivated by conservation issues and is keen on ecological science, improving resource management and marine conservation.\n Get in touch  for more info and to discuss project options."
  },
  {
    "objectID": "posts/2020-12-16-encouragement-not-praise/index.html",
    "href": "posts/2020-12-16-encouragement-not-praise/index.html",
    "title": "Encouragement not praise",
    "section": "",
    "text": "An important goal for me as supervisor of PhD students is to build their confidence. Building confidence in a scientific topic comes through growing a student’s knowledge in a discipline and its methods. Growth in confidence can also come through how a supervisor mentors their student. Compliments can be part of that growth.\nI only recently realised there is an important difference between praise and encouragement, particularly when it comes to building confidence.\nA fundamental difference is that the praise you give reflects your own values, whereas encouragement asks someone to identify what they value. In this sense praise could be seen as coming from a selfish point of view (I value that, therefore you should to), whereas encouragement shows interest in what someone else values, without passing your own judgement on those values.\nLet me give some examples.\nSay a senior professor approaches a student and she says “Those analyses you are doing are groundbreaking. You are so clever with statistics. I am so proud of how much you have learned here.”\nThis is praise. In fact, what I wrote is so corny you might start to wonder if they are trying to flatter you to get something out of you.\nThe praise can be more subtle than my example. One tell-tale sign is labels. The prof is labelling your work (and by implication you) as groundrbreaking, they are labelling as someone who is clever at statistics. They are saying what you are doing is making them proud. This is praise and it all comes from a place of the values they hold for you and your work."
  },
  {
    "objectID": "posts/2020-12-16-encouragement-not-praise/index.html#praise-confidence-and-manipulation",
    "href": "posts/2020-12-16-encouragement-not-praise/index.html#praise-confidence-and-manipulation",
    "title": "Encouragement not praise",
    "section": "Praise, confidence and manipulation",
    "text": "Praise, confidence and manipulation\nPraise can be well-intentioned and I don’t mean to knock people who praise (I compliment my students often!). Praise can also build confidence and many good people who mean the best for you will praise you.\nThe problem with praise is that it can also build dependency of the student on the supervisor. The supervisor is setting up a situation where the student needs to come to the supervisor to build their self-worth. The student’s self-worth is portrayed through the supervisors eyes, and so is external to the student themselves.\nPraise isn’t effective at building intrinsic self-worth.\nYou could say the entirety of academic culture is built on praise. You wouldn’t be far wrong. Academic culture places immense value on status symbols like fancy journals, titles, commendations from peers and awards. These things are all labels for the value we’re told we should place in a scientist’s career.\nPraise is frequently employed by academics to ensure other people say ‘yes’ to an ask. These are traps academics set for each other (I’ve been known to set them on occasion too!). A colleague once wanted my help for their conference so they wrote to me saying: “I’m looking for thought leaders to help organize this conference…”\nThis use of praise not necessarily a bad thing, organizing conferences is an important part of a career. But as it happens I didn’t have the time or inclination to help with that particular conference. So I laughed to myself and turned the opportunity down. (If you are are sucker for saying ‘yes’, try ithinkwell’s advice about “not saying yes”).\nBut praise is also occurs in more extreme and harmful cases of manipulation. In fact, in recent news about bullying in academia, ‘praise’ has been frequently mentioned by the victims. In these extreme cases praise becomes one of the tools a bully uses to break down a student’s confidence. The bullying will follow a cycle of praising to create dependence, but then put-downs to further suppress any hope the student had of developing their own self-worth and thus gaining independence from the bully.\nSo notice the things people say to you and think critically. Is it praise? Are they labelling you? If they are praising you, what is the context? It may be because they truly want you to feel good about what you’ve accomplished, like your parents telling you they are proud of the award of your PhD. It may be because they need your help with something, which could be good for your career, or a time-consuming distraction. Or it may be they are trying to manipulate you in more serious way."
  },
  {
    "objectID": "posts/2020-12-16-encouragement-not-praise/index.html#encouragement-and-self-worth",
    "href": "posts/2020-12-16-encouragement-not-praise/index.html#encouragement-and-self-worth",
    "title": "Encouragement not praise",
    "section": "Encouragement and self-worth",
    "text": "Encouragement and self-worth\nSo what’s the alternative to praise for supervisors who want to build a student’s intrinsic self-worth?\nEncouragement is about helping someone find their own intrinsic values for what they are doing. To give some examples, lets revisit our Prof from before, but re-write what she said as encouragement:\n“What do you think are the most important next steps for solving the statistical issues our field faces? Do you enjoy the work on analysis you are doing? What are you most proud of in the paper you just published?”\nNow these questions are asking the student what their intrinsic values are. The student isn’t being labelled. The prof isn’t making any value judgements on the work. They are helping the student identify their own values.\nIt could also be encouragement for the Prof to say “Thank you for presenting your work, it has been really helpful for my own research to learn about these new techniques.” Here the Prof is sharing how she was encouraged by the presentation, without offering any labels for the student’s work.\nUltimately encouragement helps someone to become more independent of what others think of them. If you deeply understand what you like doing and what you value most, then you can pursue career satisfaction without the need for external validation.\nEndnote I learned these ideas about praise vs encouragement in an odd way. I was listening to a parenting podcast about raising confident children and it was talking about praise vs encouragement. I wouldn’t normally draw parallels between students and children! But ‘praise’ has also come up in several recent bullying cases and at the same time I’ve been ‘encouraged’ to use encouragement in leadership courses I’ve taken. In fact, many global leaders say having children was a turning point that enabled them to be better leaders. Just goes to show we should be supporting parents of all genders to stay in academia."
  },
  {
    "objectID": "posts/2015-07-19-brewer_colour_palettes/2015-07-19-brewer_colour_palettes.html",
    "href": "posts/2015-07-19-brewer_colour_palettes/2015-07-19-brewer_colour_palettes.html",
    "title": "Making continuous colour palettes with RColorBrewer",
    "section": "",
    "text": "Making continuous colour palettes with RColorBrewer\n\n\nThe RColorBrewer package makes choosing colours for your graphs easy. If you follow the link on the help page for the handy brewer.pal function, you can get to a web app that lets you choose the type of palette (linear, diverging, qualitative), the number of colours and other options, like palettes that will be visable to people who are red-green colour blind (about 4% of the population).This all extremely useful, but RColorBrewer palettes are generally limited to a maximum of 10 different colours. What if you want to have a continuous scale? Here I run through how to make a colour ramp from a Brewer palette.Note though, that the designers of RColorBrewer limited the number of colours as a deliberate choice. The colour palettes are mainly designed for maps, and in general, people can’t keep more than about 7 categories in their head at any one time. Hence, before you create your figure, think about how you want to represent on your figure. For instance, say you are plotting elevation. Choosing a few colours will emphasis zones in elevation, for instance, maybe you want to represent different vegetation zones that change with altitude. Whereas, a continuous palette will obviously represent the data as more of a gradient.\n\n\n\nColour Brewer categorical colours\n\nlibrary(RColorBrewer)\nbluecols &lt;- brewer.pal(9, 'Blues')\n\nWhich just makes a pie with nine different shades of blue. ### A colour ramp We can use the colorRampPalette() function to create a new function, here newcol(). newcol() takes a single integer as an argument, which outputs the colour ramp using our blue palette.\n\nnewcol &lt;- colorRampPalette(bluecols)\nncols &lt;- 100\nbluecols2 &lt;- newcol(ncols)#apply the function to get 100 colours\n\nAnd that is it. We now have the RColorBrewer palette with 100 different shades of blue."
  },
  {
    "objectID": "posts/2018-07-24-new-phd-topics-on-wetlands/index.html",
    "href": "posts/2018-07-24-new-phd-topics-on-wetlands/index.html",
    "title": "New PhD opportunities to work on conserving the health of coastal wetlands",
    "section": "",
    "text": "We are seeking PhD students to conduct research with the new Global Wetlands (GLoW) program at Griffith University.\nThe Global Wetlands program aims to promote the health of coastal wetlands globally. Wetlands studied include mangroves, seagrass and saltmarsh ecosystems.\nThis interdisciplinary program brings together staff working on coastal wetlands science, conservation planning, ecological modelling, machine learning and science communication. We have collaborators in seven countries.\nSuccessful PhD applicants will receive $10,000 additional funds to support field work and collaborative travel.\nThe ideal student will be motivated to conduct research in one or more of the fields of: biodiversity conservation, field ecology, ecological modelling, biogeography or wetlands ecology.\nInterested candidates should read the project descriptions below and contact the relevant staff member in the first instance. Please also read the guidelines for PhD applicants at the bottom of this page before contacting GLoW staff.\nApplications are due 2nd October 2018."
  },
  {
    "objectID": "posts/2018-07-24-new-phd-topics-on-wetlands/index.html#phd-project-opportunities",
    "href": "posts/2018-07-24-new-phd-topics-on-wetlands/index.html#phd-project-opportunities",
    "title": "New PhD opportunities to work on conserving the health of coastal wetlands",
    "section": "PhD project opportunities",
    "text": "PhD project opportunities\n\nPromoting the resilience of coastal wetlands to land-based threats\nThis project will expand Dr Tulloch’s models of land-based stressors to coastal ecosystems. The student will use the models at a large scale to prioritise actions on land that can protect coastal wetlands.\nContact: Dr Viv Tulloch-McShane\n\n\nTrade-offs among the coastal ecosystem services provided by wetlands\nThis study can involve modelling and/or field components to study synergies and trade-offs in the delivery of ecosystem services from coastal wetlands.\nContact: Dr Viv Tulloch-McShane\n\n\nGlobal shifts in grazer functions under multiple stressors\nThis experimental study will aim to sample marine grazers from multiple locations internationally to understand functional responses of grazers to multiple stressors.\nContact: Ryan Pearson\n\n\nFinding function among structure: developing and validating indicators of ecosystem health in coastal wetlands\nContact: Dr Michael Sievers\n\n\nHabitat suitability models for prioritisation of global action in coastal wetlands\nThis project aims to model current and future habitat suitability of wetland plants and animals. Current records of species occurrence (macroinvertebrates, fish, etc) would be supplemented field surveys. Models will be used to generate predictions and maps of biodiversity response under various scenarios, and to prioritise cost-effective management actions that promote the health of coastal wetlands.\nContact: Michael Sievers\n\n\nValidating multi-stressor null models on coastal seagrass habitats\nMarine systems are under increasing threat from a number of human activities. This project will aim to use a combination of field and experimental studies to quantify the types of interactions found between a range of stressors impacting seagrass habitats. These findings can help improve and inform management practices for conservation.\nContact: Mischa Turschwell\n#### Automation of environmental monitoring\nEnvironmental monitoring is on the brink of a step change in effectiveness. Building on GLOW’s automated fish identification system, the student will use ecological and statistical frameworks to determine how best to automate the gathering, quality assurance, and analysis, of environmental data.\nContact: Professor Rod Connolly\n\n\nMapping the world’s mangrove ecosystem services\nMangroves are one of the most valuable ecosystems on Earth, providing us with a range of services including carbon sequestration, coastal protection, fisheries, and water quality improvement. The ultimate goal of the project is to develop a global map of ecosystem services of mangrove which could aid in the better management, conservation, and restoration of these valuable ecosystems.\nContact: Dr Fernanda Adame"
  },
  {
    "objectID": "posts/2018-07-24-new-phd-topics-on-wetlands/index.html#criteria-for-potential-phd-applicants",
    "href": "posts/2018-07-24-new-phd-topics-on-wetlands/index.html#criteria-for-potential-phd-applicants",
    "title": "New PhD opportunities to work on conserving the health of coastal wetlands",
    "section": "Criteria for potential PhD applicants",
    "text": "Criteria for potential PhD applicants\nPlease do not contact us unless you meet the below criteria. We will not be considering students for applications that do not meet these criteria. Further details on Griffith University’s application process can be found here.\n\nAustralian students\nAustralian citizens and permanent residents should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters by research) in a relevant field. Ideally, they will also have published a lead authored paper in an internationally recognized peer-reviewed journal.\n\n\nInternational students\nInternational applicants must have been awarded a 1st Class Honours degree or equivalent (e.g. Masters by research) in a relevant field. They must be lead author on at least one publication in an internationally recognized peer-reviewed journal. To be considered, journals should be internationally recognized, for instance they should have an Impact Factor rating from ISI."
  },
  {
    "objectID": "posts/2018-12-17-finding-confidence-in-your-phd/index.html",
    "href": "posts/2018-12-17-finding-confidence-in-your-phd/index.html",
    "title": "Finding confidence in your PhD and overcoming imposter syndrome",
    "section": "",
    "text": "2018 has been a big year for me, at work and home. I started several major grants, including starting new PhD students and hiring my first ever postdoc. Then very quickly I also hired my 2nd and 3rd ever postdocs. All this hiring meant I had to chair interview panels for the first time.\nI don’t think the first few candidates realised, but I was as nervous as they were.\nAt home my second son was born. Cradling his floppy little head in hospital I had that feeling again that there was something wrong that society would let such a naive Dad look after this child’s fragile life. I often feel like I’m learning to be a Dad by ‘trial and error’ and that someone will pull me up for poor parent skills, like not holding a baby properly.\nAll these feelings of not being capable may be imposter syndrome. Imposter syndrome is the feeling that you are a fraud, and that all you’ve achieved is false. You feel like you will eventually be caught out by someone and everything will come tumbling down. You have this feeling even though there is quantitative evidence that you are in fact the real deal (ok so I did win the grants, and yes I am a capable father who has raised another child).\nImposter syndrome is becoming increasingly well recognized in academic circles, because it often affects successful people. I think there is evidence that women are more prone to it than men, though we men can certainly be affected too.\nSo an important development goal for someone’s PhD is not just to grow their knowledge, but also their confidence in that knowledge. This might include the confidence to present seminars, conduct statistics, write papers, debate, do job interviews, or even just speak with colleagues.\nLearning to be humble about your knowledge can be important too, though I think this need is rarer in early career scientists.\nMany people with imposter syndrome may not even realise they can overcome it. If they are even aware of their own low confidence they may see it as an inherent trait, like it’s part of their DNA. In fact, you can write your own story about who you are.\nI want to share here how I grappled with imposter syndrome and how I’ve learned to be more confident and overcome low self-esteem.\nI’ll focus on the parts of the story I think are lessons that others can act on. I hope this helps you."
  },
  {
    "objectID": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#my-story-about-low-self-esteem-and-cultivating-confidence",
    "href": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#my-story-about-low-self-esteem-and-cultivating-confidence",
    "title": "Finding confidence in your PhD and overcoming imposter syndrome",
    "section": "My story about low self-esteem and cultivating confidence",
    "text": "My story about low self-esteem and cultivating confidence\nI did my PhD in a very successful lab group. I remember three months into my PhD feeling extremely stressed. Many of my peers who had started 6 months before me had submitted their first PhD papers to journals like Science. I was asking myself how I deserved to be there. I was months in and I hadn’t submitted a paper yet!\nA few years later I shared this experience with some of my peers. They laughed. Apparently just before I started the lab lead had printed my CV and left it on the tea-room table. The other students had been somewhat intimidated by some of my academic achievements in undergrad.\nThe lesson I learned was that we often compare ourselves against other people’s achievements and find ourselves to be lacking. We fail to remember that we too have been successful, but in different ways. It is like looking at someone’s Instagram feed: They select only the amazing things to post about their holiday, and you think they have a much more interesting life than you have. If you talk to them though, you find out that the holiday was mostly mundane, or downright dreadful.\nIt is important to learn that you can write your own story of what success is, and you can write this story around your own strengths.\nIf you are not sure what your strengths are, find a trusted friend or colleague and ask them. Another way to find your strengths is to write down your ‘weaknesses’. Your strengths will often be the flip side of your weaknesses. So if one of your weaknesses was taking on too much for other people, maybe your strength is teamwork.\nI don’t know why I thought I was failing so early on in my PhD. I think it goes back to low self-esteem from my teenage years. I was often bullied in high school. Rarely did I think I was ‘good’ at anything. When I did well in class, I usually put it down to luck or hard work."
  },
  {
    "objectID": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#you-can-choose-peers-that-help-your-confidence-not-hinder-it",
    "href": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#you-can-choose-peers-that-help-your-confidence-not-hinder-it",
    "title": "Finding confidence in your PhD and overcoming imposter syndrome",
    "section": "You can choose peers that help your confidence, not hinder it",
    "text": "You can choose peers that help your confidence, not hinder it\nStudy and academia played a key role in growing my confidence. As I progressed to the later years of high-school I escaped from some of the social circles that were putting me down. Those later years also saw a greater emphasis on learning and study and, for me, access to some great mentors who encouraged my interest in ecology and science.\nStudying at university meant I found peers who also shared my passion for ecology and science. I gained the respect of my peers for the knowledge I did have. For instance, I often helped my peers study for exams. This really helped me develop more confidence.\nFinding a positive group of peers and mentors is important for growing your confidence. In my PhD and onto my postdoc I had finally found a community that grew my confidence, rather than putting me down like in high-school. Many of them became close friends.\nAs my Phd went on I started to ask the other students or postdocs in the lab for help on everything from admin to statistics to finding a house. This helped me find a community among other PhD students and postdocs who become a crucial support network. I learned from them that others who seemed so successful had also faced the same fears of failure as me, but overcame them.\nSo a community of support can help you grow your confidence. Often all it takes is to ask someone for help."
  },
  {
    "objectID": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#mentors-that-let-you-make-mistakes",
    "href": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#mentors-that-let-you-make-mistakes",
    "title": "Finding confidence in your PhD and overcoming imposter syndrome",
    "section": "Mentors that let you make mistakes",
    "text": "Mentors that let you make mistakes\nFinding and maintaining positive mentors is also important too. I’ve had many. I looked back at the acknowledgements from my PhD and a quote about my supervisors stands out:\n\n“I could not have emerged from this PhD confident about making a career in scientific research without their guidance, but also the freedom they gave me to make mistakes.”\n\nBecause we all make mistakes sometimes, I think it is important for supervisors to let you make them, or, if not, find your own mentors that help you reconcile and learn from them.\nIf your supervisor is putting you down, maybe you need to find another mentor. Or be brave and ask them to be more positive. Criticism is hardwired into academic culture (and so it should be), but sometimes supervisors can be more critical and negative than they realise.\nAlong the way in my PhD I got to present at conferences. I was well prepared and I did a good enough job to get positive feedback from other scientists. I also got some papers through peer-review. Some times the reviews (and even coauthors!) were hyper-negative. Sometimes they were positive. But the end result of the published paper was always a boost to my confidence. So putting the work in to making these achievements happen was crucial to building my confidence.\nThrough my PhD I started learning the R programming language, which has become instrumental to my career. As I got better I realised it was knowledge other people wanted to have too. So into my postdoc I started teaching R to others, something I continue to this day.\nI’ve found teaching something you know well, even to just one or two people, is a great way to build your confidence. It is like the Instagram situation in reverse, you see other people are hungry for what you have."
  },
  {
    "objectID": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#imposter-syndrome-can-be-an-extreme-sport",
    "href": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#imposter-syndrome-can-be-an-extreme-sport",
    "title": "Finding confidence in your PhD and overcoming imposter syndrome",
    "section": "Imposter syndrome can be an extreme sport",
    "text": "Imposter syndrome can be an extreme sport\nI love extreme sports like surfing, mountain biking, rock climbing and writing.\nWriting is an extreme sport?\nI forget where I read it, but someone said that writing novels is an extreme, adrenalin fueled, sport. An author spends years putting together a novel before it is published. Every time they have little idea how it will be received, they may get critical acclaim, bomb reviews, or worse still, be totally ignored.\nI think writing papers can be the same too. I never really now if a paper will fly through review, or totally flunk. Imposter syndrome puts the fear in me. And I have learned to love running straight into it.\n\nI have been helped to overcome imposter syndrome by the realization that confronting fears in any form is a kind of sport\n\nThis might seem odd to you, but hey I’m a Dad, I don’t get as much time surfing and climbing as I used too, so writing is my way to get a rush now.\nPart of this realization is that the consequences of mistakes usually aren’t that high. I’m fortunate (or getting old!) to have the experience of making mistakes and surviving many times. Usually the outcome is just being slightly embarrassed, but then you correct them and nothing else happens. Unlike extreme sports athletes, we don’t get physical wounds from errors in equations, hidden flaws in experimental designs or missing references.\nThese blogs, especially the personal ones that make me feel vulnerable, are another way I practice ‘extreme’ writing."
  },
  {
    "objectID": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#experimenting-with-your-career-and-learning-from-mistakes",
    "href": "posts/2018-12-17-finding-confidence-in-your-phd/index.html#experimenting-with-your-career-and-learning-from-mistakes",
    "title": "Finding confidence in your PhD and overcoming imposter syndrome",
    "section": "Experimenting with your career and learning from mistakes",
    "text": "Experimenting with your career and learning from mistakes\nThe final tool that’s helped me with imposter syndrome is seeing everything as a series of experiments.\nSome people say we should embrace failure as a way of learning. I prefer to see failure as part of experimentation.\nI recognized that the change in my job from being an postdoc to a supervisor meant this year would be a big year of learning. I reconciled with myself that there would undoubtedly be mistakes, and while I would try to avoid them, I would learn from those that I did make.\nSo next time you hesitate because of the risk of failure, try looking at the challenge as an experiment. Ask yourself, how might I run this experiment to maximize the learning, but minimise the costs of failure?\nFor instance, I recently submitted a paper on some eco-stats that were pretty new to me (and ecologists). But I posted the manuscript on a pre-print server and sought pre-review feedback before submitting it. I made some helpful corrections that way.\nI’ll leave you with a story from a friend. My friend met someone who was selling a book series about how to raise your kids. The salesman made a convincing sales pitch to my friend about how the books would direct her to raise smart and well balanced children. But at the end my friend said she didn’t want the books. The salesman asked why and my friend said:\n\n“These books would take the trial and error out of parenting. My children are my experiments, I don’t want to lose that joy.”"
  },
  {
    "objectID": "posts/2018-03-29-glow-postdoc-positions/index.html",
    "href": "posts/2018-03-29-glow-postdoc-positions/index.html",
    "title": "Four new postdoc and science communicator positions to work on the global health of wetlands",
    "section": "",
    "text": "Applications for these jobs have now closed, but I have kept the post here for reference\nGLOW is a program developing and applying globally applicable assessments of coastal wetland health to drive actions and investments towards protection, restoration and improvement of wetlands worldwide. We are hiring three research fellows in ecology, statistical modelling and software engineering, and a science communicator, to work on this project.\nSee below to apply."
  },
  {
    "objectID": "posts/2018-03-29-glow-postdoc-positions/index.html#program-overview",
    "href": "posts/2018-03-29-glow-postdoc-positions/index.html#program-overview",
    "title": "Four new postdoc and science communicator positions to work on the global health of wetlands",
    "section": "Program overview",
    "text": "Program overview\nCoastal wetlands – mangroves, seagrass, saltmarsh and estuaries – are valuable ecosystems with high biodiversity. Long favoured as sites for human settlements, they provide us with safe ports, fisheries resources, and access to fresh water. They are critically important in the production of seafood and they assist in protecting coastlines from extreme weather events, now and as sea levels rise.\n\nHowever as the world’s coastal cities continue to grow and industrialise, many coastal wetlands are facing major declines in ecosystem health. We therefore propose to shine a spotlight on where the problems are, what needs to be done to protect what we have, and where rehabilitation will be most effective.\n​To achieve this, we are assessing coastal wetland health around the world, to:\n-highlight the most urgent issues facing coastal wetlands -guide targeted investment and practical actions towards protection and rehabilitation\nWe will:\n-develop and apply globally applicable measures of coastal wetland ecosystem health, -identify locations most in need of protection and rehabilitation, as a guide to where investment in conservation actions can be most effective, -build web-based scenario models to help conservation-focussed projects identify cost effective options for protection and rehabilitation.\nWe are kicking off the project at six trial locations: Portugal, South Africa, India, China, Australia, Vanuatu. And then expanding to the rest of the world’s coastal wetlands. Positions available\nTo achieve the Program’s objectives, Griffith University is recruiting a science team capable of solving significant marine science challenges.\nThis is an opportunity to be part of a major initiative promoting the health of key ecosystems – mangroves, saltmarsh, seagrass and estuaries – and to advance the science of urbanisation, multiple stressors, climate responses and automated monitoring. Researchers will work in a supportive team with complementary skills in ecology, statistical modelling, big data, automation and science communication.\nThis four-year, internationally funded program is based at Griffith University’s largest campus on the Gold Coast, southeast Queensland, Australia. This is a hub for marine science, in a city surrounded by surf beaches, estuarine wetlands and montane rainforest, and with easy access to two international airports. ​ ## Positions available:\nMarine Ecologist\nStatistical Ecologist\nScience Communicator\nSoftware Engineer\nUse individual links above for more information. For enquiries, contact Program Director, Professor Rod Connolly (r.connolly@griffith.edu.au), or Research Leader, Dr Chris J Brown (chris.brown@griffith.edu.au)."
  },
  {
    "objectID": "posts/2018-10-01-how-I-became-a-scicomm/index.html",
    "href": "posts/2018-10-01-how-I-became-a-scicomm/index.html",
    "title": "How I became a science communicator without realizing it",
    "section": "",
    "text": "Would you consider yourself to be a science communicator, that is not just writing papers but doing other activities like doing interviews on radio?\n\n\n\nPhoto: Tom Rayner\n\n\nThis story is about how I became a science communicator, without ever intending too.\nThis blog is based on a talk I gave for the launch of the Australian Rivers Institute’s new science magazine.\nI used to think scicomm was doing interviews for print, radio and TV to try and reach general public. I definitely didn’t want to do interviews growing up. I wanted to be a marine biologist and help ocean ecosystems. Looking back though, it turns out I was always a keen science communicator.\nIn high school I took a creative writing course, as a kind of break from all the science and maths I was doing. My peers in the course were writing beautiful poems about love, drinking and drugs (we were 16!). I was the nerdy kid imagining I was a journalist for Scientific American, writing non-fiction magazine articles about cool science.\nI never got very good marks in writing, but my creative writing teacher (Karen Clark) said something that has stuck in my head all these years. “keep practicising writing Chris, you will get good at it” (or something like that, it was a few years ago).\nWhen I went on to do my PhD at UQ, writing become about papers and very technical.\nDuring my PhD I was invited by Ben Halpern to attend training on science communications at the University of California, Santa Barbara.\nIt was there I learned about Nancy Baron’s work “Escape from the Ivory Tower”. I became convinced that it was crucial for us to escape the ivory tower if science was to inform better policies and decisions about the environment. I thought I should try to do more scicomm.\nThe course was a lot about media interviews. I did a mock interview with an actual journalist from Scientific American, and failed badly. I felt that if this was what scicomm was about, it definitely wasn’t for me. At the course they also taught us about Twitter and I became interested in how we could connect with other people through social media.\nI started tweeting in my first postdoc. But more importantly, I started seeing a lot of blogs on twitter. Now as a young postdoc at UQ I felt my emerging identity as an independent thinker and scientist was lost in UQ’s academic machine.\nI realised that starting my own blog was a way I could present the identity I wanted to present to the world with full control. And share my own thoughts. So I started blogging.\nAt the same time I was still trying to escape from the ivory tower and the opportunity arose to do an interview for some North Queensland papers on my new study. When I arrived at the office next data the Uni’s media person had posted the article on the message board. Turns out the newspaper had also asked some local fisherman what they thought of my study. They had printed a quote in large font:\n\n“These guys have no brains”\n\nYep, interviews and scicomm weren’t for me.\nBut I kept blogging.\nAnd my blogs were being read. I would meet people overseas at workshops and they would say ‘hey I read your blog’. That was exciting. But I was really only reaching other scientists.\nBy the time I came to ARI I was becoming addicted to blogging. So much so that now I write about a PhD thesis worth of words in blogs every 18 months.\nI hope one day I bump into my creative writing teacher on the street so I can tell her “hey, I practiced and I became a Writer!”\nThings changed when I came to the Australian Rivers Institute and I started to identify as a science communicator. I think it is indicative of the wonderful supportive culture we have at ARI. The institute is great at supporting the career development of its ECRs.\nAt ARI I set up the institute’s blog and twitter feed (where I also do 10s of blogs per year and edit many more). I would help my colleagues write blogs about their science papers in a more accessible form. And I found I really enjoyed it.\nI also learned there never was an ivory tower at ARI. Everyone was doing such great applied work, such as that the work Stuart Bunn and Michele Burford do with the National Environmental Science Program. Under this funding they are working directly with stakeholders to set science priorities and do the research that was needed.\nThat really changed my perspective on scicomm, it wasn’t just about interviews with journalists, it could be more targeted, directly speaking to the people that needed the research the most.\nThen Liz O’Brien from Griffith’s Research Office encouraged me to apply for a Tall Young Poppy Award in science media and communication and I won one. The recognition really helped me to start to identify as a communicator.\nSo then we went further and set up the ARI Scicomm cadets program. So we are currently training four of our ECRs in science communications skills, everything from tweeting to engaging with stakeholders. And they are running some amazing activities like Man’s international Cyanobacteria Twitter conference that is coming up next month.\nThen Prof Rod Connolly invited me to join an application for funding that has become the Global Wetlands Project. We hired a full-time science communicator. He’s here today – Tom Rayner – and is doing some amazing things that more strategic than I ever dreamed scicomm could be. We have integrated scicomm into our research from day 1, and have a strategy for everything from training ECRs to how to facilitate a two-way dialogue between researchers and stakeholders.\nTom and I are currently working on how we will monitor the project’s impact with stakeholders, so we can map the project’s pathways to impact over its 4 year duration.\nWith all of this interest in scicomm, I am very pleased to be at ARI as a researcher and science communicator for the launch of ARI’s magazine.\nThe magazine for me represents much of what ARI is about. It is showing to the world that not all scientists are stuck in the ivory tower.\nIt gives our early career researchers a voice so they can start to create their own research identity.\nIt is about reaching out to stakeholders and starting a conversation that will hopefully lead to better more useful science and more effective decision about our environment. People are less likely to raise comments like ‘they have no brains’ if they’ve had the chance to contribute to the science from day one.\nSo now you’ve heard my story, I’ll ask you again, are you a science communicator?"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html",
    "title": "Data Wrangling using R",
    "section": "",
    "text": "Data Wrangling Using R"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#introduction",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#introduction",
    "title": "Data Wrangling using R",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\nWhat is data wrangling?Wikipedia defines it as “Data wrangling or data wrangling is loosely the process of manually converting or mapping data from one ‘raw’ form into another format that allows for more convenient consumption of the data with the help of semi-automated tools”. Have you ever been faced with a dataset that was in the wrong format for the statistical package you wanted to use? Well data-wrangling is about solving that problem.If you have ever had to deal with large data-sets you may realise that data wrangling can take a considerable amount of time and skill with spreasheets programs like excel. Data wrangling is also dangerous for your analysis- if you stuff something up, like accidentally deleting some rows of data, it can affect all your results and the problem can be hard to detect.\n\n\n\nThere are better ways of wrangling data than with spreadsheets, and one way is with R.\n\n\nR is fast becoming the most powerful and flexible programming package for environmental data analysis. It is also totally free. R is a powerful language for data wrangling because1. It is relatively fast to process commands2. You can create repeatable scripts3. You can trace errors back to their source3. You can share your wrangling scripts with other people4. You can conveniently search large databases for errors5. Having your data in R opens up a huge array of cutting edge analysis tools.\n\n\nA core principle of science is repeatability. Creating and sharing your wrangling scripts helps you to fulfil the important principle of repeatability. It makes you a better scientist and can also make you a more popular scientist: a well thought out script for a common wrangling problem may be widely used by others. In fact, these days it is best practice to include your scripts with publications.Most statistical methods in R require your data is input in a certain ‘tidy’ format. This course will cover how to use R to easily convert your data into a ‘tidy’ format. Steps include restructuring existing datasets and combining different data-sets. We will also create some data summaries and plots, which are easy once you have ‘tidy’ data. The course will be useful for people who want to explore, analyse and visualise their own field and experimental data in R. The skills covered are also a useful precusor for management of very large datasets in R.\n\n\n\n\nThe main principles I hope you will learn are\n\n\n\nData wrangling in R is safe, fast, reliable and repeatable\n\n\nCoding aesthetics for readable and repeatable code\n\n\n\nTo download the data you need click here\n\n\nThe download also includes a portable html of this course.\n\n\nLet’s begin!"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#basic-calculations",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#basic-calculations",
    "title": "Data Wrangling using R",
    "section": "\nBasic calculations\n",
    "text": "Basic calculations\n\n\nI am going to assume you are using RStudio. However, you can also use the R app and a text editor if you prefer.Open up RStudio. R is a programming language, which means there are very few commands available in the menus, rather you have to type in commands into the ‘Console’ (notice there is a window called ‘Console’). The hard part about learning to use R is knowning what the commands are that you want and how to structure the commands in a way R understands. RStudio does however, ease the transition to programming by providing you with some drop down menus.Let’s get started with R, doing some basic calculations in the console. In the console type\n\n10 - 1\n\nAnd the console returns an answer (9). You have just asked the R program to compute this (easy) sum. Look carefully at the console and note that each line starts with a &gt; this means the console is ready to take input. If you just type 10 - and hit enter, you will see the next line begins with a +, meaning the console is waiting for you to finish this equation. Type any number and hit return and you will get the answer followed by a &gt;.You can also ask the console for help, for instance try:\n\nhelp('+')\n\nNotice a new ‘Help’ window opens up, with details of how to use arithmetic operators in R. These help files are very useful, but also tend to contain a lot of technical jargon. In learning to use R, you will begin to understand this jargon, making it much easier for you to learn more in the future. But to start off, if things don’t make sense at first reading, try web searches, referring to a beginners book or just experiment with code in different ways to see what happens.R contains most functions and constants for maths you can think of, for instance try:\n\nsqrt(9)\n3^2\nexp(1)\npi"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#storing-numbers-as-variables",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#storing-numbers-as-variables",
    "title": "Data Wrangling using R",
    "section": "\nStoring numbers as variables\n",
    "text": "Storing numbers as variables\n\n\nIt is well and good to be able to do calculations in R, but what if we want to save our number and reuse it later?That is what variables are for. Try typing this into your console:\n\nx &lt;- 3^2\n\nNote that the console returns absolutely nothing, but it is ready for more input (&gt;). Try typing x and the console reminds us that three squared in fact equals 9. To run through it point by point, the command we are giving says: compute 3^2 and assign (the assign command is this &lt;-) the answer to x. The spaces around the assign don’t matter, you can leave them out. However, I prefer to have the spaces because it makes the code more readable. You may also see some people use = instead of &lt;-. Both are ok, but for technical reasons that will be explained later, it is better to use &lt;- for assigning values to variables.If we want R to return the answer when the calculation is computed then enclose the sum in brackets like this (x &lt;- 3^2).Now, what if we try to recall a variable that does not exist? For instance type y. R reminds us, in red font, that we never created a variable called y.Now we have created x we can use it in algebraic equations. For instance try z &lt;- x*3 and you will see we get variable z that equals 27.We can also assign words to variables:\n\nv &lt;- 'today'\n\nNote that we enclosed our word in inverted commas. If we didn’t do this, R would try to assign the value of the variable today to v.A few tips on naming variables. You can name variables whatever you want (e.g. you could use nine instead of x), but there are a few rules: (1) you can’t use special characters in variable names (other than . and _) and (2) you must start a variable name with a letter. It is also a good idea to avoid naming variables as constants or functions that already exist in R, like pi, e, help."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#vectors-variables-with-more-than-one-number",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#vectors-variables-with-more-than-one-number",
    "title": "Data Wrangling using R",
    "section": "\nVectors: variables with more than one number\n",
    "text": "Vectors: variables with more than one number\n\n\nSo far our variables only have one number. Note that when we recall a value, e.g. x the console prints [1] 9. The [1] means that the first number stored in x is 9. We can store more than one number or word in a variable using the c() function:\n\n x &lt;- c(22, 23, 24)\nx\nv &lt;- c('yesterday', 'today', 'tomorrow')\n\nWhich creates variables each with three values. Note again I use spaces purely for clarity, they are not essential.We can index variables to access certain numbers using []\n\nx[1]\nv[2]\n\nreturns the first value of x and the second value of v. We can also use indices to recall multiple values at once. For instance try\n\nx[1:3]\nx[]\nx[c(1,3)]\n\nDoes the output make sense?"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#logical-indexing",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#logical-indexing",
    "title": "Data Wrangling using R",
    "section": "\nLogical indexing\n",
    "text": "Logical indexing\n\n\nWe can also use logicals (TRUE/FALSE) for indexing. To see how this works, let’s pose R some logical questions\n\n42 == 54\n42 == 7*6\nx[1] &gt; 10\nx[1] != 22\n\nThe relational operators used here ask questions such as 42 == 52 means ‘does 42 equal 54?’. To see more of them type help(‘==’). You will see they return TRUE or FALSE depending on whether the relation is true or false.Logicals are very useful because we can use them for indexing. For instance\n\nx&gt;23\n\nreturns a vector of T/F, telling us that the first two values of x are not greater than 23 but the last value is. We can store this logical vector and use it to return values of x that are greater than 23:\n\nix &lt;- x&gt;23\nx[ix]\n\nWe can also apply this idea to words\n\niv &lt;- v == 'today'\nv[iv]\n\nThere are other specialised functions if you want to do partial searches for words (like grep()), but we won’t cover those in this basic course.One final thing, if you don’t like the logicals, you can also use the which() function, which turns logicals into indices\n\nix &lt;- which(x&gt;23)\nix # the third value of x is greater than 23\nx[ix] #the third value of x is 24."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#scripts",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#scripts",
    "title": "Data Wrangling using R",
    "section": "\nScripts\n",
    "text": "Scripts\n\n\nSo far we have just been typing everything into the console. However, we might want to save our commands for later. To do this go to the menu and select file -&gt; New file -&gt; R Script. It will open a text editor. From now on, type all your code into this editor. To send the code to the console, click on a line, or select a section and hit ctrl-R on windows or cmnd-enter on mac.We can put comments in the script, to remind us what it does. R will not evaluate comments, which are preceeded by an #. For instance, start your script by typing:\n\n# R data wrangling course\n# Your name\n# 23 Jun 2015\n\nI recommend using comments frequently throughout your code. It will make life a lot easier when you come back to a piece of code after a few days, because the comments will remind you what the code does. It will also make it easier to share your code with collaborators and make your code more repeatable, which as I said, is a core principle of science. Another way to make your code more readable is to break up sections of code within your script, for instance here is how I often delineate sections in my code\n\n#-----------------------#\n#   MAJOR HEADING     #\n#-----------------------#\n\n#\n# Minor heading\n#\n\nNow we have a script with something worth saving, you should go to the menu and select ‘file’ -&gt; ‘save as’ to save the script for later. Next time you come back to your analysis, you can just open this file, select everything and hit run. Then you will be back to where you left off last time. Further, you can share your script with other people or publish it with your paper.Don’t forget what I said about repeatability and readability! It can take time to organise your script carefully, but it is worth it. I encourage you to include ample comments and section headers as we progress with the rest of the course.\n\n\n\n\n\nReading in data\n\n\nR really becomes useful when we can read in our own datasets. To start, we are going to read in a dataset of fish surveys, dat.csv. This is a practice data set and is very small (only 21 rows). You might think wrangling this data would be easier in a spreadsheet, like excel. We will develop our wrangling skills on this small dataset then test them on a much larger dataset, which would be tricky to handle in a spreadsheet.The first step is to tell R where on your computer the dataset is stored. In your script type this (but replace the path with that for your own computer)\n\n#\n#DATA IMPORT\n#\nsetwd('/Users/Documents/R data wrangling')\n\nsetwd() stands for ‘set working directory’. You will need to replace the folder path in the ’‘ with the path on your computer. On windows you can right click on the file and select ‘properties’, on mac use ‘get info’, then cut and paste the path. Note that if you cut and paste on a windows computer you will have to replace the &lt;/code&gt; with /. I also have put a code header in here as an example. I won’t do this below, but I suggest you put in code headers for each new section we cover, it will make it easier for you to come back to this script later.We can see the files in this folder by typing list.files(). The first data we will use is called dat.csv and can be read in as a dataframe like this\n\nfishdat &lt;- read.csv('dat.csv', header = TRUE, stringsAsFactors = FALSE)\n\nNotice we used the assign operator again, but this time assigned the input data to the name fishdat.read.csv is function that reads a csv file and stores it in R’s local memory. Functions are simply algorithms that R has stored, which perform a task for you, in this case reading a csv file. Functions always begin with a name followed by (), which contain some arguments. In the case of the read.csv() function, the arguments are the name of the data file (‘dat.csv’), header = TRUE and stringsAsFactors = FALSE. The header argument simply says the first row is a row of variable names (so don’t treat them as measurements). The strings as factors arguments tells R to treat strings (ie words) as strings and not to group them by names. You will see why we do this later. Also notice we use = to specify the function’s arguments. As I said above, it is best to use &lt;- for assigning variable names and save = for function arguments.If you need more help on any function in R type its name prefixed with a question mark, for instance ?read.csv.Now lets look at fishdat. Either type fishdat into the console, or on RStudio, go to the Environment window and double click on ‘fishdat’ under ‘Data’. You will see a table of data like this (I have appended a truncated version here)\n\n\nfishdat is what we call ‘tidy data’. Each row is an observation. Here we have multiple sites, each with five transects. We have five variables site, the names of sites, transect the transect number, fish_abund the abundance of fish at that transect and dN15_SG the ratio of nitrogen stable isotopes of a seagrass sample from that transect (high nitrogen isotope values indicate the influence of pollution).\n\n\n\n\nA note about Excel files\n\n\nIn short, don’t use .xlsx or .xls files for saving data. The problem with .xls and .xlsx files are that they store extra info with the data that makes files larger than neccessary and Excel formats can also unwittingly alter your data!This kind of thing can really mess you up. A stable way to save your data is as a .csv which stands for comma seperated values. These are simply values seperated by commas and rows defined by returns. If you select ‘save as’ in excel, you can choose .csv as one of the options. Try opening the .csv file I have given you using a text editor and you will see it is just words, numbers and commas. Obviously using the csv format means you need to avoid commas in your variable names and values, because all commas will be interpreted as spaces between cells in the table.As an example of the problems with excel files, type some numbers into an excel spreadsheet then click ‘format’ -&gt; ‘cells’ and change the category to ‘time’. Now save it as a .csv and open it with a text editor. Your numbers have changed from what you typed in. Because excel automatically detects number types, if you enter something that looks like dates it might decide to change them all to dates and in doing so alter your data.\n\n\n\n\n\n\nAccessing variables in the dataframe\n\n\nThe rules we learned above for accessing values of a variable also apply to dataframes. Except now we have both rows and columns. Rows and columns are indexed by using a comma as a separator. Try\n\nfishdat[1,3]\nfishdat[,2]\nfishdat[1,]\nfishdat[ ,c(1,3)]\n\nWe can also access variables by their names:\n\nfishdat$site\nfishdat$fish_abund[1:5]\nfishdat[,'transect']\n\n\n\nBasic data summaries\n\n\nWe can get a summary of the data frame by typing:\n\nhead(fishdat) #first 6 rows by default\nhead(fishdat, 10) #first ten rows\n\nUseful if the dataframe is very large!We can also do some statistical summaries. This is handy to check the data is ready in correctly and even to start some analysis. For instance:\n\nnrow(fishdat)\nmean(fishdat$fish_abund)\nsd(fishdat$fish_abund) #standard deviation of abundances\ntable(fishdat$site) #number of transects per site\ntable(fishdat$transect) #number of sites per transect\n\nDid you notice a mistake in fishdat?Yes, R distinguishes between upper and low case, so it is treating sites b and B as different sites. They are meant to be the same. We will find out how to correct this typo in a moment. But first, we need to locate its position.We can locate the indices for certain variables using logical relations, for instance\n\nfishdat$site == 'A'\n\nReturns a vector of TRUE and FALSE, with the same number of rows as the dataframe. TRUE occures where the site is A and FALSE where the site isn’t A. The double equals above is used to ask a question (ie is site A?). If you prefer row indices to T/F, you can enclose the logicals in the which() function:\n\nwhich(fishdat$site == 'A')\n\nSo the first five rows are site A. There are other sorts of logical quesions for instance:\n\nwhich(fishdat$site !='A')\nwhich(fishdat$dN15_SG &gt; 11.99)\nwhich(fishdat$dN15_SG &gt;= 11.99)\n\n\n\nCorrecting mistakes in data frames\n\n\nEarlier we noticed a typo where one transect at site ‘B’ was mistakenly typed as site ‘b’. We should identify which ‘b’ is wrong and correct it.\n\nierror &lt;- which(fishdat$site == 'b') #Find out which row number is wrong\nfishdat$site[ierror]\nfishdat$site[ierror] &lt;- 'B' #Change it to the correct upper case. \n\nWe could also have used the toupper() function to change the case of ‘b’ (or infact the case of every value in site. You might be thinking, ‘I could have easily changed that error in a spreasheet’. Well you could have for a dataframe with 20 observations, but what if the data had 1 million observations?## Saving our corrected data frame Now we have corrected our dataframe, we may want to save it for later. We can do this with write.csv() which is basically the opposite of `read.csv()\n\nwrite.csv(fishdat, 'dat_corrected.csv', row.names = FALSE)\n\nWhich saves a new csv file to our current working directory (have a look in your folder and you should see it there). If you want to know what your current working directory is, type getwd().\n\n\n\n\nBasic plots\n\n\nLets do a few graphical summaries. Try these\n\nhist(fishdat$fish_abund)\nplot(fishdat$fish_abund, fishdat$dN15_SG)\n\nWhich plots a histogram and an xy plot of fish abundances against the nitrogen isotope ratios for seagrass at each transect. Looks like there is a relationship. We can improve our plot slightly by using some extra arguments to plot()\n\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n\n\n\n\nThe pch command changes the symbol type. In fact, a whole host of graphical parameters are available with the par() function (try ?par to see them all). For instance\n\npar(las=1) #plot text the right way up\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n\npar(mfrow = c(1,2)) #plot window with two columns \nhist(fishdat$fish_abund, xlab ='Fish abundance', main ='Histogram')\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15, col ='steelblue')\n\n\n\nLoading packages\n\n\nMany users of R also develop add on packages, which they then share with the world. Today we are going to use some packages that make data wrangling easy. First you need to install them to your computer (you will need a web connection to do this). I will run through package installs on the projector. Today we need the tidyr (for tidying dataframes) and dplyr (‘data pliers’) packages. Once you have installed these, you need to load them for each new R session like this:\n\nlibrary(tidyr)\nlibrary(dplyr)\n\nThe red warning just means some functions in R’s base packages have been replaced with those in dplyr, nothing for us to worry about here.All the operations we will perform in tidyr and dplyr can be done using R’s base packages. However, we are using these special packages because they make data wrangling easy, intuitive and aesthetic. Remember I said that a good script can make you a popular scientist? Well dplyr and tidyr are good examples of that. They do some tasks we could do already in R but makes them easier and faster.\n\n\n\n\nFiltering a dataframe using dplyr\n\n\nWe can filter a dataframe with dplyr to say, just get the rows for one site\n\nfish_A &lt;- filter(fishdat, site == 'A')\nfish_A\n\nWhich says, take the fishdat dataframe and filter it for rows that have site==‘A’. One way this could be useful would be to help us make a plot where the sites are different colours, for instance\n\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abunance', pch = 15, col ='grey')\npoints(fish_A$dN15_SG, fish_A$fish_abund, pch = 15, col = 'red')\n\n\n\n\nHere we have plotted all the points (an easy way to get the axes to scale correctly) and then used points() to add points for site A, but changed their colours. Can you work out how to add the other three sites as different colours?\n\n\n\n\nSummaries using dplyr\n\n\nSummarising data is straightforward with dplyr. What follows is somewhat similar to making a pivot table in excel, but without all the clicking. First we need to specify what we are summarising over, using the group_by() function\n\ngrps &lt;- group_by(fishdat, site)\ngrps\n\ngrps is similar to the orginal data frame, but an extra property has been added ‘Groups:site’. Now we have specified a grouping variable, we can summarise() by sites, for instance the mean values of the nitrogen isotopes and fish abundances\n\nsummarise(grps, mean_dN = mean(dN15_SG)) #just nitrogen\nsummarise(grps, mean_dN = mean(dN15_SG), mean_abund = mean(fish_abund)) #means for nitrogen and fish\n\nHere is what our data summary looks like\n\n## Source: local data frame [4 x 3]\n##\n##   site mean_dN mean_abund\n## 1    A   9.666        6.4\n## 2    B  15.260       18.6\n## 3    C  10.878        4.2\n## 4    D  14.752       19.6\n\nYou could summarise using all sorts of other functions, like sd() or min(). Try it for yourself.Let’s store our new data summary then, use its variables in some plots\n\ndatsum &lt;- summarise(grps, mean_dN = mean(dN15_SG), mean_abund = mean(fish_abund))\n#Make a plot with all variables\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n#Add the means summaries as red points\npoints(datsum$mean_dN, datsum$mean_abund, pch = 16, col ='red')\n\n\n\nAesthetics in multi-line coding\n\n\nNotice that we just did a series of data-wrangling steps. First we identified groups then we created some summaries. We could write this code all on one line like this\n\nsummarise(group_by(fishdat, site), mean_dN = mean(dN15_SG))\n\nWhich will return the same summary as above. But, notice we have nested a function inside a function! Not very aesthetic and difficult for others to follow. One solution would be to do the code over multiple lines, like we did above\n\ngrps &lt;- group_by(fishdat, site)\nsummarise(grps, mean_dN = mean(dN15_SG))\n\nOne clever thing that the creaters of dplyr did was to add data ‘pipes’ to improve the aesthetics of multi-function commands. Pipes look like this %&gt;% and work like this\n\nfishdat %&gt;% group_by(site) %&gt;% summarise(mean_dN = mean(dN15_SG))\n\nPipes take the output of the first function and apply it as the first argument of the second function and so on. Notice that we skip specifying the dataframe in group_by() and summarise() because the data was specified at the start of the pipe. The pipe is like saying, take fishdat pipe it into group_by(), then pipe the result into summarise(). Now our code reads like a sentence ‘take fishdat, do this, then this, then this’. Rather than before which was more like ‘apply this function to the output of another function that was applied to fishdat’.\n\n\n\n\nJoining dataframes\n\n\nOur fish abundances and seagrass isotopes aren’t the end point of our data analysis. We also want to know how they relate to some variables we measured at each site. To see these load\n\nsitedat &lt;- read.csv('Sitesdat.csv', header = TRUE, stringsAsFactors = FALSE)\nsitedat\n\ndplyr has a range of functions for joining dataframes. All are prefixed with join. Here we use the inner_join() to find out about the others look at the help file (?inner_join)\n\ndatnew &lt;- inner_join(fishdat, sitedat, by ='site')\ndatnew\n\nWe have joined fishdat and sitedat by their common variable site. Notice the join automatically replicates distance and depth across all the transects, keeping out dataframe ‘tidy’. One analysis we could do is plot the seagrass isotope ratios by distance (which represents distance from a sewage plant).\n\nplot(datnew$distance, datnew$dN15_SG, xlab ='Distance from sewage', ylab ='Nitrogen isotope ratio of seagrass')\n\n\n\n\nLooks like the pollution source is enriching the nitrogen in the seagrass.As a challenge, can you think of a way of plotting the mean values of dN15_SG at each distance?\n\n\n\n\nTidying messy data\n\n\nWe have yet more data. Our collaborator has helped us out and taken three fish of different sizes from each site (small, medium and large) and run isotope analyses on those. This is great, because it means we can see if fish isotopes follow the same pattern as the seagrass.Let’s look at the data\n\nfishiso &lt;- read.csv('FishIsodat.csv', header = TRUE, stringsAsFactors = FALSE)\nfishiso\n\nIt seems our collaborator was not familiar with the ‘tidy data’ format. Notice that values for the isotope ratios at each site are stored as separate columns. We will need to tidy this data if we are to proceed. Luckily we have the tidyr() package handy and one function gather() will be particularly handy here. Check out the vignettes other useful functions to tidy data)\n\n(isotidy &lt;- gather(fishiso, site, dN_fish, A:D))\n##      size site dN_fish\n## 1   small    A   10.04\n## 2  medium    A   11.21\n## 3   large    A   10.77\n## 4   small    B   16.10\n## 5  medium    B      NA\n## 6   large    B   16.50\n## 7   small    C   12.23\n## 8  medium    C   13.18\n## 9   large    C   13.20\n## 10  small    D   14.93\n## 11 medium    D   14.44\n## 12  large    D   13.98\n\nThe tidy data looks much better. What gather() has done is take the fishiso dataframe, creating new variables site and dN_fish and made them from the values in columns A to D.Notice the NA for the medium fish at site B. Unfortunately, we couldn’t catch a fish of that size at site B. We have encoded the missing value as NA when we entered the data. NA is the correct way to indicate a missing value to R.\n\n\n\n\nSummaries with missing values\n\n\nLet’s create a summary of our new fish isotope ratio data, using our new piping skills.\n\n(fishiso_mn &lt;- isotidy %&gt;% group_by(size) %&gt;% summarise(mean_dN = mean(dN_fish)))\n## Source: local data frame [3 x 2]\n##\n##     size mean_dN\n## 1  large 13.6125\n## 2 medium      NA\n## 3  small 13.3250\n\nWhich says, take isotidy, group it by size then make a summary of mean values for dN_fish. Well we get the means, but not for the size category with the missing value. Why? R’s default setting for taking a mean across a vector with a missing value is the return NA. R does this to make sure we are aware there are missing samples (and therefore, our design is unbalanced). We can change this default behaviour however, and ask R to ignore the NA by adding the argument na.rm = TRUE\n\n(fishiso_mn &lt;- isotidy %&gt;% group_by(size) %&gt;% summarise(mean_dN = mean(dN_fish, na.rm = TRUE)))\n## Source: local data frame [3 x 2]\n##\n##     size  mean_dN\n## 1  large 13.61250\n## 2 medium 12.94333\n## 3  small 13.32500\n\nWe have our mean for the medium fish, but be aware, it was taken from only three sites. Let’s also do some means by sites (across sizes)\n\nsites_iso_mn &lt;- isotidy %&gt;% group_by(site) %&gt;% summarise(mean_dNfish = mean(dN_fish, na.rm = TRUE))\n\n\n\nBringing it all together\n\n\nNow we have combined all our data frames, let’s see if we can plot nitrogen isotope ratios for both fish and seagrass on the same plot, as a function of distance from the sewage plant. First, join the mean values for fish isotope ratios to the site data\n\nsites_means &lt;- inner_join(sitedat, sites_iso_mn, by = 'site')\n\nThen join our new sites dataframe with the summary of fishdat.\n\n(sites_means2 &lt;- inner_join(sites_means, datsum, by = 'site'))\n##   site distance depth mean_dNfish mean_dN mean_abund\n## 1    A        5    10    10.67333   9.666        6.4\n## 2    B       12    11    16.30000  15.260       18.6\n## 3    C        6     4    12.87000  10.878        4.2\n## 4    D       11     3    14.45000  14.752       19.6\n\nNote, you could try doing this all in one line, using pipes.Now lets put it all together in a plot\n\npar(las = 1)\nplot(sites_means2$distance, sites_means2$mean_dN, xlab = 'Distance from sewage plant', ylab = 'N isotope ratio', pch = 16, ylim = c(5, 20), col = 'springgreen3', cex = 1.5, cex.lab = 1.3)\npoints(sites_means2$distance, sites_means2$mean_dNfish, xlab = 'Distance from sewage plant',  ylab = 'N isotope ratio', pch = 16, col ='steelblue1', cex = 1.5)\nlegend('topleft', legend = c('Seagrass', 'Fish'), pch = 16, col =c('springgreen3','steelblue1'), cex = 1.5)\n\n\n\n\nTo explain some new arguments above, ylim sets the y-limits, cex sets the scaling of points and cex.lab resizes the axes labels. The legend() function lets us add a legend to the plot. Note, if you want to try different colours, type colors() to see a full list. The RColorBrewer package is also extremely handy for picking colours. Finally, you should also plot the correlation of mean fish versus mean seagrass isotope ratios. I will leave that to you to work out.\n\n\n\n\nAdvanced tasks\n\n\nIf you have cruised through the first part of the course, have a go with this advanced challenge question. I have given you some extra dataframes to practice your data wrangling skills with. These are much larger, so you will need to use R to find errors in them. The dataframes are: Fish_wghts.csv which records individual weights and sexes for fish caught in 100 different trawls; and Trawl_sites.csv which records for each trawl: its distance from shore and its depth. We want to know how the mean weight of individual fish varies with distance from shore, how the total population biomass varies with distance from shore and if the the different sexes have different weights. Beware: there are two errors in the fish_wghts file that you will need to correct!\n\n\n\n\nConclusion\n\n\nI hope I have convinced you of how useful R can be for data wrangling. R might be hard at first, but the more you use R the easier it will become. If you can use R you will find data management much more straightforward and repeatable. Aim to write aesthetic scripts and you can avoid many head-aches when you realise there are errors in your database- with a neat script you can trace back to the problem and correct it.Data wrangling is a skill that take times to learn. But I believe it is worthwhile. Being good at data wrangling will make you a sought after collaborator and a better scientist."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#reading-in-data",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#reading-in-data",
    "title": "Data Wrangling using R",
    "section": "\nReading in data\n",
    "text": "Reading in data\n\n\nR really becomes useful when we can read in our own datasets. To start, we are going to read in a dataset of fish surveys, dat.csv. This is a practice data set and is very small (only 21 rows). You might think wrangling this data would be easier in a spreadsheet, like excel. We will develop our wrangling skills on this small dataset then test them on a much larger dataset, which would be tricky to handle in a spreadsheet.The first step is to tell R where on your computer the dataset is stored. In your script type this (but replace the path with that for your own computer)\n\n#\n#DATA IMPORT\n#\nsetwd('/Users/Documents/R data wrangling')\n\nsetwd() stands for ‘set working directory’. You will need to replace the folder path in the ’‘ with the path on your computer. On windows you can right click on the file and select ‘properties’, on mac use ‘get info’, then cut and paste the path. Note that if you cut and paste on a windows computer you will have to replace the &lt;/code&gt; with /. I also have put a code header in here as an example. I won’t do this below, but I suggest you put in code headers for each new section we cover, it will make it easier for you to come back to this script later.We can see the files in this folder by typing list.files(). The first data we will use is called dat.csv and can be read in as a dataframe like this\n\nfishdat &lt;- read.csv('dat.csv', header = TRUE, stringsAsFactors = FALSE)\n\nNotice we used the assign operator again, but this time assigned the input data to the name fishdat.read.csv is function that reads a csv file and stores it in R’s local memory. Functions are simply algorithms that R has stored, which perform a task for you, in this case reading a csv file. Functions always begin with a name followed by (), which contain some arguments. In the case of the read.csv() function, the arguments are the name of the data file (‘dat.csv’), header = TRUE and stringsAsFactors = FALSE. The header argument simply says the first row is a row of variable names (so don’t treat them as measurements). The strings as factors arguments tells R to treat strings (ie words) as strings and not to group them by names. You will see why we do this later. Also notice we use = to specify the function’s arguments. As I said above, it is best to use &lt;- for assigning variable names and save = for function arguments.If you need more help on any function in R type its name prefixed with a question mark, for instance ?read.csv.Now lets look at fishdat. Either type fishdat into the console, or on RStudio, go to the Environment window and double click on ‘fishdat’ under ‘Data’. You will see a table of data like this (I have appended a truncated version here)\n\n\nfishdat is what we call ‘tidy data’. Each row is an observation. Here we have multiple sites, each with five transects. We have five variables site, the names of sites, transect the transect number, fish_abund the abundance of fish at that transect and dN15_SG the ratio of nitrogen stable isotopes of a seagrass sample from that transect (high nitrogen isotope values indicate the influence of pollution).\n\n\n\n\nA note about Excel files\n\n\nIn short, don’t use .xlsx or .xls files for saving data. The problem with .xls and .xlsx files are that they store extra info with the data that makes files larger than neccessary and Excel formats can also unwittingly alter your data!This kind of thing can really mess you up. A stable way to save your data is as a .csv which stands for comma seperated values. These are simply values seperated by commas and rows defined by returns. If you select ‘save as’ in excel, you can choose .csv as one of the options. Try opening the .csv file I have given you using a text editor and you will see it is just words, numbers and commas. Obviously using the csv format means you need to avoid commas in your variable names and values, because all commas will be interpreted as spaces between cells in the table.As an example of the problems with excel files, type some numbers into an excel spreadsheet then click ‘format’ -&gt; ‘cells’ and change the category to ‘time’. Now save it as a .csv and open it with a text editor. Your numbers have changed from what you typed in. Because excel automatically detects number types, if you enter something that looks like dates it might decide to change them all to dates and in doing so alter your data.\n\n\n\n\n\n\nAccessing variables in the dataframe\n\n\nThe rules we learned above for accessing values of a variable also apply to dataframes. Except now we have both rows and columns. Rows and columns are indexed by using a comma as a separator. Try\n\nfishdat[1,3]\nfishdat[,2]\nfishdat[1,]\nfishdat[ ,c(1,3)]\n\nWe can also access variables by their names:\n\nfishdat$site\nfishdat$fish_abund[1:5]\nfishdat[,'transect']\n\n\n\nBasic data summaries\n\n\nWe can get a summary of the data frame by typing:\n\nhead(fishdat) #first 6 rows by default\nhead(fishdat, 10) #first ten rows\n\nUseful if the dataframe is very large!We can also do some statistical summaries. This is handy to check the data is ready in correctly and even to start some analysis. For instance:\n\nnrow(fishdat)\nmean(fishdat$fish_abund)\nsd(fishdat$fish_abund) #standard deviation of abundances\ntable(fishdat$site) #number of transects per site\ntable(fishdat$transect) #number of sites per transect\n\nDid you notice a mistake in fishdat?Yes, R distinguishes between upper and low case, so it is treating sites b and B as different sites. They are meant to be the same. We will find out how to correct this typo in a moment. But first, we need to locate its position.We can locate the indices for certain variables using logical relations, for instance\n\nfishdat$site == 'A'\n\nReturns a vector of TRUE and FALSE, with the same number of rows as the dataframe. TRUE occures where the site is A and FALSE where the site isn’t A. The double equals above is used to ask a question (ie is site A?). If you prefer row indices to T/F, you can enclose the logicals in the which() function:\n\nwhich(fishdat$site == 'A')\n\nSo the first five rows are site A. There are other sorts of logical quesions for instance:\n\nwhich(fishdat$site !='A')\nwhich(fishdat$dN15_SG &gt; 11.99)\nwhich(fishdat$dN15_SG &gt;= 11.99)\n\n\n\nCorrecting mistakes in data frames\n\n\nEarlier we noticed a typo where one transect at site ‘B’ was mistakenly typed as site ‘b’. We should identify which ‘b’ is wrong and correct it.\n\nierror &lt;- which(fishdat$site == 'b') #Find out which row number is wrong\nfishdat$site[ierror]\nfishdat$site[ierror] &lt;- 'B' #Change it to the correct upper case. \n\nWe could also have used the toupper() function to change the case of ‘b’ (or infact the case of every value in site. You might be thinking, ‘I could have easily changed that error in a spreasheet’. Well you could have for a dataframe with 20 observations, but what if the data had 1 million observations?## Saving our corrected data frame Now we have corrected our dataframe, we may want to save it for later. We can do this with write.csv() which is basically the opposite of `read.csv()\n\nwrite.csv(fishdat, 'dat_corrected.csv', row.names = FALSE)\n\nWhich saves a new csv file to our current working directory (have a look in your folder and you should see it there). If you want to know what your current working directory is, type getwd().\n\n\n\n\nBasic plots\n\n\nLets do a few graphical summaries. Try these\n\nhist(fishdat$fish_abund)\nplot(fishdat$fish_abund, fishdat$dN15_SG)\n\nWhich plots a histogram and an xy plot of fish abundances against the nitrogen isotope ratios for seagrass at each transect. Looks like there is a relationship. We can improve our plot slightly by using some extra arguments to plot()\n\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n\n\n\n\nThe pch command changes the symbol type. In fact, a whole host of graphical parameters are available with the par() function (try ?par to see them all). For instance\n\npar(las=1) #plot text the right way up\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n\npar(mfrow = c(1,2)) #plot window with two columns \nhist(fishdat$fish_abund, xlab ='Fish abundance', main ='Histogram')\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15, col ='steelblue')\n\n\n\nLoading packages\n\n\nMany users of R also develop add on packages, which they then share with the world. Today we are going to use some packages that make data wrangling easy. First you need to install them to your computer (you will need a web connection to do this). I will run through package installs on the projector. Today we need the tidyr (for tidying dataframes) and dplyr (‘data pliers’) packages. Once you have installed these, you need to load them for each new R session like this:\n\nlibrary(tidyr)\nlibrary(dplyr)\n\nThe red warning just means some functions in R’s base packages have been replaced with those in dplyr, nothing for us to worry about here.All the operations we will perform in tidyr and dplyr can be done using R’s base packages. However, we are using these special packages because they make data wrangling easy, intuitive and aesthetic. Remember I said that a good script can make you a popular scientist? Well dplyr and tidyr are good examples of that. They do some tasks we could do already in R but makes them easier and faster.\n\n\n\n\nFiltering a dataframe using dplyr\n\n\nWe can filter a dataframe with dplyr to say, just get the rows for one site\n\nfish_A &lt;- filter(fishdat, site == 'A')\nfish_A\n\nWhich says, take the fishdat dataframe and filter it for rows that have site==‘A’. One way this could be useful would be to help us make a plot where the sites are different colours, for instance\n\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abunance', pch = 15, col ='grey')\npoints(fish_A$dN15_SG, fish_A$fish_abund, pch = 15, col = 'red')\n\n\n\n\nHere we have plotted all the points (an easy way to get the axes to scale correctly) and then used points() to add points for site A, but changed their colours. Can you work out how to add the other three sites as different colours?\n\n\n\n\nSummaries using dplyr\n\n\nSummarising data is straightforward with dplyr. What follows is somewhat similar to making a pivot table in excel, but without all the clicking. First we need to specify what we are summarising over, using the group_by() function\n\ngrps &lt;- group_by(fishdat, site)\ngrps\n\ngrps is similar to the orginal data frame, but an extra property has been added ‘Groups:site’. Now we have specified a grouping variable, we can summarise() by sites, for instance the mean values of the nitrogen isotopes and fish abundances\n\nsummarise(grps, mean_dN = mean(dN15_SG)) #just nitrogen\nsummarise(grps, mean_dN = mean(dN15_SG), mean_abund = mean(fish_abund)) #means for nitrogen and fish\n\nHere is what our data summary looks like\n\n## Source: local data frame [4 x 3]\n##\n##   site mean_dN mean_abund\n## 1    A   9.666        6.4\n## 2    B  15.260       18.6\n## 3    C  10.878        4.2\n## 4    D  14.752       19.6\n\nYou could summarise using all sorts of other functions, like sd() or min(). Try it for yourself.Let’s store our new data summary then, use its variables in some plots\n\ndatsum &lt;- summarise(grps, mean_dN = mean(dN15_SG), mean_abund = mean(fish_abund))\n#Make a plot with all variables\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n#Add the means summaries as red points\npoints(datsum$mean_dN, datsum$mean_abund, pch = 16, col ='red')\n\n\n\nAesthetics in multi-line coding\n\n\nNotice that we just did a series of data-wrangling steps. First we identified groups then we created some summaries. We could write this code all on one line like this\n\nsummarise(group_by(fishdat, site), mean_dN = mean(dN15_SG))\n\nWhich will return the same summary as above. But, notice we have nested a function inside a function! Not very aesthetic and difficult for others to follow. One solution would be to do the code over multiple lines, like we did above\n\ngrps &lt;- group_by(fishdat, site)\nsummarise(grps, mean_dN = mean(dN15_SG))\n\nOne clever thing that the creaters of dplyr did was to add data ‘pipes’ to improve the aesthetics of multi-function commands. Pipes look like this %&gt;% and work like this\n\nfishdat %&gt;% group_by(site) %&gt;% summarise(mean_dN = mean(dN15_SG))\n\nPipes take the output of the first function and apply it as the first argument of the second function and so on. Notice that we skip specifying the dataframe in group_by() and summarise() because the data was specified at the start of the pipe. The pipe is like saying, take fishdat pipe it into group_by(), then pipe the result into summarise(). Now our code reads like a sentence ‘take fishdat, do this, then this, then this’. Rather than before which was more like ‘apply this function to the output of another function that was applied to fishdat’.\n\n\n\n\nJoining dataframes\n\n\nOur fish abundances and seagrass isotopes aren’t the end point of our data analysis. We also want to know how they relate to some variables we measured at each site. To see these load\n\nsitedat &lt;- read.csv('Sitesdat.csv', header = TRUE, stringsAsFactors = FALSE)\nsitedat\n\ndplyr has a range of functions for joining dataframes. All are prefixed with join. Here we use the inner_join() to find out about the others look at the help file (?inner_join)\n\ndatnew &lt;- inner_join(fishdat, sitedat, by ='site')\ndatnew\n\nWe have joined fishdat and sitedat by their common variable site. Notice the join automatically replicates distance and depth across all the transects, keeping out dataframe ‘tidy’. One analysis we could do is plot the seagrass isotope ratios by distance (which represents distance from a sewage plant).\n\nplot(datnew$distance, datnew$dN15_SG, xlab ='Distance from sewage', ylab ='Nitrogen isotope ratio of seagrass')\n\n\n\n\nLooks like the pollution source is enriching the nitrogen in the seagrass.As a challenge, can you think of a way of plotting the mean values of dN15_SG at each distance?\n\n\n\n\nTidying messy data\n\n\nWe have yet more data. Our collaborator has helped us out and taken three fish of different sizes from each site (small, medium and large) and run isotope analyses on those. This is great, because it means we can see if fish isotopes follow the same pattern as the seagrass.Let’s look at the data\n\nfishiso &lt;- read.csv('FishIsodat.csv', header = TRUE, stringsAsFactors = FALSE)\nfishiso\n\nIt seems our collaborator was not familiar with the ‘tidy data’ format. Notice that values for the isotope ratios at each site are stored as separate columns. We will need to tidy this data if we are to proceed. Luckily we have the tidyr() package handy and one function gather() will be particularly handy here. Check out the vignettes other useful functions to tidy data)\n\n(isotidy &lt;- gather(fishiso, site, dN_fish, A:D))\n##      size site dN_fish\n## 1   small    A   10.04\n## 2  medium    A   11.21\n## 3   large    A   10.77\n## 4   small    B   16.10\n## 5  medium    B      NA\n## 6   large    B   16.50\n## 7   small    C   12.23\n## 8  medium    C   13.18\n## 9   large    C   13.20\n## 10  small    D   14.93\n## 11 medium    D   14.44\n## 12  large    D   13.98\n\nThe tidy data looks much better. What gather() has done is take the fishiso dataframe, creating new variables site and dN_fish and made them from the values in columns A to D.Notice the NA for the medium fish at site B. Unfortunately, we couldn’t catch a fish of that size at site B. We have encoded the missing value as NA when we entered the data. NA is the correct way to indicate a missing value to R.\n\n\n\n\nSummaries with missing values\n\n\nLet’s create a summary of our new fish isotope ratio data, using our new piping skills.\n\n(fishiso_mn &lt;- isotidy %&gt;% group_by(size) %&gt;% summarise(mean_dN = mean(dN_fish)))\n## Source: local data frame [3 x 2]\n##\n##     size mean_dN\n## 1  large 13.6125\n## 2 medium      NA\n## 3  small 13.3250\n\nWhich says, take isotidy, group it by size then make a summary of mean values for dN_fish. Well we get the means, but not for the size category with the missing value. Why? R’s default setting for taking a mean across a vector with a missing value is the return NA. R does this to make sure we are aware there are missing samples (and therefore, our design is unbalanced). We can change this default behaviour however, and ask R to ignore the NA by adding the argument na.rm = TRUE\n\n(fishiso_mn &lt;- isotidy %&gt;% group_by(size) %&gt;% summarise(mean_dN = mean(dN_fish, na.rm = TRUE)))\n## Source: local data frame [3 x 2]\n##\n##     size  mean_dN\n## 1  large 13.61250\n## 2 medium 12.94333\n## 3  small 13.32500\n\nWe have our mean for the medium fish, but be aware, it was taken from only three sites. Let’s also do some means by sites (across sizes)\n\nsites_iso_mn &lt;- isotidy %&gt;% group_by(site) %&gt;% summarise(mean_dNfish = mean(dN_fish, na.rm = TRUE))\n\n\n\nBringing it all together\n\n\nNow we have combined all our data frames, let’s see if we can plot nitrogen isotope ratios for both fish and seagrass on the same plot, as a function of distance from the sewage plant. First, join the mean values for fish isotope ratios to the site data\n\nsites_means &lt;- inner_join(sitedat, sites_iso_mn, by = 'site')\n\nThen join our new sites dataframe with the summary of fishdat.\n\n(sites_means2 &lt;- inner_join(sites_means, datsum, by = 'site'))\n##   site distance depth mean_dNfish mean_dN mean_abund\n## 1    A        5    10    10.67333   9.666        6.4\n## 2    B       12    11    16.30000  15.260       18.6\n## 3    C        6     4    12.87000  10.878        4.2\n## 4    D       11     3    14.45000  14.752       19.6\n\nNote, you could try doing this all in one line, using pipes.Now lets put it all together in a plot\n\npar(las = 1)\nplot(sites_means2$distance, sites_means2$mean_dN, xlab = 'Distance from sewage plant', ylab = 'N isotope ratio', pch = 16, ylim = c(5, 20), col = 'springgreen3', cex = 1.5, cex.lab = 1.3)\npoints(sites_means2$distance, sites_means2$mean_dNfish, xlab = 'Distance from sewage plant',  ylab = 'N isotope ratio', pch = 16, col ='steelblue1', cex = 1.5)\nlegend('topleft', legend = c('Seagrass', 'Fish'), pch = 16, col =c('springgreen3','steelblue1'), cex = 1.5)\n\n\n\n\nTo explain some new arguments above, ylim sets the y-limits, cex sets the scaling of points and cex.lab resizes the axes labels. The legend() function lets us add a legend to the plot. Note, if you want to try different colours, type colors() to see a full list. The RColorBrewer package is also extremely handy for picking colours. Finally, you should also plot the correlation of mean fish versus mean seagrass isotope ratios. I will leave that to you to work out.\n\n\n\n\nAdvanced tasks\n\n\nIf you have cruised through the first part of the course, have a go with this advanced challenge question. I have given you some extra dataframes to practice your data wrangling skills with. These are much larger, so you will need to use R to find errors in them. The dataframes are: Fish_wghts.csv which records individual weights and sexes for fish caught in 100 different trawls; and Trawl_sites.csv which records for each trawl: its distance from shore and its depth. We want to know how the mean weight of individual fish varies with distance from shore, how the total population biomass varies with distance from shore and if the the different sexes have different weights. Beware: there are two errors in the fish_wghts file that you will need to correct!\n\n\n\n\nConclusion\n\n\nI hope I have convinced you of how useful R can be for data wrangling. R might be hard at first, but the more you use R the easier it will become. If you can use R you will find data management much more straightforward and repeatable. Aim to write aesthetic scripts and you can avoid many head-aches when you realise there are errors in your database- with a neat script you can trace back to the problem and correct it.Data wrangling is a skill that take times to learn. But I believe it is worthwhile. Being good at data wrangling will make you a sought after collaborator and a better scientist."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#accessing-variables-in-the-dataframe",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#accessing-variables-in-the-dataframe",
    "title": "Data Wrangling using R",
    "section": "\nAccessing variables in the dataframe\n",
    "text": "Accessing variables in the dataframe\n\n\nThe rules we learned above for accessing values of a variable also apply to dataframes. Except now we have both rows and columns. Rows and columns are indexed by using a comma as a separator. Try\n\nfishdat[1,3]\nfishdat[,2]\nfishdat[1,]\nfishdat[ ,c(1,3)]\n\nWe can also access variables by their names:\n\nfishdat$site\nfishdat$fish_abund[1:5]\nfishdat[,'transect']"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#basic-data-summaries",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#basic-data-summaries",
    "title": "Data Wrangling using R",
    "section": "\nBasic data summaries\n",
    "text": "Basic data summaries\n\n\nWe can get a summary of the data frame by typing:\n\nhead(fishdat) #first 6 rows by default\nhead(fishdat, 10) #first ten rows\n\nUseful if the dataframe is very large!We can also do some statistical summaries. This is handy to check the data is ready in correctly and even to start some analysis. For instance:\n\nnrow(fishdat)\nmean(fishdat$fish_abund)\nsd(fishdat$fish_abund) #standard deviation of abundances\ntable(fishdat$site) #number of transects per site\ntable(fishdat$transect) #number of sites per transect\n\nDid you notice a mistake in fishdat?Yes, R distinguishes between upper and low case, so it is treating sites b and B as different sites. They are meant to be the same. We will find out how to correct this typo in a moment. But first, we need to locate its position.We can locate the indices for certain variables using logical relations, for instance\n\nfishdat$site == 'A'\n\nReturns a vector of TRUE and FALSE, with the same number of rows as the dataframe. TRUE occures where the site is A and FALSE where the site isn’t A. The double equals above is used to ask a question (ie is site A?). If you prefer row indices to T/F, you can enclose the logicals in the which() function:\n\nwhich(fishdat$site == 'A')\n\nSo the first five rows are site A. There are other sorts of logical quesions for instance:\n\nwhich(fishdat$site !='A')\nwhich(fishdat$dN15_SG &gt; 11.99)\nwhich(fishdat$dN15_SG &gt;= 11.99)"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#correcting-mistakes-in-data-frames",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#correcting-mistakes-in-data-frames",
    "title": "Data Wrangling using R",
    "section": "\nCorrecting mistakes in data frames\n",
    "text": "Correcting mistakes in data frames\n\n\nEarlier we noticed a typo where one transect at site ‘B’ was mistakenly typed as site ‘b’. We should identify which ‘b’ is wrong and correct it.\n\nierror &lt;- which(fishdat$site == 'b') #Find out which row number is wrong\nfishdat$site[ierror]\nfishdat$site[ierror] &lt;- 'B' #Change it to the correct upper case. \n\nWe could also have used the toupper() function to change the case of ‘b’ (or infact the case of every value in site. You might be thinking, ‘I could have easily changed that error in a spreasheet’. Well you could have for a dataframe with 20 observations, but what if the data had 1 million observations?## Saving our corrected data frame Now we have corrected our dataframe, we may want to save it for later. We can do this with write.csv() which is basically the opposite of `read.csv()\n\nwrite.csv(fishdat, 'dat_corrected.csv', row.names = FALSE)\n\nWhich saves a new csv file to our current working directory (have a look in your folder and you should see it there). If you want to know what your current working directory is, type getwd()."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#basic-plots",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#basic-plots",
    "title": "Data Wrangling using R",
    "section": "\nBasic plots\n",
    "text": "Basic plots\n\n\nLets do a few graphical summaries. Try these\n\nhist(fishdat$fish_abund)\nplot(fishdat$fish_abund, fishdat$dN15_SG)\n\nWhich plots a histogram and an xy plot of fish abundances against the nitrogen isotope ratios for seagrass at each transect. Looks like there is a relationship. We can improve our plot slightly by using some extra arguments to plot()\n\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n\n\n\n\nThe pch command changes the symbol type. In fact, a whole host of graphical parameters are available with the par() function (try ?par to see them all). For instance\n\npar(las=1) #plot text the right way up\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n\npar(mfrow = c(1,2)) #plot window with two columns \nhist(fishdat$fish_abund, xlab ='Fish abundance', main ='Histogram')\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15, col ='steelblue')"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#loading-packages",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#loading-packages",
    "title": "Data Wrangling using R",
    "section": "\nLoading packages\n",
    "text": "Loading packages\n\n\nMany users of R also develop add on packages, which they then share with the world. Today we are going to use some packages that make data wrangling easy. First you need to install them to your computer (you will need a web connection to do this). I will run through package installs on the projector. Today we need the tidyr (for tidying dataframes) and dplyr (‘data pliers’) packages. Once you have installed these, you need to load them for each new R session like this:\n\nlibrary(tidyr)\nlibrary(dplyr)\n\nThe red warning just means some functions in R’s base packages have been replaced with those in dplyr, nothing for us to worry about here.All the operations we will perform in tidyr and dplyr can be done using R’s base packages. However, we are using these special packages because they make data wrangling easy, intuitive and aesthetic. Remember I said that a good script can make you a popular scientist? Well dplyr and tidyr are good examples of that. They do some tasks we could do already in R but makes them easier and faster."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#filtering-a-dataframe-using-dplyr",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#filtering-a-dataframe-using-dplyr",
    "title": "Data Wrangling using R",
    "section": "\nFiltering a dataframe using dplyr\n",
    "text": "Filtering a dataframe using dplyr\n\n\nWe can filter a dataframe with dplyr to say, just get the rows for one site\n\nfish_A &lt;- filter(fishdat, site == 'A')\nfish_A\n\nWhich says, take the fishdat dataframe and filter it for rows that have site==‘A’. One way this could be useful would be to help us make a plot where the sites are different colours, for instance\n\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abunance', pch = 15, col ='grey')\npoints(fish_A$dN15_SG, fish_A$fish_abund, pch = 15, col = 'red')\n\n\n\n\nHere we have plotted all the points (an easy way to get the axes to scale correctly) and then used points() to add points for site A, but changed their colours. Can you work out how to add the other three sites as different colours?"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#summaries-using-dplyr",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#summaries-using-dplyr",
    "title": "Data Wrangling using R",
    "section": "\nSummaries using dplyr\n",
    "text": "Summaries using dplyr\n\n\nSummarising data is straightforward with dplyr. What follows is somewhat similar to making a pivot table in excel, but without all the clicking. First we need to specify what we are summarising over, using the group_by() function\n\ngrps &lt;- group_by(fishdat, site)\ngrps\n\ngrps is similar to the orginal data frame, but an extra property has been added ‘Groups:site’. Now we have specified a grouping variable, we can summarise() by sites, for instance the mean values of the nitrogen isotopes and fish abundances\n\nsummarise(grps, mean_dN = mean(dN15_SG)) #just nitrogen\nsummarise(grps, mean_dN = mean(dN15_SG), mean_abund = mean(fish_abund)) #means for nitrogen and fish\n\nHere is what our data summary looks like\n\n## Source: local data frame [4 x 3]\n##\n##   site mean_dN mean_abund\n## 1    A   9.666        6.4\n## 2    B  15.260       18.6\n## 3    C  10.878        4.2\n## 4    D  14.752       19.6\n\nYou could summarise using all sorts of other functions, like sd() or min(). Try it for yourself.Let’s store our new data summary then, use its variables in some plots\n\ndatsum &lt;- summarise(grps, mean_dN = mean(dN15_SG), mean_abund = mean(fish_abund))\n#Make a plot with all variables\nplot(fishdat$dN15_SG, fishdat$fish_abund, xlab = 'N Isotope ratio', ylab ='Fish abundance', pch = 15)\n#Add the means summaries as red points\npoints(datsum$mean_dN, datsum$mean_abund, pch = 16, col ='red')"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#aesthetics-in-multi-line-coding",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#aesthetics-in-multi-line-coding",
    "title": "Data Wrangling using R",
    "section": "\nAesthetics in multi-line coding\n",
    "text": "Aesthetics in multi-line coding\n\n\nNotice that we just did a series of data-wrangling steps. First we identified groups then we created some summaries. We could write this code all on one line like this\n\nsummarise(group_by(fishdat, site), mean_dN = mean(dN15_SG))\n\nWhich will return the same summary as above. But, notice we have nested a function inside a function! Not very aesthetic and difficult for others to follow. One solution would be to do the code over multiple lines, like we did above\n\ngrps &lt;- group_by(fishdat, site)\nsummarise(grps, mean_dN = mean(dN15_SG))\n\nOne clever thing that the creaters of dplyr did was to add data ‘pipes’ to improve the aesthetics of multi-function commands. Pipes look like this %&gt;% and work like this\n\nfishdat %&gt;% group_by(site) %&gt;% summarise(mean_dN = mean(dN15_SG))\n\nPipes take the output of the first function and apply it as the first argument of the second function and so on. Notice that we skip specifying the dataframe in group_by() and summarise() because the data was specified at the start of the pipe. The pipe is like saying, take fishdat pipe it into group_by(), then pipe the result into summarise(). Now our code reads like a sentence ‘take fishdat, do this, then this, then this’. Rather than before which was more like ‘apply this function to the output of another function that was applied to fishdat’."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#joining-dataframes",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#joining-dataframes",
    "title": "Data Wrangling using R",
    "section": "\nJoining dataframes\n",
    "text": "Joining dataframes\n\n\nOur fish abundances and seagrass isotopes aren’t the end point of our data analysis. We also want to know how they relate to some variables we measured at each site. To see these load\n\nsitedat &lt;- read.csv('Sitesdat.csv', header = TRUE, stringsAsFactors = FALSE)\nsitedat\n\ndplyr has a range of functions for joining dataframes. All are prefixed with join. Here we use the inner_join() to find out about the others look at the help file (?inner_join)\n\ndatnew &lt;- inner_join(fishdat, sitedat, by ='site')\ndatnew\n\nWe have joined fishdat and sitedat by their common variable site. Notice the join automatically replicates distance and depth across all the transects, keeping out dataframe ‘tidy’. One analysis we could do is plot the seagrass isotope ratios by distance (which represents distance from a sewage plant).\n\nplot(datnew$distance, datnew$dN15_SG, xlab ='Distance from sewage', ylab ='Nitrogen isotope ratio of seagrass')\n\n\n\n\nLooks like the pollution source is enriching the nitrogen in the seagrass.As a challenge, can you think of a way of plotting the mean values of dN15_SG at each distance?"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#tidying-messy-data",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#tidying-messy-data",
    "title": "Data Wrangling using R",
    "section": "\nTidying messy data\n",
    "text": "Tidying messy data\n\n\nWe have yet more data. Our collaborator has helped us out and taken three fish of different sizes from each site (small, medium and large) and run isotope analyses on those. This is great, because it means we can see if fish isotopes follow the same pattern as the seagrass.Let’s look at the data\n\nfishiso &lt;- read.csv('FishIsodat.csv', header = TRUE, stringsAsFactors = FALSE)\nfishiso\n\nIt seems our collaborator was not familiar with the ‘tidy data’ format. Notice that values for the isotope ratios at each site are stored as separate columns. We will need to tidy this data if we are to proceed. Luckily we have the tidyr() package handy and one function gather() will be particularly handy here. Check out the vignettes other useful functions to tidy data)\n\n(isotidy &lt;- gather(fishiso, site, dN_fish, A:D))\n##      size site dN_fish\n## 1   small    A   10.04\n## 2  medium    A   11.21\n## 3   large    A   10.77\n## 4   small    B   16.10\n## 5  medium    B      NA\n## 6   large    B   16.50\n## 7   small    C   12.23\n## 8  medium    C   13.18\n## 9   large    C   13.20\n## 10  small    D   14.93\n## 11 medium    D   14.44\n## 12  large    D   13.98\n\nThe tidy data looks much better. What gather() has done is take the fishiso dataframe, creating new variables site and dN_fish and made them from the values in columns A to D.Notice the NA for the medium fish at site B. Unfortunately, we couldn’t catch a fish of that size at site B. We have encoded the missing value as NA when we entered the data. NA is the correct way to indicate a missing value to R."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#summaries-with-missing-values",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#summaries-with-missing-values",
    "title": "Data Wrangling using R",
    "section": "\nSummaries with missing values\n",
    "text": "Summaries with missing values\n\n\nLet’s create a summary of our new fish isotope ratio data, using our new piping skills.\n\n(fishiso_mn &lt;- isotidy %&gt;% group_by(size) %&gt;% summarise(mean_dN = mean(dN_fish)))\n## Source: local data frame [3 x 2]\n##\n##     size mean_dN\n## 1  large 13.6125\n## 2 medium      NA\n## 3  small 13.3250\n\nWhich says, take isotidy, group it by size then make a summary of mean values for dN_fish. Well we get the means, but not for the size category with the missing value. Why? R’s default setting for taking a mean across a vector with a missing value is the return NA. R does this to make sure we are aware there are missing samples (and therefore, our design is unbalanced). We can change this default behaviour however, and ask R to ignore the NA by adding the argument na.rm = TRUE\n\n(fishiso_mn &lt;- isotidy %&gt;% group_by(size) %&gt;% summarise(mean_dN = mean(dN_fish, na.rm = TRUE)))\n## Source: local data frame [3 x 2]\n##\n##     size  mean_dN\n## 1  large 13.61250\n## 2 medium 12.94333\n## 3  small 13.32500\n\nWe have our mean for the medium fish, but be aware, it was taken from only three sites. Let’s also do some means by sites (across sizes)\n\nsites_iso_mn &lt;- isotidy %&gt;% group_by(site) %&gt;% summarise(mean_dNfish = mean(dN_fish, na.rm = TRUE))"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#bringing-it-all-together",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#bringing-it-all-together",
    "title": "Data Wrangling using R",
    "section": "\nBringing it all together\n",
    "text": "Bringing it all together\n\n\nNow we have combined all our data frames, let’s see if we can plot nitrogen isotope ratios for both fish and seagrass on the same plot, as a function of distance from the sewage plant. First, join the mean values for fish isotope ratios to the site data\n\nsites_means &lt;- inner_join(sitedat, sites_iso_mn, by = 'site')\n\nThen join our new sites dataframe with the summary of fishdat.\n\n(sites_means2 &lt;- inner_join(sites_means, datsum, by = 'site'))\n##   site distance depth mean_dNfish mean_dN mean_abund\n## 1    A        5    10    10.67333   9.666        6.4\n## 2    B       12    11    16.30000  15.260       18.6\n## 3    C        6     4    12.87000  10.878        4.2\n## 4    D       11     3    14.45000  14.752       19.6\n\nNote, you could try doing this all in one line, using pipes.Now lets put it all together in a plot\n\npar(las = 1)\nplot(sites_means2$distance, sites_means2$mean_dN, xlab = 'Distance from sewage plant', ylab = 'N isotope ratio', pch = 16, ylim = c(5, 20), col = 'springgreen3', cex = 1.5, cex.lab = 1.3)\npoints(sites_means2$distance, sites_means2$mean_dNfish, xlab = 'Distance from sewage plant',  ylab = 'N isotope ratio', pch = 16, col ='steelblue1', cex = 1.5)\nlegend('topleft', legend = c('Seagrass', 'Fish'), pch = 16, col =c('springgreen3','steelblue1'), cex = 1.5)\n\n\n\n\nTo explain some new arguments above, ylim sets the y-limits, cex sets the scaling of points and cex.lab resizes the axes labels. The legend() function lets us add a legend to the plot. Note, if you want to try different colours, type colors() to see a full list. The RColorBrewer package is also extremely handy for picking colours. Finally, you should also plot the correlation of mean fish versus mean seagrass isotope ratios. I will leave that to you to work out."
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#advanced-tasks",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#advanced-tasks",
    "title": "Data Wrangling using R",
    "section": "\nAdvanced tasks\n",
    "text": "Advanced tasks\n\n\nIf you have cruised through the first part of the course, have a go with this advanced challenge question. I have given you some extra dataframes to practice your data wrangling skills with. These are much larger, so you will need to use R to find errors in them. The dataframes are: Fish_wghts.csv which records individual weights and sexes for fish caught in 100 different trawls; and Trawl_sites.csv which records for each trawl: its distance from shore and its depth. We want to know how the mean weight of individual fish varies with distance from shore, how the total population biomass varies with distance from shore and if the the different sexes have different weights. Beware: there are two errors in the fish_wghts file that you will need to correct!"
  },
  {
    "objectID": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#conclusion",
    "href": "posts/2015-06-23-R_data_wrangling_course/2015-06-23-R_data_wrangling_course.html#conclusion",
    "title": "Data Wrangling using R",
    "section": "\nConclusion\n",
    "text": "Conclusion\n\n\nI hope I have convinced you of how useful R can be for data wrangling. R might be hard at first, but the more you use R the easier it will become. If you can use R you will find data management much more straightforward and repeatable. Aim to write aesthetic scripts and you can avoid many head-aches when you realise there are errors in your database- with a neat script you can trace back to the problem and correct it.Data wrangling is a skill that take times to learn. But I believe it is worthwhile. Being good at data wrangling will make you a sought after collaborator and a better scientist."
  },
  {
    "objectID": "posts/2021-12-07-experiment-rules-of-thumb/index.html",
    "href": "posts/2021-12-07-experiment-rules-of-thumb/index.html",
    "title": "Tips for the design of ecological experiments",
    "section": "",
    "text": "Are you planning an experiment in the lab or field, or an observational study? Its a good idea to talk to a statistician before you start.\nStudents often ask me for help with statistics after they’ve collected their data. However, it’s common they need help with statistics because of issues with the experimental (or observational) design.\nA stats model can never completely fix design flaws. So below are some tips, based on my experiences helping students analyze their datasets.\nExperimental design is complex because ultimately you are balancing logistical practicalities (like cost of sampling) with with is theoretically optimal. This blog is a starting point, but I recommend reading deeper into the references below.\nFor a more complete discussion of experimental design I recommend Quinn and Keough’s classic book: Experimental Design and Data Analysis for Biologists.\nSee also this recent review of four important logical principles that need to be met in design to infer causality.\nThis is a working document for my students, so please write me (chris.brown@griffith.edu.au) if you have suggestions for improvements on the below.\n\n\nJust quickly, below I’ll refer to:\n‘Experiments’ as manipulative experiments in the lab or field. Field studies in particular may also have some elements of ‘Observational’ studies, so there is a grey-area with:\n‘Observational studies’ as field studies that sample across gradients of natural variation to try and identify effects on a response variable (AKA mensurative experiments).\n‘Environmental gradients’ as gradients in a covariate of interest, e.g. rainfall or fishing pressure. These can occur naturally in the field, or be the result of manipulation in an experiment.\n‘Samples’ are the units of observation.\n‘Response’ is the variable(s) of interest that you hypothesize are affected by the treatments and/or covariates.\n‘Covariates’, ‘predictors’ or ‘explanatory’ variables are the hypothesized drivers of changes in the response. Usually we’d use predictor or explanatory variable in the case of a manipulative experiment, whereas any term is fine in the case of an observational study.\n‘Confounded’ means that you cannot distinguish the effects of two predictors. In a field observational study we could say that two covariates are ‘collinear’.\nAll of the experimental design concepts below also apply to analysis of field surveys or pre-existing datasets (e.g. citizen science data or meta-analyses). In these cases you are limited in the pre-existing ‘treatments’ available to you, so you will need to constrain your interpretation based on experimental design principles.\n\n\n\n\n\nUndoubtedly things will fail. Plants die when they’re not meant to, you can’t visit a site because of a suspicious landholder, your volunteer stuffs up a bunch of measurements, someone turns the power off in the lab… There’s a long list of reasons that your design won’t turn out like you planned.\nSo make sure your design is robust to the loss of some of your samples.\n\n\n\nIdeally you’ll have relatively equal sampling intensities across different treatment levels, or along an environmental gradient of interest (but see below).\nIn the old days having equal sample sizes for each treatment level, a ‘balanced design’ was critical for straightforward stats (e.g. with Type I ANOVA). This rule isn’t so strict with modern GLMs and extensions.\nHowever, it is still important to have sufficient sampling across a gradient or levels of a treatment. You need this to be able to estimate the treatment’s effect relative to background variation.\nIts usually straightforward in laboratory experiments to have equal sample sizes of each treatment level. In observational studies, balancing sampling can be more challenging.\nA common issue is insufficient sampling at all combinations of environmental gradients. As an example, I’ve often worked on analyses of how pollution and fishing pressure affect coastal fish abundance. Unfortunately, these gradients are highly confounded: polluted sites are nearer the coast and tend also to have higher fishing pressure. This means it is impossible to separate the effects of pollution and fishing pressure on fish abundance.\nAn improved observational design would include sites with low pollution and high fishing pressure and vice-versa (if they exist). This would enable us to statistically separate the effects of pollution and fishing, as well as estimate their interaction.\n\n\n\n“Stratify experiments with respect to your knowledge and randomise with respect to your ignorance” - Professor Alan T. James, via Bill Venables.\nYou have two broad options for dealing with background variation. You can try and control them, so they are constant across all samples. For instance, in an experiment you may keep temperature constant, so you can examine the effects of nutrients on algal growth.\nControlled designs have greater power to detect changes of interest. However, they are less general (the results of the algae experiment above only applies at one temperature) and more susceptible to overestimating the effect of a covariate.\nAn alternative option is to randomize your sampling with respect to background sources of variation. Such a design will have reduced power compared to a controlled design, but the effect sizes you estimate will be more general and realistic (typically they are smaller).\nIf you go with randomisation you may want to try increase replication, to address power issues.\n\n\n\nRandom sampling is the basic assumption underlying most stats tests.\nWatch out for sample selection methods that masquerade as random sampling. Ad-hoc sampling, like throwing a quadrat around the intertidal zone, can result in biased inferences.\nFor instance, in a mangrove forest you’d have to be careful to sample both sparse and dense patches of forest. Identifying sample locations beforehand with randomly picked GPS points is one way to address this.\nAnother example is citizen science data, which tends to be heavily biased by accessibility of sites.\nStats can help overcome non-random sampling - you can control statistically for non-random designs if you have some idea of the drivers of sampling (like distance to cities for citsci). But you’ll need to know the drivers well enough to model them.\n\n\n\nPsuedo-replication means your samples aren’t truly independent. The classic example is a design of fish in tanks. If you have 10 tanks and 5 fish in each tank, your replicates are tanks, fish in tanks are psuedo-reps (because they share the same water).\nThis issue can be hidden. For instance, climate change tank experiments often have a single water circulation system per treatment level (e.g. controlling OA or temperature). This means all the replicate tanks on the same system aren’t truly independent samples. Solutions are to have one circulation system per treatment (but expensive!) or run the experiment multiple times to get temporally independent samples.\nIf you have psuedo-replication its not the end of the world. Mixed effects models (e.g. with lme4) are how you model that.\n\n\n\nOnce you’ve identified psuedo-replication you may want to use a blocking design to deal with the issue.\nFor example, say you are applying different nutrient and temperature treatments to see how it affects algae growth. You can only grow 10 samples of algae per week and your target is 50 samples. So you will run the experiment over 5 weeks, doing 10 samples per week.\nIdeally, you will want to run every nutrient and temperature treatment every week, so you can separate the effect of ‘week’ from your treatment effects.\nBut say you only have one incubation chamber for maintaining a constant temperature. So each week can only be one temperature. Then you’ll want to either: run the same temperature over multiple weeks or, run a gradient of temperatures (see below). Within each week you will still run every nutrient level. You will also want to randomize the order that the temperature treatments are applied.\nThis will ensure you can estimate the temperature effect and how it applies across different nutrient levels. Multiple samples across weeks will let you remove (statistically) that nuisance variable.\n\n\n\nEasy answer: maximize replication over measuring ‘everything’ at each sample.\nInitial plans I see from students, especially for field designs, often involve measuring a lot of variables at very few sites. But, such designs end up with have low power and a lot of confounded variables, because you end up with more covariates to test than samples to test them.\nYou have a better chance of getting a useful and interesting result if you focus on just a few major patterns. Then you can sample more sites and get replication numbers up.\nAs an example, imagine you can sample 3 sites per day to do rapid assessments of insect taxa diversity and forest structure, or take 2 days to make a detailed assessment of a single site, including document all plant and insect species. The rapid assessment could get you to 42 sites over 3 weeks, versus only 7 sites for the detailed assessment. 42 sites is getting up towards a sample size that is useful for regression modelling (e.g. with glm), whereas 7 is far from enough.\nOf course the replication level you need depends on how variable the system is, but in my (marine fish biased) experience of analyzing ecological field experiments, ~50 independent sites is usually a minimum.\nI’ll add that if you can easily sample more covariates, go for it. You may want to condition on them to statistically control for them when you do the analysis.\n\n\n\nDeveloping a conceptual model of your system, through talking with experts and reading the literature, can help you identify key predictors and refine your research question. This means you can better target your sampling design.\n\n\n\nClosely related to the above idea, I often see students intensely replicating their designs at very few sites (e.g. doing many days of sampling at a single site, or numerous transects per site). But these are psuedoreps. Your study will be more general if you can cover more sites with less replication at each site.\nAiming for greater replication can also help you obtain a deeper understanding of your study topic. You’ll see more different circumstances which may help you formulate better hypotheses.\n\n\n\nEasy answer: gradient designs are better for prediction, and designing for prediction is the future of ecology.\nBut, replicated designs have more power to detect monotonically increasing responses to the predictor.\nSay we are doing an experiment measuring how fast algae grow in flasks under different nutrient levels. We can fit 30 flasks in the lab. Here’s three ways to assign them to nutrient levels: Gradient (every flask gets a different nutrient level), replicated (just two nutrient levels), or hybrid of gradient and replicated.\n What should you do?\nThe replicated design is more typical of ecological experiments. This design gives you the greatest power to detect an effect of nutrients on algae growth (meaning best chance to find p&lt;0.05 if there is an effect).\nReplicated designs may also be preferred for practical reasons, for instance, it may be expensive or time consuming to have multiple levels of the treatment (e.g. requiring multiple different incubators in a warming experiment).\nBUT, there’s a catch. What if your ecological response shows a humped or saturating relationship with the predictor variable? Then your replicated design implies a straight-line relationship between the two levels, e.g. here are results of a gradient vs replicated design for a curved relationship:\n\nSo your ability to predict the ecological response is very poor with the replicated design.\nThis studies provides a series of analyses with simulated and real data to compare our ability to predict with different designs.\nThe above graph assumes you at equal intervals along the gradient. If you know there are regions of rapid change in your response, you could intensify your sampling around those regions.\nHybrid designs are a compromise on prediction and power and may work well in many cases.\nEcology desperately needs to become more ‘predictive’ for a whole heap of reasons, including being able to inform ecosystem management, predict effects of multiple stressors, and for testing fundamental theories. So I encourage you to consider a gradient or hybrid design.\nWhen designing a gradient, look out for high leverage points, e.g. one temperature extreme 5 degrees greater than any of your other treatments/surveys (with no replication) will have a high influence on your fitted model.\n\n\n\nThe classic example is early ocean acidification experiments that acidified water in experimental treatments to what would be equivalent to atmospheric concentrations of CO2 of 1000ppm (we’re currently at about 420ppm and projected to reach ~500-600ppm by 2050).\nSo treatments should be environmentally relevant. BUT, the increasing prevalence of extreme weather events means that you may want to consider some extreme scenarios, e.g. for heatwaves. In some fields not enough experiments consider the impacts of extreme events. For instance, a gradient design could consider both contemporary as well as projected mid-range and extremes values of the predictor.\n\n\n\nI often hear from students questions like ‘this might be a dumb question but, why do we always do 3 transects per site?’.\nDumb questions are great because they reveal tacit knowledge of experts and hidden assumptions. And sometimes they reveal important assumptions that could overturn or invalidate a study’s findings. Like the emperors new clothes.\nSo don’t be afraid to ask ‘dumb’ questions about experimental design.\n\n\n\n\nDesigning experiments takes careful thought and planning. The skill of design is finding where statistical and logical requirements meet practical realities.\nThe above are some of my tips, but there are of course always exceptions, the best you can do is read the literature, consult people with experience in your field and ask lots of questions.\nIf you have advice to add, or don’t agree with any of the above, feel free to let me know: chris.brown@griffith.edu.au"
  },
  {
    "objectID": "posts/2021-12-07-experiment-rules-of-thumb/index.html#some-definitions",
    "href": "posts/2021-12-07-experiment-rules-of-thumb/index.html#some-definitions",
    "title": "Tips for the design of ecological experiments",
    "section": "",
    "text": "Just quickly, below I’ll refer to:\n‘Experiments’ as manipulative experiments in the lab or field. Field studies in particular may also have some elements of ‘Observational’ studies, so there is a grey-area with:\n‘Observational studies’ as field studies that sample across gradients of natural variation to try and identify effects on a response variable (AKA mensurative experiments).\n‘Environmental gradients’ as gradients in a covariate of interest, e.g. rainfall or fishing pressure. These can occur naturally in the field, or be the result of manipulation in an experiment.\n‘Samples’ are the units of observation.\n‘Response’ is the variable(s) of interest that you hypothesize are affected by the treatments and/or covariates.\n‘Covariates’, ‘predictors’ or ‘explanatory’ variables are the hypothesized drivers of changes in the response. Usually we’d use predictor or explanatory variable in the case of a manipulative experiment, whereas any term is fine in the case of an observational study.\n‘Confounded’ means that you cannot distinguish the effects of two predictors. In a field observational study we could say that two covariates are ‘collinear’.\nAll of the experimental design concepts below also apply to analysis of field surveys or pre-existing datasets (e.g. citizen science data or meta-analyses). In these cases you are limited in the pre-existing ‘treatments’ available to you, so you will need to constrain your interpretation based on experimental design principles."
  },
  {
    "objectID": "posts/2021-12-07-experiment-rules-of-thumb/index.html#tips",
    "href": "posts/2021-12-07-experiment-rules-of-thumb/index.html#tips",
    "title": "Tips for the design of ecological experiments",
    "section": "",
    "text": "Undoubtedly things will fail. Plants die when they’re not meant to, you can’t visit a site because of a suspicious landholder, your volunteer stuffs up a bunch of measurements, someone turns the power off in the lab… There’s a long list of reasons that your design won’t turn out like you planned.\nSo make sure your design is robust to the loss of some of your samples.\n\n\n\nIdeally you’ll have relatively equal sampling intensities across different treatment levels, or along an environmental gradient of interest (but see below).\nIn the old days having equal sample sizes for each treatment level, a ‘balanced design’ was critical for straightforward stats (e.g. with Type I ANOVA). This rule isn’t so strict with modern GLMs and extensions.\nHowever, it is still important to have sufficient sampling across a gradient or levels of a treatment. You need this to be able to estimate the treatment’s effect relative to background variation.\nIts usually straightforward in laboratory experiments to have equal sample sizes of each treatment level. In observational studies, balancing sampling can be more challenging.\nA common issue is insufficient sampling at all combinations of environmental gradients. As an example, I’ve often worked on analyses of how pollution and fishing pressure affect coastal fish abundance. Unfortunately, these gradients are highly confounded: polluted sites are nearer the coast and tend also to have higher fishing pressure. This means it is impossible to separate the effects of pollution and fishing pressure on fish abundance.\nAn improved observational design would include sites with low pollution and high fishing pressure and vice-versa (if they exist). This would enable us to statistically separate the effects of pollution and fishing, as well as estimate their interaction.\n\n\n\n“Stratify experiments with respect to your knowledge and randomise with respect to your ignorance” - Professor Alan T. James, via Bill Venables.\nYou have two broad options for dealing with background variation. You can try and control them, so they are constant across all samples. For instance, in an experiment you may keep temperature constant, so you can examine the effects of nutrients on algal growth.\nControlled designs have greater power to detect changes of interest. However, they are less general (the results of the algae experiment above only applies at one temperature) and more susceptible to overestimating the effect of a covariate.\nAn alternative option is to randomize your sampling with respect to background sources of variation. Such a design will have reduced power compared to a controlled design, but the effect sizes you estimate will be more general and realistic (typically they are smaller).\nIf you go with randomisation you may want to try increase replication, to address power issues.\n\n\n\nRandom sampling is the basic assumption underlying most stats tests.\nWatch out for sample selection methods that masquerade as random sampling. Ad-hoc sampling, like throwing a quadrat around the intertidal zone, can result in biased inferences.\nFor instance, in a mangrove forest you’d have to be careful to sample both sparse and dense patches of forest. Identifying sample locations beforehand with randomly picked GPS points is one way to address this.\nAnother example is citizen science data, which tends to be heavily biased by accessibility of sites.\nStats can help overcome non-random sampling - you can control statistically for non-random designs if you have some idea of the drivers of sampling (like distance to cities for citsci). But you’ll need to know the drivers well enough to model them.\n\n\n\nPsuedo-replication means your samples aren’t truly independent. The classic example is a design of fish in tanks. If you have 10 tanks and 5 fish in each tank, your replicates are tanks, fish in tanks are psuedo-reps (because they share the same water).\nThis issue can be hidden. For instance, climate change tank experiments often have a single water circulation system per treatment level (e.g. controlling OA or temperature). This means all the replicate tanks on the same system aren’t truly independent samples. Solutions are to have one circulation system per treatment (but expensive!) or run the experiment multiple times to get temporally independent samples.\nIf you have psuedo-replication its not the end of the world. Mixed effects models (e.g. with lme4) are how you model that.\n\n\n\nOnce you’ve identified psuedo-replication you may want to use a blocking design to deal with the issue.\nFor example, say you are applying different nutrient and temperature treatments to see how it affects algae growth. You can only grow 10 samples of algae per week and your target is 50 samples. So you will run the experiment over 5 weeks, doing 10 samples per week.\nIdeally, you will want to run every nutrient and temperature treatment every week, so you can separate the effect of ‘week’ from your treatment effects.\nBut say you only have one incubation chamber for maintaining a constant temperature. So each week can only be one temperature. Then you’ll want to either: run the same temperature over multiple weeks or, run a gradient of temperatures (see below). Within each week you will still run every nutrient level. You will also want to randomize the order that the temperature treatments are applied.\nThis will ensure you can estimate the temperature effect and how it applies across different nutrient levels. Multiple samples across weeks will let you remove (statistically) that nuisance variable.\n\n\n\nEasy answer: maximize replication over measuring ‘everything’ at each sample.\nInitial plans I see from students, especially for field designs, often involve measuring a lot of variables at very few sites. But, such designs end up with have low power and a lot of confounded variables, because you end up with more covariates to test than samples to test them.\nYou have a better chance of getting a useful and interesting result if you focus on just a few major patterns. Then you can sample more sites and get replication numbers up.\nAs an example, imagine you can sample 3 sites per day to do rapid assessments of insect taxa diversity and forest structure, or take 2 days to make a detailed assessment of a single site, including document all plant and insect species. The rapid assessment could get you to 42 sites over 3 weeks, versus only 7 sites for the detailed assessment. 42 sites is getting up towards a sample size that is useful for regression modelling (e.g. with glm), whereas 7 is far from enough.\nOf course the replication level you need depends on how variable the system is, but in my (marine fish biased) experience of analyzing ecological field experiments, ~50 independent sites is usually a minimum.\nI’ll add that if you can easily sample more covariates, go for it. You may want to condition on them to statistically control for them when you do the analysis.\n\n\n\nDeveloping a conceptual model of your system, through talking with experts and reading the literature, can help you identify key predictors and refine your research question. This means you can better target your sampling design.\n\n\n\nClosely related to the above idea, I often see students intensely replicating their designs at very few sites (e.g. doing many days of sampling at a single site, or numerous transects per site). But these are psuedoreps. Your study will be more general if you can cover more sites with less replication at each site.\nAiming for greater replication can also help you obtain a deeper understanding of your study topic. You’ll see more different circumstances which may help you formulate better hypotheses.\n\n\n\nEasy answer: gradient designs are better for prediction, and designing for prediction is the future of ecology.\nBut, replicated designs have more power to detect monotonically increasing responses to the predictor.\nSay we are doing an experiment measuring how fast algae grow in flasks under different nutrient levels. We can fit 30 flasks in the lab. Here’s three ways to assign them to nutrient levels: Gradient (every flask gets a different nutrient level), replicated (just two nutrient levels), or hybrid of gradient and replicated.\n What should you do?\nThe replicated design is more typical of ecological experiments. This design gives you the greatest power to detect an effect of nutrients on algae growth (meaning best chance to find p&lt;0.05 if there is an effect).\nReplicated designs may also be preferred for practical reasons, for instance, it may be expensive or time consuming to have multiple levels of the treatment (e.g. requiring multiple different incubators in a warming experiment).\nBUT, there’s a catch. What if your ecological response shows a humped or saturating relationship with the predictor variable? Then your replicated design implies a straight-line relationship between the two levels, e.g. here are results of a gradient vs replicated design for a curved relationship:\n\nSo your ability to predict the ecological response is very poor with the replicated design.\nThis studies provides a series of analyses with simulated and real data to compare our ability to predict with different designs.\nThe above graph assumes you at equal intervals along the gradient. If you know there are regions of rapid change in your response, you could intensify your sampling around those regions.\nHybrid designs are a compromise on prediction and power and may work well in many cases.\nEcology desperately needs to become more ‘predictive’ for a whole heap of reasons, including being able to inform ecosystem management, predict effects of multiple stressors, and for testing fundamental theories. So I encourage you to consider a gradient or hybrid design.\nWhen designing a gradient, look out for high leverage points, e.g. one temperature extreme 5 degrees greater than any of your other treatments/surveys (with no replication) will have a high influence on your fitted model.\n\n\n\nThe classic example is early ocean acidification experiments that acidified water in experimental treatments to what would be equivalent to atmospheric concentrations of CO2 of 1000ppm (we’re currently at about 420ppm and projected to reach ~500-600ppm by 2050).\nSo treatments should be environmentally relevant. BUT, the increasing prevalence of extreme weather events means that you may want to consider some extreme scenarios, e.g. for heatwaves. In some fields not enough experiments consider the impacts of extreme events. For instance, a gradient design could consider both contemporary as well as projected mid-range and extremes values of the predictor.\n\n\n\nI often hear from students questions like ‘this might be a dumb question but, why do we always do 3 transects per site?’.\nDumb questions are great because they reveal tacit knowledge of experts and hidden assumptions. And sometimes they reveal important assumptions that could overturn or invalidate a study’s findings. Like the emperors new clothes.\nSo don’t be afraid to ask ‘dumb’ questions about experimental design."
  },
  {
    "objectID": "posts/2021-12-07-experiment-rules-of-thumb/index.html#summary",
    "href": "posts/2021-12-07-experiment-rules-of-thumb/index.html#summary",
    "title": "Tips for the design of ecological experiments",
    "section": "",
    "text": "Designing experiments takes careful thought and planning. The skill of design is finding where statistical and logical requirements meet practical realities.\nThe above are some of my tips, but there are of course always exceptions, the best you can do is read the literature, consult people with experience in your field and ask lots of questions.\nIf you have advice to add, or don’t agree with any of the above, feel free to let me know: chris.brown@griffith.edu.au"
  },
  {
    "objectID": "posts/2021-06-01-STARS/index.html",
    "href": "posts/2021-06-01-STARS/index.html",
    "title": "A comparison of terra and stars packages",
    "section": "",
    "text": "The other day I wrote a comparison of raster and terra packages. The other newish raster package on the block is stars. So I go through the same code here.\nIf you are starting out with stars, read their manual first.\nBut first some key differences.\nstars is set-up to work with spatio-temporal arrays (terra also has better capacity to work with multilayer rasters than raster did). stars deals with more complex data types than either raster or terra, for instance, stars can handle rotated grids.\nstars works will with sf, and many sf functions have methods for stars objects (like st_bbox and st_transform). In contrast, my current experience of trying to use terra and sf is a bit more awkward. I had to do lots of class conversions of terra objects to sf objects and back again.\nstars isn’t as well documented as raster or terra. Some colleagues told me they’ve had unresolvable issues with stars and reverted to raster. I don’t know the details though.\nThere is a ‘babel’ for stars to raster function names though.\n\n\nData input and plots are quite similar to sf:\nlibrary(stars)\nr &lt;- read_stars(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nplot(r)\n\nst_bbox(r)\n\n##   xmin   ymin   xmax   ymax\n##  82.50 -72.25 181.25  -9.75\nNow let’s crop and reproject it, it looks almost identical to reprojecting a polygon in sf:\n#create an extent object\next2 &lt;- st_bbox(r)\n\n#constrain it in x direction\next2[1] &lt;- 120\next2[3] &lt;- 170\n\nr2 &lt;- st_crop(r, ext2)\n\nr3 &lt;- st_transform(r2, \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\")\n\nplot(r3)\n\n\n\n\nLet’s compare the three packages for speed at projecting data, an often time-consuming task.\nlibrary(microbenchmark)\n\nr_terra &lt;- terra::rast(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nr_raster &lt;- raster::raster(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\n\nrobin_proj &lt;- \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\"\n\ntout &lt;- microbenchmark(\nraster::projectRaster(r_raster, crs=robin_proj),\nterra::project(r_terra, robin_proj),\nst_transform(r, crs = robin_proj),\ntimes = 10\n)\ntout\n\n## Unit: milliseconds\n##                                               expr       min        lq\n##  raster::projectRaster(r_raster, crs = robin_proj) 330.49234 348.60983\n##                terra::project(r_terra, robin_proj)  66.47145  67.78466\n##                  st_transform(r, crs = robin_proj)  19.03904  19.91101\n##       mean    median        uq       max neval\n##  506.93616 505.29937 521.27790 874.32388    10\n##   69.90088  68.87271  69.09739  80.66674    10\n##   23.38595  20.64656  22.88241  42.57163    10\nSo terra was about 8.5 times faster than raster and stars was about 3 times faster than terra (so about 26 times faster than raster).\n\n\n\nThe answer here obviously depends on what packages you want to use. A key one for me is tmap for mapping:\nlibrary(tmap)\ntm_shape(r) +\n  tm_raster()\n Apparently you can coerce stars objects into raster objects, but I couldn’t figure out easily how to do that (with the 10 minutes I had to find out).\n\n\n\nIts a confusing world of raster packages now, with raster, terra and stars all viable options. A decision about your ‘go to’ package needs more exploration than my brief blog and will depend on what types of operations you are doing.\nThe real test will come when analyzing large and complex datasets. The comparison I’ve made here is very shallow, it doesn’t explore the full functionality of terra or stars, which is much deeper than raster. So I’m interested to hear from folk how they go with more sophisticated applications."
  },
  {
    "objectID": "posts/2021-06-01-STARS/index.html#data-reading-and-plotting-in-stars",
    "href": "posts/2021-06-01-STARS/index.html#data-reading-and-plotting-in-stars",
    "title": "A comparison of terra and stars packages",
    "section": "",
    "text": "Data input and plots are quite similar to sf:\nlibrary(stars)\nr &lt;- read_stars(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nplot(r)\n\nst_bbox(r)\n\n##   xmin   ymin   xmax   ymax\n##  82.50 -72.25 181.25  -9.75\nNow let’s crop and reproject it, it looks almost identical to reprojecting a polygon in sf:\n#create an extent object\next2 &lt;- st_bbox(r)\n\n#constrain it in x direction\next2[1] &lt;- 120\next2[3] &lt;- 170\n\nr2 &lt;- st_crop(r, ext2)\n\nr3 &lt;- st_transform(r2, \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\")\n\nplot(r3)"
  },
  {
    "objectID": "posts/2021-06-01-STARS/index.html#how-do-terra-raster-and-stars-compare-for-speed",
    "href": "posts/2021-06-01-STARS/index.html#how-do-terra-raster-and-stars-compare-for-speed",
    "title": "A comparison of terra and stars packages",
    "section": "",
    "text": "Let’s compare the three packages for speed at projecting data, an often time-consuming task.\nlibrary(microbenchmark)\n\nr_terra &lt;- terra::rast(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nr_raster &lt;- raster::raster(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\n\nrobin_proj &lt;- \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\"\n\ntout &lt;- microbenchmark(\nraster::projectRaster(r_raster, crs=robin_proj),\nterra::project(r_terra, robin_proj),\nst_transform(r, crs = robin_proj),\ntimes = 10\n)\ntout\n\n## Unit: milliseconds\n##                                               expr       min        lq\n##  raster::projectRaster(r_raster, crs = robin_proj) 330.49234 348.60983\n##                terra::project(r_terra, robin_proj)  66.47145  67.78466\n##                  st_transform(r, crs = robin_proj)  19.03904  19.91101\n##       mean    median        uq       max neval\n##  506.93616 505.29937 521.27790 874.32388    10\n##   69.90088  68.87271  69.09739  80.66674    10\n##   23.38595  20.64656  22.88241  42.57163    10\nSo terra was about 8.5 times faster than raster and stars was about 3 times faster than terra (so about 26 times faster than raster)."
  },
  {
    "objectID": "posts/2021-06-01-STARS/index.html#will-stars-be-compatible-with-other-packages-i-use",
    "href": "posts/2021-06-01-STARS/index.html#will-stars-be-compatible-with-other-packages-i-use",
    "title": "A comparison of terra and stars packages",
    "section": "",
    "text": "The answer here obviously depends on what packages you want to use. A key one for me is tmap for mapping:\nlibrary(tmap)\ntm_shape(r) +\n  tm_raster()\n Apparently you can coerce stars objects into raster objects, but I couldn’t figure out easily how to do that (with the 10 minutes I had to find out)."
  },
  {
    "objectID": "posts/2021-06-01-STARS/index.html#summary",
    "href": "posts/2021-06-01-STARS/index.html#summary",
    "title": "A comparison of terra and stars packages",
    "section": "",
    "text": "Its a confusing world of raster packages now, with raster, terra and stars all viable options. A decision about your ‘go to’ package needs more exploration than my brief blog and will depend on what types of operations you are doing.\nThe real test will come when analyzing large and complex datasets. The comparison I’ve made here is very shallow, it doesn’t explore the full functionality of terra or stars, which is much deeper than raster. So I’m interested to hear from folk how they go with more sophisticated applications."
  },
  {
    "objectID": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html",
    "href": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html",
    "title": "Vibe coding with AI agents is not for scientists",
    "section": "",
    "text": "Vibe coding is the idea of creating software with large language models (e.g. chat GPT, Claude), using only prompting and no, or very little human written code.\nCo-founder of openAI says of vibe coding: “It’s not really coding - I just see things, say things, run things, and copy-paste things, and it mostly works.”.\nThe problem with vibe coding for scientists is modelling requires precision in coding and statistical logic. Small differences in data wrangling or model formulation can result in logically flawed results.\nMy experience of vibe coding is that it tends to make code that runs, but has mistakes in the critical places. This is very problematic if you do not manually check all the logic."
  },
  {
    "objectID": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#what-is-vibe-coding",
    "href": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#what-is-vibe-coding",
    "title": "Vibe coding with AI agents is not for scientists",
    "section": "What is vibe coding",
    "text": "What is vibe coding\nVibe coding has gained popularity with agentic AI systems, such as Github Copilot, Cline, Roo Code and Claude Code. These are systems that take your initial instructions and then iterate it through creating it and debugging it with minimal or no human intervention.\nAnthropic claims that their latest model can work for hours on software development without need for human intervention. The way it works is that the LLM returns the results of its code to itself, checks those results, then decides on the next action to take.\nAs an ecological modeller I thought these tools could be useful for developing routine R code. Common tasks include data wrangling, building and running various versions of a regression model and then generating plots.\nSo I’ve been experimenting with agentic AI, including with Cline, Roo Code and Github Copilot for ecological statistics."
  },
  {
    "objectID": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#flawed-logic",
    "href": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#flawed-logic",
    "title": "Vibe coding with AI agents is not for scientists",
    "section": "Flawed logic",
    "text": "Flawed logic\nThe clincher from the quote above is “…and it mostly works.”\nI’ve found for statistics the bits that don’t work are the critical science parts. So vibe coding tends to produce code that runs and produces believable plots, but results that are logically flawed.\nHere’s an example of how vibe coding can produce flawed science:\nI’m analysing some turtle monitoring data to calculate mean number of nests per night. The problem is that zeros, nights with no nests, are implicit. Monitoring was done every night for three months, and the field team records nesting events the observe.\nI tried vibe coding to calculate mean number of nests per night. It just calculated the mean number out of the total number of unique dates in the dataset. What we actually need to do first is add zeros, nights without nesting, back into the data first. Otherwise, we get a mean that isn’t comparable across years or sites"
  },
  {
    "objectID": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#over-engineering",
    "href": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#over-engineering",
    "title": "Vibe coding with AI agents is not for scientists",
    "section": "Over-engineering",
    "text": "Over-engineering\nYou might say - so just go back and fix the error. That’s exactly what I did. However, herein lies a problem.\nAI agents tend to totally over-engineer R code, producing scripts that are very robust to programmatic errors, but also very lengthy (see example below). There’s an overwhelming amount of code to go through to find these small but significant mistakes.\nAI has a great breadth of knowledge, so a lengthy vibe coding session will produce a diversity functions and coding logic. You need to be experienced to interpet a lot of what it produces.\nYour typical R user knows enough to analyse their data and make some plots. They are going to be overwhelmed trying to interpret 100s of lines of R code."
  },
  {
    "objectID": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#how-to-use-ai-agents-safely",
    "href": "posts/2025-06-08-vibe-coding-not-for-scientists/index.html#how-to-use-ai-agents-safely",
    "title": "Vibe coding with AI agents is not for scientists",
    "section": "How to use AI agents safely",
    "text": "How to use AI agents safely\nAI agents are still very useful. But for scientific coding they need careful oversight. You can either:\n\nLet it run autonomously, then carefully go back through what its created and review everything\nCheck and review it step-by-step, and redirect it\n\nIf you are getting out of your depth, then I suggest stopping and reviewing the code until you are sure what it does. You can also prompt the agent to add checks in (like checking number of samples matches your expectations).\nRemember that the commercial agents, like Claude code or github copilot, are designed for software engineers. So they write robust R code, but they are also not very good at statistics.\n\nExample of over engineering\nAI Agents love to cover all the bases. This can be nice, and I’ve learnt some new tricks this way. However, its total overkill if you just want to create a few plots. Here’s one example where I was getting copilot to do an multivariate analysis. It started by creating this whole script, complete with custom functions, just to load a few packages:\n\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",  # For data manipulation and visualization\n  \"vegan\",      # For community analysis\n  \"ggplot2\",    # For data visualization\n  \"ggrepel\",    # For non-overlapping text labels\n  \"cluster\",    # For cluster analysis\n  \"knitr\",      # For report generation\n  \"kableExtra\"  # For table formatting in reports\n)\n\n# Function to check and install packages\ninstall_if_missing &lt;- function(pkg) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    cat(paste0(\"Installing package: \", pkg, \"\\n\"))\n    install.packages(pkg, dependencies = TRUE)\n  } else {\n    cat(paste0(\"Package already installed: \", pkg, \"\\n\"))\n  }\n}\n\n# Install missing packages\ncat(\"Checking and installing required packages...\\n\")\nfor (pkg in required_packages) {\n  install_if_missing(pkg)\n}\n\n# Load the packages to verify installation\ncat(\"\\nLoading packages to verify installation...\\n\")\nfor (pkg in required_packages) {\n  cat(paste0(\"Loading package: \", pkg, \"\\n\"))\n  library(pkg, character.only = TRUE)\n}\n\ncat(\"\\nAll required packages are installed and loaded.\\n\")\ncat(\"You can now proceed with running the analysis scripts.\\n\")"
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html",
    "title": "Time sinks for data wrangling",
    "section": "",
    "text": "Estimating the time it will take to complete a project is an important part of being a successful PhD student, researcher or consultant. A major time sink on many analytical projects is data wrangling.\nOften the wrangling can take longer than the analysis.\nHere’s some good questions to ask yourself and your collaborators. They will help you better estimate the time you need for data wrangling."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#who-has-the-data",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#who-has-the-data",
    "title": "Time sinks for data wrangling",
    "section": "Who has the data?",
    "text": "Who has the data?\nIf you are starting a collaboration, find out where the data is stored before starting. Many times collaborators have told me they ‘have’ or ‘can get’ the data. We start and I find out they don’t actually, literally, have the data, they just know someone who has it.\nSo begins a lengthy negotiation about data access with the person that actually has the data. Another consideration in such cases is whether your use is consistent with original ethics approval for the data collection (if it was needed)."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#is-the-data-real",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#is-the-data-real",
    "title": "Time sinks for data wrangling",
    "section": "Is the data real?",
    "text": "Is the data real?\nSeriously, if its not your data, are you sure its real data? Ok this situation is rare (I hope), but it does happen that collaborators share false data."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#how-is-the-data-recorded-and-stored",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#how-is-the-data-recorded-and-stored",
    "title": "Time sinks for data wrangling",
    "section": "How is the data recorded and stored?",
    "text": "How is the data recorded and stored?\nIf collaborating to get data, find out how it is stored and recorded. Is it in pdf tables, is it a single spreadsheet with good meta-data, or is it a series of nested excel sheets with cross-referenced formulas?"
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#has-the-data-been-analyzed-before",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#has-the-data-been-analyzed-before",
    "title": "Time sinks for data wrangling",
    "section": "Has the data been analyzed before?",
    "text": "Has the data been analyzed before?\nIf it has, it is usually already cleaned and easier to work with. Also check what it has been used for. If it has been used just to generate high level summary stats, it might not be clean for detailed analysis.\nIf its been analyzed in R or python before, even better. It is probably clean and R ready."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#how-big-ie-computer-memory-is-the-data",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#how-big-ie-computer-memory-is-the-data",
    "title": "Time sinks for data wrangling",
    "section": "How big (ie computer memory) is the data?",
    "text": "How big (ie computer memory) is the data?\nBigger datasets are harder to clean and error check. Very small data can be checked manually. Larger datasets can easily be checked with a combination of manual checking, graphs and programming.\nVery large dataset (ie bigger than your RAM, like gigabytes) present new problems. R will take a while to chug through processing steps. This can really slow down debugging, especially for complex tasks.\nYou may also have to develop special strategies, like breaking your data into chunks for error checking and debugging of analysis.\nAll in all this means a 500kb file takes about the same amount of time to wrangle as a 100mb file, whereas a 1 gigabyte file might take 10 times longer."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#does-the-project-involve-spatial-data",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#does-the-project-involve-spatial-data",
    "title": "Time sinks for data wrangling",
    "section": "Does the project involve spatial data?",
    "text": "Does the project involve spatial data?\nSpatial data records dependencies between data points (coordinates). This can present new problems. You’ll obviously need to use specialist GIS tools. Processing steps can be much slower, because the algorithms to cope with spatial dependency just take longer. For example, the maths of a spatial join is much more complex (geometry) than that for a regular join of two dataframes on a single variable."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#does-the-project-involve-temporal-data",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#does-the-project-involve-temporal-data",
    "title": "Time sinks for data wrangling",
    "section": "Does the project involve temporal data?",
    "text": "Does the project involve temporal data?\nDates and times are hard to work with. First there can be gaps in a time-series. How will you deal with that? Maybe you need to impute values before analysis?\nSecond, the programming of dates is just hard. I don’t know anyone that likes it. For example, what day is 30 days before the 15th March? Depends on whether its a leap year right? Annoying details like this can really slow you dowe.\nAlso timezones! People always send me data with times, but neglect to tell me what time zone its in (or I forget to ask perhaps)."
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#are-you-joining-multiple-datasets",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#are-you-joining-multiple-datasets",
    "title": "Time sinks for data wrangling",
    "section": "Are you joining multiple datasets?",
    "text": "Are you joining multiple datasets?\nThis is a big one. Joining datasets is a major, major, timesink.\nSo much so, you could basically say my niche in my field is joining different datasets in new ways. We can get new interesting results, but it can be hard (which is why not everyone is doing it).\nFirst, joins present opportunities for errors. If you are doing a simple join on a shared variable (e.g. with dplyr’s inner_join), but the variable is coded inconsistently across dataframes you might loose data. This might be hard to detect in large datasets.\nSecond, joins might be based more loosely on spatial or temporal similarities. These can get fiddly. Say you want to extract wave height data at some diver survey sites. Just use the coordinates right? Well, what if the wave data doesn’t extend all the way to the coast, or the resolution is too ‘blocky’ and it misses some dive sites? Now you have to invent a way to find wave values near to dive sites, ideally only when wave data are missing. It gets fiddly.\nThe same issue happens with joins on times/dates. Maybe rainfall isn’t measured on the exact day you had bird counts. So perhaps now you impute rainfall to the missing day, but what algorithm of the 1000s possible are you going to use for the imputation?"
  },
  {
    "objectID": "posts/2020-05-01-time-sinks-data-wrangling/index.html#summary",
    "href": "posts/2020-05-01-time-sinks-data-wrangling/index.html#summary",
    "title": "Time sinks for data wrangling",
    "section": "Summary",
    "text": "Summary\nSo think carefully about the issues your project’s data might present. If you’re still not sure how long it might take, then try this formula recommended by a friend:\n“Time to write-up a PhD thesis: Guess how long you will need to write up your thesis. Add 3 months. Double it. Drink heavily”"
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "",
    "text": "VScode has many extensions that let you create and run entire workflows via using prompts to a large language model. Its not widely used in the R community yet, but I expect it will be soon. You can create your entire R project, interpret the results and write a draft of your findings without writing any R code.\nMost of these tools are not available (as of writing) in RStudio, or have only limited functionality. So you need to use a different IDE (Integrated Development Environment) to run your R code. Here I’ll explain how to set-up VSCode (a popular IDE) so you can use Cline."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#software-requirements",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#software-requirements",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Software requirements",
    "text": "Software requirements\nTo set up VScode for R and Cline, you’ll need:\n\nR programming language\nVScode text editor\nR extension for VScode\nCline AI assistant extension for VScode\n\nNote that if you computer is controlled centrally by an IT department, you may need to request admin access to install software, or email IT and ask for them to come and help you."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-r",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-r",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Install R",
    "text": "Install R\n\nGo to the official R project website: https://www.r-project.org/\nClick the “download R” link in the Getting Started section\nChoose a CRAN mirror close to your location\nDownload the appropriate R installer for your operating system\nRun the installer and follow the prompts to complete installation"
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#r-packages",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#r-packages",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "R packages",
    "text": "R packages\n\nOpen R or RStudio\nInstall language server install.packages(\"languageserver\")\nInstall httpgd install.packages(\"httpgd\") (this helps improve plots in VScode). NOTE that httpgd seems to often be removed from CRAN, then come back again, I’m not sure why… If you are having trouble you can try install from a different repo, see instructions here: https://community.r-multiverse.org/httpgd"
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-vscode",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-vscode",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Install VScode",
    "text": "Install VScode\n\nGo to the official VScode website: https://code.visualstudio.com/\nClick the big blue “Download” button\nDownload the appropriate VScode installer for your operating system\nRun the installer and follow the prompts\nLaunch VScode once installation is complete"
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-r-extension",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-r-extension",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Install R extension",
    "text": "Install R extension\n\nOpen VScode\nOpen the Extensions view in VScode (click the boxes on left hand side)\nSearch for “R” in the extensions marketplace\nSelect the “R” extension published by REditorSupport\nClick the “Install” button\nRestart VScode after installation if prompted\n\nMore info on vscode and R here"
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#connect-r-and-vscode",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#connect-r-and-vscode",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Connect R and VScode",
    "text": "Connect R and VScode\n\nOpen a new terminal in VScode (Terminal &gt; New Terminal)\nCheck that R is installed by running: R --version\nType R to open the R console in the terminal\nNow open any R script in VS code (File &gt; Open)\nRun some R code to check that VS code can connect to R in the terminal. Use the shortcut Ctrl+Enter/Cmd+Enter or press the play button in the top right of the script editor.\n\nIf R is not found then open extensions (left hand side, boxes icon), filter by ‘enabled’ then click the R extension. Now click the cog icon in the R extension and select ‘settings’ from the dropdown. Search for ‘rpath’. Check that it has the correct path to R on your computer. You can find the path by opening a terminal and typing which R (on mac) or in a windows terminal where R.\nWhile you have the extension settings open search for ‘httgp’ and make sure Plot: Use Httpgd is enabled."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-gen-ai-extensions",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#install-gen-ai-extensions",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Install gen AI extensions",
    "text": "Install gen AI extensions\n\nOpen the Extensions view in VScode (Ctrl+Shift+X)\nSearch for the genAI assistant of your choice. I’m use Roo Code currently. Cline is another popular choice.\nSelect the extension\nClick the “Install” button\nThe extension icon (e.g. a Roo if using Roo Code) should appear in the VScode sidebar"
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#set-up-extension-in-vscode",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#set-up-extension-in-vscode",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Set up extension in VScode",
    "text": "Set up extension in VScode\n\nClick on the extension icon (e.g. a roo for roo code or robot for cline) on the left hand side\nClick the cog (if the settings don’t open automatically)\nSelect your API provider and cut and paste the API key into the box (see next step if you don’t have one)."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#get-an-api-key",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#get-an-api-key",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Get an API key",
    "text": "Get an API key\nAn API key is like a password that allows the AI assistant (e.g. roo code) to send your prompt to a large language model. Your key should be kept private. Usually you’ll have to buy some credits. These allow you to send prompts to the LLM. You’ll be paying per prompt.\nNow you need to choose your large language model provider. I’m currently using OpenRouter and Anthropic, which have a diversity of models for generating text, code and reading images. Do some web searching to find out the latest info on providers and models.\nYou choose depends on what you want to do and your budget. Some providers offer a free tier. You’ll need to web search for the latest info on this.\nYou can get the links to any supported provider from your extension settings, just select the provider and then a link will appear to sign-up for that provider.\nOnce you’ve chosen a provider, create an account and follow their instructions for creating an API key. You will probably also need to buy some credit to use the model.\nNote that if you already have a subscription account, e.g. for chatGPT, you will get need an API key to use Cline. This might be in a different section of your account settings. If you are using a subscription service you are probably paying monthly to use the provider’s web app. We need the API key to be able to use the model directly via code."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#youre-all-set",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#youre-all-set",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "You’re all set",
    "text": "You’re all set\nNow you’re set and ready to start using Cline to code and write text."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#issues-and-tips",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#issues-and-tips",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Issues and tips",
    "text": "Issues and tips\nThis is just a list of issues I’ve had and how I’ve solved them.\nPlotting If your R plots look weird (like tiny font), make sure httpgp is enabled. Go back to steps above and see how to do that.\nViewing data There are various extensions for viewing csv and excel files. It is worth looking into these so that when you do View(dat) in R you get a nice table. Some also allow editing.\nGetting help to install software My computer is somewhat locked down by IT, so getting this set-up was a bit fiddly and required a few requests to IT to install software.\nR markdown There are options in the R extension settings for how to knit markdown. You may need to configure these if you want to knit markdown docs from VScode. If you are having trouble knitting markdown it may mean that the path to pandoc is not set correctly. There is some helpful instructions here\nR terminal crashes If I run too much R code at once (like selecting a big block then running) the terminal tends to crash. Initially I see a little highlighted box saying ‘PTY HOST’. Then I need to close all the terminals (with the bin icon) and start again. Try radian if this is a problem. You can also code run line-by-line or source whole scripts from the terminal (which works fine). I tried debugging this by increasing the buffer but to on avail.\nShortcut keys (on osx) cmd-/ to comment uncomment lines. cmd-shift-p to open the command palette, cmd-b to open the file explorer, cmd-enter to run lines or selection of R code, cmd-shift-c to open terminal in new window, cntrl-shift-` to open a new terminal in vs code."
  },
  {
    "objectID": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#installing-radian-optional",
    "href": "posts/2025-06-12-setting-up-vscode-r-genAI/index.html#installing-radian-optional",
    "title": "Setting up VScode for R and generative AI tools",
    "section": "Installing radian (optional)",
    "text": "Installing radian (optional)\nRadian is a terminal editor that is a bit nicer than the base R one. It does autocomplete in the terminal (like Rstudio does in the console), colours code/brackets etc… and allows multi-line editing in the terminal.\nTo set this up, install radian (you need python to do this). More instructions here.\nThen go to the terminal and find the path where radian is installed (e.g. which radian on mac or where radian on windows).\nNow open your settings in VScode (cmd-,) and search for ‘rterm’ (stands for ‘R Terminal’, don’t change the rpath which we set just before). Add the path to radian to the rterm setting. Also search for the setting ‘R: Bracketed Paste’ and make sure it is enabled."
  },
  {
    "objectID": "posts/2017-09-07-bayesian-kriging-thoughts/index.html",
    "href": "posts/2017-09-07-bayesian-kriging-thoughts/index.html",
    "title": "Some thoughts about Bayesian Kriging in INLA",
    "section": "",
    "text": "I have been playing around with spatial modelling in the R INLA package. This blog just records a few thoughts I have had about using INLA for kriging (spatial interpolation). I am keen to discuss these ideas with others.\nKriging is super useful tool for ‘filling in the gaps’ between sampling sites. e.g. see this map. Handy if you want to make a map, or need to match up two spatial data sets that overlap in extent, but have samples at different locations.\nYou can do kriging the old fashioned way in R or even in ARC GIS. The advantage of using INLA though is that you can use Bayesian inference to do your kriging.\nThis means you can interpolate non-normal error structures (like counts or presence/absence data). You can also include other fixed covariates, like a spatial layer of temperatures. Or you can even include, other random effects. For instance, you could do spatio-temporal kriging if you had time-series at each of your sites.\nFor a simple tutorial on spatial modelling in INLA see here or my blog series on making maps from geotagged photos.\nSorry, there will be no code or figures with this post. I can’t share the data for the example I am working on yet. Will post it in a few months if I can.\nSo here are some thoughts I have had while working with my latest INLA model."
  },
  {
    "objectID": "posts/2017-09-07-bayesian-kriging-thoughts/index.html#grid-scale-for-interpolations",
    "href": "posts/2017-09-07-bayesian-kriging-thoughts/index.html#grid-scale-for-interpolations",
    "title": "Some thoughts about Bayesian Kriging in INLA",
    "section": "Grid scale for interpolations",
    "text": "Grid scale for interpolations\nThe standard advice, that I read somewhere on the INLA page was that the grid size or triangulation you use should have edges that are somewhat smaller than the spatial range parameter (as explained in my blog).\nHowever, I have been finding that tweeking the edge size (e.g. making the cutoff parameer in inla.mesh.2d larger) can be useful for visualising spatial trends at different scales.\nIf you make your grid size much larger than the distance between the closest sites, you get a much better picture of large scale spatial trends.\nSay you have highly clustered sampling and there is a large amount of variability within clusters that swamps large-scale trends. For instance you may be measuring stuff on coral reefs as you sail your boat around. It is likely you will hit up a bunch of sites nearby to each other, then sail off to another reef some distance away then survey another bunch of sites.\nThe end result will be lots of clusters of samples that have large distances between them. Coral reef data are typically highly variable, even at small spatial scales and large-scale (100s km) trends are often quite weak (though real!). Fish surveys are the worst, because fish swim around, often in schools, so often true ecological trends are well hidden under a stack load of observer error.\nIf you interpolate reef data with a very small grid, that matches the scale parameter, you end up with a very patchy looking map. Whereas, if you use a large grid, you get a much smoother looking map that just captures the weaker, but perhaps more important, broadscale trends.\nThis map will hide the considerable local scale variation, so it is worth plotting that as a separate map too.\nEffectively, this is just like doing averages on a small vs large grid.\nI can’t find it right now, but someone did write a guidance tutorial for setting up grids/triangulations for INLA. If you know where it is, let me know and I will post it."
  },
  {
    "objectID": "posts/2017-09-07-bayesian-kriging-thoughts/index.html#smoothing-parameter",
    "href": "posts/2017-09-07-bayesian-kriging-thoughts/index.html#smoothing-parameter",
    "title": "Some thoughts about Bayesian Kriging in INLA",
    "section": "Smoothing parameter",
    "text": "Smoothing parameter\nI hadn’t played around much with the alpha parameter in INLA’s Matern model before. You change it when specifying the matern mesh before fitting your model inla.spde2.matern(mesh, alpha = 1.9).\nLarge values (approaching 2) give smoother trend, whereas values closer to zero will show more of the bumps in a spatial field. Of course, choice of this parameter will interact with how you define your grid.\nIt can make a big difference to your resulting interpolation. I am not sure much advice exists on how to choose it. I would be interested to hear from others what they think.\nTwo ways to choose this parameter would be based on aesthetics of the resulting map (if you are just making a map), or perhaps using cross-validation or information criteria to choose the parameter."
  },
  {
    "objectID": "posts/2017-09-07-bayesian-kriging-thoughts/index.html#playing-with-priors-to-get-shrinkage-to-a-null-hypothesis",
    "href": "posts/2017-09-07-bayesian-kriging-thoughts/index.html#playing-with-priors-to-get-shrinkage-to-a-null-hypothesis",
    "title": "Some thoughts about Bayesian Kriging in INLA",
    "section": "Playing with priors to get shrinkage to a null hypothesis",
    "text": "Playing with priors to get shrinkage to a null hypothesis\nSome of the INLA developers and co recently published a cool paper about choosing priors for hyperparameters. The hyperpriors control how much your spatial surface can vary. They are quite hard to set based on true ‘prior evidence’, because they are abstract parameters.\nSimpson and co give an example of the variance parameter in a single dimensional model, but the same idea applies to spatial gradients too. (I also have a blog about hyperpriors for time-series here).\nIf you make your hyperprior on spatial/temporal variance very peaked near zero with a long tail, then effectively it is like saying there is probably no spatial variance and a small chance of lots of spatial variance.\nThe effect in your model will be the spatial trend ‘shrinks’ toward a flat gradient, unless your data are strong enough to overcome the no gradient prior. This is like having a null hypothesis of no gradients.\ne.g. see the last figure in my blog here where a stronger prior effectively fits a model to a time-series with a slope of zero (ie no temporal trend).\nAn even more useful way of thinking about this is that you can play around with the priors to get the desired level of smoothing in your spatial (or temporal) model.\nI think this idea is pretty radical. We are shifting away from thinking that priors should be unininformative if you have no prior data (like in the bad old days of the Winbugs gamma(0.001, 0.001) prior for variances), and towards a philosophy where some priors can be tuned and (specifically hyperpriors for variance parameters) should be very very informative.\nThis is a pretty powerful idea. In effect an informative prior can encode our null hypothesis and a stronger prior will require stronger trends in the data to ‘break away from’ the null hypothesis. If you really want to show your data contain a powerful trend, then try to smash it with a very informative prior that peaks near zero variance.\nAs a hint of a paper I want to work on soon, imagine you have super variable data that have stack loads of observer error and weak ecological trends (like surveys of schooling fish on coral reefs). You want to estimate the ecological gradient under all that noise. Well you could get prior information on the extent of spatial variation attributable to true ecological variable. You could then use that prior information to inform a variance prior that helps you estimate the latent spatial field for true ecological gradient. In effect, you are removing the observer signal.\nThat’s all for now. Email me or tweet me if you like this, or think I am talking rubbish."
  },
  {
    "objectID": "posts/2017-03-29-locator-to-clipboard/index.html",
    "href": "posts/2017-03-29-locator-to-clipboard/index.html",
    "title": "A fast method to add annotations to a plot",
    "section": "",
    "text": "Making professional looking plots in R can be fiddly. One task that I often spend ages doing is manually finding coordinates to add labels. Wouldn’t it be nice if you could just send the coordinates directly to your text editor?\nI did some searching and found on stackoverflow that you can send R objects to the clipboard. So here is my solution using that trick.\n\n\n\nAdding text to the right position on a plot can be a real hassle. Here I show how to use a simple function to click on a figurea and put coordinates onto your clipboard.\n\n\nYou can get R to send data directly to the clipboard using the pipe command. Below is a little function I wrote that takes coordinates from locator() and sends them to your clipboard. Then you can just hit cmd-v to paste them into your text editor (nb I understand this may need some slight modifications to work on linux or windows, I use OSX):\nloccopy &lt;- function(n, digits = 2){\n    data &lt;- locator(n)\n    data &lt;- round(cbind(data$x, data$y), digits)\n    clip &lt;- pipe(\"pbcopy\", \"w\")\n    write.table(data, file = clip, col.names = F, row.names = F)\n    close(clip)\n}\nLet’s test it out:\nset.seed(42)\nplot(runif(100))\nloccopy(1)\nNow hit cmd-v (or equivalent on your OS).\n69.23 0.84\nLet’s add a label using our fast method for coordinates:\ntext(69.23, 0.84, \"Unusual data point\", pos =4, offset = 0)\nThe pos=4 and offset=0 ensures that the text goes directly to the right of our coordinates.\nThat’s it. Hope it helps speed up your workflow."
  },
  {
    "objectID": "posts/2017-03-29-locator-to-clipboard/index.html#a-fast-method-to-add-annotations-to-a-plot",
    "href": "posts/2017-03-29-locator-to-clipboard/index.html#a-fast-method-to-add-annotations-to-a-plot",
    "title": "A fast method to add annotations to a plot",
    "section": "",
    "text": "Making professional looking plots in R can be fiddly. One task that I often spend ages doing is manually finding coordinates to add labels. Wouldn’t it be nice if you could just send the coordinates directly to your text editor?\nI did some searching and found on stackoverflow that you can send R objects to the clipboard. So here is my solution using that trick.\n\n\n\nAdding text to the right position on a plot can be a real hassle. Here I show how to use a simple function to click on a figurea and put coordinates onto your clipboard.\n\n\nYou can get R to send data directly to the clipboard using the pipe command. Below is a little function I wrote that takes coordinates from locator() and sends them to your clipboard. Then you can just hit cmd-v to paste them into your text editor (nb I understand this may need some slight modifications to work on linux or windows, I use OSX):\nloccopy &lt;- function(n, digits = 2){\n    data &lt;- locator(n)\n    data &lt;- round(cbind(data$x, data$y), digits)\n    clip &lt;- pipe(\"pbcopy\", \"w\")\n    write.table(data, file = clip, col.names = F, row.names = F)\n    close(clip)\n}\nLet’s test it out:\nset.seed(42)\nplot(runif(100))\nloccopy(1)\nNow hit cmd-v (or equivalent on your OS).\n69.23 0.84\nLet’s add a label using our fast method for coordinates:\ntext(69.23, 0.84, \"Unusual data point\", pos =4, offset = 0)\nThe pos=4 and offset=0 ensures that the text goes directly to the right of our coordinates.\nThat’s it. Hope it helps speed up your workflow."
  },
  {
    "objectID": "posts/2025-10-04-running-github-copilot-CLI-agents-from-R/index.html",
    "href": "posts/2025-10-04-running-github-copilot-CLI-agents-from-R/index.html",
    "title": "Automating the Github Copilot Agent from the command line with Copilot CLI",
    "section": "",
    "text": "Github Copilot CLI (Command Line Interface) recently became available. It is an AI agent, meaning it can use tools autonomously in a loop. It has two modes:\n\nAn interactive mode that is like a terminal version of the Copilot agent that runs in the chat window\nA programmatic mode that can be run with shell scripts.\n\nThe Copilot CLI is available with paid versions of github copilot.\nThe programmatic mode interested me, because it allows you to write scripts that call github copilot agents. This means you could run replicate agents on the same problem, then gather there results for analysis.\nI, of course, want to do everything from the R program, because I’m not great with unix code or Python. Once you’ve setup the copilot CLI, its very easy to run it from R:\n\ncopilot_cmd &lt;- \"copilot -p 'Set-up this project directory with a readme.md file, and directories for outputs, scripts, plots.' --allow-all-tools\"\n\nsystem(copilot_cmd)\n\nThis will run the agent autonomously in the current working directory of your R session.\nNow I wouldn’t recommend using --allow-all-tools like this however. There are important security considerations. Like prompt injection attacks where an Agent goes on the web and gets tricked into doing something bad to your computer when it reads some malicious content. Likewise, agents can just stuff up and just delete or overwrite a bunch of files you wanted to keep.\nYou do need to allow some tools however, otherwise you might as well run the agent in interactive mode (because you’ll have to manually approve every tool use). This defeats the time-saving goal of running agents in a loop.\nHere’s my current set-up:\ncopilot -p 'A prompt here' --allow-all-tools --deny-tool 'shell(cd)' --deny-tool 'shell(git)' --deny-tool 'shell(pwd)' --deny-tool 'fetch' --deny-tool 'extensions' --deny-tool 'websearch' --deny-tool 'githubRepo'\"\nI allow all tools, then prevent tools related to changing working directory, accessing the web or viewing directory context.\nThere is still some risk here. e.g., if you want it to run Rscripts you might want to include in your prompt something like ‘Use Rscript my-script.R to run R scripts from the terminal.’ The R scripts could include anything (including connecting to the web).\nHere’s an example in action. I use sprintf to format the terminal command with the prompt, tools and sub-directory path.\ncopilot_prompt &lt;- \"Set-up this project directory with a readme.md file, and directories for outputs, scripts, plots. Then create example data to illustrate a poisson GLM. Make prediction plots. Use `Rscript 'my-script.R'` to run R files. \"\n\ncopilot_tools &lt;- \"--allow-all-tools --deny-tool 'shell(cd)' --deny-tool 'shell(git)' --deny-tool 'shell(pwd)' --deny-tool 'fetch' --deny-tool 'extensions' --deny-tool 'websearch' --deny-tool 'githubRepo'\"\n\nsubdir_path &lt;- \"dir1\"\n\n copilot_cmd &lt;- sprintf(\n    \"cd '%s' && copilot -p '%s' %s\",\n    subdir_path,\n    copilot_prompt,\n    copilot_tools\n  )\n\nsystem(copilot_cmd)\n\nFrom here it would be easy to create a loop over different subdirectory paths, and run a separate agent in each one. The advantage of cding into each path before the agent opens is that the agent can’t then see context from other sub-directories. So you get independent agent runs.\nYou could use this to see how different prompts perform, or do complex prompt strategies like ‘tree of thought’. In tree of thought you ask an agent to create the same thing many times, it will do it slightly differently each time. Then you aggregate the results and pick the most common or most accurate one.\nMy one gripe is that Github have provided us with woeful documentation of what the tool names are. I’m guessing the tools are what you can see if you open the chat window, click ‘Agent’ mode, then click the tool symbol.\nI’d love to have some default tool sets that simplify the tool permissions. For instance a tool set that locks down all web access."
  },
  {
    "objectID": "posts/2025-11-02-should-we-teach-coding/index.html",
    "href": "posts/2025-11-02-should-we-teach-coding/index.html",
    "title": "Should we still teach R coding in this age of genAI?",
    "section": "",
    "text": "I often get asked if we should still be teaching coding skills to students and researchers now that we have generative AI tools that can write code for us.\nThe general consensus is yes, we should still teach coding. For an ecology specific argument see here.\nTeaching coding is teaching people how to think logically and structure problems. These skills are essential not only in coding, but also for good science and problem solving in general.\nSo you need to learn to code to learn to think logically and structure problems.\nA more practical standpoint is that LLMs are good at generating code that they have seen a lot of examples of in their data. But they are not so good at creative work on novel topics or niche areas.\nFor example, AI coding assistants struggle to make a simple tmap (the R mapping package) work well. tmap was updated recently, meaning the examples in the LLM’s training are out of date.\nThe coding assistants also seem to default to code patterns they are most familiar with, like ggplot2 syntax. Not all of this works with tmap.\nIn general, I find they perform much better at statistical modelling in R than they do with complex geospatial analyses.\nThis practical issue may increasingly become less relevant as LLMs get better and we create better resources to inform their actions (like a tmap specific guide for LLMs to read before advising you, or web searches of the tmap page).\nBut it is still likely that frontier modelling will require deep human engagement with code. So learning to code is important for researchers.\nI’ll leave the final say to Andrej Karpathy, former head of AI at Tesla and founding member of OpenAI’s research group.\nIn a recent podcast interview he talks about a repo he made to help teach people how to work with LLMs. His advice to learners is literally to re-write his repo by hand, not even to cut and paste.\nHe goes on to explain that AI agents are good for very standard code or code that is common on the internet. But they perform poorly for creating new code, or code that contradicts common patterns.\nFor any research code he most commonly uses the code auto-complete features that AI assistants have, rather than AI agents.\nHe suggests that to really learn something you should code it yourself, using examples only as references.\nKarpathy came up with the term ‘vibe-coding’ which means creating code by prompting, without reviewing the code. Usually an AI agent is used. So its saying a lot if he thinks that learning means doing it the old fashioned way of writing code yourself."
  },
  {
    "objectID": "posts/2022-10-08-new-vlogs-impact-conservation-mapping/index.html",
    "href": "posts/2022-10-08-new-vlogs-impact-conservation-mapping/index.html",
    "title": "New vlogs on conservation mapping",
    "section": "",
    "text": "Chris recently headed to Heron Island for a research workshop and field visit to work with a team of national and international experts on mapping and restoration.\nHe captured series of interviews with experts covering topics including blue carbon, mapping under-appreciated ecosystems, mapping the world’s coral reefs and tracking mangrove restoration efforts.\nCheck out the series of videos on Chris’ Conservation Hackers YouTube site. And more will be released soon."
  },
  {
    "objectID": "posts/2017-05-25-decisions-making-hacks/index.html",
    "href": "posts/2017-05-25-decisions-making-hacks/index.html",
    "title": "Hacks for making time-effective and lower stress decisions",
    "section": "",
    "text": "Decisions are a fundamental currency of human life. Let me give you some examples.\nFeel exhausted at the end of the work day?\nYou probably made too many decisions. In fact, our brains have limited computational power and there is a finite reserve of decisions we can make in any given day. In fact, big decisions can use just as much brain power as small ones (here’s a good guide to that phenomenon).\nThat old debate about gender equity? It might seem like it is about wages, actually it is about equal rights to make decisions. In our society, the more money you make, the more right you have to make decisions.\nA senior colleague is offering you an “opportunity” (AKA asking you do to something for them)? They are delegating decisions, either because they don’t know how, or they don’t have the time to make the decisions themselves.\n“Opportunities” like organising a symposium are packed full of decisions, like what food to order, who to invite….\nStruggling to find motivation to finish writing a paper or report?\nThat is because writing is packed full of decisions, like what words to use and where to put them. It is a major brain drain to make all these decisions, so we are inclined to put them off.\nFeeling stressed about the end of your PhD approaching?\nAt least part of the stress is knowing that you will have some major life changes coming up, that will require lots of decisions. Like where to live, where to work and so on.\nI am actually in the process of making a major life decision myself - we are looking to buy our first house.\nSo here are some hacks I have developed over the years that help to reduce stress and make decisions more time effective.\n\n\nYou can’t realise you’re stressed about decisions until you realise you’re stressed. You also need self-awareness to know what decisions are stressing you.\nThe best way to get self awareness is to practice mindfullness and/or mediation. Seriously. Many professional sports people consider mediation a critical part of their training.\nSo why not other types of professionals?\nAn easy way to be more mindful is to acknowledge to yourself how you are feeling. Actually say the words, “I am feeling stressed out by…” (or at least actively thinking them). Do that for a few minutes and you might learn alot about what decisions matter to you right now.\nMost large institutions will also offer a free and anonymous counciling service for staff. These are useful in my experience because the counciller will help you develop tools for being more self aware.\n\n\n\nBig decisions, like deciding where to buy or rent a house, or what job to go for are hard because they are actually composed of many small decisions.\nSo break them up into chunks. For instance, decide what suburb you want to be in first. Then what size house and so on.\n\n\n\nOften decisions are hard because the outcomes are uncertain.\nFor instance, you are not sure where to start addressing a critique on your work (like reviews on a paper) because you are not sure of the best way to make ammends with the critic.\nSo break the problem into smaller parts, address the first part and then test it. Do this iteratively until you reach an adequate solution.\nFor instance, if a reviewer didn’t like the method you used, perhaps it was because you explained it poorly. So edit the explanation and show your edits to someone else. If it works for them, you might have solved the problem. If it doesn’t work, maybe its time to try and change the method.\nSelf awareness is very helpful here too. You might not realise that the uncertainty of a decision is stressing you out until you conciously analyse your feelings on the topic.\n\n\n\nAnother obvious way to deal with uncertainty in the outcome of decisions is to do more research!\nFor instance, we want to buy a house, but don’t want to get ripped off. So we need to research what people are paying. Simple.\n\n\n\nOften other people are making the same decisions as you, often with better knowledge. Perhaps they are more experienced, or have had more time to learn about the decision.\nSo find those people and copy them.\nIn doing so though, we haven’t avoided a decision, we have just changed the question from ‘what to do?’, into ‘who to copy?’\nSo we may turned our decision about what statistical method to use on our data into a decision about determining who is an expert in analysing that type of data.\nThis type of decision is often much easier to solve. For instance, you can easily ask around or just look at what the experts in your field are doing.\n\n\n\nOften in life and your career you will have to repeatedly make similar versions of the same decision.\nFor these things it is a good idea to develop a ‘rule of thumb’ to follow.\nFor instance, if you are always analysing a certain type of data, then develop a robust approach to that type of analyse you plan on reusing.\nAnother good example is shopping for food (supermarkets are decision overload). You might have a rule of thumb that you buy the cheapest version of a given product, or the cheapest organic version if you are into organic food and so on.\n\n\n\nIf you copy yourself enough you will develop muscle memory. This is a very time and energy effective way to make decsions.\nFor instance, when I surf I don’t have to think about when to paddle for a wave and when to pop up onto my feet. I just know from having done it thousands of times.\nSome types of analysis also just flow out of my fingers into code without me thinking about it. This is because I have done them so many times.\nThe cost is that you have to invest extensive training into getting muscle memory. So it is only worthwhile doing this deliberately for decisions you think you will have to make very frequently.\n\n\n\nWhen someone asks you to do something, they are asking you to make decisions for them. This will cost you, because you are effectively trading off your time and energy for making other decisions.\nSo if you get asked to do things a lot, learn how to say ‘no’ gracefully. This book helped me with that.\n\n\n\nIf people are delegating decisions to you, why not delegate some of your decisions to someone else?\nEven better, if someone asks you to do something for them, ask for some decision making power in return.\nYou could ask them to do something, ask for their staff’s time to help you with something or ask for money to fund someone to help you.\n\n\n\nSometimes if you put off decisions, they will make themselves. Though this is not always an advisable hack.\nFor instance, if I put off choosing a house to buy I will never own a house. I would consider this a poor outcome.\nHowever, if I put off answering relatively unimportant emails I often find the problems resolve themselves.\n\n\n\nA sub-category of putting it off is sleeping on it. Particularly if you feel weary, your brain probably just needs a break from making decisions.\nShort naps during the day can help stretch out your decision making powers too.\n\n\n\nSometimes big decisions have to be made really fast. If so, you want to pre-approve the decision with yourself.\nFor instance, bidding on an auction for a house is pretty huge and stressful decision.\nThe way to solve this one is to have already decided you want the house and what you are willing to pay.\nThis means searching not for the ‘one ideal house’, because you may lose many auctions. Instead you should have a category of house types that you consider ‘suitable for living’. For instance, 3 bedroom houses in a local suburb.\nSo long as the type of house and the price remain within your parameters then you keep bidding. If the price exceeds your pre-approved boundaries, then you stop.\n\n\n\nMy final hack is to add some fun decisions to your day. If you have a day packed full of decisions, the fun decisions will push out some of the more stressful (and hopefully useless) ones.\nFor instance, I usually ride a fast road bike to work. I recently bought a mountain bike so I could have the flexibility to take different off-road routes on my commute.\nThis added decisions to my day - which bike should I ride and which trails should I take?\nWhen I ride home from work I often find I needlessly think about work the next day, when instead I should deal with the work decisions when I am actually in the office.\nNow when I ride home I find myself thinking about what trail to ride on and what bike to take.\nAs a result I get home more refreshed and less stressed."
  },
  {
    "objectID": "posts/2017-05-25-decisions-making-hacks/index.html#hacks-for-making-time-effective-and-lower-stress-decisions",
    "href": "posts/2017-05-25-decisions-making-hacks/index.html#hacks-for-making-time-effective-and-lower-stress-decisions",
    "title": "Hacks for making time-effective and lower stress decisions",
    "section": "",
    "text": "Decisions are a fundamental currency of human life. Let me give you some examples.\nFeel exhausted at the end of the work day?\nYou probably made too many decisions. In fact, our brains have limited computational power and there is a finite reserve of decisions we can make in any given day. In fact, big decisions can use just as much brain power as small ones (here’s a good guide to that phenomenon).\nThat old debate about gender equity? It might seem like it is about wages, actually it is about equal rights to make decisions. In our society, the more money you make, the more right you have to make decisions.\nA senior colleague is offering you an “opportunity” (AKA asking you do to something for them)? They are delegating decisions, either because they don’t know how, or they don’t have the time to make the decisions themselves.\n“Opportunities” like organising a symposium are packed full of decisions, like what food to order, who to invite….\nStruggling to find motivation to finish writing a paper or report?\nThat is because writing is packed full of decisions, like what words to use and where to put them. It is a major brain drain to make all these decisions, so we are inclined to put them off.\nFeeling stressed about the end of your PhD approaching?\nAt least part of the stress is knowing that you will have some major life changes coming up, that will require lots of decisions. Like where to live, where to work and so on.\nI am actually in the process of making a major life decision myself - we are looking to buy our first house.\nSo here are some hacks I have developed over the years that help to reduce stress and make decisions more time effective.\n\n\nYou can’t realise you’re stressed about decisions until you realise you’re stressed. You also need self-awareness to know what decisions are stressing you.\nThe best way to get self awareness is to practice mindfullness and/or mediation. Seriously. Many professional sports people consider mediation a critical part of their training.\nSo why not other types of professionals?\nAn easy way to be more mindful is to acknowledge to yourself how you are feeling. Actually say the words, “I am feeling stressed out by…” (or at least actively thinking them). Do that for a few minutes and you might learn alot about what decisions matter to you right now.\nMost large institutions will also offer a free and anonymous counciling service for staff. These are useful in my experience because the counciller will help you develop tools for being more self aware.\n\n\n\nBig decisions, like deciding where to buy or rent a house, or what job to go for are hard because they are actually composed of many small decisions.\nSo break them up into chunks. For instance, decide what suburb you want to be in first. Then what size house and so on.\n\n\n\nOften decisions are hard because the outcomes are uncertain.\nFor instance, you are not sure where to start addressing a critique on your work (like reviews on a paper) because you are not sure of the best way to make ammends with the critic.\nSo break the problem into smaller parts, address the first part and then test it. Do this iteratively until you reach an adequate solution.\nFor instance, if a reviewer didn’t like the method you used, perhaps it was because you explained it poorly. So edit the explanation and show your edits to someone else. If it works for them, you might have solved the problem. If it doesn’t work, maybe its time to try and change the method.\nSelf awareness is very helpful here too. You might not realise that the uncertainty of a decision is stressing you out until you conciously analyse your feelings on the topic.\n\n\n\nAnother obvious way to deal with uncertainty in the outcome of decisions is to do more research!\nFor instance, we want to buy a house, but don’t want to get ripped off. So we need to research what people are paying. Simple.\n\n\n\nOften other people are making the same decisions as you, often with better knowledge. Perhaps they are more experienced, or have had more time to learn about the decision.\nSo find those people and copy them.\nIn doing so though, we haven’t avoided a decision, we have just changed the question from ‘what to do?’, into ‘who to copy?’\nSo we may turned our decision about what statistical method to use on our data into a decision about determining who is an expert in analysing that type of data.\nThis type of decision is often much easier to solve. For instance, you can easily ask around or just look at what the experts in your field are doing.\n\n\n\nOften in life and your career you will have to repeatedly make similar versions of the same decision.\nFor these things it is a good idea to develop a ‘rule of thumb’ to follow.\nFor instance, if you are always analysing a certain type of data, then develop a robust approach to that type of analyse you plan on reusing.\nAnother good example is shopping for food (supermarkets are decision overload). You might have a rule of thumb that you buy the cheapest version of a given product, or the cheapest organic version if you are into organic food and so on.\n\n\n\nIf you copy yourself enough you will develop muscle memory. This is a very time and energy effective way to make decsions.\nFor instance, when I surf I don’t have to think about when to paddle for a wave and when to pop up onto my feet. I just know from having done it thousands of times.\nSome types of analysis also just flow out of my fingers into code without me thinking about it. This is because I have done them so many times.\nThe cost is that you have to invest extensive training into getting muscle memory. So it is only worthwhile doing this deliberately for decisions you think you will have to make very frequently.\n\n\n\nWhen someone asks you to do something, they are asking you to make decisions for them. This will cost you, because you are effectively trading off your time and energy for making other decisions.\nSo if you get asked to do things a lot, learn how to say ‘no’ gracefully. This book helped me with that.\n\n\n\nIf people are delegating decisions to you, why not delegate some of your decisions to someone else?\nEven better, if someone asks you to do something for them, ask for some decision making power in return.\nYou could ask them to do something, ask for their staff’s time to help you with something or ask for money to fund someone to help you.\n\n\n\nSometimes if you put off decisions, they will make themselves. Though this is not always an advisable hack.\nFor instance, if I put off choosing a house to buy I will never own a house. I would consider this a poor outcome.\nHowever, if I put off answering relatively unimportant emails I often find the problems resolve themselves.\n\n\n\nA sub-category of putting it off is sleeping on it. Particularly if you feel weary, your brain probably just needs a break from making decisions.\nShort naps during the day can help stretch out your decision making powers too.\n\n\n\nSometimes big decisions have to be made really fast. If so, you want to pre-approve the decision with yourself.\nFor instance, bidding on an auction for a house is pretty huge and stressful decision.\nThe way to solve this one is to have already decided you want the house and what you are willing to pay.\nThis means searching not for the ‘one ideal house’, because you may lose many auctions. Instead you should have a category of house types that you consider ‘suitable for living’. For instance, 3 bedroom houses in a local suburb.\nSo long as the type of house and the price remain within your parameters then you keep bidding. If the price exceeds your pre-approved boundaries, then you stop.\n\n\n\nMy final hack is to add some fun decisions to your day. If you have a day packed full of decisions, the fun decisions will push out some of the more stressful (and hopefully useless) ones.\nFor instance, I usually ride a fast road bike to work. I recently bought a mountain bike so I could have the flexibility to take different off-road routes on my commute.\nThis added decisions to my day - which bike should I ride and which trails should I take?\nWhen I ride home from work I often find I needlessly think about work the next day, when instead I should deal with the work decisions when I am actually in the office.\nNow when I ride home I find myself thinking about what trail to ride on and what bike to take.\nAs a result I get home more refreshed and less stressed."
  },
  {
    "objectID": "posts/2016-06-21-ICRS-day2/index.html",
    "href": "posts/2016-06-21-ICRS-day2/index.html",
    "title": "International Coral Reef Symposium Day 2",
    "section": "",
    "text": "21 June 2016, Honolulu Hawaii\nNote that I write this as speakers talk, so let me know if you think I have missed something, see mistakes or want me to remove something.\nSee also my blog on ICRS: Key insights that will be updated daily.\nKey themes emerging today (in the sessions I attended) included: * Making decisions for better management of coral reefs quickly, rather than waiting for complete scientific understanding. * We need to find more nuanced ways to communicate science, in particular uncertainty, to the public and decision makers. Often our key messages are lost because they are too dumbed down."
  },
  {
    "objectID": "posts/2016-06-21-ICRS-day2/index.html#plenary-from-charles-birkeland",
    "href": "posts/2016-06-21-ICRS-day2/index.html#plenary-from-charles-birkeland",
    "title": "International Coral Reef Symposium Day 2",
    "section": "Plenary from Charles Birkeland",
    "text": "Plenary from Charles Birkeland\nBirkeland spoke about the success of fishery management in Palau and other Pacific Island nations.\nSuccess comes from making decisions on a rapid time-scale. Examples of management success come from places where fishers and regulators (often the same people!) don’t wait to confirm impacts, they act first, then establish impacts later.\n\n\n\nHow some of the #ICRS2016 followers on twitter feel about eating seafood. Source: http://twitter.com\n\n\nBirkeland said we often don’t need precautionary management, we need data-less management.\nAn example was night-time scuba spear fishing in Solomon Islands. Fishers came to the Governor and expressed concern that fish stocks were declining because of the spear fishing. Overnight, the Governor banned night time spear fishing.\nRapid management decisions in Palau and other nations are in contrast to fishers managed under western laws. For instance, in Guam action to reduce overfishing was a long and slow process, because the scientists pushing for changes had to work their way through the courts and have laws changed. In the meantime several fisheries were severely overexploited.\nBirkeland then explored the role of humans in coral reef food-webs. He argued that reef food-webs have relatively low productivity compared to other ecosystems. Therefore, fishing on reefs should be subsistence only.\nHe argued that coral reef food-webs are not exporting energy, they in fact use more energy than they can produce through benthic production. On reefs you often observe inverted biomass pyramids at any one instant in time. For instance, coral polyps are inverted biomass triangles - more biomass in animal cells than in algae - 5% is producers, 95% is animal.\nImportantly, reef food-webs must be importing energy and cannot sustain fishery exports\n\n\n\nWhy we shouldn’t eat reef fish - from Birkeland’s talk. Source: http://twitter.com\n\n\nBecause reef food-webs are importing energy they cannot sustain much fishing pressure. Palau can sustainably support local subsistence fishing, which is limited. Now the problem is feeding the tourists. E.g. Palau has about ~21 000 locals vs ~161 000 tourists annually. Birkeland argued therefore that we should feed the tourists pelagic fish.\nPelagic fish are faster growing and have higher reproductive potential at an early age than reef fish. For instance, yellow fin tuna are extremely fast growing and reach 30 pounds in their first year.\nThe experience in Palau was also that buying fish was also inefficient economically because they had to sell 5kg of locally caught fish to buy one small can of tuna. Therefore, the government banned export of reef fish.\nBirkeland moved on to explain how Palau is a world leader on reef management. For instance, they were the first nation to specify their EEZ to be a shark haven (in 2009). Many other Pacific Island countries have now followed suit.\n\n\n\nOrder of priorities for when we should eat tropical fish - from Birkeland’s talk. Source: http://twitter.com\n\n\nWe have seen some reef fish management successes in other nations. For instance, in West Hawaii they have a very successful fishery management council that has increased the total value of fishery value 4x over the last ~20 years. The success comes from good leadership (from Bill Walsh) and that the government backed up the formation of the fishery council.\nFurther, the fishers avoid taking the biggest fish - they primarily want intermediate sized fish for aquariums. Bigger fish produce exponentially more eggs, so this helps keep the fishery productive.\nBirkeland said we should ‘work with, not against coral reef fisheries’. Doing so requires (in a nutshell) village level management, saving the big fish, avoid exporting reef fish, keep the biggest slowest growing fish (e.g. humphead wrasse) for special events, encourage pelagic fish as food for tourists and put a cap on tourists, focus on high value tourism.\n\n\n\nSome of Birkeland’s take home messages. Source: http://twitter.com\n\n\nBirkeland’s talk was inspiring for its mix of lessons learned from Palau and the important role of reef ecology in understanding management success and failures. Let’s hope that other nations can follow Palau’s example in implementing more successful reef fishery management for the future."
  },
  {
    "objectID": "posts/2016-06-21-ICRS-day2/index.html#watershed-impacts-on-coral-reefs",
    "href": "posts/2016-06-21-ICRS-day2/index.html#watershed-impacts-on-coral-reefs",
    "title": "International Coral Reef Symposium Day 2",
    "section": "Watershed impacts on coral reefs",
    "text": "Watershed impacts on coral reefs\nQuantifying the impacts of land-based stresses to coral reefs is challenging reef scientists to get out of the water and head up onto the land. One of the key challenges is just figuring out where pollutants come from. In the watershed impacts session we saw a number of innovative techniques presented for tracing pollutants to its sources.\nTracing pollutants will help management identify locations where action is needed, such as improving waste water treatment.\nMost from The Nature Conservancy is tracing sewage pollutants to a reef in Hawaii. Corals on the reef have been in decline over several years. They used a technique that is new to marine systems - Microbial source tracking. They took water samples and grew the bacteria. They then used genetic techniques to detect the presence of human gut bacteria.\nBy combining the microbial source tracking with other tracing techniques, they were able to show that groundwater is transporting sewage onto coral reefs, resulting in coral disease and growth deformities. They then developed maps of their water quality indicators to create a map to communicate the results to the community.\nPait spoke about numerous pollutants in the Virgin Islands that are occurring at levels above those recommended for human consumption. These include metals from boat yards and chemicals like PCBs. This information fed directly into the local watershed management plan.\nLewis will be reconstructing land-use change in Palau using evidence from coral skeletons. She is also using novel evidence sources too, like the number of permits for annually allocated for heavy machinery. She is starting her dissertation, so we look forwards to hearing more as her work progresses."
  },
  {
    "objectID": "posts/2016-06-21-ICRS-day2/index.html#informing-management-decisions",
    "href": "posts/2016-06-21-ICRS-day2/index.html#informing-management-decisions",
    "title": "International Coral Reef Symposium Day 2",
    "section": "Informing management decisions",
    "text": "Informing management decisions\nCoral reef scientists are increasingly branching out to use decision science to help communicate the actions we need to take to conserve reefs to managers.\nThis session was packed, standing room only, indicating the interest in the science of managing reefs.\nA key insight from this session was developing science to provide direct advice to managers. We need to do more than document declines, we need to show how we can reverse them.\nBut a challenge to achieving effective decision science is obtaining sufficient data to evaluate the effectiveness of management actions. For instance, we need monitoring that is designed to establish whether management actions are slowing or reverse declines (see Ahmadia’s talk from yesterday)\n\n\n\nKen Anthony on managing the Great Barrier Reef Marine Park. Source: http://twitter.com\n\n\nAnother key emphasis was on managing local stressors (e.g. nutrients) in the face of climate change, which can’t be managed locally. (A problem Cote, Darling and I explain in more detail here).\nIn fact, Global vs local stressors was the theme of the morning’s session.\nKen Anthony from Australian Institute of Marine Sciences spoke about the importance of acting to save reefs, even with imperfect scientific information. He spoke about how uncertainty can paralyse decision makers into not taking any action. We need to recognise that not taking acting is a decision in itself.\nHoegh-Guldberg spoke about the global risk of climate change to coral reefs. He said we need to limit global warming to 1.5 degrees to avoid massive degradation of coral reefs - less than the current policy target of 2 degrees. In the meantime we need to do what we can to sustain ecosystems through local management while the climate stablises.\nMcleod from The Nature Conservancy started with a quote: ‘insanity is doing the same thing over and over again and expecting the same result’. We keep telling ourselves that if we design protected areas well enough, we will ensure reef resilience to climate change. But reefs continue to decline.\n\n\n\nHoegh-Guldberg on global warming. Source: http://twitter.com\n\n\nMcleod was asked by a journalist if it is true that incorporating resilience into management is making a difference to the health of reefs. She admitted that she stumbled. We don’t yet have the data to evaluate if resilience management is working, even though we have been working on these issues for over a decade.\nShe argued that to justify our efforts we need to establish whether ‘resilience management’ is working, or not.\nThe written word can’t communicate what how compelling a speaker Mcleod is. She had no slides but the audience was enwrap. Anthony joked at the completion of her talk that he would vote for her if she ran for president.\nMcleod finished early and there were numerous questions. Someone (Maria Beger?) asked what is the difference between resilience management and normal management. Mcleod answered that resilience management has a specific focus on ecosystem function, not just the stresses, including actions like protecting herbivorous fish on coral reefs.\nSalm from The Nature Conservancy started by emphasising that we often don’t have the time to do the science to support decisions when working with local governance. He asked how much science is enough and how much is too much?\n\n\n\nQuote from Salm. Source: http://twitter.com\n\n\nSalm said we need science to be designed to be nimble so it can feed into governance structures. One way to do this is to involve local people and tourists in data collection. This can help us get the data we need to evaluate whether management is working.\nHe gave an example of how very straightforward science - involving community monitoring of species recovering in protected areas - had convinced the community that they need more protected areas to aid recovery of overfished species.\nJill Harris from WWF started with a great summary of how conservation prioritisation works. Managers have limited resources, so we need to target management efforts for protection at areas where the management actions will have the greatest benefits.\nTo prioritise management actions to maximise their effectiveness, we will often want to target areas with the greatest local threats (that can be managed) and avoid areas threatened by climate change (which can’t be managed locally).\nHarris looked at level of threat across protected areas in Indonesia and showed protection is biased towards reefs that have lower potential for management benefits.\nHarris also highlighted the importance of considering the local situation. They identified priority areas for protection, but also considered what type of management was most likely to work given local context.\nNick Wolff (University of Queensland and recently moved to The Nature Conservancy) started by describing how science is portraying a very bad story for decline of the Great Barrier Reef. In contrast, Australian Government policy documents are very optimistic that management of fisheries and catchments can secure the future health of the reef.\nWolff is using novel methods to establish the degree that local management can influence the future health of the reef. Stay tuned for his upcoming publications.\nApproaches to decision problems\nA key theme emerging in the afternoon was how to communicate more nuanced messages about coral reef conservation to the public and policy-makers. There is wide-spread concern that the key points of science may often be lost once it is communicated outside the scientific community.\nKareiva (current chief scientist of The Nature Conservancy) started his talk with the message box, a tool to help scientists communicate their science. He said that it was a good start, but that it may miss key points that scientists often want to communicate: thresholds, impact and uncertainty.\nAs an example of why thresholds and uncertainty are important, he recommended you read about this experiment. The experiment shows that if the threshold for a disaster is uncertain, multiple players fail to come to a collective agreement.\nHe hypothesised that unfortunately, science often gets boiled down into one of three stories that are then communicated to the public:\n\nIt’s not as bad as you think, we’re ok\nIt’s really bad and we need to change everything\nIts worse than you think, we are doomed\n\nIf we want to communicate more sophisticated messages, we need to understand how public, media, policy-makers read our papers an interpret them. Kareiva said this will be the focus on ongoing work.\n\n\n\nSource: http://twitter.com\n\n\nPandolfi led his talk by asking how can we communicate a more nuanced message of science to the public?\nFor instance, as a good example, Pandolfi gives the media communication of the 2016 coral bleaching of the Great Barrier Reef. There has been widespread bleaching and coral mortality, but the pattern is spatially variable, with some areas hit harder than others. However, even in those areas that had been spared severe bleaching mortality, coral health is poor. Pandolfi contended that the nuances in this event were well represented in the media.\nHe wants us to think more about how to communicate uncertainty to the public.\nCrow White from CalPoly wants us to use ‘trade-off analysis’ to understand how to balance multiple objectives for ocean management. He gives four steps to developing a trade-off analysis:\n\nDevelop models of ocean uses\nEstimate sector responses to alternate forms of management\nCompare outcomes of management in relation to management\nIdentify management strategy that maximises positive benefits to all sectors.\n\nTrade-offs analysis can help us find compromise solutions for management of multiple sectors that are often better than the status-quo. They can thus be useful for communicating science to decision makers and stakeholders.\nKirstin Oleson and Megan Barnes and from the University of Hawaii are using trade-off analysis to identify optimal actions for managing sediment impacts on coral reefs. Importantly, unsealed roads in the tropics are a key source of sediments to coral reefs. There are too many roads to seal them all, so managers need to identify priorities to seal to avoid sediment impacts.\nTrade-off analysis let them identify the optimal strategy for road repair across a range of budgets for road repair. They are now applying this approach to a broader range of land-use management activities.\nAbdulla talked about resilience management in the Maldives. Growing tourism is increasing pressure on reef fisheries to feed tourists. People are also switching to fishing herbivorous fish because other species are depleted.\nThe Maldives are remote, but unprotected, with 0.04% reserve coverage. They are looking at marine managed areas as an approach to management, rather than no-take areas, which are unlikely to be effective in the region. Due to the spatial spread of communities and reefs the effective management requires a decentralized approach.\nLaurance McCook talked about how to operationalize the precautionary principle for management of coral reefs. He presented a convincing case for an approach that better communicates and uncertainty for decision-makers, especially when there are divergent scientific views.\nHe gave an example of dredging impacts on the Great Barrier Reef. A key step was to convene a panel of experts that represented the range of views on impacts of a new development. This meant they could clarify what the panel agreed and did not agree on and make more rapid progress in the debate. It also enabled them to prioritise future research efforts on the unknown impacts.\nI have learned some important lessons on communicating my science to decision makers from this session. I hope I can carry these through to my own work.\nThat’s all from me today. Tomorrow I will be co-chairing a symposium on Ridge to Reef management approaches, so hope to see you there.\nMistakes in this blog, thoughts on this blog, want me to add or remove something? If you have anything to say, feel free to email me."
  },
  {
    "objectID": "posts/2022-02-25-ambitious-targets/index.html",
    "href": "posts/2022-02-25-ambitious-targets/index.html",
    "title": "Ambitious conservation action sees a brighter future for mangroves and seagrass",
    "section": "",
    "text": "Ambitious restoration of mangroves and seagrasses could lead to substantial recovery of these ecosystems new research has found.\nPublished in Current Biology, the study estimated potential recovery of mangroves and seagrass under a range of protection and restoration strategies for 2030, 2050 and 2070, milestone years for international biodiversity commitments, and beyond.\n“Protection alone is unlikely to drive sufficient recovery,” said Dr Christina Buelow from the Global Wetlands Project and the Griffith University Coastal and Marine Research Centre.\n“Our research suggests that if ambitious action is taken to both protect and restore, gains of up to 5% for mangroves and 35% for seagrasses could be achieved by 2050.\n“There is an urgent need to halt and reverse loss of mangroves and seagrass to continue to benefit from the services these ecosystems provide to coastal communities, such as enhancing coastal resilience and contributing to climate stability.”\nConservation action is needed to maintain and recover mangrove and seagrass ecosystems world-wide, but until now the expected outcomes of different protection and restoration strategies has remained unclear.\n“Pairing global observations of mangrove and seagrass coverage change through time with modelled changes we demonstrated that only protection and restoration combined can support substantial gains in coverage of these ecosystems into the future,” said Professor Rod Connolly, Director of the Coastal and Marine Research Centre and the Global Wetlands Project.\n“Our research can be used to set global conservation targets for coastal ecosystem recovery that are not only scientifically-sound, but are also have the necessary ambition required to inspire coordinated international action,” he said.\nAssociate Professor Chris Brown, also from The Global Wetlands Project and the Coastal and Marine Research Centre, added that “we need to be really ambitious if we are going to fight climate change and save coastal ecosystems.”\n“Imagine if we put the kind of effort we are putting into space programs into protecting the Earth’s ecosystems – that’s what I mean by being ambitious.\n“We need to save Earth’s ecosystems from the unprecedented scale of human pressures they face and help secure the stable climate that humanity needs to thrive.\n“Our research provides the scientific basis to predict the outcomes we can expect for these ecosystems if they are ambitious. We hope this helps motivate action towards international targets for ecosystem protection, such as the UN Sustainable Development Goals and the Post-2020 Global Biodiversity Framework, which envisions a future where humans live in harmony with nature.”"
  },
  {
    "objectID": "posts/2020-09-04-correcting-with-glms/index.html",
    "href": "posts/2020-09-04-correcting-with-glms/index.html",
    "title": "Correcting for confounded variables with GLMs",
    "section": "",
    "text": "General (and generalized) linear models can be useful for analyzing field data, where sampling is often distributed unevenly across different environmental gradients or treatment groups. They help us correct for confounded gradients and discover effects that are hidden in plots of raw data.\nFor instance, we used GLMs in a meta-analysis of the rates species are shifting their ranges under climate change. We used the GLMs to correct for differences in ways different studies had measured species ranges, so we could then study the unique effects of ecological variables.\nYou can also think of these GLMs with multiple covariates as ‘statistically’ (rather than experimentally) controlling for the effects of each variable when looking at the effects of the other variables.\nIn this post I’ll demonstrate this application for statistical controls with a simple example of fish abundance observed in two regions.\n\n\n\nWe’ll start by simulating some data. Let’s say the data represent fish abundance at sites. Fish vary in the area of fish habitat available at that site. There are also two bioregions we’ve sampled. We’ll assume the fish only occur on one bioregion, even though habitat is available in both bioregions.\nWe’ll assume data are poisson distributed, to account for the fact that fish numbers can’t be &lt;0, and that they should be integers.\nTo simulate the data we will specify an intercept and an habitat effect b_habitat. We’ll assume two bioregions, so we only need to specify one bioregion effect (which is the mean difference from bioregion B and A)\nnsites &lt;- 88\n\n#Effects\nintercept &lt;- 0\nb_habitat &lt;- 1\nb_region &lt;- -20\nThen we need to make up the covariate data.\nset.seed(50)\nregion &lt;- c(rep(0, nsites/2), rep(1, nsites/2))\n\nhabitat_area &lt;- rnorm(nsites, 1.5, 0.5)\nhabitat_area &lt;- habitat_area*(region + 1)\nWe made it so habitat area was on average twice as big in the second region.\nNow assemble a ‘design matrix’ (X) and matrix of effects (ie our ). These help us then make predictions of mean abundance with a simple matrix multiplication:\ncoefs_true &lt;- matrix(c(intercept, b_habitat, b_region))\nX &lt;- matrix(cbind(1, habitat_area, region), ncol = 3)\nNow make the ‘true’ means for our simulation and the fish abundance data:\ny_mean &lt;- exp(X %*% coefs_true)\ny &lt;- rpois(nsites, y_mean)\nWe took an exponent to ensure positive values.\nIf this confuses you so far, you can read more about linear models and then generalized linear models on my other blogs.\n\n\n\nplot(habitat_area, y)\n\nSo not strong evidence of any effect of habitat area on abundance. What about if we colour points by bioregion?\nplot(habitat_area, y, col = region + 1)\n\nSo it appears the red region (the region with 1 in the region variable) has not habitat relationship, whereas the black region (region with 0 in region variable) has a positive relationship. This occurs because we set the region intercept to -20 (and a poisson with mean exp(-20) will predict basically always zero).\n\n\n\nNow let’s fit a GLM assuming we don’t know about bioregion\nm1 &lt;- glm(y ~ habitat_area, family = \"poisson\")\nsummary(m1)\n\n##\n## Call:\n## glm(formula = y ~ habitat_area, family = \"poisson\")\n##\n## Deviance Residuals:\n##    Min      1Q  Median      3Q     Max  \n## -3.071  -1.783  -1.281   0.229   7.194  \n##\n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   1.91396    0.15321  12.493  &lt; 2e-16 ***\n## habitat_area -0.53476    0.08272  -6.464 1.02e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## (Dispersion parameter for poisson family taken to be 1)\n##\n##     Null deviance: 401.78  on 87  degrees of freedom\n## Residual deviance: 354.11  on 86  degrees of freedom\n## AIC: 501.23\n##\n## Number of Fisher Scoring iterations: 7\nHabitat is significant (p &lt; 0.001), but the estimate is in the wrong direction! It is saying there are fewer fish with more habitat. This is because the second region has on average twice as much habitat, but our fish doesn’t ever occur there.\nTo see this, we can do a plot with the visreg package:\nlibrary(visreg)\nvisreg(m1, scale = \"response\")\n\nSo predicting an effect of habitat area that decreases with habitat area.\n\n\n\nOk, so now try including region:\nm2 &lt;- glm(y ~ habitat_area + region, family = \"poisson\")\nsummary(m2)\n\n##\n## Call:\n## glm(formula = y ~ habitat_area + region, family = \"poisson\")\n##\n## Deviance Residuals:\n##      Min        1Q    Median        3Q       Max  \n## -2.43472  -0.26179  -0.00006  -0.00002   2.46518  \n##\n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)     0.2340     0.2294   1.020    0.308    \n## habitat_area    0.8851     0.1319   6.713 1.91e-11 ***\n## region        -23.0326  2100.7940  -0.011    0.991    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## (Dispersion parameter for poisson family taken to be 1)\n##\n##     Null deviance: 401.777  on 87  degrees of freedom\n## Residual deviance:  54.311  on 85  degrees of freedom\n## AIC: 203.43\n##\n## Number of Fisher Scoring iterations: 18\nHabitat area is significant and positive now. Note also the effect size (0.885) is within the error bounds (SE = 0.132) for the true effect size (=1). Let’s also confirm our better model (most parsimonious) with the AIC (you can read more about what the AIC is on this blog).\nAIC(m1, m2)\n\n##    df      AIC\n## m1  2 501.2268\n## m2  3 203.4282\nSo there’s basically no evidence for the model without bioregion compared to the model with that effect.\nBut notice that the region effect is approximately correct (-23), but the standard error is huge and, consequently, the region effect is non-signficant (p = 0.99). This happens because the abundance is always zero in region 2, so the poisson model can’t get a good estimate of the mean abundance. Put another way, the poisson must have mean &gt;0, but it can’t distinguish between a mean of 0.000000001 versus say 0.0001 (they both predict basically all zeros), so the SE are very broad.\nWe see this if we do a plot, and habitat area appears insignificant:\nvisreg(m2, xvar = \"habitat_area\")\n\nNow replot the predictions, asking for habitat area just in the first bioregion (labelled 0 in the data):\nvisreg(m2, xvar = \"habitat_area\",\n       cond = list(region = 0), scale = \"response\")\n\nNow we see the trend.\nSo our GLM has corrected for the absence of fish in bioregion 2 and correctly identified the positive relationship between abundance and fish biomass. The GLM with the region effect also gets the magnitude of the trend approximately correct (within the margin of error from the true value)."
  },
  {
    "objectID": "posts/2020-09-04-correcting-with-glms/index.html#simulate-data",
    "href": "posts/2020-09-04-correcting-with-glms/index.html#simulate-data",
    "title": "Correcting for confounded variables with GLMs",
    "section": "",
    "text": "We’ll start by simulating some data. Let’s say the data represent fish abundance at sites. Fish vary in the area of fish habitat available at that site. There are also two bioregions we’ve sampled. We’ll assume the fish only occur on one bioregion, even though habitat is available in both bioregions.\nWe’ll assume data are poisson distributed, to account for the fact that fish numbers can’t be &lt;0, and that they should be integers.\nTo simulate the data we will specify an intercept and an habitat effect b_habitat. We’ll assume two bioregions, so we only need to specify one bioregion effect (which is the mean difference from bioregion B and A)\nnsites &lt;- 88\n\n#Effects\nintercept &lt;- 0\nb_habitat &lt;- 1\nb_region &lt;- -20\nThen we need to make up the covariate data.\nset.seed(50)\nregion &lt;- c(rep(0, nsites/2), rep(1, nsites/2))\n\nhabitat_area &lt;- rnorm(nsites, 1.5, 0.5)\nhabitat_area &lt;- habitat_area*(region + 1)\nWe made it so habitat area was on average twice as big in the second region.\nNow assemble a ‘design matrix’ (X) and matrix of effects (ie our ). These help us then make predictions of mean abundance with a simple matrix multiplication:\ncoefs_true &lt;- matrix(c(intercept, b_habitat, b_region))\nX &lt;- matrix(cbind(1, habitat_area, region), ncol = 3)\nNow make the ‘true’ means for our simulation and the fish abundance data:\ny_mean &lt;- exp(X %*% coefs_true)\ny &lt;- rpois(nsites, y_mean)\nWe took an exponent to ensure positive values.\nIf this confuses you so far, you can read more about linear models and then generalized linear models on my other blogs."
  },
  {
    "objectID": "posts/2020-09-04-correcting-with-glms/index.html#plot-simulations",
    "href": "posts/2020-09-04-correcting-with-glms/index.html#plot-simulations",
    "title": "Correcting for confounded variables with GLMs",
    "section": "",
    "text": "plot(habitat_area, y)\n\nSo not strong evidence of any effect of habitat area on abundance. What about if we colour points by bioregion?\nplot(habitat_area, y, col = region + 1)\n\nSo it appears the red region (the region with 1 in the region variable) has not habitat relationship, whereas the black region (region with 0 in region variable) has a positive relationship. This occurs because we set the region intercept to -20 (and a poisson with mean exp(-20) will predict basically always zero)."
  },
  {
    "objectID": "posts/2020-09-04-correcting-with-glms/index.html#naive-glm",
    "href": "posts/2020-09-04-correcting-with-glms/index.html#naive-glm",
    "title": "Correcting for confounded variables with GLMs",
    "section": "",
    "text": "Now let’s fit a GLM assuming we don’t know about bioregion\nm1 &lt;- glm(y ~ habitat_area, family = \"poisson\")\nsummary(m1)\n\n##\n## Call:\n## glm(formula = y ~ habitat_area, family = \"poisson\")\n##\n## Deviance Residuals:\n##    Min      1Q  Median      3Q     Max  \n## -3.071  -1.783  -1.281   0.229   7.194  \n##\n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   1.91396    0.15321  12.493  &lt; 2e-16 ***\n## habitat_area -0.53476    0.08272  -6.464 1.02e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## (Dispersion parameter for poisson family taken to be 1)\n##\n##     Null deviance: 401.78  on 87  degrees of freedom\n## Residual deviance: 354.11  on 86  degrees of freedom\n## AIC: 501.23\n##\n## Number of Fisher Scoring iterations: 7\nHabitat is significant (p &lt; 0.001), but the estimate is in the wrong direction! It is saying there are fewer fish with more habitat. This is because the second region has on average twice as much habitat, but our fish doesn’t ever occur there.\nTo see this, we can do a plot with the visreg package:\nlibrary(visreg)\nvisreg(m1, scale = \"response\")\n\nSo predicting an effect of habitat area that decreases with habitat area."
  },
  {
    "objectID": "posts/2020-09-04-correcting-with-glms/index.html#region-adjusted-glm",
    "href": "posts/2020-09-04-correcting-with-glms/index.html#region-adjusted-glm",
    "title": "Correcting for confounded variables with GLMs",
    "section": "",
    "text": "Ok, so now try including region:\nm2 &lt;- glm(y ~ habitat_area + region, family = \"poisson\")\nsummary(m2)\n\n##\n## Call:\n## glm(formula = y ~ habitat_area + region, family = \"poisson\")\n##\n## Deviance Residuals:\n##      Min        1Q    Median        3Q       Max  \n## -2.43472  -0.26179  -0.00006  -0.00002   2.46518  \n##\n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)     0.2340     0.2294   1.020    0.308    \n## habitat_area    0.8851     0.1319   6.713 1.91e-11 ***\n## region        -23.0326  2100.7940  -0.011    0.991    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## (Dispersion parameter for poisson family taken to be 1)\n##\n##     Null deviance: 401.777  on 87  degrees of freedom\n## Residual deviance:  54.311  on 85  degrees of freedom\n## AIC: 203.43\n##\n## Number of Fisher Scoring iterations: 18\nHabitat area is significant and positive now. Note also the effect size (0.885) is within the error bounds (SE = 0.132) for the true effect size (=1). Let’s also confirm our better model (most parsimonious) with the AIC (you can read more about what the AIC is on this blog).\nAIC(m1, m2)\n\n##    df      AIC\n## m1  2 501.2268\n## m2  3 203.4282\nSo there’s basically no evidence for the model without bioregion compared to the model with that effect.\nBut notice that the region effect is approximately correct (-23), but the standard error is huge and, consequently, the region effect is non-signficant (p = 0.99). This happens because the abundance is always zero in region 2, so the poisson model can’t get a good estimate of the mean abundance. Put another way, the poisson must have mean &gt;0, but it can’t distinguish between a mean of 0.000000001 versus say 0.0001 (they both predict basically all zeros), so the SE are very broad.\nWe see this if we do a plot, and habitat area appears insignificant:\nvisreg(m2, xvar = \"habitat_area\")\n\nNow replot the predictions, asking for habitat area just in the first bioregion (labelled 0 in the data):\nvisreg(m2, xvar = \"habitat_area\",\n       cond = list(region = 0), scale = \"response\")\n\nNow we see the trend.\nSo our GLM has corrected for the absence of fish in bioregion 2 and correctly identified the positive relationship between abundance and fish biomass. The GLM with the region effect also gets the magnitude of the trend approximately correct (within the margin of error from the true value)."
  },
  {
    "objectID": "posts/2017-09-18-emotions-of-programming-rstats/index.html",
    "href": "posts/2017-09-18-emotions-of-programming-rstats/index.html",
    "title": "Stay motivated with R programming by knowing your emotions",
    "section": "",
    "text": "We don’t usually think of programming as an ‘emotional’ experience. It is all code and computer logic and if using R, probably statistics.\nWe often think of programmers as emotionless human-machines, who, powered by coffee, program late into the night, far removed from any subjective feelings.\nBut most of us aren’t like this. Programming can be intensely emotional - I often confront these emotions when teaching R courses, especially when teaching newcomers.\nRecognising the emotions you face when programming can actually help you be a more productive coder. Here’s how."
  },
  {
    "objectID": "posts/2017-09-18-emotions-of-programming-rstats/index.html#emotions-associated-with-learning-to-program",
    "href": "posts/2017-09-18-emotions-of-programming-rstats/index.html#emotions-associated-with-learning-to-program",
    "title": "Stay motivated with R programming by knowing your emotions",
    "section": "Emotions associated with learning to program",
    "text": "Emotions associated with learning to program\nSome common feelings new users report are:\n\nFrustration\nAnger\nIntrigue\nElation\nBewilderment\n\nAll of these emotions, whether you see them as ‘positive’ or ‘negative’ can actually motivate or demotivate us. Let me give two examples.\nElation is commonly described as a ‘positive’ emotion, which is usually associated with cracking a difficult programming problem. For instance, maybe you just got your linear regression model to work and the results look really interesting.\nElation can motivate you to keep trying new things, because it is rewarding. It can also be demotivating. If you have prematurally declared a problem solved, the feeling of elation may mean you miss a mistake, or a better solution.\nFrustration is probably the most common feeling I hear about (as a teacher of R). Frustration can be demotivating - “I just can’t fix this!”. Or it can be motivating - “I need to get this fixed and I’m not getting up until I do”. In fact for me, frustration is often a driving force behind solving difficult coding problems. Frustration motivates me to keep trying until its solved."
  },
  {
    "objectID": "posts/2017-09-18-emotions-of-programming-rstats/index.html#benefits-of-being-self-aware",
    "href": "posts/2017-09-18-emotions-of-programming-rstats/index.html#benefits-of-being-self-aware",
    "title": "Stay motivated with R programming by knowing your emotions",
    "section": "Benefits of being self-aware",
    "text": "Benefits of being self-aware\nBeing self-aware of how you feel can help you be a more productive programmer.\nSay you are elated about getting an analysis coded. But knowing you are perhaps over-excited, you stop yourself before you send the results straight to your supervisor. You realise you need to double check that the data were entered correctly and that the model fits the data appropriately before you can be confident in sharing your conclusions.\nFrustration as a motivating force can also be a trap. It is helpful to realise when frustration is driving you too hard to try and solve a problem.\nOften you will find that the solution ‘just comes to you’, as soon as you step away from the computer, perhaps to take a stroll.\nFrustration can also lead you into a pointless dogfight with small bugs, causing you to lose sight of the bigger picture. A good example is the Folk Theorem of Statistical computing:\n\n“When you have computational problems, often there’s a problem with your model” Andrew Gelman\n\nI have often run across this issue when I am trying to fit standard models (e.g. GLMs) to new types of data. Often the solution lies in the bigger picture - the need for a new type of model."
  },
  {
    "objectID": "posts/2017-09-18-emotions-of-programming-rstats/index.html#the-emotional-cycle-of-programming-r",
    "href": "posts/2017-09-18-emotions-of-programming-rstats/index.html#the-emotional-cycle-of-programming-r",
    "title": "Stay motivated with R programming by knowing your emotions",
    "section": "The emotional cycle of programming R",
    "text": "The emotional cycle of programming R\nAs you learn to program it is also worth taking note of how your emotions change over time.\nA typical sequence of feelings when learning a new technique in R (e.g. a new package) looks like:\n\nIntrigue, when you discover an interesting new technique you think will be useful.\nFrustration as you try to make the method work with your data.\nSatisfaction when you get the method to work.\nBoredom once you have mastered the method, but still need to tidy up the small details.\n\nKnowing where you fall on this cycle can help you overcome the pitfuls of each step and keep motivated."
  },
  {
    "objectID": "posts/2017-09-18-emotions-of-programming-rstats/index.html#emotionally-intelligent-programmers",
    "href": "posts/2017-09-18-emotions-of-programming-rstats/index.html#emotionally-intelligent-programmers",
    "title": "Stay motivated with R programming by knowing your emotions",
    "section": "Emotionally intelligent programmers",
    "text": "Emotionally intelligent programmers\nSomeone recently asked via twitter what is your best advice was for a newbie to learn programming?.\nMost of the responses related to learning materials, or the technical aspects of programming. However, at a deeper level, learning to programming means finding ways to keep yourself motivated enough to keep trying. So I think, before we consider technical advice, we need to think about what will keep people going.\nSo here is the tweet-worth of advice I posted:\n\nLearning to code is emotional- frustration, joy, boredom… Find yourself a support network of like minded coders to keep motivated\n\nHaving other people who can help you is great, not only because you can pool thoughts to solve problems, but because they can be empathetic to your challenges. Sometimes having some understanding is all you need to keep going and solve a coding problem.\nI will end this post on piece of advice posted by the person (Brian P. Hogan) that asked the question above, I think its telling:\n\n“Software dev is 20% code and 80% people. Level up communication skills and show love to everyone.” Brian P. Hogan\n\nSo remember, computer code might be emotionless, but you shouldn’t expect yourself to be."
  },
  {
    "objectID": "posts/2025-10-14-forage-fish-seabirds/index.html",
    "href": "posts/2025-10-14-forage-fish-seabirds/index.html",
    "title": "Forage fish fisheries and marine predators",
    "section": "",
    "text": "Below are some links for my class on this topic\nWord match game quiz\n\nMatch sets of four words.\nWhen you’ve picked four words hit ‘submit’ to see if you are correct.\nYou get four guesses\nRefresh your browser to start again\nWhen you’ve completed each quiz, click ‘next level’ for the next quiz.\n\nSeabird block game\n\nMove your seabird around to catch enough fish to lay an egg, then raise the chick to fledging."
  },
  {
    "objectID": "posts/2016-03-15-create-rstats-culture/index.html",
    "href": "posts/2016-03-15-create-rstats-culture/index.html",
    "title": "How to create a culture of using R programming at your institution",
    "section": "",
    "text": "How to create a culture of using R programming at your institution\nPromoting the use of the R programming language at your institution can bring many benefits. Your colleagues will be able to access cutting edge statistical analyses that have just been developed. It can improve flexibility in data-analysis, allowing the combination of old tools to invent new methods. Proficient R programming can also save a lot of time, because the scripts you create are a repeatable blueprint of your analysis. Scripting is also a key skill for dealing with very large datasets, that are unmanageable using point and click interfaces. Finally, Rstats is fast becoming the gold standard for analysis and many academic journals encourage submission of Rstats code with manuscripts, particularly for papers about new methods.\nR is somewhat infamous for it’s steep learning curve. If you are new to R, getting started can be very daunting and you will want help. So how can you create a ‘culture’ of using R in our institutions? Here I describe some of the lessons I have learned about creating a culture of R.\n\n1. Run an introductory course on R\nHaving an experienced instructor teach a course on how to get started with R can be the best way to start. Many people may not have encountered a programming language before, so just opening up the R program can be daunting - they don’t know where to start. An instructed course is a great way to step people through this process.\nYou can make a lot of progress learning R in just one day, for instance see some of the introductory courses I run.\nThe best courses will be tailored to your research discipline. I find that people ‘get it’ better if they are solving problems they will encounter in their own work. When I was learning R, I learned less from text books that worked from medical examples when compared to texts that worked from ecological examples.\nSo recruit a local R user to teach a course for your institution. Getting someone local will also help build a support network and will help ensure they are familiar with the problems faced in your discipline.\n\n\n2. Create an R support network at your institution\nA support network is key. People will typically be using R on their own. When they run across problems they will need help. R’s help files are not that useful, until you learn to speak the language - they tend to have a lot of stats and programming jargon in them. Web searches are also difficult, until you know the right terms to search for. Also, help files and web searches have no empathy. Often what aspiring R programmers need is a friendly and encouraging ear to vent their frustrations on!\nThere are a few ways to create good support networks, try some or all of these:\n\nAn introductory R course is a great way for new R users to meet and can create the beginnings of an R community\nAn email group is also helpful. R users can ask for help on their problems and share useful articles and advice they come across as they learn R. Of course the normal etiquette of group email lists applies.\nOrganise regular meet-ups where R users come with their problems and a solution is crowd sourced. Ideally, some experienced R users will attend too. I have attended/organised several of these over the years and find they are great for my own knowledge. By participating and helping people new to R I get a greater exposure to the types of statistical problems ecologists need to solve. One of the most enjoyable R groups I have attended was the Stats-Beerz group at Simon Fraser University. They combine a casual R programming help session with craft beers (which I think proves that R is the language of choice for hipsters).\n\n\n\n3. Focus training at the junior levels\nThe junior staff and students typically have the most time to learn new skills. They are also usually the people collecting, entering and analysing data. Focus R training at these people. They are most likely to integrate R into their work habits. That said, if more senior staff want to learn R, then certainly encourage that too. Presence of senior staff at R courses and meet-ups sends a message that R programming is an important skill to learn.\n\n\n4. Promote an R friendly computing environment\nSome institutions I have worked at in the past did not allow installation of 3rd party software on the institute’s computers without IT support. Once you install R, you will probably want to download and install new packages as new problems arise or you want to try a new type of analysis. Requiring an IT staff member to help install every new R package you want is very frustrating and really slows down the process of problem solving in R. Hopefully these days are over, but if your institute requires IT support to install new software, you need to fight against it.\n\n\n5. Promote R at all levels\nSenior staff need to see value in their staff and students using R to allow a productive R culture to develop. In a successful R culture, senior staff may not know how to use R themselves, but they will be encouraging junior staff and students to be using R in their work. Senior staff also typically have more access to the resources required to provide R training and facilitate support networks. In my experience most senior staff already see the value in R. If you want to develop an R culture at your institution, all you need do is approach them with ideas for training.\nGood luck with developing an R programming culture for your institute. A successful R culture will benefit you and your colleagues for years to come."
  },
  {
    "objectID": "posts/2023-12-01-providing-written-feedback/index.html",
    "href": "posts/2023-12-01-providing-written-feedback/index.html",
    "title": "Tips for providing written feedback to students",
    "section": "",
    "text": "If you are a supervisor or teacher you will need to provide written feedback to students. This is a skill that takes time to develop. Here are some tips I’ve developed over the years. As a student there are also ways you can encourage better feedback from your supervisors.\nIt helps to read some books like Schimel’s book on ‘writing papers that get cited and grants that get funded’ or Steven Pinker ‘Sense of Style’. These give you the words to describe what is wrong and how to fix it (before those books I new a sentence could be improved and I knew how to improve it, but I couldn’t explain to the student why or how to do it).\n\nJust as cooks, musicians and ballplayers have to master some lingo to be able to share their tips and learn from others, so writers can benefit by knowing the names of materials they work with and how they do their jobs.\n\nSteven Pinker - Sense of Style\n\nTips for supervisors\nProvide comments on their writing rather than just editing. Explain why it doesn’t work and how it can be improved.\nRemember to give positive criticism as well. saying ‘good’ is ok, but that’s harder for the student to interpret. Better to use encouragement like ‘After reading this I now understand X for the first time, thankyou!’ or ‘what helped me understand this was that you broke a complex idea down into several smaller ideas in separate sentences’.\nOn the flip side, if you think something is wrong with the writing, don’t just leave a vague comment like ‘vague’. Explain what is wrong and how it can be improved. Even if you are not sure what is wrong, you can still leave a comment like: “I found this sentence hard to understand, can you explain in plain words what you mean?”.\nFocus on big picture of ideas and overall structure first. Once that is good then narrow in on things like grammar and sentence structure (though sometimes the sentences are so bad you do need to fix those so you can understand what the ideas are). Therefore I encourage starting with outlines of topic sentences.\nActually a common problem is not having topic sentences, outlining can help with that.\nEncourage your student to send you lots of small bits of writing often, rather than you spending 2 hours editing a draft once every few months. Lots of small changes are easier to learn than getting dumped with heaps of comments all at once.\nNotice common mistakes and problems and point them out to the student, along with a solution. These are often the areas where they can make the fastest improvements.\nIts common that you’ll have to show them something several times before they learn. That’s just part of the process. Be patient.\nWhen they are stressed or time poor they may default back to bad habits, so just talk about that with them when you notice it, like “I noticed you are doing XXX again, what’s going on for you right now?”\nSometimes sloppy writing can be a sign of a deeper issue, such as stress about a deadline or a personal issue. Noticing early warning signs of stress and anxiety will help you better support your students.\nIts ok to leave some mistakes or areas of improvement until next time. You don’t have to rip apart every draft and smother it in red corrections.\nRemember academic culture is a culture of criticism. But our natural tendency is to to require five positive comments to offset a single negative comment. So it might take some time for your student to get used to extensive criticism on a draft. This is why I prefer to only provide a handful of criticisms (with improvements of course) per draft. (and to request frequent small amounts of editing, rather than ‘all at once’).\n\n\nTips for getting good feedback\nIf you are seeking feedback, then there are some ways you can encourage feedback that is more useful.\nRead the books I mentioned above and practice the advice they give.\nIt helps to tell your supervisor (or collaborator) what you want. Rather than sending a whole draft, send them part of a draft. Or if you do send the whole draft let them know what you specifically want feedback on. For instance, the overall structure, the methodological approach, the relevant literature, or the grammar and writing style.\nIts faster for them if they just need to comment on one aspect. Be sure to let them know in the email, so they are more likely to respond quickly. Lots of small chunks of feedback are also easier for you to learn from.\nRather than saying “I want feedback on X, Y and Z aspects of my draft” you can say “how can I improve X, Y and Z aspects of my draft?” This will encourage constructive feedback. Someone could legitimately respond to the first question with a list of things that are wrong. Whereas, for the second question you are asking them to tell you how you can improve.\nYou can also ask “what have I done really well in this draft?” to encourage positive feedback. Other ways of asking this question are: “how did this help you understand my topic?”, “what did you learn from reading this draft?”\nYou can also encourage chunking of your feedback by asking questions like “what is the most important thing I can do to improve this draft?” or “what is the most important thing I can do to improve my writing?”, or “what are three things I should focus on first?”\nEncourage them when they give you helpful feedback! Supervisors are often still learning how to give feedback. It’s really helped me that my students have told me when my feedback has helped, and how it helped them."
  },
  {
    "objectID": "posts/2015-07-15-about_this_page/2015-07-15-about_this_page.html",
    "href": "posts/2015-07-15-about_this_page/2015-07-15-about_this_page.html",
    "title": "Welcome to my new webpage",
    "section": "",
    "text": "Chris’ new Seascape Models webpage\n\n\nWelcome to my new webpage. I got tired of the limitations imposed by my old google sites page. I was looking for a new alternative, that would allow me to post my own html and css scripts and I stumbled across  Jekyll  for making pages hosted on  Github (via  Sean Anderson’s excellent webpage.)\n\n\nI hope you find the material on my webpage interesting and useful. I have divided posts into Research posts, that focus on short posts summarising my research, and Code posts that provide hints and tricks for programming and data analysis.\n\n\nNext step: get listed on RBloggers.\n\n\nCheers, Chris"
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "",
    "text": "Here is my recommended reading list for students just starting a PhD in some aspect of quantitative ecology (updated from the list published Feb 2020). I’ve written this reading list to keep in mind that many new PhDs may not have a lot of math or stats from their earlier degrees."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#the-role-of-statistics-in-science",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#the-role-of-statistics-in-science",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "The role of statistics in science",
    "text": "The role of statistics in science\nShipley, Cause and Correlation in Biology\nChapters 1 and 2 give a general introduction to the modern philosophy of science, with a focus on statistics, cause and correlation. This is a great introduction to how we use correlation to infer causation in modern science, and how science often progresses by building evidence for or against multiple competing hypotheses.\nChapters 3 onwards are more focused on the methods of structural equation modelling and will be of most interest to students who will be using statistical models such as GLMs.\nHilborn and Mangel, The Ecological Detective\nThe first few chapters are worth reading for an excellent introduction to the philosophy of science. Later chapters focus on theory and application of Bayesian statistical methods."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#bayesian-statistics",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#bayesian-statistics",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\nMcElreath, Statistical Rethinking\nIf you are going to be doing anything with Bayesian methods, then this book is gold. McElreath uses easily understood analogies to break down the jargon and complexity of Bayesian models and make them accessible to a general science audience.\nYou can even watch this whole book in his lecture series on YouTube."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#ecological-modelling",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#ecological-modelling",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "Ecological modelling",
    "text": "Ecological modelling\nOtto and Day, A biologists guide to mathematical modelling in Ecology and Evolution.\nI’d recommend the first few chapters for everyone again, because they will give you a good overview of what modelling is about. Later chapters will be good for students who will be specifically using process models (e.g. with differential equations) in their PhD."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#writing-and-presenting",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#writing-and-presenting",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "Writing and presenting",
    "text": "Writing and presenting\nThere are many books on writing, but one that is tried and tested and very popular in my lab is Joshua Schimel’s “Writing Science”. It teaches you the techniques of writing good science, from structuring a whole paper or grant all the way down to how to write more effective sentences.\nPresentation Zen, including the book of that name and the videos has advice for how to be a more effective presenter (tip, don’t ever start a presentation by opening powerpoint!)."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#data-visualisation",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#data-visualisation",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "Data visualisation",
    "text": "Data visualisation\nThere are lots of great books on this topic, but one of my favourites is Cairo, The Truthful Art.\nCairo uses really compelling examples to show how we can use data visualisation and simple statistics to accurately communicate scientific knowledge."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#skills-for-a-successful-phd",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#skills-for-a-successful-phd",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "Skills for a successful PhD",
    "text": "Skills for a successful PhD\nYou can’t go past Gardiner and Kearn’s books. There’s a whole bunch to recommend here, but the Seven Secrets is probably a good place to start."
  },
  {
    "objectID": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#your-field-of-study",
    "href": "posts/2022-03-02-new-phd-reading-quant-ecol/index.html#your-field-of-study",
    "title": "Reading for new quantitative ecology PhD students updated",
    "section": "Your field of study",
    "text": "Your field of study\nYou should of course be also reading papers in your specific field of study. A few tips for finding these are:\n\nLook in the top review journals, like Trends in Ecology and Evolution and Biological Reviews, for recent articles in your general area of interest. These journals in particular have high communication standards and articles are pitched at a more general audience than in specialist journals.\nSearch (e.g. with web of science) for review articles on your topic in discipline specific journals. For instance, if its conservation planning, then look in Conservation Letters, Conservation Biology and Biological Conservation.\nMake sure you read the new and old literature. One way to identify very influential older papers is to sort results of a lit search by citation count. When you start your PhD you have your chance to read some of the classics. You probably won’t feel you have time to read this sort of thing later on (though don’t let me hold you back if you want to!).\nAsk your peers for their favourite papers to read.\nFollow up on citations. For instance, if you see an article that is repeatedly cited"
  },
  {
    "objectID": "posts/2025-09-17-ai-generated-scicomm-games/index.html",
    "href": "posts/2025-09-17-ai-generated-scicomm-games/index.html",
    "title": "AI generated scicomm games",
    "section": "",
    "text": "This week I ran a talk and workshop for the Tasmanian Student Science Communicators Network.\nThe workshop was about creating gamifying learning. So I thought it would be fun to design some games then create them live as web apps using an AI agent.\nThe particpants designed games on a piece of paper. The aim was to communicate different concepts about sustainable management of fisheries through a game.\nI then took the images and a simple description of game play and plugged it into the Roo Code AI agent.\nHere’s the results, they are simple html and javascript based games.\nThanks to the workshop participants for generously contributing their ideas.\nHere’s the links to the games and the game rules (which incidentally are also the prompts I used)"
  },
  {
    "objectID": "posts/2025-09-17-ai-generated-scicomm-games/index.html#fish-in-a-pond",
    "href": "posts/2025-09-17-ai-generated-scicomm-games/index.html#fish-in-a-pond",
    "title": "AI generated scicomm games",
    "section": "Fish in a pond",
    "text": "Fish in a pond\nIn this game the player is working to replenish the fish population and racing a truck that is taking fish out of the ocean.\nThe ocean is the top bucket.\nThe bottom bucket is like candy candy crush. The player is trying to match four symbols. Every time they match a symbol new fish are born and top-up the fish in the ocean.\nThe game ends if the the ocean runs out of fish. If the player gets the fish over 80% full then the truck speeds up."
  },
  {
    "objectID": "posts/2025-09-17-ai-generated-scicomm-games/index.html#fish-crush",
    "href": "posts/2025-09-17-ai-generated-scicomm-games/index.html#fish-crush",
    "title": "AI generated scicomm games",
    "section": "Fish crush",
    "text": "Fish crush\nIn this game the player is to make a large amount of money. They are working to look after fish in a pond, then they catch the fish to make money.\nThey are aiming to keep the pond at about 50% full and have a profitable fishery.\nThey have to look after the environment around the pond so the fish are healthy. This means preventing pollution and making sure the water is good.\nThey can catch fish to make money. If they catch too many fish the population collapses.\nWinning the game should require looking after the environment around the pond. It should also require catching fish at a slow steady rate, not too much, not too litte."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html",
    "title": "Quick tips for facilitating a good meeting",
    "section": "",
    "text": "In roughly chronological order.\nThis advice comes from perspective of trying to get contributions from diverse research groups, that often mix career stages and cultural backgrounds. But much of it is useful in any context (e.g. more formal contexts.)"
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#before-the-meeting-starts",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#before-the-meeting-starts",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Before the meeting starts",
    "text": "Before the meeting starts\nBe prepared.\n\nTry to learn participants names and something about their background.\nReflect on expectations participants might have for the meeting these might be in terms of outcomes, but also cultural expectations in terms of behaviour.\nSet a meeting agenda. Send it around to participants ahead of time.\nSend them any background reading ahead of time (but keep it short and assume that most haven’t read it)\nDesignate a note-taker (if needed). You can’t facilitate well and take notes.\n\nShow up early, so you can familiarise yourself with the room set-up.\nIf its online, familiarise yourself with the software features before the meeting (where chat box is, where Q&A box is, what people see when you speak, is sharing enabled etc…)\nPrepare your introductory and closing notes.\nCheck the agenda with someone more experienced if need be - people often try to cram too much into a meeting. One way to save meeting time for discussion is to pre-send reading for the group."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#starting-the-meeting",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#starting-the-meeting",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Starting the meeting",
    "text": "Starting the meeting\nCome prepared with an opening that will set the scene and topic of the meeting. Remind people of the time slot for the meeting, and its goals. Remind them of expectations e.g. who is speaking, what to do if you disagree (if neccessary).\nAs facilitator people will look to you for leadership on how the meeting goes. Your approach will affect:\n\nhow fast people talk\nwhether it is safe to pause between comments, or whether someone grabs that free air space and starts talking\nwho overtalks who\nwho talks; if its ok to talk if you are shy or less senior.\n\nIf you don’t provide that leadership, then that role will default to the most senior person or the person who talks the most.\nSo grab your facilitator role by the reins and show the group how its going to be."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#during-the-meeting",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#during-the-meeting",
    "title": "Quick tips for facilitating a good meeting",
    "section": "During the meeting",
    "text": "During the meeting\nBe present. Keep on your toes. By this I mean actually listen to what other people are saying, rather than thinking about what you will say when other people talk.\n\nEngaging the participants\nEngaging everyone helps everyone feel heard and helps get contributions from all participants.\nShow other people you are listening: eye contact, nodding, hand gestures.\nReflect back to people what they’ve just said. This is active listening. For instance, if Alice described how she was satisified with the progress on the project” you could respond “Thanks Alice, I see that you’re happy with how the project is going” (rather than just nodding).\nDraw in people, especially the quiet ones. For instance, if Selena describes an issue she is having with the data analysis on the project you could hand it over to Takuya: “Takuya, I know you’ve written about network modelling, do you have experiences from that work you could share to help Selena”\nIf you don’t know their backgrounds that is ok to! You can just ask someone to reflect like: “Takuya, what do you think about what Selena just said” or “Takuya, can you describe the problem Selena is having in your own words?”\nThis reflection opportunity can be helpful to engage people, but also see problems from different perspectives.\nIf you start asking people to reflect on each other’s comments, everyone will pay more attention to what is being said.\n\n\nYou don’t need to have all the answers!\nA common mistake as facilitator is to think you need to answer the questions that are posed. Then the facilitator ends up talking with their own answers. You don’t answer them.\nYou are there to help the group find a solution. So if a problem/question comes up, use your facilitator skills to kick it onto someone else like “Muhammad, what do you think about Maria’s question?”\nIf the group solves a problem, even if you knew the answer, then you are doing successful facilitation."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#dealing-with-problems",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#dealing-with-problems",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Dealing with problems",
    "text": "Dealing with problems\n\nPeople who talk too much\nThe tactful way is to smoothly transition the conversation away from them, by taking over their sentences. Its hard to put this in text but here goes:\nOvertalker: “What I’m talking about is the absolute lack of data in…” Facilitator (finishes the sentence) “the models. That’s a really important insight John. I’d like to pass on to Eva for her perspective on this issue”.\nBut this is partly highly cultural. e.g. in some cultures the senior members need to be let to speak as much as they want, in other cultures it might be more appropriate to let minority groups speak first. Some cultures this is fast and seamless. Others you would leave a long pause between sentences.\n\n\nPeople who talk too much in online meetings\nThis is really difficult. And we all tend to overtalk in online meetings because we aren’t getting as much visual feedback from our participants! If you have advice let me know. A couple tips:\n\nGet cameras turned on so people get more visual feedback.\nBe engaged, overstate your body language (nodding etc…) so the speaker knows you are listening\nUse emoticons and reactions\n\nSometimes I just talk over them like in the ‘people who talk too much’ example. The key thing is to show you’ve heard by reflecting their thoughts back to them in your own words. If you abruptly change topics it may seem rude.\n\n\nPeople who don’t engage or who are shy\nSee notes above about engaging particpants. Just try to be aware of who they are and keep a mental checklist of who is doing the talking.\n\n\nTransitioning the conversation\nWhat to do if the conversation is going in the wrong direction, or you are running out of time?\nFollow above advice for people who talk too much to transition the conversation to where you want it to go.\nJust remember to acknowledge the person you are taking the platform from. A bad example\n“So we are running out of time, let’s move on”\nGood example:\n“Thanks for those important insights Anya. I think we need to follow-up to properly explore those ideas. Now I’d like to just take us back to the agenda…”\n\n\nStrong emotions and conflict\nSometimes people express anger, frustration or deep sadness in meetings. Sometimes there is conflict that goes beyond healthy collegial debate. These situations can be difficult and need to be treated case-by-case, particularly if they involve ongoing issues in group dynamics.\nThe only general point I can make is to acknowledge the feelings. For example, you could say, “Rachael, are you feeling angry about what was just said?”\nThis helps the person feel heard. But also helps you take the floor back from the conflict.\nHow you go from there depends on the situation. If its sadness (like a colleague drops the news that they lost a friend recently) and you are in a supportive group, you could offer condolences and reflections as a group, just generally show kindness.\nIf its serious conflict yuo might like to ask the person to come outside for a private chat (and delegate facilitation to someone else while you are gone).\nI’ll leave that there, as I am no expert on conflict resolution."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#save-time-to-wrap-up-and-finish-on-time",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#save-time-to-wrap-up-and-finish-on-time",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Save time to wrap up and finish on time",
    "text": "Save time to wrap up and finish on time\nIts polite to finish on time. You should aim to finish early, to allow time to wrap up. Also people may have ‘last thoughts’ just when you were going to finish. For example for a 50 minute meeting start at 11am:\n11:35am - Start bringing the conversation back to the agenda 11:40am - Ask for any last thoughts 11:45am - Provide wrap-up, reflection, next steps and actions."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#wrapping-up",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#wrapping-up",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Wrapping up",
    "text": "Wrapping up\nSave time to wrap-up the meeting. This can include:\n\nReflections on key things that were discussed or learned\nAcknowledging different people’s contributions\nThanking people for contributions, especially if they put a lot of effort in (e.g. planning) or brought up something that was emotionally difficult (like sharing a failure that the group can learn from)\nAddressing the agenda items\n\nIf it is an action orientated meeting, you should also finish by stating the next steps."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#online-meetings",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#online-meetings",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Online meetings",
    "text": "Online meetings\nI much prefer in-person meetings. Online meetings are more less like conversations and more like he talks, she talks, he talks, she talks.\nMeetings that are simultaneously online and in person are the worst.\nA few tips:\n\nDelegate someone to be the ‘chat facilitator’. Their job is to keep an eye on the chat and bring any points to your attention. Some people won’t or can’t talk online and may type instead. So its still good for you as facilitator to raise their points like: “Ahhh, I see Caitie has asked a great question online, Alyssa she wants to know if…”\nIf it is simultaneously online and in person you need an ‘online’ faciliator to help bring in those people (remind you, monitor chat, perhaps talk to online participants directly)\nCameras on is better for engagement\nResist the temptation to check your email when other people are talking (I once did this with screen sharing on, oops). You need to be engaged and staying present."
  },
  {
    "objectID": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#conclusion",
    "href": "posts/2025-08-21-quick-tips-for-faciltating-meetings/index.html#conclusion",
    "title": "Quick tips for facilitating a good meeting",
    "section": "Conclusion",
    "text": "Conclusion\nFacilitation is a foundational skill for most careers. But few of us practice it or put much thought into it. It does take practice and commitment to do it well. But its also rewarding to see your team flourish and everyone get more out of meetings."
  },
  {
    "objectID": "posts/2015-11-03-decra/index.html",
    "href": "posts/2015-11-03-decra/index.html",
    "title": "New fellowship on fisheries and their habitats",
    "section": "",
    "text": "New fellowship: Discovery Early Career Research Award\nI am pleased to announce I was awarded a national fellowship from the Australian Research Council recently. I will be funded through to 2018 to pursue a new research direction in the impacts of habitat change on fisheries. My funding also includes a PhD scholarship, so I will be advertising for a PhD student soon. Get in touch if you are interested in modelling of marine ecosystems.\nHere is an excerpt from my project proposal.\nSustaining fisheries is important for economies, livelihoods and human health, but globally numerous fisheries are threatened by rapid loss of aquatic habitat. The role of habitat in fishery declines is poorly quantified, and we need new models that integrate existing datasets to attribute change in a fishery to change in its habitats. The project will develop a new statistical analysis to examine the role of habitat loss in the global status of fisheries. Subsequently, novel numerical techniques will be used to explore how multiple human impacts to habitats affect fisheries. These approaches will be used to predict how human impacts to habitats affect fisheries through time and space."
  },
  {
    "objectID": "posts/2020-06-15-PhD-projects-available/index.html",
    "href": "posts/2020-06-15-PhD-projects-available/index.html",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "",
    "text": "Current June 2020 until January 2021\nOur lab uses quantitative tools, including statistics, AI and numerical models, to help conservation managers address a range of issue facing marine ecosystems. Below are two current projects where we are recruiting students."
  },
  {
    "objectID": "posts/2020-06-15-PhD-projects-available/index.html#conservation-and-restoration-of-coastal-habitats",
    "href": "posts/2020-06-15-PhD-projects-available/index.html#conservation-and-restoration-of-coastal-habitats",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Conservation and restoration of coastal habitats",
    "text": "Conservation and restoration of coastal habitats\nWork with the team on Global Wetlands Project to help solve pressing challenges for conservation of coastal wetland habitats. PhD and Honours projects available to work on predicting the impact of multiple interacting stressors and assessing the impact of habitat loss on the sustainability of fisheries. Email Chris (chris.brown@griffith.edu.au) for more information. PhD and honours projects available."
  },
  {
    "objectID": "posts/2020-06-15-PhD-projects-available/index.html#how-can-marine-conservation-recover-from-the-economic-recession-and-decline-in-ecotourism",
    "href": "posts/2020-06-15-PhD-projects-available/index.html#how-can-marine-conservation-recover-from-the-economic-recession-and-decline-in-ecotourism",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "How can marine conservation recover from the economic recession and decline in ecotourism?",
    "text": "How can marine conservation recover from the economic recession and decline in ecotourism?\nMany marine conservation initiatives are supported by ecotourism, but that industry has been hit hard by COVID19 and travel restrictions. PhD projects are available to work with international agencies to understand how conservation of marine life has been affected and plan recovery strategies. This is an opportunity to develop new science and engage in meaningful conservation actions to address a global issue. Contact Chris Brown (chris.brown@griffith.edu.au) for more information. PhD and honours projects available."
  },
  {
    "objectID": "posts/2020-06-15-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "href": "posts/2020-06-15-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Criteria for potential PhD applicants",
    "text": "Criteria for potential PhD applicants\nPlease consider the below criteria before contacting us. We will not be considering students for applications that do not meet these criteria, because they will not be competitive for funded positions. Further details on Griffith University’s application process can be found here.\nOur primary research methods are quantitative tools. Potential applicants should have either experience in quantiative methods like R, or a very strong desire to learn more quantitative skills. Chris does not supervise projects that are primarily field or lab based, except as cosupervisor with other group leaders.\nOur students come with training in a diverse range of fields, including field ecology, maths and IT. Our lab group is outstanding at supporting quantitative training and current PhD students have ample opportunities to learn quantitative skills by working with their peers, postdoctoral researchers, Chris and through specialized training courses.\n\nAustralian students\nAustralian citizens and permanent residents should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters by research) in a relevant field.\n\n\nInternational students\nInternational applicants must have been awarded a 1st Class Honours degree or equivalent (e.g. Masters with a 40 credit point research thesis) in a relevant field. They must be lead author on at least one publication in an internationally recognized peer-reviewed journal. To be considered, journals should be internationally recognized, for instance they should have an Impact Factor rating from ISI or be listed by Scimago."
  },
  {
    "objectID": "posts/2021-10-02-are-grant-applications-fair/index.html",
    "href": "posts/2021-10-02-are-grant-applications-fair/index.html",
    "title": "Are lengthy grant applications fair?",
    "section": "",
    "text": "In a previous post I looked at how long it takes to write for a major national grant and the strategies I use to get the application done to a decent standard.\nIn this post I’ll discuss specifically how the complex process of writing ARC grant applications can impact equity in scientific careers and individual career outcomes.\nARC grant applications are very complex, because there are a large number of sections you will be evaluated on. There’s project design, institutional support statement, budget, budget justification, industry partner support letters (for some grants), investigator career opportunities, investigator career impact, investigator citation metrics, … it goes on and on.\nI can see how applications have become so complex. All the sections are there to make sure the assessors fairly consider all aspects of your project design and career track record. They are also there to make sure that spending of public money is well justified.\nFor instance, in an ARC you’ll write a ‘research opportunity performance evaluation’, where you can talk about your best research outcomes, but also how breaks in your career may have slowed you down."
  },
  {
    "objectID": "posts/2021-10-02-are-grant-applications-fair/index.html#how-complexity-becomes-unfair",
    "href": "posts/2021-10-02-are-grant-applications-fair/index.html#how-complexity-becomes-unfair",
    "title": "Are lengthy grant applications fair?",
    "section": "How complexity becomes unfair",
    "text": "How complexity becomes unfair\nBut all this complexity makes for a very lengthy and time consuming application, which I think then makes it an unfair process in a different way.\nAcross all of my applications I was employed full-time on a university position with time specifically allocated for grant writing. I had tenure for some of these application.\nI have a young family, so my time outside of work hours is significantly constrained. This meant I had to be very organized. But I did have the luxury of a job that paid me to write grants.\nMany early career researchers and researchers who are trying to return to science after a career break don’t have this luxury. This may mean they need to write the application on their own time, either because they’ve don’t have an academic job, or their present job doesn’t allow time for grant writing.\nI’ve also seen the process suck the soul out of people. Here’s a common situation: a postdoc’s research contract is ending soon and their supervisor doesn’t have ongoing funding for their position. So they write for an early career fellowship.\nBut chances are the postdoc won’t win the grant (the overall success rate is about 15-20%) and the time invested in the application is time lost from chasing up other higher probability job opportunities.\nThe stress of this situation is amplified by uncertainty about when the grant will actually be announced.\nSo, if you are going to play a high stakes game, it’s a good idea to have a reliable plan B."
  },
  {
    "objectID": "posts/2021-10-02-are-grant-applications-fair/index.html#is-there-another-way",
    "href": "posts/2021-10-02-are-grant-applications-fair/index.html#is-there-another-way",
    "title": "Are lengthy grant applications fair?",
    "section": "Is there another way?",
    "text": "Is there another way?\nGrant applications could be simplified. This would save researcher time (also often paid for by public money!) and make grant writing more accessible to those who are disadvantaged by not having sufficient time.\nFrom what I’ve seen of the Discovery grant applications in Canada, they are far simpler than Australian Discovery grants.\nThis year a petition from ARC tracker was signed by hundreds of researchers. Some of the recommendations in the petition included simplifying the grant application process.\nOf course, the simplification would have to be done carefully so that other types of bias aren’t amplified (like those relating to performance relative to career opportunity).\nAnother approach is to use ‘grant lotteries’, where funding is allocated randomly to a pool of grants that are deemed fundable. That way you can have a shorter application that has to tick off on some minimal standards. Then leave the rest to an unbiased lottery.\nTo sum up, as grant applications become increasingly complex, it increasingly favours people with the time and support networks to develop the application. This means good researchers with great ideas who don’t get the time will be at a disadvantage. That isn’t in the interest of high quality science."
  },
  {
    "objectID": "posts/2021-12-04-asking-answering-questions/index.html",
    "href": "posts/2021-12-04-asking-answering-questions/index.html",
    "title": "Asking and answering questions at seminars",
    "section": "",
    "text": "In our lab meeting this week we practise asking and answering questions or seminars. Asking well framed questions at seminars takes practise so after a discussion of different styles of question we practised asking each other mock questions. We also practised answering difficult questions. I strongly encourage my students to ask questions at seminars, they are an important aspect of scientific dialogue, but it can take some practice.\nIn this blog I describe some of the key discussion points and things we learned."
  },
  {
    "objectID": "posts/2021-12-04-asking-answering-questions/index.html#asking-good-questions",
    "href": "posts/2021-12-04-asking-answering-questions/index.html#asking-good-questions",
    "title": "Asking and answering questions at seminars",
    "section": "Asking good questions",
    "text": "Asking good questions\nWe first discussed some of the different types of questions the team had observed at seminars.\nMany felt that a good question should be an actual genial question about the science that was presented. Some of the lab members related times they had heard ‘questions’ which were actually comments or comments shallowly framed as questions. Sometimes the person in the audience just wants to talk about their own work. If that is the case, they should wait till their own seminar or leave the point for discussion after the seminar rather than stating it in the presenter’s question time.\nA style of question that can be difficult for the presenter to interpret is when the questioner is unsure of their question, so they speak to figure out what their actual question is. One lab member had noticed that people taking notes during seminars tended to have better frame questions. So taking notes can help you come up with a clearer question.\nGood questions often start with a compliment to this presenter will stop this helps put the presenter at ease, especially if it’s a difficult question that they might feel defensive about.\ne.g. “That was a very clear explanation of how dentrification operates in mangroves thankyou. I wonder though how much error there is in denitrification measurements made using the isotope method and if these errors could affect your conclusion that denitrification rates are greater in mangroves than other coastal wetlands?”\nA compliment that contains a short amount of context about the question can also help jog both the presenter and the audience’s minds about which aspect of the presentation this question relates to.\nAnother style of question that some team members have enjoyed our questions that suggests you directions of research.\ne.g. “I didn’t realize mangroves could remove nitrogen pollution from the water, I wonder if we could also apply your methods to measure nitrogen mitigation in seagrass meadows?”\nLab members also related bad questions they had heard, including questions that were about the presenter’s personal appearance or other aspects that didn’t relate to the science they were presenting.\nAnother style of questioning that can come off badly is to doggedly pursue the same question over and over. One example was a presenter that was questioned on the robustness of the t-tests they had used. The presenter answered that they were the correct method, however the audience member kept following up with more and more questions insisting that the t-tests were incorrectly applied. It became clear that the presenter didn’t have sufficient expertise to answer these statistical questions. In such a situation it would have been more appropriate for the questioner to discuss it further with the presenter after the seminar finished."
  },
  {
    "objectID": "posts/2021-12-04-asking-answering-questions/index.html#responding-to-difficult-questions",
    "href": "posts/2021-12-04-asking-answering-questions/index.html#responding-to-difficult-questions",
    "title": "Asking and answering questions at seminars",
    "section": "Responding to difficult questions",
    "text": "Responding to difficult questions\nWe also practised asking difficult questions of each other and answering those.\nSome tips for difficult questions include thanking the questioner and asking to discuss the topic later after the seminar. For a question you’re not sure about you could admit that you’re not sure about the answer and offer to look into it and follow up with the questioner later on.\nWhat you should avoid is trying to dodge a question you don’t know the answer to. It will be obvious to the audience that you are bullshitting.\nIt’s important to remember that we are scientists not politicians. This means we don’t have to know everything, we are allowed to be wrong and we allowed to change our minds in the face of new evidence. So if someone points out a legitimate flaw in your research during question time you’re allowed to agree with them. In fact, the sign of a good scientist is that they know the limits of their own knowledge.\nResponding to difficult questions, especially questions that catch you off guard or sound antagonistic, takes practise. So I recommend you doing a mock ‘questions time’ that includes some tough questions, with your colleagues or lab group so you can practice.\nFor instance, one mock question I asked one lab member about their work with an industry partner was along the lines of an accusation: “your work is just helping this industry green-wash their dirty operations”. He answered: “In our work we are engaging with stakeholders on all sides of this contentious issue. The scientific process is helping improve the transparency of industry operations and create a dialogue between different sides.”\nComing up with well-framed answers to difficult questions can take thought, so it is worth practising for your specific topic."
  },
  {
    "objectID": "posts/2018-02-16-the-90hr-work-week/index.html",
    "href": "posts/2018-02-16-the-90hr-work-week/index.html",
    "title": "The fallacy of the 90 hour work week",
    "section": "",
    "text": "You might often hear senior academics say you need to work extreme hours to ‘make it’ in academia. Such comments often elicit considerable debate.\nFor instance, a recent survey found that academics at one university work an average of 60 hours per week.\nSome commentors noted that such long work hours were a fact of life, and that you will need to work them too if you were to be competive for jobs.\nOther commentors noted that statements about extreme work hours contribute to excluding under-represented groups (e.g. mothers) from acdemia.\nThere are several reasons I believe that this debate about how many work hours is enough is both unhelpful and counter-productive."
  },
  {
    "objectID": "posts/2018-02-16-the-90hr-work-week/index.html#people-exaggerate-their-work-hours",
    "href": "posts/2018-02-16-the-90hr-work-week/index.html#people-exaggerate-their-work-hours",
    "title": "The fallacy of the 90 hour work week",
    "section": "People exaggerate their work hours",
    "text": "People exaggerate their work hours\nMany people are inclined to exaggerate about their true work hours. I once heard an academic tell a group of early-career researchers that 90 hour work weeks were neccessary to make it. But various studies have shown that people tend to exaggerate their true work hours.\nFor instance, in a recent interview with Indra Nooyi (CEO of Pepsi co), she claimed to sleep just 4 hours a night and work 20 hour days 7 days a week. In the same interview she also talks about playing a daily match of tennis. So clearly she is exaggerating, even if slightly (I am sure she does work impossibly long hours by most people’s standards).\nWhen speaking about our work hours we have a strong cultural incentive to exaggerate. Indra Nui probably doesn’t want her shareholders thinking she slacks off. And young academics may often be concerned about looking good to their seniors, for all important reference letters.\nIf you really want to know how many hours you’re working I would recommend keeping a detailed log of your time and where you spend it."
  },
  {
    "objectID": "posts/2018-02-16-the-90hr-work-week/index.html#is-it-even-possible-to-work-90-hour-weeks-consistently",
    "href": "posts/2018-02-16-the-90hr-work-week/index.html#is-it-even-possible-to-work-90-hour-weeks-consistently",
    "title": "The fallacy of the 90 hour work week",
    "section": "Is it even possible to work 90 hour weeks consistently?",
    "text": "Is it even possible to work 90 hour weeks consistently?\nI don’t actually think that such a work load would be sustainable from a health point of view.\nSure some weeks you might work 90 hours (I never have though). Years and years ago I was doing field work on seals and we probably worked 12 hour days for 7 days straight (=84 hours), to keep up with all the seal counts and tagging we had to do. However, this was an exception and that work load only lasted a few weeks during the busy period of seal breeding.\nSo I think it is fair to say you might need to do long weeks sometimes (especially if you are a field biologist), but those hours needn’t be the norm."
  },
  {
    "objectID": "posts/2018-02-16-the-90hr-work-week/index.html#an-implicit-assumption",
    "href": "posts/2018-02-16-the-90hr-work-week/index.html#an-implicit-assumption",
    "title": "The fallacy of the 90 hour work week",
    "section": "An implicit assumption",
    "text": "An implicit assumption\nWhen a senior academic makes a statement about having to work long-hours, they are making an implicit assumption that they were successful because of the long hours they put in, therefore you need to put in long hours too.\nI don’t think this is true. Academic success is defined by productivity, not work hours. And the hours in, productivity out correlation isn’t perfect, as I will explain below."
  },
  {
    "objectID": "posts/2018-02-16-the-90hr-work-week/index.html#efficiency-versus-productivity",
    "href": "posts/2018-02-16-the-90hr-work-week/index.html#efficiency-versus-productivity",
    "title": "The fallacy of the 90 hour work week",
    "section": "Efficiency versus productivity",
    "text": "Efficiency versus productivity\nKey determinants of academic productivity (e.g. papers written per year) are hours worked and your efficiency at work.\nWhat I have observed is that many people are inefficient at work. So to compete with their peers, they need to work long hours.\nI think most people need training to become more efficient. If our mentors think working long-hours is neccessary, that suggests they could also improve their efficiency. So you should seek alternative advice. A good place to start for acadmeics is the iThinkwell webpage and books.\nI also found the book The 4 Hour Work Week helpful for thinking about how I could improve efficiency.\n(A few tips, cut out those low-value time-consuming projects or collaborations, put email low on your priority list of things to do and delegate wherever possible and, do the hardest and most important task first thing in the day).\nIn fact a Swedish study that shorter work hours actually increased the productivity of their workforce, because people were happier and healthier and took less sick days.\nTo improve your efficiency it can be helpful to log your activities at work. For instance, I learned that one of my pubs required about 100 hours of work to reach the point of submission. Looking at my data, I realised that a large portion of the time was spent following up on interesting, but not useful, ideas about complicated new analyses. As a result I have now changed my work practices to put greater emphasis on pre-planning of analyses, to stop myself from following multiple false leads.\nRemember too that there have been massive changes in technology since many of our mentors were trained. Technology can be a hinderance to productivity (email, social media!) but also a massive help (also email and social media!). You will need to do some experimentation to make technology work for your productivity."
  },
  {
    "objectID": "posts/2018-02-16-the-90hr-work-week/index.html#what-is-work-anyway",
    "href": "posts/2018-02-16-the-90hr-work-week/index.html#what-is-work-anyway",
    "title": "The fallacy of the 90 hour work week",
    "section": "What is work anyway?",
    "text": "What is work anyway?\nMy final point is that statements like “I worked 60 hours a week so therefore you have to too” are not realistic to how different people work or even define work.\nThe boundary between work and non-work is not clear to me. If I think about an analysis on the ride home, is that work? or is it commuting? Or if we invite some scientist friends around for dinner and talk about science, is that work? What about watching nature documentaries with my son?\nIf you followed the tax office’s definition of work related activities, you could even classify some holidays as ‘work hours’. For instance, if I go on a diving holiday I am eligible for some tax deductions, because that counts as maintaining my work skills (I’m marine biologist).\nOr this webpage I maintain (its mostly a hobby I do outside of official work hours), should I count that towards work hours? Not everyone spends time maintaining their own webpage and blog rolls, but that doesn’t mean they aren’t successful in their careers.\nEvery one is different in how they work, so at the end of the day, it is not helpful to offer prescriptions about how many hours someone must work to be successful."
  },
  {
    "objectID": "posts/2024-01-06-programming-ocean-conservation-video-game/index.html",
    "href": "posts/2024-01-06-programming-ocean-conservation-video-game/index.html",
    "title": "Making an ocean science video game",
    "section": "",
    "text": "I programmed my own video game. Its called ‘otter vs. urchins’. It’s a space invaders style game, where sea urchins rain down on a kelp forest. The player is an otter and has to protect the kelp forest by munching and crushing the urchins.\nWhen I say ‘programmed my own game’, I actually mean ‘wrote instructions for the Copilot AI to program the game’.\nThe game is written in Python, with the pygame package. Not a programming language I know well.\nDoesn’t matter, because copilot knows Python and can follow my instructions. I could leverage the AI to help me build the game and it helped me learn how to write my own python code.\nIt was super fun to develop and I got my kids involved, who love having their own homemade game. They drew the game ‘art’ (texta drawings that I digitised) and recorded the sounds (me munching on lettuce to make a crunching urchin sound).\nPygame makes it easy to integrate the custom media into your game. And the AI meant we could go very quickly from a gaming idea to a working game.\nMy bigger agenda is to use video games as learning tools. They allow students to fail often and fail safely. Failing safely is one of the fastest ways to learn.\nSo I’ve wanted to develop my own games to teach ocean science for a while. But I’ve been put-off by the computer programming involved. I’ve tried using Rshiny, because I know that language well. But its not well set-up for game simulation.\nLooking forwards to making more games that are fun and next time hopefully with some educational value."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "",
    "text": "I recently signed up for a Slack channel, a kind of chat room for work projects that seems to be popular with programmers and that some academics are picking up for their lab groups now.\nSigning up for Slack made me realise how much I have been using e-media a range of e-media tools in the pipeline from ideas generation to publication and sharing of research. I wanted to document some of those tools and their uses in the pipeline here.\nWe will work through the process from ideas generation to research communication, which wraps back around to generating new ideas.\nI will focus on the tools I use, though I recognise there are many similar options to the specific brand names I will give."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html#generating-ideas",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html#generating-ideas",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "1. Generating ideas",
    "text": "1. Generating ideas\nKey tools: Twitter, blogging, and comments on blogs/papers.\nDiscussions started on Twitter, perhaps when someone shares a new research paper, can sometimes inspire new ideas. This has happened to me and other people have reported it too.\nBlogging is great if you have a longer idea you want to seek feedback on. In fact, we should perhaps just get rid of ‘perspectives’ papers and make them blogs intead. Perspectives tend to present opinion rather than hard evidence. They suggest interesting ideas that need to be tested. Arguably putting them in our academic journals gives them the pretence of being evidence based science. Writing them as blog posts would also save time and hassle with the review process.\nComments sections at the end of blogs and on some online journals are also an opportunity to inpsire new research directions."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html#developing-the-paper-story-methods-analysis-and-interpretation",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html#developing-the-paper-story-methods-analysis-and-interpretation",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "2. Developing the paper: story, methods, analysis and interpretation",
    "text": "2. Developing the paper: story, methods, analysis and interpretation\nKey tools: Video-conferencing, Slack and similar, Dropbox, Google docs, Github, Stackoverflow, and boring old email.\nOnce you have the idea you move to a phase of developing the work with a smaller group of people who will generally your coauthors.\nAt this stage the process usually moves to platforms with more restricted viewing (though their is no reason it couldn’t also be totally open).\nI used to use Skype alot, but the quality seems so poor now I have mostly dropped that. I am yet to find a good alternative. Actually, we mostly use phone calls for meetings these days.\nI only started using Slack this week, but is seems to have great potential. You can set up “exclusive” chat rooms for sharing documents and code and discussing an idea. We are currently trialling Slack in my lab group as a way to keep in touch and share ideas.\nI hope that Slack can cut back on alot of the emailing we do as a lab and make our conversations, well, more conversational.\nGithub is like social media for programmers (this blog is hosted there). I haven’t used it in a collaborative mode much, but it is a popular way to collaboratively work on a programming project.\nStackoverflow and related sites are really useful for finding help on technical programming or statistical issues. I use them a lot, mostly to read other people’s solutions, but you can pose your own questions on there too.\nNeed I go into Dropbox and Google drive? I will say you can integrate them with Slack, so that opens up some more convenient ways to use them and keep updated about changes.\nActually, on the topic of Dropbox, I have found it handy for working on collaborative analysis remotely with folk who aren’t that keen on learning Github. I can update the code that generates resulst and share it straight to dropbox, where they can view results."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html#refining-the-project",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html#refining-the-project",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "3. Refining the project",
    "text": "3. Refining the project\nKey tools: As above but also blogging.\nAs you refine your project, most of the key e-media tools are the same as point 2. However, I would add blogging, or a blog post here.\nIt can be useful to seek feedback from the scientific community at this stage. You could post a blog or an early draft of your article on your blog and ask for comments.\nFor instance, while I was writing a paper that commented on [government fisheries policy] I posted an early (but complete) draft on this blog. I asked people to comment on it and got some good feedbacks about gaps in my argument."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html#pre-prints",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html#pre-prints",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "4. Pre-prints",
    "text": "4. Pre-prints\nKey tools: biorXiv (for biological scientists) or arXiv\nThis year I have been posting some of my papers on biorXiv (e.g. this one) before I submit them for publication. The reason I do this is that I found myself using a lot of statsy biorXiv pre-prints, because the methods I wanted to try weren’t published in the peer-reviewed literature yet.\nI think pre-prints have the potential to speed up the scientific process, because you can (carefully) start to make use of science while it is still traversing its way through the often slow peer-review process.\nThere are some caveats you should consider before embarking on this option, but so far I haven’t noticed any downsides. There was recently a blog debate about their pros and cons, I think Scientist Sees Squirrel was involved, I can remember who else."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html#publication",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html#publication",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "5. Publication",
    "text": "5. Publication\nNot too much to say here, you are likely pretty familiar with the tedius process of uploading papers to journal submission sites. My first PhD paper had something like 12 co-authors, and I think it took me a whole day to get it in the journal’s system.\nOne wish I have is that journal submission sites would have an algorithm that just auto-detects the papers title, authors, affiliations (argh, these can be super annoying to enter), abstract etc… It wouldn’t be that hard to do."
  },
  {
    "objectID": "posts/2017-11-17-emedia-and-publishing/index.html#promotion",
    "href": "posts/2017-11-17-emedia-and-publishing/index.html#promotion",
    "title": "e-media tools for the creation, publication and promotion of academic papers",
    "section": "6. Promotion",
    "text": "6. Promotion\nKey tools: Twitter, blogging, Github.\nYour paper is now published and we have come full circle back to Twitter and blogging.\nIt might seem odd to mention Github here, but if you do any programming, share your code on Github and it will make your paper even more useful to other people.\nPosting a plain English description of your key findings and their implications is a great way to help share your research. It might help your research actually get used in an applied sense, because a non-technical specialist will be able to understand how they can use your research.\nIt can also help you communicate to other scientists, because we can’t be bothered reading techincal papers all the time either. Sometimes we just want to know the headlines.\nMy institute’s blog is about 2 years old now and those that have got involved like it and keep coming back for more.\nTwitter is useful to communicate your blog, or just a link to your paper directly. While it feels somewhat narcissistic to Tweet ones own papers, I often find these are the ones that get the most interest from other people.\nTwitter and blogging continue the conversation about your paper, which is why I say we have come full circle - hopefully in sharing your research you will now inspire new research directions.\n\nThat’s all except to say I don’t have a comment section on my blog (I can’t be bothered managing the comments), so hit me up on Twitter if you want to start a discussion about this."
  },
  {
    "objectID": "posts/2021-08-12-future-fellowship/index.html",
    "href": "posts/2021-08-12-future-fellowship/index.html",
    "title": "New fellowship on predicting coastal ecological futures in an era of unprecedented change",
    "section": "",
    "text": "We are entering a new era of science where ecosystem change is uncharted by historical observations. Observations are the foundation of predictive science, so how can we predict a future that is outside the envelope of the environments?\nThis is the question I will address in a new 2021 Australian Research Council Future Fellowship. Specifically, I will focus on predicting future change for coastal ecosystems.\nWatch a short video about the project\nHumanities footprint on the Earth’s ecosystem has never been greater. The oceans are increasingly being industrialized, and climate change means the footprint of human economic activity now touches even the most remote ecosystems.\nCoastal ecosystems exemplify the challenges we face in managing our growing impacts. Coasts are the thin line on which much of the human population and industrial activity rests. This human activity places great pressures on coastal ecosystems, and in many places, we are losing species and the ecological functions that support human livelihoods, like fisheries.\nPredicting the future for coastal ecosystems is critical for creating a sustainable relationship between humanity and nature. Ecological predictions allow us to identify options for better managing our relationship with ecosystems.\nReliable predictions are needed to help managers mitigate the risks of future human activities to the environment. In this four-year fellowship I will lead a team to develop new modelling techniques for predicting ecosystem change.\nI will show how we can define the limits of prediction by studying our ability to predict change in environments that have never been seen. These predictions will help us transition to a sustainable relationship with coastal ecosystems.\nIf you are interested in further study on this topic a couple of PhD positions have just opened up. See here for further details"
  },
  {
    "objectID": "posts/2020-03-29-home-isolation-work-success-plan/index.html",
    "href": "posts/2020-03-29-home-isolation-work-success-plan/index.html",
    "title": "Home isolation work success plan",
    "section": "",
    "text": "So Australia is entering into a COVID lockdown and I have my two young kids home with me full time. That means every weekday I have roles as a Dad, a husband, a primary school teacher, a carer, a lecturer, a researcher, a cleaner, a cook and a grad student supervisor. I get asked to be a dinosaur most days too.\nSo I think I need a plan to be successful. I thought I’d write down my plan for being successful at work now, so I can read it in a few months time and laugh at myself.\nThis is will start of mild and get more and more extreme, so be warned.\n\n\nEveryone is doing this. It seems self evident. We have coffee with my research team every morning on Microsoft Teams, I attend when I’m not being the Dad/primary school teacher, and sometimes when I am.\nConversations still aren’t as great as they are face-to-face. One thing I’ve noticed is that listening, I mean really listening, is harder. And I think listening, particularly to my grad students and their concerns, is more important than ever. So something I need to figure out.\n\n\n\nI hope to get more proactive with the online stuff soon. Conferences, workshops have been cancelled. I’m hoping aspects of them can still go ahead online. In a few months I want to reschedule my R workshops that were canned, and do them interactively online. Some goes for research workshops\n\n\n\nNot really sure how this will happen. My wife works too, so my work schedule is currently mornings or afternoons (whatever she isn’t working) + evenings.\nWorking and being a school teacher is mentally exhausting!\nFinding it hard to find time for exercise, so this will have to be integrated into play with the kids.\n\n\n\nThis is more achievable. By work balance I mean doing less of the admin I dislike and more of the research/teaching I do like, like workshops with undergrads, R coding and writing.\nWhat’s been great about the shift online so far is that meetings are being replaced with ‘doing research’. Now I have less meetings with students, but more email requests to look R code (which we share on a server). So I get to dip into their R projects, do a bit of coding, leave some comments and then follow up with a call later.\nHopefully it helps them learn too. I’ll keep my eye on that.\n\n\n\nThis is the extreme bit. I’m trying a 2 nap + core sleep schedule, so I can work into the evenings."
  },
  {
    "objectID": "posts/2020-03-29-home-isolation-work-success-plan/index.html#home-isolation-work-success-plan",
    "href": "posts/2020-03-29-home-isolation-work-success-plan/index.html#home-isolation-work-success-plan",
    "title": "Home isolation work success plan",
    "section": "",
    "text": "So Australia is entering into a COVID lockdown and I have my two young kids home with me full time. That means every weekday I have roles as a Dad, a husband, a primary school teacher, a carer, a lecturer, a researcher, a cleaner, a cook and a grad student supervisor. I get asked to be a dinosaur most days too.\nSo I think I need a plan to be successful. I thought I’d write down my plan for being successful at work now, so I can read it in a few months time and laugh at myself.\nThis is will start of mild and get more and more extreme, so be warned.\n\n\nEveryone is doing this. It seems self evident. We have coffee with my research team every morning on Microsoft Teams, I attend when I’m not being the Dad/primary school teacher, and sometimes when I am.\nConversations still aren’t as great as they are face-to-face. One thing I’ve noticed is that listening, I mean really listening, is harder. And I think listening, particularly to my grad students and their concerns, is more important than ever. So something I need to figure out.\n\n\n\nI hope to get more proactive with the online stuff soon. Conferences, workshops have been cancelled. I’m hoping aspects of them can still go ahead online. In a few months I want to reschedule my R workshops that were canned, and do them interactively online. Some goes for research workshops\n\n\n\nNot really sure how this will happen. My wife works too, so my work schedule is currently mornings or afternoons (whatever she isn’t working) + evenings.\nWorking and being a school teacher is mentally exhausting!\nFinding it hard to find time for exercise, so this will have to be integrated into play with the kids.\n\n\n\nThis is more achievable. By work balance I mean doing less of the admin I dislike and more of the research/teaching I do like, like workshops with undergrads, R coding and writing.\nWhat’s been great about the shift online so far is that meetings are being replaced with ‘doing research’. Now I have less meetings with students, but more email requests to look R code (which we share on a server). So I get to dip into their R projects, do a bit of coding, leave some comments and then follow up with a call later.\nHopefully it helps them learn too. I’ll keep my eye on that.\n\n\n\nThis is the extreme bit. I’m trying a 2 nap + core sleep schedule, so I can work into the evenings."
  },
  {
    "objectID": "posts/2018-08-28-five-key-skills-for-interviews/index.html",
    "href": "posts/2018-08-28-five-key-skills-for-interviews/index.html",
    "title": "Five key skills to learn in your PhD and demonstrate in job interviews",
    "section": "",
    "text": "Our PhD student Laura Griffiths, who has worked in government and as a consultant prior to her PhD led a discussion in our lab meeting on skills to demonstrate for job interviews.\nWe discussed how you learn many skills in a PhD, beyond just the academic and science specific ones. Laura showed the other students how it can be invaluable to demonstrate those skills in job interviews, particularly outside of academia.\nHere follows the guest post by Laura Griffiths."
  },
  {
    "objectID": "posts/2018-08-28-five-key-skills-for-interviews/index.html#laura-griffiths-notes-on-five-key-skills",
    "href": "posts/2018-08-28-five-key-skills-for-interviews/index.html#laura-griffiths-notes-on-five-key-skills",
    "title": "Five key skills to learn in your PhD and demonstrate in job interviews",
    "section": "Laura Griffiths notes on five key skills",
    "text": "Laura Griffiths notes on five key skills\n\n1. Technical Skills and Training\nThis relates specifically to your area of expertise. You need to communicate your knowledge and key concepts of your specific field. This will be highlighted in your CV but it’s possible that not all of your CV will be read. So the interview will provide an opportunity to emphasise your skills and training, as well as discuss other aspects not mentioned.\nA tip from this blog: Before going into the interview, make a list of the less obvious but equally important skills you’ve acquired over the years, whether through formal training or self-development. These could include proficiency in different software programmes, or internal training specific to your current or previous organisation.\n\n\n2. Communication skills\nThis is an important skill to show how you communicate verbally. Your CV and published papers will demonstrate your written communication skills, so the emphasis here is on verbal.\nInterviewers want to know how you express and articulate ideas succinctly and how you interact and connect to others, so use appropriate verbal language and use body language that suggests engagement. Your experience presenting at conferences, workshops, teaching, participating in ‘in house’ discussions (lab groups, etc) is a good way to show case these skills.\nIt is also useful to show how you are capable of tailoring your communication skills for different audiences – managers, peers, public, etc.\nA tip from Michael Page: Before an interview, practice talking about your experience out loud. Try to keep answers clear, concise and to the point. Good communication also requires you to be an excellent listener, so always pay close attention to exactly what’s being asked of you. Questions to prepare for are “Tell me about yourself” or “Tell me about your background”. See Link to Michael Page at bottom of article for more info.\n\n\n3. Problem Solving and Critical Thinking\nYou need to display an ability to take a logical and analytical approach to solving problems and resolving issues. It’s also good to show that you can approach problems from different angles.\nIt doesn’t just have to apply to technical problems but also social problems, etc. Critical thinking skills are an integral part of any post graduate study so this is a good way to showcase these skills and show how you have presented solutions back up by data and ideas to solve the problem.\n\n\n4. Professionalism and Leadership\nThis is all encompassing and includes how you can demonstrate professionalism, ie showing that you can prioritise, work efficiently and productively, and manage your time well.\nIt’s also good to be able to show employers how you decide what is important to focus on and get done, and how you go about meeting deadlines. It includes manners, having a positive attitude, being flexible, how you can motivate teams and other colleagues, etc.\nLeadership is about assigning and delegating tasks well, setting deadlines and leading by good example. Not all jobs are seeking leadership roles but having leadership as a skill can help you create opportunities.\nProfessionalism can be demonstrated by learning about the company/department you are being interviewed by. Show your enthusiasm and motivation to be part of their team.\nTip from Michael Page: In an interview discuss situations where you demonstrated these qualities with a positive effect. For example, if you do pride yourself on your people skills then you may want to talk about a time when you led a team, helped a colleague or were commended for your excellent service.\n\n\n5. Emotional Intelligence\nThis is an important skills across the board as developing and building relationships is crucial for any work environment.\nHere you need to demonstrate the following - how you develop relationships and network, how you interact with others, how you guide and monitor reactions of others, judge impacts, sensing contexts, etc. You could give an example of how you are sensitive to the needs of others – eg how you diffused heated discussion, how you played the role of a mediator, or simply how you interact in a team environment (listen and respond)."
  },
  {
    "objectID": "posts/2018-04-13-how-to-use-the-AIC/index.html",
    "href": "posts/2018-04-13-how-to-use-the-AIC/index.html",
    "title": "How do I interpret the AIC",
    "section": "",
    "text": "My student asked today how to interpret the AIC (Akaike’s Information Criteria) statistic for model selection. We ended up bashing out some R code to demonstrate how to calculate the AIC for a simple GLM (general linear model). I always think if you can understand the derivation of a statistic, it is much easier to remember how to use it.\nNow if you google derivation of the AIC, you are likely to run into a lot of math. But the principles are really not that complex. So here we will fit some simple GLMs, then derive a means to choose the ‘best’ one.\nSkip to the end if you just want to go over the basic principles.\nBefore we can understand the AIC though, we need to understand the statistical methodology of likelihoods.\n\n\nSay you have some data that are normally distributed with a mean of 5 and an sd of 3:\nset.seed(126)\nn &lt;- 50 #sample size\na &lt;- 5\nsdy &lt;- 3\ny &lt;- rnorm(n, mean = a, sd = sdy)\nhist(y)\n\nNow we want to estimate some parameters for the population that y was sampled from, like its mean and standard devaiation (which we know here to be 5 and 3, but in the real world you won’t know that).\nWe are going to use frequentist statistics to estimate those parameters. Philosophically this means we believe that there is ‘one true value’ for each parameter, and the data we observed are generated by this true value.\nm1 &lt;- glm(y ~ 1, family = \"gaussian\")\nsm1 &lt;- summary(m1)\nThe estimate of the mean is stored here coef(m1) =4.38, the estimated variance here sm1$dispersion= 5.91, or the SD sqrt(sm1$dispersion) =2.43. Just to be totally clear, we also specified that we believe the data follow a normal (AKA “Gaussian”) distribution.\nWe just fit a GLM asking R to estimate an intercept parameter (~1), which is simply the mean of y. We also get out an estimate of the SD (= \\(\\\\sqrt variance\\)) You might think its overkill to use a GLM to estimate the mean and SD, when we could just calculate them directly.\nWell notice now that R also estimated some other quantities, like the residual deviance and the AIC statistic.\nsummary(m1)\n\n##\n## Call:\n## glm(formula = y ~ 1, family = \"gaussian\")\n##\n## Deviance Residuals:\n##     Min       1Q   Median       3Q      Max\n## -5.7557  -0.9795   0.2853   1.7288   3.9583\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)   4.3837     0.3438   12.75   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## (Dispersion parameter for gaussian family taken to be 5.910122)\n##\n##     Null deviance: 289.6  on 49  degrees of freedom\n## Residual deviance: 289.6  on 49  degrees of freedom\n## AIC: 233.72\n##\n## Number of Fisher Scoring iterations: 2\nYou might also be aware that the deviance is a measure of model fit, much like the sums-of-squares. Note also that the value of the AIC is suspiciously close to the deviance. Despite its odd name, the concepts underlying the deviance are quite simple.\nAs I said above, we are observing data that are generated from a population with one true mean and one true SD. Given we know have estimates of these quantities that define a probability distribution, we could also estimate the likelihood of measuring a new value of y that say = 7.\nTo do this, we simply plug the estimated values into the equation for the normal distribution and ask for the relative likelihood of 7. We do this with the R function dnorm\nsdest &lt;- sqrt(sm1$dispersion)\ndnorm(7, mean = coef(m1), sd = sdest)\n\n## [1] 0.09196167\nFormally, this is the relative likelihood of the value 7 given the values of the mean and the SD that we estimated (=4.8 and 2.39 respectively if you are using the same random seed as me).\nYou might ask why the likelihood is greater than 1, surely, as it comes from a probability distribution, it should be &lt;1. Well, the normal distribution is continuous, which means it describes an infinte set of possible y values, so the probability of any given value will be zero. The relative likelihood on the other hand can be used to calculate the probability of a range of values.\nSo you might realise that calculating the likelihood of all the data would be a sensible way to measure how well our ‘model’ (just a mean and SD here) fits the data.\nHere’s what the likelihood looks like:\nplot(y, dnorm(y, mean = coef(m1), sd = sdest), ylab = \"Likelihood\")\n\nIt’s just a normal distribution.\nTo do this, think about how you would calculate the probability of multiple (independent) events. Say the chance I ride my bike to work on any given day is 3/5 and the chance it rains is 161/365 (like Vancouver!), then the chance I will ride in the rain[1] is 3/5 * 161/365 = about 1/4, so I best wear a coat if riding in Vancouver.\nWe can do the same for likelihoods, simply multiply the likelihood of each individual y value and we have the total likelihood. This will be a very small number, because we multiply a lot of small numbers by each other. So one trick we use is to sum the log of the likelihoods instead of multiplying them:\ny_lik &lt;- dnorm(y, mean = coef(m1), sd = sdest, log = TRUE)\nsum(y_lik)\n\n## [1] -114.8636\nThe larger (the less negative) the likelihood of our data given the model’s estimates, the ‘better’ the model fits the data. The deviance is calculated from the likelihood and for the deviance smaller values indicate a closer fit of the model to the data.\nThe parameter values that give us the smallest value of the -log-likelihood are termed the maximum likelihood estimates.\n\n\n\nNow say we have measurements and two covariates, x1 and x2, either of which we think might affect y:\na &lt;- 5\nb &lt;- 3\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nsdy &lt;- 1\ny &lt;- a + b*x1 + rnorm(n, sd = sdy)\npar(mfrow = c(1,2))\nplot(x1, y)\nplot(x2, y)\n\nSo x1 is a cause of y, but x2 does not affect y. How would we choose which hypothesis is most likely? Well one way would be to compare models with different combinations of covariates:\nm1 &lt;- glm(y ~ x1)\nm2 &lt;- glm(y ~ x2)\nm3 &lt;- glm(y ~ x1 + x2)\nNow we are fitting a line to y, so our estimate of the mean is now the line of best fit, it varies with the value of x1. To visualise this:\nplot(x1, y)\nlines(x1, predict(m1))\n\nThe predict(m1) gives the line of best fit, ie the mean value of y given each x1 value. We then use predict to get the likelihoods for each model:\nsm1 &lt;- summary(m1)\nsum(dnorm(y, mean = predict(m1), sd = sqrt(sm1$dispersion), log = TRUE))\n\n## [1] -125.6214\n\nsm2 &lt;- summary(m2)\nsum(dnorm(y, mean = predict(m2), sd = sqrt(sm2$dispersion), log = TRUE))\n\n## [1] -247.8059\n\nsm3 &lt;- summary(m3)\nsum(dnorm(y, mean = predict(m3), sd = sqrt(sm3$dispersion), log = TRUE))\n\n## [1] -125.4843\nThe likelihood of m1 is larger than m2, which makes sense because m2 has the ‘fake’ covariate in it. The likelihood for m3 (which has both x1 and x2 in it) is fractionally larger than the likelihood m1, so should we judge that model as giving nearly as good a representation of the data?\nBecause the likelihood is only a tiny bit larger, the addition of x2 has only explained a tiny amount of the variance in the data. But where do you draw the line between including and excluding x2? You run into a similar problem if you use R^2 for model selection.\nSo what if we penalize the likelihood by the number of paramaters we have to estimate to fit the model? Then if we include more covariates (and we estimate more slope parameters) only those that account for a lot of the variation will overcome the penalty.\nWhat we want a statistic that helps us select the most parsimonious model.\n\n\n\nOne way we could penalize the likelihood by the number of parameters is to add an amount to it that is proportional to the number of parameters. First, let’s multiply the log-likelihood by -2, so that it is positive and smaller values indicate a closer fit.\nLLm1 &lt;- sum(dnorm(y, mean = predict(m1), sd = sqrt(sm1$dispersion), log = TRUE))\n-2*LLm1\n\n## [1] 251.2428\nWhy its -2 not -1, I can’t quite remember, but I think just historical reasons.\nThen add 2*k, where k is the number of estimated parameters.\n-2*LLm1 + 2*3\n\n## [1] 257.2428\nFor m1 there are three parameters, one intercept, one slope and one standard deviation. Now, let’s calculate the AIC for all three models:\n-2*LLm1 + 2*3\n\n## [1] 257.2428\n\nLLm2 &lt;- sum(dnorm(y, mean = predict(m2), sd = sqrt(sm2$dispersion), log = TRUE))\n-2*LLm2 + 2*3\n\n## [1] 501.6118\n\nLLm3 &lt;- sum(dnorm(y, mean = predict(m3), sd = sqrt(sm3$dispersion), log = TRUE))\n-2*LLm3 + 2*4\n\n## [1] 258.9686\nWe see that model 1 has the lowest AIC and therefore has the most parsimonious fit. Model 1 now outperforms model 3 which had a slightly higher likelihood, but because of the extra covariate has a higher penalty too.\nYou don’t need to do these calculations yourself everytime you want an AIC. The function AIC(m3) will just give it to you.\n\n\n\nSo to summarize, the basic principles that guide the use of the AIC are:\n\nLower indicates a more parsimonious model, relative to a model fit with a higher AIC.\nIt is a relative measure of model parsimony, so it only has meaning if we compare the AIC for alternate hypotheses (= different models of the data).\nWe can compare non-nested models. For instance, we could compare a linear to a non-linear model.\nThe comparisons are only valid for models that are fit to the same response data (ie values of y).\nModel selection conducted with the AIC will choose the same model as leave-one-out cross validation (where we leave out one data point and fit the model, then evaluate its fit to that point) for large sample sizes.\nYou shouldn’t compare too many models with the AIC. You will run into the same problems with multiple model comparison as you would with p-values, in that you might by chance find a model with the lowest AIC, that isn’t truly the most appropriate model.\nWhen using the AIC you might end up with multiple models that perform similarly to each other. So you have similar evidence weights for different alternate hypotheses. In the example above m3 is actually about as good as m1.\nYou should correct for small sample sizes if you use the AIC with small sample sizes, by using the AICc statistic.\nYou can access the AIC in R with the AIC() function.\n\nMessage me on Twitter if you have comments on this post.\n[1] Assuming it rains all day, which is reasonable for Vancouver."
  },
  {
    "objectID": "posts/2018-04-13-how-to-use-the-AIC/index.html#explaining-likelihoods",
    "href": "posts/2018-04-13-how-to-use-the-AIC/index.html#explaining-likelihoods",
    "title": "How do I interpret the AIC",
    "section": "",
    "text": "Say you have some data that are normally distributed with a mean of 5 and an sd of 3:\nset.seed(126)\nn &lt;- 50 #sample size\na &lt;- 5\nsdy &lt;- 3\ny &lt;- rnorm(n, mean = a, sd = sdy)\nhist(y)\n\nNow we want to estimate some parameters for the population that y was sampled from, like its mean and standard devaiation (which we know here to be 5 and 3, but in the real world you won’t know that).\nWe are going to use frequentist statistics to estimate those parameters. Philosophically this means we believe that there is ‘one true value’ for each parameter, and the data we observed are generated by this true value.\nm1 &lt;- glm(y ~ 1, family = \"gaussian\")\nsm1 &lt;- summary(m1)\nThe estimate of the mean is stored here coef(m1) =4.38, the estimated variance here sm1$dispersion= 5.91, or the SD sqrt(sm1$dispersion) =2.43. Just to be totally clear, we also specified that we believe the data follow a normal (AKA “Gaussian”) distribution.\nWe just fit a GLM asking R to estimate an intercept parameter (~1), which is simply the mean of y. We also get out an estimate of the SD (= \\(\\\\sqrt variance\\)) You might think its overkill to use a GLM to estimate the mean and SD, when we could just calculate them directly.\nWell notice now that R also estimated some other quantities, like the residual deviance and the AIC statistic.\nsummary(m1)\n\n##\n## Call:\n## glm(formula = y ~ 1, family = \"gaussian\")\n##\n## Deviance Residuals:\n##     Min       1Q   Median       3Q      Max\n## -5.7557  -0.9795   0.2853   1.7288   3.9583\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)   4.3837     0.3438   12.75   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## (Dispersion parameter for gaussian family taken to be 5.910122)\n##\n##     Null deviance: 289.6  on 49  degrees of freedom\n## Residual deviance: 289.6  on 49  degrees of freedom\n## AIC: 233.72\n##\n## Number of Fisher Scoring iterations: 2\nYou might also be aware that the deviance is a measure of model fit, much like the sums-of-squares. Note also that the value of the AIC is suspiciously close to the deviance. Despite its odd name, the concepts underlying the deviance are quite simple.\nAs I said above, we are observing data that are generated from a population with one true mean and one true SD. Given we know have estimates of these quantities that define a probability distribution, we could also estimate the likelihood of measuring a new value of y that say = 7.\nTo do this, we simply plug the estimated values into the equation for the normal distribution and ask for the relative likelihood of 7. We do this with the R function dnorm\nsdest &lt;- sqrt(sm1$dispersion)\ndnorm(7, mean = coef(m1), sd = sdest)\n\n## [1] 0.09196167\nFormally, this is the relative likelihood of the value 7 given the values of the mean and the SD that we estimated (=4.8 and 2.39 respectively if you are using the same random seed as me).\nYou might ask why the likelihood is greater than 1, surely, as it comes from a probability distribution, it should be &lt;1. Well, the normal distribution is continuous, which means it describes an infinte set of possible y values, so the probability of any given value will be zero. The relative likelihood on the other hand can be used to calculate the probability of a range of values.\nSo you might realise that calculating the likelihood of all the data would be a sensible way to measure how well our ‘model’ (just a mean and SD here) fits the data.\nHere’s what the likelihood looks like:\nplot(y, dnorm(y, mean = coef(m1), sd = sdest), ylab = \"Likelihood\")\n\nIt’s just a normal distribution.\nTo do this, think about how you would calculate the probability of multiple (independent) events. Say the chance I ride my bike to work on any given day is 3/5 and the chance it rains is 161/365 (like Vancouver!), then the chance I will ride in the rain[1] is 3/5 * 161/365 = about 1/4, so I best wear a coat if riding in Vancouver.\nWe can do the same for likelihoods, simply multiply the likelihood of each individual y value and we have the total likelihood. This will be a very small number, because we multiply a lot of small numbers by each other. So one trick we use is to sum the log of the likelihoods instead of multiplying them:\ny_lik &lt;- dnorm(y, mean = coef(m1), sd = sdest, log = TRUE)\nsum(y_lik)\n\n## [1] -114.8636\nThe larger (the less negative) the likelihood of our data given the model’s estimates, the ‘better’ the model fits the data. The deviance is calculated from the likelihood and for the deviance smaller values indicate a closer fit of the model to the data.\nThe parameter values that give us the smallest value of the -log-likelihood are termed the maximum likelihood estimates."
  },
  {
    "objectID": "posts/2018-04-13-how-to-use-the-AIC/index.html#comparing-alternate-hypotheses-with-likelihoods",
    "href": "posts/2018-04-13-how-to-use-the-AIC/index.html#comparing-alternate-hypotheses-with-likelihoods",
    "title": "How do I interpret the AIC",
    "section": "",
    "text": "Now say we have measurements and two covariates, x1 and x2, either of which we think might affect y:\na &lt;- 5\nb &lt;- 3\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nsdy &lt;- 1\ny &lt;- a + b*x1 + rnorm(n, sd = sdy)\npar(mfrow = c(1,2))\nplot(x1, y)\nplot(x2, y)\n\nSo x1 is a cause of y, but x2 does not affect y. How would we choose which hypothesis is most likely? Well one way would be to compare models with different combinations of covariates:\nm1 &lt;- glm(y ~ x1)\nm2 &lt;- glm(y ~ x2)\nm3 &lt;- glm(y ~ x1 + x2)\nNow we are fitting a line to y, so our estimate of the mean is now the line of best fit, it varies with the value of x1. To visualise this:\nplot(x1, y)\nlines(x1, predict(m1))\n\nThe predict(m1) gives the line of best fit, ie the mean value of y given each x1 value. We then use predict to get the likelihoods for each model:\nsm1 &lt;- summary(m1)\nsum(dnorm(y, mean = predict(m1), sd = sqrt(sm1$dispersion), log = TRUE))\n\n## [1] -125.6214\n\nsm2 &lt;- summary(m2)\nsum(dnorm(y, mean = predict(m2), sd = sqrt(sm2$dispersion), log = TRUE))\n\n## [1] -247.8059\n\nsm3 &lt;- summary(m3)\nsum(dnorm(y, mean = predict(m3), sd = sqrt(sm3$dispersion), log = TRUE))\n\n## [1] -125.4843\nThe likelihood of m1 is larger than m2, which makes sense because m2 has the ‘fake’ covariate in it. The likelihood for m3 (which has both x1 and x2 in it) is fractionally larger than the likelihood m1, so should we judge that model as giving nearly as good a representation of the data?\nBecause the likelihood is only a tiny bit larger, the addition of x2 has only explained a tiny amount of the variance in the data. But where do you draw the line between including and excluding x2? You run into a similar problem if you use R^2 for model selection.\nSo what if we penalize the likelihood by the number of paramaters we have to estimate to fit the model? Then if we include more covariates (and we estimate more slope parameters) only those that account for a lot of the variation will overcome the penalty.\nWhat we want a statistic that helps us select the most parsimonious model."
  },
  {
    "objectID": "posts/2018-04-13-how-to-use-the-AIC/index.html#the-aic-as-a-measure-of-parsimony",
    "href": "posts/2018-04-13-how-to-use-the-AIC/index.html#the-aic-as-a-measure-of-parsimony",
    "title": "How do I interpret the AIC",
    "section": "",
    "text": "One way we could penalize the likelihood by the number of parameters is to add an amount to it that is proportional to the number of parameters. First, let’s multiply the log-likelihood by -2, so that it is positive and smaller values indicate a closer fit.\nLLm1 &lt;- sum(dnorm(y, mean = predict(m1), sd = sqrt(sm1$dispersion), log = TRUE))\n-2*LLm1\n\n## [1] 251.2428\nWhy its -2 not -1, I can’t quite remember, but I think just historical reasons.\nThen add 2*k, where k is the number of estimated parameters.\n-2*LLm1 + 2*3\n\n## [1] 257.2428\nFor m1 there are three parameters, one intercept, one slope and one standard deviation. Now, let’s calculate the AIC for all three models:\n-2*LLm1 + 2*3\n\n## [1] 257.2428\n\nLLm2 &lt;- sum(dnorm(y, mean = predict(m2), sd = sqrt(sm2$dispersion), log = TRUE))\n-2*LLm2 + 2*3\n\n## [1] 501.6118\n\nLLm3 &lt;- sum(dnorm(y, mean = predict(m3), sd = sqrt(sm3$dispersion), log = TRUE))\n-2*LLm3 + 2*4\n\n## [1] 258.9686\nWe see that model 1 has the lowest AIC and therefore has the most parsimonious fit. Model 1 now outperforms model 3 which had a slightly higher likelihood, but because of the extra covariate has a higher penalty too.\nYou don’t need to do these calculations yourself everytime you want an AIC. The function AIC(m3) will just give it to you."
  },
  {
    "objectID": "posts/2018-04-13-how-to-use-the-AIC/index.html#aic-basic-principles",
    "href": "posts/2018-04-13-how-to-use-the-AIC/index.html#aic-basic-principles",
    "title": "How do I interpret the AIC",
    "section": "",
    "text": "So to summarize, the basic principles that guide the use of the AIC are:\n\nLower indicates a more parsimonious model, relative to a model fit with a higher AIC.\nIt is a relative measure of model parsimony, so it only has meaning if we compare the AIC for alternate hypotheses (= different models of the data).\nWe can compare non-nested models. For instance, we could compare a linear to a non-linear model.\nThe comparisons are only valid for models that are fit to the same response data (ie values of y).\nModel selection conducted with the AIC will choose the same model as leave-one-out cross validation (where we leave out one data point and fit the model, then evaluate its fit to that point) for large sample sizes.\nYou shouldn’t compare too many models with the AIC. You will run into the same problems with multiple model comparison as you would with p-values, in that you might by chance find a model with the lowest AIC, that isn’t truly the most appropriate model.\nWhen using the AIC you might end up with multiple models that perform similarly to each other. So you have similar evidence weights for different alternate hypotheses. In the example above m3 is actually about as good as m1.\nYou should correct for small sample sizes if you use the AIC with small sample sizes, by using the AICc statistic.\nYou can access the AIC in R with the AIC() function.\n\nMessage me on Twitter if you have comments on this post.\n[1] Assuming it rains all day, which is reasonable for Vancouver."
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "",
    "text": "Here are the notes from my talk on dataviz at the 2018 UQ Winterschool in Bioinformatics.\nThese notes run through some of the principles I present and also show R code for generating data visuals.\nIf you want to take the dataviz quiz first, then click here. We will look at the results later using R.\n{% include r-courses-ad.html %}"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#data-visuals-in-r-2018",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#data-visuals-in-r-2018",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "",
    "text": "Here are the notes from my talk on dataviz at the 2018 UQ Winterschool in Bioinformatics.\nThese notes run through some of the principles I present and also show R code for generating data visuals.\nIf you want to take the dataviz quiz first, then click here. We will look at the results later using R.\n{% include r-courses-ad.html %}"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#graphics-packages-in-r",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#graphics-packages-in-r",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Graphics packages in R",
    "text": "Graphics packages in R\nThe dominant options are the base graphics R comes shipped with and the ggplot2 package."
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#plot-your-data",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#plot-your-data",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Plot your data",
    "text": "Plot your data\n\nMake your own Datasaurus dozen\nThe datasaurus is a great example of why you should view your data, invented by Alberto Cairo. See Steph Locke’s code and package on github for making this in R.\nlibrary(datasauRus)\ndatnames &lt;- rev(unique(datasaurus_dozen$dataset))\nnlevels &lt;- length(datnames)\n\nfor (i in 1:nlevels){\n  i &lt;- which(datasaurus_dozen$dataset == datnames[i])\n  plot(datasaurus_dozen$x[i], datasaurus_dozen$y[i],\n     xlab = \"x\", ylab = \"y\",   las = 1)\n  Sys.sleep(1)\n}\n\nConvince yourself that the mean, sd and correlation is the same in all of these plots:\nlibrary(dplyr)\ndatasaurus_dozen %&gt;% group_by(dataset) %&gt;%\n    summarize(meanx = mean(x), meany = mean(y),\n              sdx = sd(x), sdy = sd(y),\n              corr = cor(x,y))\nWe can also save these as .png images to make a .gif image (see also here)\nfor (ilvs in 1:nlevels){\n  i &lt;- which(datasaurus_dozen$dataset == datnames[ilvs])\n  thiscol &lt;- ifelse(datnames[ilvs] == \"dino\", \"darkseagreen\", \"grey20\")\n  png(filename = paste0(\"datasaurus/\",datnames[ilvs],\".png\"))\n  plot(datasaurus_dozen$x[i], datasaurus_dozen$y[i],\n     xlab = \"x\", ylab = \"y\",   las = 1,\n      xlim = c(10, 105), ylim = c(-5, 105), col = thiscol, pch = 16)\n  dev.off()\n}"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#clarity-not-simplicity",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#clarity-not-simplicity",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Clarity not simplicity",
    "text": "Clarity not simplicity\nI give the example of the famous ‘hockey stick’ graph of Northern Hemisphere temperatures."
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#dataviz-are-models",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#dataviz-are-models",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Dataviz are models",
    "text": "Dataviz are models\n\nAny visualization is a model\n\nAlberto Cairo 2016\n\nDifferent viz models for the same data\n\n\nThree ways of visualising the same x-y data\nEach of these graphs of the same data has a slightly different interpretation.\nx &lt;- 1:100\ny &lt;- x + rnorm(100, sd=30)\nplot(x,y, pch = 16, col = grey(0.5, 0.5))\n\nmod1 &lt;- lm(y ~ x)\nplot(x,y, col = 'white')\nabline(mod1, lwd = 3, col = 'red')\n\nlibrary(MASS)\nfilled.contour(kde2d(x,y), scale = F)\n\nplot(x,y, pch = 16, col = grey(0.5, 0.5))\nabline(mod1, lwd = 3, col = 'red')"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#models-help-clarify-complex-data-sets",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#models-help-clarify-complex-data-sets",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Models help clarify complex data-sets",
    "text": "Models help clarify complex data-sets\n\nEffect size often has to been seen to be understood\nWhen doing confirmatory analysis, we might want to know how strong an effect is. Data viz is very useful for this task. Lets compare two datasets that have similar p-values, but very different effect sizes\nset.seed(42)\nx &lt;- rnorm(1000)\nset.seed(420)\ny &lt;- 5*x + 3 + rnorm(1000, sd = 15)\nset.seed(420)\ny2 &lt;- 5*x + 3 + rnorm(1000, sd = 1)\n\nmod1 &lt;- lm(y ~ x)\nmod2 &lt;- lm(y2 ~ x)\n\n#Compare the pvalues on the slopes\nsummary(mod1)\n\n##\n## Call:\n## lm(formula = y ~ x)\n##\n## Residuals:\n##     Min      1Q  Median      3Q     Max\n## -43.201 -10.330   0.395   9.634  46.694\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   2.8054     0.4614   6.080 1.71e-09 ***\n## x             4.2096     0.4603   9.145  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 14.59 on 998 degrees of freedom\n## Multiple R-squared:  0.07732,    Adjusted R-squared:  0.07639\n## F-statistic: 83.63 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod2)\n\n##\n## Call:\n## lm(formula = y2 ~ x)\n##\n## Residuals:\n##      Min       1Q   Median       3Q      Max\n## -2.88004 -0.68868  0.02634  0.64229  3.11291\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  2.98703    0.03076   97.11   &lt;2e-16 ***\n## x            4.94731    0.03069  161.21   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 0.9724 on 998 degrees of freedom\n## Multiple R-squared:  0.963,  Adjusted R-squared:  0.963\n## F-statistic: 2.599e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow = c(1,2))\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\nplot(x,y2, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod2, lwd = 2, col = 'red')"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#superplots",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#superplots",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Superplots",
    "text": "Superplots\nAndrew Gelman coined the term superplots for plotting different models on multiple panels of a graph so you can visually compare them.\nFor instance, say we have several time-series and we want to know if they deviate from each other signficantly. An easy way to compare them is to fit splines to each time-series and then just plot them next to each other, with SEs. Then we can compare visually for ‘signficant’ differences.\nHere’s some code to simulate three made-up series. The first two have the same trend, but different observation errors, the third has a different trend:\ntmax &lt;- 50\ndrift &lt;- c(10, -5)\nsd &lt;- 40\nsdobs &lt;- 200\nset.seed(5)\nyzero &lt;-  cumsum(rnorm(n=tmax, mean=drift[1], sd=sd))\ny1 &lt;- yzero + rnorm(n = tmax, mean = 0, sd = sdobs)\ny2 &lt;- yzero + rnorm(n = tmax, mean = 0, sd = sdobs)\ny3 &lt;- cumsum(rnorm(n=tmax, mean=drift[2], sd=sd)) +\n  rnorm(n = tmax, mean = 0, sd = sdobs)\ndat &lt;- data.frame(ts = rep(letters[1:3], each = tmax), x = rep(1:tmax, 3), y = c(y1, y2, y3))\nWe can easily plot these three series using ggplot2 and automatically add a spline.\nlibrary(ggplot2)\nggplot(dat, aes(x = x, y = y)) +\n    geom_point() +\n    facet_grid(.~ts) +\n    stat_smooth(method = \"loess\", se = TRUE) +\n   theme(axis.text = element_text(size=14),\n        axis.title = element_text(size=16,face=\"bold\"),\n          strip.text.x = element_text(size = 16),\n          panel.background = element_rect(fill = 'white', colour = 'white'))"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#length-is-most-accurate",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#length-is-most-accurate",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Length is most accurate",
    "text": "Length is most accurate\nWays of comparing data in order from most accurate (top) to more generic (bottom)."
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#comparing-volume-and-area",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#comparing-volume-and-area",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Comparing volume and area",
    "text": "Comparing volume and area\nCompare these. Note that if we compare circles we should use area, not the radius or diameter to scale their size.\nn &lt;- c(10, 5)\nbarplot(n, col = 'skyblue', xaxt = 'n', yaxt = 'n')\n\nrad1 &lt;- 1\narea1 &lt;- pi*(rad1^2)\narea2 &lt;- area1/2\nrad2 &lt;- sqrt(area2/pi)\n\npar(mfrow = c(1,2), mar = c(0,0,0,0))\npie(1, col = 'skyblue', labels = NA, border = NA, radius = rad1)\npie(1, col = 'skyblue', labels = NA, border = NA, radius = rad2)\n\n\nExploration of data\nLet’s create a point cloud to demonstrate some data exploration techniques\nset.seed(42)\nx &lt;- rnorm(1000)\ny &lt;- 5*x + 3 + rnorm(1000, sd = 15)\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\n\nCan’t see alot here. A linear model might help us explore if there is any trend going on:\nmod1 &lt;- lm(y ~ x)\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\n\nxnew &lt;- seq(min(x), max(x), length.out = 100)\npmod &lt;- predict(mod1, newdata =data.frame(x=xnew),  se = T)\nlines(xnew, pmod$fit + pmod$se.fit, lwd = 2, col = 'red', lty = 2)\nlines(xnew, pmod$fit - pmod$se.fit, lwd = 2, col = 'red', lty = 2)\n\nWhat about identifying extreme points, that may be worth investigating further? We can pick out points that are greater than 2SDs from the trend:\nmodresid &lt;- resid(mod1)\nsd2 &lt;- sd(modresid)*2\nipt &lt;- which(abs(modresid) &gt; sd2)\n\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\npoints(x[ipt], y[ipt], pch = 16, col = rgb(1,0,0, 0.6))\n\n\n\nThe problem with bar charts\nHere is an interesting blog on the problem with barcharts.\nFirst, let’s generate some data to play with, it has three variables x, y and z.\nn &lt;- 40\nx &lt;- rnorm(n, mean = 20, sd = 8)\ny &lt;- rnorm(n, mean = 20, sd = 3)\nz &lt;- rnorm(n, mean = 10, sd = 8)\n\ndat &lt;- data.frame(grp = c(rep(\"x\", n), rep(\"y\", n), rep(\"z\", n)),\n                          val = c(x,y,z))\nNow compare these three charts, the first is just the means:\nsdat &lt;- dat %&gt;% group_by(grp) %&gt;%\n  summarize(val = mean(val))\n\nplot(1:3, sdat$val, type = 'p', xaxt = \"n\", xlab = '', ylab = '',\n     las = 1, cex.axis = 1.5, pch = 16, cex = 2)\naxis(1, at = 1:3, labels = sdat$grp, cex.axis = 1.5)\nThe second has standard errors and so captures some of the variation in our data :\nggplot(dat, aes(x = grp, y = val)) +\n  stat_summary(fun.data = \"mean_se\", size = 1.3) +\n  xlab(\"Group\") +\n  ylab(\"Values\") +\n  theme_bw() +\n  theme(axis.text=element_text(size=16),\n       axis.title=element_text(size=18))\n\nThe third has boxplots, so is perhaps the most truthful, because it shows quantiles and the full range of the data (which is much broader than just the SEs).\nggplot(dat, aes(x = grp, y = val)) +\n  geom_boxplot(color = \"black\") +\n  xlab(\"Group\") +\n  ylab(\"Values\") +\n  theme_bw() +\n  theme(axis.text=element_text(size=16),\n        axis.title=element_text(size=18))\n\n\n\nDon’t waste digital ink\nPlots with less ‘add-ons’ tend to communicate the key message more clearly. For instance, just like excel plots dont:\nx &lt;- rnorm(100)\ndat &lt;- data.frame(x = x, y = 0.25*x + rnorm(100, sd = 0.2))\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_excel() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\nYou can get additional themes for ggplot2 using this excellent package.\nA cleaner view:\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_base() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\nOr simply:\nplot(dat$x, dat$y, xlab = \"x\", ylab = \"y\", las = 1)\n\nA good principle is to not use ‘ink’ on figures that isn’t needed to communicate your message. Tufte takes the ‘less ink’ philosophy to the extreme:\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_tufte() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\n\n\nWhen is ggplot2 appropriate, or when should I use base R?\nIn general I think ggplot2 is appropriate for problems of intermediate complexity. Like this:\n Base R is great if you just want to plot a barplot quickly, or do an x-y plot. ggplot2 comes into its own for slight more complex plots, like having multiple panels for different groups or colouring lines by a 3rd factor. But once you move to really complex plots, like overlaying a subplot on a map, ggplot2 becomes very difficult, if not impossible. At that point it is better to move back to Base R. ggplot2 can also get very fiddly if you are very specific about your plots, like having certain colours, or the labels in a certain way.\nAs an example, ggplot2 is great for data like this:\nx1 &lt;- rnorm(30)\ngrps &lt;- letters[c(rep(1, 10), rep(2, 10), rep(3, 10))]\ny1 &lt;- x1 + c(rep(1, 10), rep(-1, 10), rep(2, 10)) + rnorm(30)\ndat &lt;- data.frame(x = x1, grps = grps, y = y1)\nhead(dat)\n\n##            x grps         y\n## 1  0.3293278    a 1.5458494\n## 2 -0.2975122    a 0.5877379\n## 3  1.1849880    a 2.2053261\n## 4  1.3095354    a 3.2023256\n## 5 -1.1704553    a 1.2776289\n## 6  0.9961189    a 1.7440880\n\nggplot(dat, aes(x = x1, y = y1, color = grps)) +\n  geom_point() + theme_bw()\n\nIt is also pretty handy for faceting:\nggplot(dat, aes(x = x1, y = y1)) +\n  geom_point() + facet_grid(.~grps)+\n  theme_bw()\n\nThe key with ggplot2 is to have your data in a data-frame.\nIn reality both ggplot2 and base R graphics are worth learning, but I would start with learning the basics of base R graphics and then move onto ggplot2 if you want to quickly plot lots of structured data-sets.\n\n\nPie graphs vs bar graphs\nIn Mariani et al. they plot rates of seafood fraud by several European countries. While its a foundational study that establishes improvement in the accuracy of food labelling, their graphics could be improved in several ways.\nFirst they use perspective pies. This makes it incredibly hard to compare the two groups (fish that are labelled/mislabelled). Humans are very bad at comparing angles and pretty bad at comparing areas. With the perspective you can’t even compare the areas properly. They do provide the raw numbers, but then, why bother with the pies?\nNote that the % pies misrepresent the data slightly because the % figures are actually odds ratios (mis-labels / correct labels), rather than percent (mis-labeels / total samples).\nSecond the pies are coloured red/green, which will be hard for red-green colourblind people to see.\nThird, they have coloured land blue on their map, so it appears to be ocean at first look.\nFourth, the map is not really neccessary. There are no spatial patterns going on that the authors want to draw attention to. I guess having a map does emphasize that the study is in Europe. Finally, the size of each pie is scaled to the sample size, but the scale bar for the sample size shows a sample of only 30, whereas most of their data are for much larger samples sizes (&gt;200). Do you get the impression from the pies that the UK has 668 samples, whereas Ireland only has 187? Therefore, from this graphic we have no idea what sample size was used in each country.\nIn fact, all the numbers that are difficult to interpret in the figure are very nicely presented in Table 1.\nBelow is a start at improving the presentation. For instance, you could do a simple bar chart, ordering by rate of mislabelling.\ncnames &lt;- c('Ireland' ,'UK','Germany','France','Spain','Portugal')\ncorrlab &lt;- c(180, 647, 145, 146, 267, 178)\nmislab &lt;- c(7, 21, 9, 4, 24, 12)\nmisrate &lt;- 100*signif(mislab / (corrlab + mislab),2)                  \ncorrrate &lt;- 100 - misrate\nord &lt;- order(misrate, decreasing = T)\ny &lt;- rbind(corrrate, misrate)\n\npar(mar = c(5,4,4,7))\nbarplot(y[,ord], names.arg = cnames[ord], col = c('skyblue','tomato'), ylab = 'Labelling rate (%)', las = 2)\nlegend(x=7.5, y = 90, legend = c(\"Mislabelled\", \"Correctly labelled\"), pch = 16, col = c('tomato','skyblue'), xpd = NA, cex = 0.7)\n\nYou could add another subfigure to this, showing the rate by different species too.\nThe barplot doesn’t communicate the sample size, but then that is probably not the main point. The sample sizes are probably best reported in the table\nIf we felt the map was essential, then putting barcharts on it would be more informative. It is not that easy to add barcharts ontop of an existing map in R, so I would recommend creating the barcharts seperately, then adding them on in Illustrator or Powerpoint.\nWe can make a basic map like this:\nlibrary(maps)\nlibrary(maptools)\nmap('world', xlim = c(-20, 20), ylim = c(35, 60), col = 'grey', fill = T)\n\nThen create some nice barcharts. We write a loop so we get one barchart for each country.\nnc &lt;- length(cnames)\npar(mfrow = c(2,3), oma = c(1,1,1,3))\nfor (i in 1:nc){\n  y &lt;- c(mislab[i], corrlab[i])\n  barplot(y, names.arg = '', las = 2, col = c('tomato','skyblue'), ylim = c(0, corrlab[i]), main = cnames[i], cex.main = 2.4, yaxt = 'n')\n  byy &lt;- signif(max(y),2)/3\n  yat &lt;- c(0, min(y), max(y))\naxis(2, at = yat, las = 2, cex.axis = 2, labels = F)  \naxis(2, at = yat[2:3], las = 2, cex.axis = 2, labels = T)  \n}\nlegend(x = 2.8, y = 500, legend = c('Fraud', 'Correct'), pch = 15, col = c('tomato','skyblue'), xpd = NA, cex = 2, bty = 'n')\n\n\n\nScaling matters\nIt can be misleading to present % and proportion data on axes that are not scaled 0 - 100%. For instance, compare these three graphs:\ny &lt;- c(70, 72, 68, 73)\nx &lt;- 1:4\nxnams &lt;- LETTERS[1:4]\n\npar(mfrow = c(1,3), oma = c(1,1,1,3), mar = c(5,6,2,2))\nplot(x,y, pch = 3, cex = 2, las = 1, xaxt  = 'n', xlab = '', ylab = 'Percent', cex.axis = 2, cex.lab = 2, tcl = 0.5, xlim = c(0, 5), col = 'red', lwd = 3)\naxis(1, at = x, labels = xnams, cex.axis = 2, tcl = 0.5)\n\nbarplot(y, names.arg = xnams, las = 1, cex.axis = 2, cex.lab = 2, cex.names = 2, ylab = 'Percent')\n\nbarplot(y, names.arg = xnams, las = 1, cex.axis = 2, cex.lab = 2, cex.names = 2, ylab = 'Percent', ylim = c(0, 100))"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#interpreting-rates",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#interpreting-rates",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Interpreting rates",
    "text": "Interpreting rates\nThe units you use affect how people interpret your graph.\nPeople are bad at interpreting rates, we just can’t get our heads around accumulation very well. Here is a numerical example. Check out the below figure and ask yourself:\n\nAt what time is the number of people in the shopping centre declining?\n\n\nWould you say it is at point A, B, C or D?\nLet’s plot the total number of people:\n\nHopefully the answer is obvious now. So the right scales can help make interpretation much easier.\nYou could also rephrase the question from when is the total number decreasing to when is the number entering less than the number of people leaving?"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#choosing-colour-scales",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#choosing-colour-scales",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Choosing colour scales",
    "text": "Choosing colour scales\nAlot of thought should go into choosing colour scales for graphs for instance- will it print ok? will colour blind people be able to see this? does the scale create artificial visual breaks in the data? Luckily there is a package to help you make the right decision for a colour scale, it is called RColorBrewer. Check out colorbrewer.org for a helpful interactive web interface for choosing colours.\nFirst let’s load some sea surface temperature data as a raster:\nlibrary(raster)\nr &lt;- raster(\"MeanAVHRRSST\")\n\nlibrary(RColorBrewer)\npar(mfrow = c(1,2))\nplot(r, col = rev(brewer.pal(11, \"Spectral\")), asp = NA)\nplot(r, col = brewer.pal(11, \"Purples\"), asp = NA)\n\n## Warning in brewer.pal(11, \"Purples\"): n too large, allowed maximum for palette Purples is 9\n## Returning the palette you asked for with that many colors\n\nUsing red-green palettes makes it hard for colour blind people. Also, using a diverging palette makes it look like there is something important about the middle point (yellow). A better palette to use would be one of the sequential ones, “Purples” shown here.\nTo make it easier to understand, let’s look at these again as contour plots. I will use a more appropriate diverging palette to the red-green one though.\nz &lt;- matrix(rep(1:10, 10), nrow = 10)\nfilled.contour(z, col = brewer.pal(9, 'Reds'), nlevels = 10)\n\nfilled.contour(z, col = brewer.pal(9, 'RdBu'), nlevels = 10)\n\nNotice the diverging pallette creates an artificial split at yellow\nOne of the only legitimate uses for pie graphs (I think) is visualising the colour scales. Here is how:\nreds &lt;- brewer.pal(9, 'Reds')\ngreens &lt;- brewer.pal(9, 'Greens')\nblues &lt;- brewer.pal(9, 'Blues')\nrdylgn &lt;- brewer.pal(9, 'RdYlGn')\nrdbu &lt;- brewer.pal(9, 'RdBu')\ndark2 &lt;- brewer.pal(8, 'Dark2')\n\npar(mfrow = c(2,3), mar = c(0,0,0,0), oma = c(0,0,0,0))\npie(rep(1, 9), col = reds)\npie(rep(1, 9), col = greens)\npie(rep(1, 9), col = blues)\npie(rep(1, 9), col = rdylgn)\npie(rep(1, 9), col = rdbu)\npie(rep(1, 9), col = dark2)"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#r-the-intergrator",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#r-the-intergrator",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "R the intergrator",
    "text": "R the intergrator\nIn my talk I give some examples of how R can integrate everything from data input, data processing, analysis, visualisation to sharing results (even as interactive web content). The results for the quiz you took above are available as a google spreadsheet. We can use R to read that data and visualise it, and even share the results as a webpage or on Twitter.\nHere are the quiz results\nYou can access a google sheet with the googlesheets package:\nlibrary(googlesheets)\ndataviz &lt;- gs_url(\"https://docs.google.com/spreadsheets/d/10i3v3NIVpgmURyLVzsiadPAMGeqa7dLFcDb9sqFe8KA/edit#gid=1513779153)\")\ngsheet &lt;- gs_read(dataviz)\nNow the column names are just the questions in the quiz, so let’s rename them for convenience.\nnewnames &lt;- c(\"timestamp\", \"shopping\",\n              \"bar_percent\",\n              \"pie_percent\",\n              \"room\",\n              \"cb_age\")\nnames(gsheet) &lt;- newnames\nNow we are ready to do our plots. For instance, to see what people said about my age:\nggplot(gsheet, aes(cb_age)) +\n  geom_histogram()\nHow did people answer the shopping quiz question?\nggplot(gsheet, aes(shopping)) +\n  geom_bar()\nWhat about looking at the accuracy of the pie chart vs bar chart question. To do that we first need to calculate an accuracy metric. We will use the dplyr package to do that:\nlibrary(dplyr)\nlibrary(tidyr)\noutcomes &lt;- c(72, 17)\ngsheet2 &lt;- gsheet %&gt;%\n  mutate(bar_accuracy = abs(bar_percent - outcomes[1]),\n    pie_accuracy = abs(pie_percent - outcomes[2])) %&gt;%\n  select(bar_accuracy, pie_accuracy, cb_age) %&gt;%\n  gather(dataviz, accuracy, -cb_age)\nHere accuracy is just the quiz answers minus the correct answer (in outcomes) and then we take its absolute value.\nWe can plot this like so:\nggplot(gsheet2, aes(dataviz, accuracy)) +\n  geom_boxplot()\nWe could also look to see how accuracy compares to guesses of my age (obviously younger guesses at my age will be more accurate ;)), and plot a model of age vs accuracy:\nggplot(gsheet2, aes(cb_age, accuracy)) +\n  geom_point() +\n  facet_grid(.~dataviz) +\n  stat_smooth()"
  },
  {
    "objectID": "posts/2018-07-04-dataviz2018_notes/index.html#resources-and-further-reading",
    "href": "posts/2018-07-04-dataviz2018_notes/index.html#resources-and-further-reading",
    "title": "A brief guide to data visuals in R in 2018",
    "section": "Resources and further reading",
    "text": "Resources and further reading\n\nThe Truthful Art by Alberto Cairo\nBooks by Edward Tufte\nTwitter #dataviz\nAn infographic of chart types: Visual vocabulary\nWTF Visualizations, on Twitter\nThe dataviz catalogue\nCleveland and McGill “Graphical Perception of data classic” study published in Science 1985"
  },
  {
    "objectID": "posts/2022-09-05-new-vlog-conservation-hackers/index.html",
    "href": "posts/2022-09-05-new-vlog-conservation-hackers/index.html",
    "title": "New vlog Conservation Hackers",
    "section": "",
    "text": "I haven’t been writing blog posts for a while because I have been spending my time on a new project: the Conservation Hackers vlog.\nI wanted to be able to share the interesting conversations I have with scientists and get the stories behind the research papers. I’m especially interested in making the connection to real world impact on the conservation of ecosystems.\nI’m currently experimenting with a number of different vlog formats, including shorts, how to videos and interviews with leading conservation scientists.\nSo head over to YouTube, check out some content and leave me a comment to let me know what you like."
  },
  {
    "objectID": "posts/2025-08-13-validating-predictive-models/index.html",
    "href": "posts/2025-08-13-validating-predictive-models/index.html",
    "title": "It’s getting harder to predict ecological change",
    "section": "",
    "text": "The increasing pace of climate change and human impacts on ecosystems is pushing ecosystems beyond the envelope of past conditions. The pace of change makes predicting that change much harder. In a new study, Assessing predictive accuracy of species abundance models in dynamic systems, we show how to test model forecasts in rapidly changing ecosystems.\nGood tests of model forecasts help us quantify uncertainty and are important for many management applications including fisheries stock forecasts, species extinction risk and climate-smart conservation planning.\nThe pace of change is a problem for making predictions because our models are trained on historical data. Model’s predict best when they are predicting to conditions they’ve ‘seen’ in the training data. Unprecedented environmental change means our models need to extrapolate. This is much harder.\nOne recent example is the deadly algal bloom across 500km of South Australia’s coast. No ones saw this coming. No-one yet knows if its likely to reoccur or not.\nWe developed a way to test models with historical data to get an idea of how well they extrapolate. This gives us a better measurement of model uncertainty that we can use to calibrate risk, e.g. to see how much we might be under-estimating extinction risk or overestimating fish stock size.\nWe illustrated our method with a 30 year timeseries of reef biota from Maria Island. We split this time-series into sections used for training and sections used for testing the model. The model is then fit to the training data and forecast to the testing data. The difference between its forecast and the test data is a measure of its errors.\nOur new method forces the model to fit to ‘out-of-date’ data. Whereas, the typical method keeps the models contemporary to the testing data. g We confirmed that for a species with a population collapse our new method gives a much more pessimistic (broader) estimate of uncertainty than the typical method would do.\nWe suggest our method be used to better estimate risk measures when using models to forecast ecological change. Our method can also be used in meta-studies that are exploring how predictability varies across different species, places and times. This is a point we’ll pick up in our next study. Read more here."
  },
  {
    "objectID": "posts/2019-04-12-negativity-bias/index.html",
    "href": "posts/2019-04-12-negativity-bias/index.html",
    "title": "Dodging negativity bias in a critical work culture",
    "section": "",
    "text": "Academic culture is critical. Criticizing each other’s work is how scientists improve hypotheses and theories. Criticism is an important part of advancing knowledge.\nIn any given week academics are force-fed criticism from student evaluations, peer-review of papers, presentations and workplace development reviews. I dish out my fare share of criticism too, though hopefully it’s mostly of the constructive flavour.\nBut academic culture is perpetrated by human individuals, who have feelings. Sometimes all the criticism can get us down.\nHere’s my thoughts on dodging the negativity bias."
  },
  {
    "objectID": "posts/2019-04-12-negativity-bias/index.html#negativity-bias",
    "href": "posts/2019-04-12-negativity-bias/index.html#negativity-bias",
    "title": "Dodging negativity bias in a critical work culture",
    "section": "Negativity bias",
    "text": "Negativity bias\nWhen the criticism seems overwhelming we may be experiencing ‘negativity bias’, which is the tendency to be more aware of negative than positive feedback.\nWe were discussing in our lab group how to be supportive of students who were facing tough parts of their PhDs. My student suggested we be more aware of negativity bias.\nBut it wasn’t clear to me what to do if we feel this bias, other than to be aware of it.\nRecently I was faced with three doses of criticism, one positive, one ambivalent and one negative. I received peer reviews on a paper I had submitted to a journal.\n(The positive one thought the paper was well written, the negative one thought it was poorly written. This seems to be a common problem for me, if anyone has any insights on why, I’d love to hear. Maybe its just that some people don’t like the writing style of an Australian who went to school in the 90s, whereas others don’t mind that style?)\nThe negative review left a bad taste and I wasted my ride home that day thinking about it. Even though I could have equally well ridden home feeling great about the positive review.\nBut then I found being more aware of my own negativity bias did help. It helped in two ways: thinking differently and acting differently."
  },
  {
    "objectID": "posts/2019-04-12-negativity-bias/index.html#thinking-differently",
    "href": "posts/2019-04-12-negativity-bias/index.html#thinking-differently",
    "title": "Dodging negativity bias in a critical work culture",
    "section": "Thinking differently",
    "text": "Thinking differently\nIt is easy to tell someone to think differently, but quite hard to do. We have to train our minds to think differently.\n\nBeing more aware to think less about it negative thoughts\nOne way is to try to think less about the negative criticism. But in trying not to think about something we tend to think about it even more.\nSo the trick (from meditation practice) is to be self-aware of what you are thinking. Every time you think the negative thought, acknowledge it and then let it go. Keep doing this over and over, eventually you will stop thinking the negative thought.\nIt can help to have a visualization to go along with this process of acknowledging and letting go. A classic is thinking of the thoughts as leaves floating down a stream. You see them, acknowledge them, then let them float on by.\n(I’m a marine biologist, so I imagine I’m scuba diving in blue water. Ctenophores are the thoughts drifting by, I watch them, then let them drift away.)\n\n\nImpersonalize it\nThe next trick is to try and make the criticism impersonal. The person may have legitimately been trying to help improve your work, only they didn’t know how to deliver the advice in a more constructive way (or sometimes they didn’t put much care into phrasing their criticism).\nI’m not saying to try and suppress any emotion you feel about the criticism. Certainly acknowledge your feelings. But then try to reword the criticism in a more constructive way.\nI often ask myself on reading paper reviews “what is the real problem here”? For instance a review may criticize your paper for having insufficient data. Perhaps the real problem is that you’ve overstated the findings of the paper, relative to the data at hand. So scaling back your discussion of the study’s implications may be the more direct solution to this issue.\nNow I’ve turned the criticism into a solution, and I feel better.\n\n\nWhy do you care anyway?\nIf negative criticism gets you down, you might go into a negative thought spiral. As an (extreme!) example: A reviewer criticizes your study’s analysis. You start to think you’re useless at analysis, and therefore a useless scientist (maybe its imposter syndrome too?). You’ve wasted years of study and your whole life on this career choice.\nBut, we can turn this negative spiral around and make it positive.\nTo do that, ask yourself the same question about why you care three times. This helps get to the root cause.\nAs an example: A reviewer criticizes my study’s analysis. Why do I care? Well I want to get the paper published. Why do I want to get the paper published? Because I put so much work into it. Why do I care that I put so much work into it? Well I want validation that my career choice and all the time I devote to it is worthwhile.\nThis process then helps me see that there are other things I value in my career, beyond publishing papers. Like seeing my students succeed, or seeing my science helping conserve nature.\nSo criticism of an analysis is a set-back, but only a small one in the scheme of things."
  },
  {
    "objectID": "posts/2019-04-12-negativity-bias/index.html#acting-differently-finding-the-sublime",
    "href": "posts/2019-04-12-negativity-bias/index.html#acting-differently-finding-the-sublime",
    "title": "Dodging negativity bias in a critical work culture",
    "section": "Acting differently, finding the sublime",
    "text": "Acting differently, finding the sublime\nFeeling small in the scheme of things can be helpful to escape negative thoughts too.\nHave you ever been awed by a starry sky, a mountain range, a kelp forest or an old growth forest? You feel so small and insignificant in the scheme of things, and it’s somehow comforting.\nYou had a sublime experience.\nFeeling sublime can help us get out of our heads and grow past negative thoughts.\nOf course, now I have a career and young children I can’t get to the sublime nearly as much as I’d like. But if you can, I recommend you make the most of it."
  },
  {
    "objectID": "posts/2019-04-12-negativity-bias/index.html#critical-and-supportive",
    "href": "posts/2019-04-12-negativity-bias/index.html#critical-and-supportive",
    "title": "Dodging negativity bias in a critical work culture",
    "section": "Critical AND supportive",
    "text": "Critical AND supportive\nMy final word is think about how your own criticism might be interpreted. What you might not mean to sound harsh, might come across that way. Writing an email or a review? Write it once, then put yourself in the receivers shoes and read it again.\nThink about the consequences of their work too and make sure the criticism is in proportion to the consequences. We’re all guilty of getting a bit too ‘excited’ about being critical when we spot a mistake. Was a student’s writing sloppy in it’s preparation? Well tell them, but you don’t need to go to town on it, after all, it’s only some bad writing and not a flaw in a crucial life-support device.\nScience does progress by criticism and academic culture needs that. But the progress will happen more effectively if we’re kind to people when we serve them a dish of critical thought."
  },
  {
    "objectID": "posts/2025-10-06-llm-tools-for-r-book/index.html",
    "href": "posts/2025-10-06-llm-tools-for-r-book/index.html",
    "title": "Resource for using Large Language Model tools in R",
    "section": "",
    "text": "If you’re interested in using Large Language Models (LLMs) with R, you should check out Luis D. Verde Arregoitia’s new online resource: Large Language Model tools for R. Its available in English and Spanish.\nThe resouce provides a comprehensive guide to working with LLMs in R, covering packages, techniques, and practical applications.\nThis is a great complement to my workshop and book that guides you through some of the options and prompt engineering for high quality R analyses.\nIf you’re looking to integrate AI capabilities into your R workflows, this book is definitely worth exploring.\nCheck it out: https://luisdva.github.io/llmsr-book/index.html"
  },
  {
    "objectID": "posts/2017-04-14-glmm-comparison/index.html",
    "href": "posts/2017-04-14-glmm-comparison/index.html",
    "title": "Choosing R packages for mixed effects modelling based on the car you drive",
    "section": "",
    "text": "There are many roads you can take to fit a mixed effects model (sometimes termed hierarchical models) in R. There are numerous packages that each deploy different engines to fit mixed effects models.\nIn this driveability review, I look at some of the dominant packages for mixed effects models The focus of this review is on the practicality of each package, rather than their numerical accuracy. Information on numerical accuracy can be found in the academic literature, see the bibliography below. I will lookat each package’s handling, speed and adaptability for tackling different types of questions.\nSummary of the packages strengths and weaknesses. Speeds are relative to lme4 in a test case problem, see below for details.\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nlme4\n\n\nrjags\n\n\nINLA\n\n\nRStan\n\n\n\n\n\n\nRelative speed\n\n\n1\n\n\n100\n\n\n10\n\n\n60\n\n\n\n\nHandling- ease of coding1\n\n\nStraightforward\n\n\nHard\n\n\nModerate\n\n\nHard\n\n\n\n\nUser manual\n\n\nAccessible\n\n\nBrief and technical\n\n\nVery technical2\n\n\nHighly accessible\n\n\n\n\nOnline documentation\n\n\nExtensive\n\n\nExtensive\n\n\nNascent\n\n\nNascent\n\n\n\n\nSupport from developers3\n\n\nGood\n\n\nGood\n\n\nOutstanding\n\n\nGood\n\n\n\n\nTesting for accuracy\n\n\nExtensive\n\n\nExtensive for some types of models\n\n\nNascent\n\n\nNascent\n\n\n\n\nAdaptability - types of models you can fit\n\n\nLimited\n\n\nYour imagination is the limit\n\n\nExtensive\n\n\nAlmost as much as JAGS\n\n\n\n\nBest for\n\n\nSimple problems, quick implementation\n\n\nCustomising complex models\n\n\nSpatio-temporal models\n\n\nDeveloping new types of models\n\n\n\n\nHow you will look driving it\n\n\nTraditional, but elegant\n\n\nYou like to take the scenic route\n\n\nEdgy and slightly mysterious\n\n\nHanging with the cool kids\n\n\n\n\n\nbut see below for some add-on packages which make life easier for STAN and JAGS\n= lots of equations\nIn my experience. But, heck these packages are all free and all give better user support than many paid services!\n\nThe first is lme4, meaning linear mixed effects models with S4 classes. lme4 is like an older model sports-car - fast, respectable, well known and able to handle common types of questions. lme4 uses maximum likelihood methods for estimation, so interpret its statistics as a frequentist.\nThe next three packages are Bayesian techniques.\nThe second package is rjags, which is an interface to the “Just Another Gibbs Sampler” program. rjags is the Land-Rover of packages for mixed effects models: well tested, able to tackle just about any terrian, but slow and prone to breaking down (usually a user error, rather than the package itself).\n\n\n\nMy take on reviewing R packages for Generalised Linear Mixed Effects. Clockwise from top left: The lme4 package is fast but a bit older, like an early noughties Ferrari; the JAGS program goes anywhere like a Range Rover, but breaks often and is slow; the Stan language is modern and efficient like a hybrid BMW and INLA is fast and modern but requires technical expertise like a Tesla.\n\n\nThe third package is INLA, standing for “Integrated Nested Laplace Approximation”. INLA is like an electric sports car: it is a modern technique that is lightning fast, but there has been limited testing to date and you better understand what is under the hood before you attempt to drive it.\nThe final package is RStan, which is the R interface to the Stan modelling language. RStan is like like BMW’s hybrid, combining computational speed with flexibility and with exceptional documentation and support.\nNow, let’s look at each packages computational speed, handling (ease of use), driver support (documentation, existing knowledge and forums) and flexibility.\nSkip to the end for a biblography of examples and some cool packages that build on these tools.\n\n\nThe frequentist and Bayesian approaches I review here come from fundamentally different statistical philosophies. Here I will tackle the approaches pragmatically, looking at estimation speed and so on, however you might exclude one or the other a-prior based on your bent towards Bayesian or frequentist thinking.\n\n\n\nI simulated a simple hierarchical data-set to test each of the models. The script is available here. The data-set has 100 binary measurements. There is one fixed covariate (continuous ) and one random effect with five groups. The linear predictor was transformed to binomial probabilities using the logit function. For the Bayesian approaches, slightly different priors were used for each package, depending on what was available. See the script for more details on priors.\nFor each package I fit the same linear model with binomial errors and a logit link. To fit the models I used the functions glmer() from lme4, inla() from INLA, custom code for rjags and the stan_glmer() from the package rstanarm to fit the Stan model (you can also write your own custom code for RStan, but for convenience I used the canned version of mixed effects models).\nComputational speeds for fitting a binomial Generalised Linear Model to 100 samples with one random and one fixed effect. Relative times are relative to lme4\n\n\n\n\nMeasurement\n\n\nlme4\n\n\nrjags\n\n\nINLA\n\n\nRStan\n\n\n\n\n\n\nSpeed (Seconds)\n\n\n0.063\n\n\n6.2\n\n\n0.59\n\n\n3.8\n\n\n\n\nSpeed (relative)\n\n\n1\n\n\n99\n\n\n9.4\n\n\n60\n\n\n\n\nSo lme4 is by far the fastest, clocking in at around 6 tenths of a second (on my 3GHz Intel i7 Macbook Pro). INLA comes in second, ten times slower than lme4, then followed an order of magnitude later by RStan and finally rjags came plodding along behind about 100 times slower than lme4.\nIf you run the code I have provided you will see that all packages come up with similar estimates for the model parameters.\n\n\n\nlme4 leads the pack when it comes to ease of use. If you have your data sorted, then you can go from zero to fitted model in one line of code. INLA is similar, however you should generally not rely on default priors but specify your own, which requires additional code and thinking.\nUse of RStan and rjags depends on how you use them. You can write your own model code, which can lead to quite lengthy scripts, but also gives you greater number of choices for how you design your model. However, there are also many packages that provide wrapper’s to RStan and rjags that will fit common types of models (like mixed effects models). These tend to be as simple as INLA’s functions, remembering that you should think carefully about prior choice.\nIf you are going to write your own models, you will need to get good at debugging, so there is an extra overhead there.\nModel checking is another important consideration.\nFor users familiar with GLMs, lme4 probably provides the most familiar support for model checking. INLA can be more difficult to use, because it requires the user to hand-code many model checking routines. INLA, rjags and RStan also require the user to carefully check that the fitting algorithms have performed properly.\nEach of these packages use different types of algorithms, so the statistics you will use to check algorithm performance are different. See each package’s documentation for further advice.\n\n\n\nlme4 and RStan have the highest quality user-manuals. Both are well written and provide numerous examples that users will find helpful.\nThe user manual for rjags and JAGS is somewhat brief, but the user can easily find many helpful examples on the web, and these packages are covered by numerous books. So if you are willing to broaden your search you should have no trouble finding help.\nRStan and INLA are both relatively new, so fewer examples can be found on the web. While RStan has an excellent manual for both the mathsy and non-mathsy types, documentation of INLA can be difficult to follow for novice users, because much of it is equations.\nWeb support and chat groups are also an important aspect of model development. Help for all packages can be found on sites like Stackoverflow, or you can post your own new questions there.\nPersonally, I have found that support for INLA, through their forum, is exceptional. On several occaisons I have posted questions for which I could not find documentation and have generally recieved a response from INLA’s developers before the end of the working day. This is amazing, because it is better than you would expect from many paid services.\nI have not participated in forums for the other packages, so I can’t add comments here.\nOne final note on support is that lme4 and JAGS are both pretty well studied approaches to modelling and there are numerous academic papers dealing with their biases and accuracy. INLA and RStan are relatively new on the scene, so for new and different types of problems you might want to run your own simulation study to check that your model is performing as expected.\n\n\n\nYour imagination is the limit with rjags - take it on well driven routes, like a mixed effects model, or off-road on new adventures with Bayesian structural equation modelling - it can do it all.\nrjags is followed closely by RStan. RStan has a few limitations, but can basically do anything JAGS can do, and often faster (e.g. this ecological study). RStan will probably eventually replace rjags, but to date rjags persists because of the extensive range of well documented examples users can build on.\nINLA comes in fourth with a diverse range of built in likelihoods and model functions (you can even ask the developers to add more and they might do it!). INLA is becoming particularly popular for modelling spatial and temporal auto-correlation, partly due to its speed. You can do these types of models with RStan and rjags but computations might be impossibly slow.\nlme4 is the least flexible of packages - though there are some options to customise it’s models. The similar nlme package also provides a range of tools for fitting random effects for spatial and temporal autocorrelation.\nThere are however, some clever tricks you can do with lme4 to fit a broader range of models. For instance, you can include a random effect where every sample is a separate group in a Poisson GLM to get a Poisson with extra variation.\n\n\n\nThat’s all. I hope this blog helps you make a good choice before investing in learning a new tool for mixed effects models. Let me know how you go and if you found this useful.\n\n\n\nrstanarm for mixed effects models coded like glm() but using Stan for estimation.\nglmmBUGS for mixed effects models coded like glm() but using JAGS for estimation.\nA new function in the mgcv package (links to pdf) for auto-writing code for Bayesian Generalised Additive Models.\nA new study showing how you can fit spatial models with barriers.\nFit Bayesian Latent Variable models for analysing multivariate data (e.g. ecological communities).\nThe GLMM in R FAQ\n\n\n\nUnless otherwise indicated, these resources are open access.\nStan user manual.\nJAGS user manual.\nIntroduction to lme4 (pdf).\nINLA project page.\nINLA forum.\nMixed Effects Models and Extensions in Ecology with R my go-to book for theory and code for fitting likelihood based mixed effects models (e.g. with lme4). You will have to buy it.\nA good example for how to design a simulation study to test your models\nExample for INLA for spatial models in ecology.\nA review of mixed effects models in fisheries science (good for other disciplines too).\nNIMBLE another R package for Bayesian modelling.\nglmmADMB another R package giving greater flexibility for fitting using maximum likelihood."
  },
  {
    "objectID": "posts/2017-04-14-glmm-comparison/index.html#choosing-r-packages-for-mixed-effects-modelling-based-on-the-car-you-drive",
    "href": "posts/2017-04-14-glmm-comparison/index.html#choosing-r-packages-for-mixed-effects-modelling-based-on-the-car-you-drive",
    "title": "Choosing R packages for mixed effects modelling based on the car you drive",
    "section": "",
    "text": "There are many roads you can take to fit a mixed effects model (sometimes termed hierarchical models) in R. There are numerous packages that each deploy different engines to fit mixed effects models.\nIn this driveability review, I look at some of the dominant packages for mixed effects models The focus of this review is on the practicality of each package, rather than their numerical accuracy. Information on numerical accuracy can be found in the academic literature, see the bibliography below. I will lookat each package’s handling, speed and adaptability for tackling different types of questions.\nSummary of the packages strengths and weaknesses. Speeds are relative to lme4 in a test case problem, see below for details.\n\n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nlme4\n\n\nrjags\n\n\nINLA\n\n\nRStan\n\n\n\n\n\n\nRelative speed\n\n\n1\n\n\n100\n\n\n10\n\n\n60\n\n\n\n\nHandling- ease of coding1\n\n\nStraightforward\n\n\nHard\n\n\nModerate\n\n\nHard\n\n\n\n\nUser manual\n\n\nAccessible\n\n\nBrief and technical\n\n\nVery technical2\n\n\nHighly accessible\n\n\n\n\nOnline documentation\n\n\nExtensive\n\n\nExtensive\n\n\nNascent\n\n\nNascent\n\n\n\n\nSupport from developers3\n\n\nGood\n\n\nGood\n\n\nOutstanding\n\n\nGood\n\n\n\n\nTesting for accuracy\n\n\nExtensive\n\n\nExtensive for some types of models\n\n\nNascent\n\n\nNascent\n\n\n\n\nAdaptability - types of models you can fit\n\n\nLimited\n\n\nYour imagination is the limit\n\n\nExtensive\n\n\nAlmost as much as JAGS\n\n\n\n\nBest for\n\n\nSimple problems, quick implementation\n\n\nCustomising complex models\n\n\nSpatio-temporal models\n\n\nDeveloping new types of models\n\n\n\n\nHow you will look driving it\n\n\nTraditional, but elegant\n\n\nYou like to take the scenic route\n\n\nEdgy and slightly mysterious\n\n\nHanging with the cool kids\n\n\n\n\n\nbut see below for some add-on packages which make life easier for STAN and JAGS\n= lots of equations\nIn my experience. But, heck these packages are all free and all give better user support than many paid services!\n\nThe first is lme4, meaning linear mixed effects models with S4 classes. lme4 is like an older model sports-car - fast, respectable, well known and able to handle common types of questions. lme4 uses maximum likelihood methods for estimation, so interpret its statistics as a frequentist.\nThe next three packages are Bayesian techniques.\nThe second package is rjags, which is an interface to the “Just Another Gibbs Sampler” program. rjags is the Land-Rover of packages for mixed effects models: well tested, able to tackle just about any terrian, but slow and prone to breaking down (usually a user error, rather than the package itself).\n\n\n\nMy take on reviewing R packages for Generalised Linear Mixed Effects. Clockwise from top left: The lme4 package is fast but a bit older, like an early noughties Ferrari; the JAGS program goes anywhere like a Range Rover, but breaks often and is slow; the Stan language is modern and efficient like a hybrid BMW and INLA is fast and modern but requires technical expertise like a Tesla.\n\n\nThe third package is INLA, standing for “Integrated Nested Laplace Approximation”. INLA is like an electric sports car: it is a modern technique that is lightning fast, but there has been limited testing to date and you better understand what is under the hood before you attempt to drive it.\nThe final package is RStan, which is the R interface to the Stan modelling language. RStan is like like BMW’s hybrid, combining computational speed with flexibility and with exceptional documentation and support.\nNow, let’s look at each packages computational speed, handling (ease of use), driver support (documentation, existing knowledge and forums) and flexibility.\nSkip to the end for a biblography of examples and some cool packages that build on these tools.\n\n\nThe frequentist and Bayesian approaches I review here come from fundamentally different statistical philosophies. Here I will tackle the approaches pragmatically, looking at estimation speed and so on, however you might exclude one or the other a-prior based on your bent towards Bayesian or frequentist thinking.\n\n\n\nI simulated a simple hierarchical data-set to test each of the models. The script is available here. The data-set has 100 binary measurements. There is one fixed covariate (continuous ) and one random effect with five groups. The linear predictor was transformed to binomial probabilities using the logit function. For the Bayesian approaches, slightly different priors were used for each package, depending on what was available. See the script for more details on priors.\nFor each package I fit the same linear model with binomial errors and a logit link. To fit the models I used the functions glmer() from lme4, inla() from INLA, custom code for rjags and the stan_glmer() from the package rstanarm to fit the Stan model (you can also write your own custom code for RStan, but for convenience I used the canned version of mixed effects models).\nComputational speeds for fitting a binomial Generalised Linear Model to 100 samples with one random and one fixed effect. Relative times are relative to lme4\n\n\n\n\nMeasurement\n\n\nlme4\n\n\nrjags\n\n\nINLA\n\n\nRStan\n\n\n\n\n\n\nSpeed (Seconds)\n\n\n0.063\n\n\n6.2\n\n\n0.59\n\n\n3.8\n\n\n\n\nSpeed (relative)\n\n\n1\n\n\n99\n\n\n9.4\n\n\n60\n\n\n\n\nSo lme4 is by far the fastest, clocking in at around 6 tenths of a second (on my 3GHz Intel i7 Macbook Pro). INLA comes in second, ten times slower than lme4, then followed an order of magnitude later by RStan and finally rjags came plodding along behind about 100 times slower than lme4.\nIf you run the code I have provided you will see that all packages come up with similar estimates for the model parameters.\n\n\n\nlme4 leads the pack when it comes to ease of use. If you have your data sorted, then you can go from zero to fitted model in one line of code. INLA is similar, however you should generally not rely on default priors but specify your own, which requires additional code and thinking.\nUse of RStan and rjags depends on how you use them. You can write your own model code, which can lead to quite lengthy scripts, but also gives you greater number of choices for how you design your model. However, there are also many packages that provide wrapper’s to RStan and rjags that will fit common types of models (like mixed effects models). These tend to be as simple as INLA’s functions, remembering that you should think carefully about prior choice.\nIf you are going to write your own models, you will need to get good at debugging, so there is an extra overhead there.\nModel checking is another important consideration.\nFor users familiar with GLMs, lme4 probably provides the most familiar support for model checking. INLA can be more difficult to use, because it requires the user to hand-code many model checking routines. INLA, rjags and RStan also require the user to carefully check that the fitting algorithms have performed properly.\nEach of these packages use different types of algorithms, so the statistics you will use to check algorithm performance are different. See each package’s documentation for further advice.\n\n\n\nlme4 and RStan have the highest quality user-manuals. Both are well written and provide numerous examples that users will find helpful.\nThe user manual for rjags and JAGS is somewhat brief, but the user can easily find many helpful examples on the web, and these packages are covered by numerous books. So if you are willing to broaden your search you should have no trouble finding help.\nRStan and INLA are both relatively new, so fewer examples can be found on the web. While RStan has an excellent manual for both the mathsy and non-mathsy types, documentation of INLA can be difficult to follow for novice users, because much of it is equations.\nWeb support and chat groups are also an important aspect of model development. Help for all packages can be found on sites like Stackoverflow, or you can post your own new questions there.\nPersonally, I have found that support for INLA, through their forum, is exceptional. On several occaisons I have posted questions for which I could not find documentation and have generally recieved a response from INLA’s developers before the end of the working day. This is amazing, because it is better than you would expect from many paid services.\nI have not participated in forums for the other packages, so I can’t add comments here.\nOne final note on support is that lme4 and JAGS are both pretty well studied approaches to modelling and there are numerous academic papers dealing with their biases and accuracy. INLA and RStan are relatively new on the scene, so for new and different types of problems you might want to run your own simulation study to check that your model is performing as expected.\n\n\n\nYour imagination is the limit with rjags - take it on well driven routes, like a mixed effects model, or off-road on new adventures with Bayesian structural equation modelling - it can do it all.\nrjags is followed closely by RStan. RStan has a few limitations, but can basically do anything JAGS can do, and often faster (e.g. this ecological study). RStan will probably eventually replace rjags, but to date rjags persists because of the extensive range of well documented examples users can build on.\nINLA comes in fourth with a diverse range of built in likelihoods and model functions (you can even ask the developers to add more and they might do it!). INLA is becoming particularly popular for modelling spatial and temporal auto-correlation, partly due to its speed. You can do these types of models with RStan and rjags but computations might be impossibly slow.\nlme4 is the least flexible of packages - though there are some options to customise it’s models. The similar nlme package also provides a range of tools for fitting random effects for spatial and temporal autocorrelation.\nThere are however, some clever tricks you can do with lme4 to fit a broader range of models. For instance, you can include a random effect where every sample is a separate group in a Poisson GLM to get a Poisson with extra variation.\n\n\n\nThat’s all. I hope this blog helps you make a good choice before investing in learning a new tool for mixed effects models. Let me know how you go and if you found this useful.\n\n\n\nrstanarm for mixed effects models coded like glm() but using Stan for estimation.\nglmmBUGS for mixed effects models coded like glm() but using JAGS for estimation.\nA new function in the mgcv package (links to pdf) for auto-writing code for Bayesian Generalised Additive Models.\nA new study showing how you can fit spatial models with barriers.\nFit Bayesian Latent Variable models for analysing multivariate data (e.g. ecological communities).\nThe GLMM in R FAQ\n\n\n\nUnless otherwise indicated, these resources are open access.\nStan user manual.\nJAGS user manual.\nIntroduction to lme4 (pdf).\nINLA project page.\nINLA forum.\nMixed Effects Models and Extensions in Ecology with R my go-to book for theory and code for fitting likelihood based mixed effects models (e.g. with lme4). You will have to buy it.\nA good example for how to design a simulation study to test your models\nExample for INLA for spatial models in ecology.\nA review of mixed effects models in fisheries science (good for other disciplines too).\nNIMBLE another R package for Bayesian modelling.\nglmmADMB another R package giving greater flexibility for fitting using maximum likelihood."
  },
  {
    "objectID": "posts/2016-06-27-ICRS-tweets/index.html",
    "href": "posts/2016-06-27-ICRS-tweets/index.html",
    "title": "Twitter stats from ICRS2016",
    "section": "",
    "text": "A few people have asked me about statistics on tweets at the International Coral Reef Symposium. You can actually download all tweets that match a certain search term and analyse them using R. I used the twitteR package to access tweets that mention the hashtag #ICRS2016 (via a custom app created in my twitter account) .\nThere were a very large number of tweets, so I limited the date range from 18th June until 25th June.\nI then ran some analyses. Here are the results.\nOver the period 18-25 June there were 14 246 tweets and re-tweets of which 5860 were original tweets. There were 1938 unique handles (ie people) tweeting to #ICRS2016.\nThe top tweeters are pictured below, including the number of tweets and RTs they made:\n\nIt was not possible to determine how many of the handles were actually at the conference, because most were not geo-tagged. Most handles only participated by retweeting something once. In fact the tweet-frequency curve looked something like the species-abundance distribution from Nancy Knowlton’s talk about rare invertebrate species on reefs:\n\nProf. Terry Hughes had the honour of the most retweeted tweet, with 111 retweets:\n A plot of the tweet and RT time-series shows distinct peaks during mornings and afternoons at the conference, with a break at lunch time and during the night:"
  },
  {
    "objectID": "posts/2016-06-27-ICRS-tweets/index.html#twitter-stats-from-the-international-coral-reef-symposium",
    "href": "posts/2016-06-27-ICRS-tweets/index.html#twitter-stats-from-the-international-coral-reef-symposium",
    "title": "Twitter stats from ICRS2016",
    "section": "",
    "text": "A few people have asked me about statistics on tweets at the International Coral Reef Symposium. You can actually download all tweets that match a certain search term and analyse them using R. I used the twitteR package to access tweets that mention the hashtag #ICRS2016 (via a custom app created in my twitter account) .\nThere were a very large number of tweets, so I limited the date range from 18th June until 25th June.\nI then ran some analyses. Here are the results.\nOver the period 18-25 June there were 14 246 tweets and re-tweets of which 5860 were original tweets. There were 1938 unique handles (ie people) tweeting to #ICRS2016.\nThe top tweeters are pictured below, including the number of tweets and RTs they made:\n\nIt was not possible to determine how many of the handles were actually at the conference, because most were not geo-tagged. Most handles only participated by retweeting something once. In fact the tweet-frequency curve looked something like the species-abundance distribution from Nancy Knowlton’s talk about rare invertebrate species on reefs:\n\nProf. Terry Hughes had the honour of the most retweeted tweet, with 111 retweets:\n A plot of the tweet and RT time-series shows distinct peaks during mornings and afternoons at the conference, with a break at lunch time and during the night:"
  },
  {
    "objectID": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html",
    "href": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html",
    "title": "Enforcing marine protected areas to reach ecological targets for recovery",
    "section": "",
    "text": "Marine protected areas are often implemented without sufficient ongoing funding to ensure their objectives are not compromised by poaching.\nIn new work we developed models to estimate the cost of enforcing reserves sufficiently to allow recovery of fish species to ecologically functional levels. Importantly, we also estimate the benefits of having community support for reserves.\nThis could help management agencies optimise their spending on community engagement activities, like education, versus spending on enforcement against poaching.\n(Open access pre-print is here)\nIn this post I wanted to tell something of the story of how this work came to be."
  },
  {
    "objectID": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#inspiration-for-a-model-of-reserve-enforcement",
    "href": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#inspiration-for-a-model-of-reserve-enforcement",
    "title": "Enforcing marine protected areas to reach ecological targets for recovery",
    "section": "Inspiration for a model of reserve enforcement",
    "text": "Inspiration for a model of reserve enforcement\n\nThe story of how this paper came to be is slightly unusual and I think, worth telling. It starts, like many good things, in the 1980s.\n\nSome time ago Hugh Possingham (Director of Science at The Nature Conservancy) was describing his PhD work on optimal foraging of nectar eating birds.\nBack in the 80s he had developed some models of how a bird would draw down on nectar in a flower. The flower then recharges itself until the bird next visits and draws off nectar again. He wanted to know the optimal timing for the bird to wait before it returned to the flower.\nMore recently, Hugh had suggested we use this model to predict outcomes for coral reefs under successive cyclones. Corals presumably grow steadily until a cyclone, which knocks them back down again. So what you get is something like in the below graph.\n\nLike all handy models, I stored it away in the back of my head, in case I had reason to use it.\nYears later Eddie Game, also of The Nature Conservancy, suggested I look into analysing some data TNC had collected on its reserves in Raja Ampat.\nRaja Ampat is the global centre of fish and coral diversity and also the site of a large multi-institutional network of protected areas (that also involved WWF among others)."
  },
  {
    "objectID": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#fiscally-sustainable-reserves",
    "href": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#fiscally-sustainable-reserves",
    "title": "Enforcing marine protected areas to reach ecological targets for recovery",
    "section": "Fiscally sustainable reserves",
    "text": "Fiscally sustainable reserves\nThe program had been successful in implementing many protected areas and was now moving to a phase where they were looking to manage them with achieve fiscal sustainability.\nObviously a key component of fiscal sustainability is how much you need to spend on costs like fuel for your staff to go and patrol the protected areas. You might also want to invest in educating people about where the reserves are and their benefits.\nAs a young diver I had been struck by how my local reserves (in Tasmania) could go from being stacked full of lobster to practically empty overnight. It only took one serious poacher to go out and catch a large number of naive lobster.\nSo when Eddie asked about modelling the Raja Ampat situation I thought back to Hugh’s model of intermittent knock-backs."
  },
  {
    "objectID": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#intermittent-poaching",
    "href": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#intermittent-poaching",
    "title": "Enforcing marine protected areas to reach ecological targets for recovery",
    "section": "Intermittent poaching",
    "text": "Intermittent poaching\nAnd what we ended up doing was modelling poaching as intermittent events that knock back fish abundance (like the bird sucking up nectar), which then recovers slowly in between poaching events.\nBut we redesigned the model so it was appropriate for fish.\nIn Indonesia you have fishing boats dubbed ‘roving bandits’ (Here’s a pdf of the original paper). These are large (often international) boats that roam around depleting local reefs of fish before they move to the next location. Unlike local fishers they have no incentive to invest in locally sustainable fishing.\nSo it made sense to model these roving bandits as intermittent poachers that removed a large biomass of fish.\nWhat the model showed was that you need to ensure poaching by roving bandits is very infrequent to allow fish biomass adequate time to recover to above baseline levels. If poaching was too frequent the reserve would never get anywhere, it was effectively a ‘paper park’.\nOne problem had me stuck however. I had to specify a parameter to relate the rate of enforcement to the rate of poaching. But I had no idea how to measure or interpret this parameter. Enter an international collaboration."
  },
  {
    "objectID": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#international-collaboration",
    "href": "posts/2018-09-24-enforcing-MPAs-for-ecological-recovery/index.html#international-collaboration",
    "title": "Enforcing marine protected areas to reach ecological targets for recovery",
    "section": "International collaboration",
    "text": "International collaboration\nTo test the model out, we used data from Raja Ampat to estimate the rate of poaching and size of local fish populations. Eddie and I brought on study co-authors Gabby Ahmadia (WWF), Purwanto (formerly TNC Indonesia) and Rizya Ardiwijaya (TNC Indonesia) to help interpret the data. I also recruited local PhD student Brett Parker to help with data analysis.\nIt was great to get to collaborate with this international team and combine our analytical know-how with local expertise on the problems at hand.\nAn important insight that emerged from this collaboration was the value of local community support. Then it struck me how to interpret the missing parameter - it reflects how much communities support reserves.\nIf a community doesn’t support a reserve they are more likely to poach, often despite of enforcement effort. Whereas, if the support the reserve they may never poach and even help by reporting on foreign poachers. This insight provided a way to interpret the new parameter I had introduced in turning the bird model into a fish model.\nSo we used the model to estimate the benefit of community support to improving fish biomass in the reserve. When local communities support reserves they are less likely to poach themselves and may contribute to surveillance by reporting on illegal poachers.\nThe benefit of community support could also be measured in terms of savings a management agency might make on its budget for enforcement.\nSo if you get a chance to read the article, take a moment to notice the reference to nectar feeding birds, and the mix of modelling and local insight.\nThe cost of enforcing a marine protected area to achieve ecological targets for the recovery of fish biomass was published online at Biological Conservation, November 2018\nIf you can’t access that journal here’s an open access pre-print) or email me for a copy."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction copy/index.html",
    "href": "posts/2025-10-23-deer-harvest-reduction copy/index.html",
    "title": "Estimating harvest levels to reduce feral deer populations in Tasmania",
    "section": "",
    "text": "Feral deer were introduced to Tasmania for hunting, but are becoming an increasing environmental, agricultural and road safety issue. They damage sensitive vegetation and even disrupt attempts to restore coastal wetlands. A recent report shows results from a new survey indicating that the deer population is booming.\n(Here’s a news article summarizing the report).\nI’m interested in the question how much harvesting is required to reduce deer populations. As a fisheries scientists, I’m usually asking the reverse question, of how much harvesting is too much. In this case, we want to overharest to reduce populations.\nOne suggestion from recreational hunting groups is to allow more hunting to control deer populations. But how much hunting would have to happen and is it feasible for recreational hunters to take that much?\nThe management problem is that deer in Tasmania are managed in zones, including sustainable hunting zones and no-go zones. The no-go zones are there to protect sennsitive enviroments, whereas the hunting zones are there to manage deer populations to allow ongoing recreational hunting.\nBut deer don’t obey jurisdictional boundaries and the new survey clearly shows the deer population is spreading. There are similar issues on the mainland with different states having different management approaches.\nThis problem is an interesting exercise in population modelling, and I cover the details below. But there are also scientific studies on the harvest rates required to stabilize deer populations. A study done in Tasmania estimated a rate of 25% of the population per year would be needed just to stabilise populations.\nGiven the estimate of 71,655 deer in 2024, that would equate to about 17,900 deer needing to be harvested each year just to stabilise the population. We’d need to harvest significantly more than that to reduce the population.\nI did some research but unfortunately couldn’t find any data on how many deer recreational hunters hunt or how many hunting licenses there are in Tasmania. But we can make some educated guesses. This study found that 28 hunters killed 128 deer over 3 years on one private reserve in Tasmania.\nThat amounts to about 1.5 deer per hunter per year. Thus, to harvest 17,900 deer per year would require about 11,933 hunters. That of course assumes the same success and participation rate. To get a substantial population decrease we’d need a much higher harvest rate, perhaps double that number of deer, meaning over 20,000 hunters would be needed.\nThis study sums it up nicely:\n” Wild deer populations in Australia and New Zealand are increasing and require active management to reduce their impacts on agriculture, natural values and human safety. It is sometimes suggested that recreational hunting, a popular activity in both countries, could be an effective tool for managing wild deer populations, but its effectiveness has seldom been demonstrated (Bengsen and Sparkes 2016). Our study showed that recreational hunting of fallow deer on a private reserve in Tasmania sustained a population density of 4–5 deer per km 2, higher than the average deer density in Tasmania. To effectively act as a control tool to reduce deer density, as proposed in the Tasmanian Wild Fallow Deer Management Plan 2022–27, recreational hunters would need to increase the number of deer killed annually well above 35% of the population, while targeting females primarily. This could be achieved by removing the ‘Partly Protected Wildlife’ status of fallow deer in Tasmania and its consequent restrictions on hunting seasons, quotas and shooting equipment (The Tasmanian Government 2023). Landholders can apply for special purposes wildlife property protection or crop protection permits to control deer on their properties, but completely removing that restriction, as was done in NSW (Bengsen and Forsyth 2019), would facilitate deer management on private land and most likely increase the annual deer harvest ”\nSource: https://doi.org/10.1111/emr.70001\nIt seems unlikely that Tasmania has that many active deer hunters that could be enabled by further removing restrictions. But more data on hunting participation is required to confirm or deny that claim.\nOther approaches such as professional culling and eradication by landholders will likely be required to achieve meaningful population reductions."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction copy/index.html#population-growth-calculations",
    "href": "posts/2025-10-23-deer-harvest-reduction copy/index.html#population-growth-calculations",
    "title": "Estimating harvest levels to reduce feral deer populations in Tasmania",
    "section": "Population growth calculations",
    "text": "Population growth calculations\nWe can do our own simple calculations to estimate the harvest rate required to reduce deer populations. Note I provide below as an educational example only. I recommend using the studies cited above for real advice on rates (as you’ll see I get quite different numbers).\nTasmania’s deer population has grown from about 53,660 to 71,655 individuals over 5 years (note there is errors around those numbers that we won’t consider here).\nPopulation growth is rarely linear. For the deer a reasonable approximation would be that deer are increasingly exponentially, meaning non-linearly at an increasing numbers of new animals each year. I make this assertion based the fact that their distribution is also growing, suggesting a growing population that is currently unhindered by environmental limits (e.g. food).\nFor growing populations its convenient to work with instantaneous rates of increase (r). r is a the rate of increase on a multiplicative (or ‘log’) scale. Let’s see how to do that.\n\nFinite rate of increase (λ)\nThe finite rate of increase over a time period is given by:\n\\[ \\lambda = \\left(\\frac{N_t}{N_0}\\right)^{1/t} \\]\nWhere:\n\n\\(N_0 = 53,660\\) (initial population)\n\\(N_t = 71,655\\) (population after 5 years)\n\\(t = 5\\) years\n\nLet’s calculate this in R:\n\n\nShow the code\n# Initial parameters\nN0 &lt;- 53660  # Initial population\nNt &lt;- 71655  # Population after 5 years\nt &lt;- 5       # Time period in years\n\n# Calculate finite rate of increase\nlambda &lt;- (Nt / N0)^(1/t)\nlambda\n\n\n[1] 1.059544\n\n\n\n\nInstantaneous rate of increase (r)\nWe can convert λ to the instantaneous rate of increase using:\n\\[ r = \\ln(\\lambda) \\]\n\n\nShow the code\n# Calculate instantaneous rate of increase\nr &lt;- log(lambda)\nr\n\n\n[1] 0.05783902\n\n\nTo convince ourselves we’ve done the math right we can check the answer to see if we recover the original 5 year growth:\n\n\nShow the code\n53660 * exp(r*5)\n\n\n[1] 71655\n\n\nwhich equals the number of deer in 2024."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction copy/index.html#what-does-this-growth-rate-look-like",
    "href": "posts/2025-10-23-deer-harvest-reduction copy/index.html#what-does-this-growth-rate-look-like",
    "title": "Estimating harvest levels to reduce feral deer populations in Tasmania",
    "section": "What does this growth rate look like?",
    "text": "What does this growth rate look like?\nLet’s visualize what this exponential growth rate means by projecting a small population forward in time. We’ll start with 5,000 individuals and see how it grows over 20 years:\n\n\nShow the code\nlibrary(ggplot2)\n# Simulate exponential growth from a small population\nyears_proj &lt;- 0:20\nN_initial &lt;- 5000\npop_projection &lt;- N_initial * exp(r * years_proj)\n\nproj_df &lt;- data.frame(\n  Year = years_proj,\n  Population = pop_projection\n)\n\n# Plot on natural scale\nggplot(proj_df, aes(x = Year, y = Population)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_point(size = 2, color = \"darkblue\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Exponential Population Growth (Natural Scale)\",\n    subtitle = paste0(\"Starting population: 5,000 | Growth rate: \", \n                     round(r * 100, 2), \"% per year\"),\n    x = \"Year\",\n    y = \"Population Size\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\nNote the upwards curve.\nThe same data on a multiplicative scale (like a scale that goes 1, 10, 100, 1000) shows the exponential growth as a straight line, which is characteristic of constant proportional growth:\n\n\nShow the code\n# Plot on log10 scale\nggplot(proj_df, aes(x = Year, y = Population)) +\n  geom_line(linewidth = 1.2, color = \"darkred\") +\n  geom_point(size = 2, color = \"darkred\") +\n  scale_y_log10(labels = scales::comma) +\n  labs(\n    title = \"Exponential Population Growth (Log10 Scale)\",\n    subtitle = paste0(\"Starting population: 5,000 | Growth rate: \", \n                     round(r * 100, 2), \"% per year\"),\n    x = \"Year\",\n    y = \"Population Size (log10 scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11)\n  )\n\n\n\n\n\n\n\n\n\nThe log scale visualization reveals that exponential growth maintains a constant rate - the population increases by the same proportion each year, even though the absolute numbers get larger and larger."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction copy/index.html#required-harvest-rate",
    "href": "posts/2025-10-23-deer-harvest-reduction copy/index.html#required-harvest-rate",
    "title": "Estimating harvest levels to reduce feral deer populations in Tasmania",
    "section": "Required harvest rate",
    "text": "Required harvest rate\nLet’s say we want to reduce the deer population from 71,655 back to 53,660 over 5 years.\nTo do this we need to calculate what harvest rate would achieve this while accounting for natural population growth.\nThe target is to have the net rate of change (growth minus harvest) result in the desired decline. We can set up the equation:\n\\[ N_t = N_0 \\times e^{(r - h) \\times t} \\]\nWhere \\(h\\) is the harvest rate we want to solve for. Rearranging:\n\\[ h = r - \\frac{\\ln(N_t / N_0)}{t} \\]\n\n\nShow the code\n# Starting population for harvest scenario\nN_start &lt;- 71655\nN_target &lt;- 53660\nt_harvest &lt;- 5\n\n# Calculate required harvest rate\ntarget_rate &lt;- log(N_target / N_start) / t_harvest\nharvest_rate &lt;- r - target_rate\nharvest_rate\n\n\n[1] 0.115678\n\n\nThe required annual harvest rate is 12%. This harvest rate offsets both the population growth and achieves the desired decline."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction copy/index.html#annual-deer-harvest-numbers",
    "href": "posts/2025-10-23-deer-harvest-reduction copy/index.html#annual-deer-harvest-numbers",
    "title": "Estimating harvest levels to reduce feral deer populations in Tasmania",
    "section": "Annual deer harvest numbers",
    "text": "Annual deer harvest numbers\nLet’s simulate how many deer need to be harvested each year to achieve this reduction:\n\n\nShow the code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\n# Simulation parameters\nyears &lt;- 0:5\npop_size &lt;- numeric(length(years))\nharvest_numbers &lt;- numeric(length(years))\n\n# Initial conditions\npop_size[1] &lt;- N_start\n\n# Simulate population with harvest\nfor (i in 2:length(years)) {\n  # Population grows\n  pop_after_growth &lt;- pop_size[i-1] * exp(r)\n  # Calculate harvest number (proportion of post-growth population)\n  harvest_numbers[i] &lt;- pop_after_growth * harvest_rate\n  # Population after harvest\n  pop_size[i] &lt;- pop_after_growth - harvest_numbers[i]\n}\n\n# Create data frame for results\nharvest_df &lt;- data.frame(\n  Year = years[2:length(years)],\n  Deer_Harvested = round(harvest_numbers[2:length(years)])\n)\n\nharvest_df\n\n\n  Year Deer_Harvested\n1    1           8782\n2    2           8229\n3    3           7710\n4    4           7224\n5    5           6769\n\n\nAs we can see, the number of deer that need to be harvested decreases each year as the population declines.\nNote we assumed harvest after population growth (breeding) each year."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction copy/index.html#visualizing-the-population-trajectory",
    "href": "posts/2025-10-23-deer-harvest-reduction copy/index.html#visualizing-the-population-trajectory",
    "title": "Estimating harvest levels to reduce feral deer populations in Tasmania",
    "section": "Visualizing the population trajectory",
    "text": "Visualizing the population trajectory\nLet’s create a visualization showing a projection for how the population changes over time with and without harvest:\n\n\nShow the code\n# Simulate population without harvest\npop_no_harvest &lt;- N_start * exp(r * years)\n\n# Create data frame for plotting\npop_df &lt;- data.frame(\n  Year = rep(years, 2),\n  Population = c(pop_size, pop_no_harvest),\n  Scenario = rep(c(\"With Harvest\", \"Without Harvest\"), \n                 each = length(years))\n)\n\n# Create plot\nggplot(pop_df, aes(x = Year, y = Population, \n                   color = Scenario)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  scale_y_continuous(labels = scales::comma, \n                    limits = c(40000, 100000)) +\n  labs(\n    title = \"Deer Population Trajectory Over 5 Years\",\n    subtitle = \"Comparing growth scenarios with and without harvest management\",\n    x = \"Year\",\n    y = \"Population Size\",\n    color = \"Scenario\",\n    linetype = \"Scenario\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"bottom\"\n  )\n\n\nIgnoring unknown labels:\n• linetype : \"Scenario\"\n\n\n\n\n\n\n\n\n\nThe plot clearly shows the diverging trajectories.\nSo why is my number of 12% per year so much lower than the 25% per year cited in the Tasmanian study?\nFor one, my calculation doesn’t account for the spatial dynamics and increasing range. Deer moving into new areas with fresh vegetation can sustain higher growth rates. For another my calculation assumes a stable age structure. This means that the proportion of juveniles to adults remains constant, which may not be the case in a rapidly growing population.\nMore sophisticated models, like the one used in this study, would be needed to capture these dynamics accurately."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "",
    "text": "Tools let AI models run code and access other applications. This can be a tool on your computer (like an R function), or access and API.\nBy creating tools you can give your LLM access to databases online (via their APIs) or let them run code on your computer, interpret the output and use it in their responses.\nThere are many ways to use tools. So far many tool examples are for computer programmers. There are some generally useful tools out there (like creating files or searching the web). However, I wanted to see if I could make a tool that was specifically useful in my research field.\nIn this tutorial I’ll look at using them to get ocean data from a public database that has an API."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#how-tools-work",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#how-tools-work",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "How tools work",
    "text": "How tools work\nThe basic example of a tool is getting the current date and time. Then when you ask for the time, instead of hallucinating a time and date, the model can use a tool to get an accurate current time and date.\nTools are defined using a Model Context Protocol, which is a standardized structure for connecting natural language models to programmatic tools (which have all the normal programming rules like being case-sensitive, syntax appropriate to the language etc).\nIts hard to find clear explanations of how they work, but here’s what I understand:\nThe tool is ‘registered’ with the model. This registering is just a prompt to the model, but it is done with a specific structure that defines the tool, its parameters, outputs and use cases. This way the model knows how to write code to run the tool.\nThen when you chat with the model it might interpret that tool use would be helpful. Like if you ask the time and it ‘knows’ about a time-date tool, it might use it. It writes the appropriate code to use the tool, sends that back to your computer (or to the API), where the code is evaluated. The output is then sent back to the model. The model interprets this into the final response it will give you.\nThere’s a good example of how tools don’t work and do work in the ellmer documentation.\nLet’s try a more interesting tool than the usual example of getting date and time."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#use-case",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#use-case",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "Use case",
    "text": "Use case\nThe remora package lets you extract ocean data from the Integrated Marine Observing System (IMOS) and Bluelink Reanalysis (BRAN) model.. We can give an LLM access to this data by creating a tool that uses remora to extract the data."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#remora",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#remora",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "remora",
    "text": "remora\nWe’ll start by making some data and testing out remora. Note that remora isn’t on cran, so you will need to install it from github.\nWhat we do below is download ocean temperatures for some made up data. We just need coordinates and dates.\nFor a more detailed example see the remora documentation\nlibrary(remora)\nlibrary(tidyverse)\nlibrary(raster)\n\n# Create a dataframe of coordinates near Tasmania with different dates in 2021\ntas_dat &lt;- data.frame(\n  datetime = ymd_hms(paste(\n        c(\"2021-02-15\", \"2021-02-15\", \"2021-02-15\", \n                   \"2021-02-15\", \"2021-02-15\"),\n                   \"10:00:00\")),\n  X = c(147.2, 148.5, 146.8, 145.3, 144.7),  # Longitudes around Tasmania\n  Y = c(-42.5, -41.8, -43.2, -42.9, -41.5)\n)\nwrite.csv(tas_dat, \"tas_dat.csv\", row.names = FALSE)\nExtracting the SST data at a given depth is simple:\ntas_temp &lt;- \n    extractBlue(df = tas_dat,\n                X = \"X\", \n                Y = \"Y\", \n                datetime = \"datetime\", \n                env_var = \"BRAN_temp\",\n                extract_depth = 30,\n                verbose = TRUE,\n                full_timeperiod = FALSE)"
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#defining-the-tool",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#defining-the-tool",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "Defining the tool",
    "text": "Defining the tool\nNow we know how to use remora to get the data, we can define a tool so the LLM can do it.\nWe’ll use the ellmer package to define the tool. First step is to make a function.\nThe ellmer documentation recommends you keep the function inputs and outputs simple. First of all, LLMs can’t handle complex R data structures (like complex lists). Second, the LLM will be consuming the inputs and outputs. This means it uses tokens, which cost you $ and use up the model’s context window (capacity for prompts).\nSo you don’t want to have gigabytes of data going into or out of the tool.\nTo get around this I’ve defined a tool that takes a path and write a file. That way the LLM doesn’t ‘see’ the data, it only sees the input and output paths and my depth request.\n#' Extracts SST data from the IMOS BlueLink database\n#'\n#' @param data_path A path to a csv dataframe\n#' @param depth The depth to extract the SST data from\n#' @return The current time in the given time zone.\nget_sst &lt;- function(data_path, depth) {\n  dat &lt;- read.csv(data_path)\n  datout &lt;- extractBlue(df = dat,\n                X = \"X\", \n                Y = \"Y\", \n                datetime = \"datetime\", \n                env_var = \"BRAN_temp\",\n                extract_depth = depth,\n                verbose = FALSE,\n                full_timeperiod = FALSE)\n  write.csv(datout, \"temp_data.csv\", row.names = FALSE)\n  return(\"File written to sst_data.csv\")\n}\nNow just test the function works:\nget_sst(\"tas_dat.csv\", 30)\nThe ellmer package has a create_tool_def that semi-automates the next step based on the roxygen documentation I wrote above the function. However, this currently seems only to work if you have an openAI API key."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#register-the-tool-with-a-chat",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#register-the-tool-with-a-chat",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "Register the tool with a chat",
    "text": "Register the tool with a chat\nStart a new chat:\nlibrary(ellmer)\nchat &lt;- chat_claude(\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n(You should probably use a strong logic model, like sonnet or GPT4.0 for tool use, but haiku worked fine for this simple one).\nNow we register the tool with the chat:\nmytool &lt;- tool(\n  get_sst,\n  \"Gets SST data from the IMOS BlueLink database\",\n  data_path = type_string(\n    \"Path to a csv dataframe that has columns datetime, X, Y\",\n    required = TRUE\n  ),\n  depth = type_number(\n    \"The depth to extract the SST data from\",\n    required = TRUE\n  )\n)\n\nchat$register_tool(mytool)\nNote the careful documentation of the tool. This is going to become prompts for the model, so use all your normal prompt strategies to help it figure out when to use the tool .\nWe also use the type_xxx functions from ellmer to define the types of the inputs. This is important to ensure the model knows how to properly write the code for the tool use."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#using-the-tool",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#using-the-tool",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "Using the tool",
    "text": "Using the tool\nThis is the easy bit:\nchat$chat(\"Can you get me SST data at 21m depth for the coordinates in tas_dat.csv?\")\nIf that works for you (as it did for me) you should have a new csv file in your working directory with the SST data."
  },
  {
    "objectID": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#conclusion",
    "href": "posts/2025-03-17-LLMs-in-R-tool-use/index.html#conclusion",
    "title": "How to get your LLM model to run and interpret R code",
    "section": "Conclusion",
    "text": "Conclusion\nWith a little bit of code you can give your LLM prompts and responses new types of capabilities.\nMy simple example could be improved by adding more parameters into the function and adding error checking and reports.\nIf we wanted to keep going we could create an analysis tool that then analyses the data from file, makes the plots, and so on.\nTool definitions are based on function documentation. This means you can take any existing R function and easily turn it into a tool.\nYou do want to be careful about how much and the types of data that go into tools. There are obviously security concerns (what if the model created malicious code?) as well as privacy concerns if you are working with sensitive data.\nAPI costs could also blow up. ellmer provides a function token_usage(), to help you keep track of spending.\nOverall, I was excited at how easy it was to make a tool with interesting applications in my field of ocean sciences."
  },
  {
    "objectID": "posts/2016-12-03-r2r-workshop/index.html",
    "href": "posts/2016-12-03-r2r-workshop/index.html",
    "title": "Protecting forests in Fiji to give coral reefs a future",
    "section": "",
    "text": "Many of us become scientists because we have witnessed the degradation of the environment and want to ‘make a difference’. I was therefore excited when the Wildlife Conservation Society (WCS) asked me for advice on developing a plan for managing the catchments and coral reefs of Vanua Levu, Fiji.\n\n\n\nLeft: Fish like this Giant Trevally prefer to hunt in clear water. Right: Streams draining forested catchments will carry clean water to the ocean.\n\n\nWCS is working with communities on island of Vanua Levu, to designate protected areas that will conserve local forests, protect clean water sources and avoid excessive runoff of soils and pollutants that may degrade coastal reefs.\nMy own career was inspired by growing up on an estuary where I couldn’t eat the local fish I caught, because of heavy metal pollution. The pollution was decades old, a result of decisions made by my parent’s and grandparent’s generations. I wanted to make sure my generation didn’t make the same mistakes.\nYet, the daily life of a university scientist often feels distant from any real change. Most early career researchers are so busy trying to chase funding and write ‘high-impact’ academic publications that they don’t have time to think about ‘real impact’ – science that helps people and ecosystems. The daily struggle to keep a job has often left me wondering if a career in science is really the most effective way to help the environment.\n\n\n\nFrom top: A Bauxite mine in Vanua Levu, the Bauxite is extracted from the top-soil. In the process much soil is released that flows into streams and out to the ocean. A river on Vanua Levu carrying sediment from mining and logging to the ocean. A reef smothered to death by sediment from on land.\n\n\nThe opportunity to advise on management plans for Fiji came through with a Science for Nature and People Partnership working group I have helped to lead (along with Carissa Klein and Hugh Possingham). These working groups are funded by the Wildlife Conservation Society (WCS) and The Nature Conservancy. They bring together scientists with the conservation staff who are working on the ground with environmental issues.\nMajor environmental issues and development pressures face the island of Vanua Levu. In particular, agriculture, logging, mining and road development cause soils and pollutants to run-off into streams where they can flow out to coral reef ecosystems. Run-off may be a major threat to coastal reefs and the local fisheries they support.\nThe hard work done by our WCS colleagues, particularly Sangeeta Mangubhai and Stacy Jupiter, to develop collaborations with communities and government on Vanua Levu meant our working group had a direct avenue to contribute toward environmental planning.\nWhen the Wildlife Conservation Society invited my colleague (Amelia Wenger from Uni of Queensland) and I to present our group’s science at a workshop with community and government leaders, I was both thrilled and daunted. Thrilled, because my science had a chance to inform environmental change. Daunted, because I had never visited Vanua Levu, but would be presenting to local people who live from the land and ocean resources I had studied remotely.\nMy own contribution before the workshop in Fiji was largely analytical. Our group brought together data from WCS to measure how land-use change has affected stream fish, coral reefs and coral reef fish (e.g. see this recent talk and paper). If I needed local advice on our work I could easily call up my WCS colleagues.\nTravelling to Nabouwalu a government centre on Vanua Levu where the workshop would be held, was something of a pilgrimage for me. On the way I finally got to see the ecosystems I had studied extensively on maps and satellite images. We flew over extensive reefs, pristine forest. We also saw the environmental damage the region will face more of in the future – Bauxite mining, logging, pine plantations, agriculture and murky rivers.\n\n\n\nDiscussing proposals for protected areas with local community members, WCS and Dr. Wenger from UQ.\n\n\nThe objective for the workshop was to bring together leaders from government departments representing these different industries and local leaders from different districts to build a spatial plan across the Bua Province (south-west portion of Vanua Levu). It is also an opportunity for communities to voice their concerns about environmental protection to central government.\nOur role was to present our work that showed the connections between land-use change and reef fish. In particular, we highlighted how actions in one district can affect the catchments and marine areas of neighbouring districts.\nAfter our initial presentations we had almost two days for community leaders and government members to discuss proposals for a new spatial plan.\nWe then developed the spatial and management plans with community members. The plans were designed through discussion around maps with marker pens. All my training in the sophisticated quantitative and computerised methods used spatial planning went out the window.\n\n\n\nClockwise from top-left: Fire threatens a pine plantation on Vanua Levu. A truck carries logs to a nearby sawmill. Farmland on Vanua Levu. Dr Wenger speaks at the workshop about balancing nature conservation with logging, farming and development.\n\n\nYou can run the computer programs for spatial planning interactively. However, in this case we had to ask the locals about where the pristine catchments were and where their drinking water came from. We also learned that one large catchment with intact forest on the south-coast had now been largely sold into logging concessions. Sadly, my earlier modelling had identified this very catchment as a major contributor to sedimentation to reefs if it was logged.\nDespite the technical simplicity of our approach, we did use many of the principles for conservation planning I had learned over the years. So science did underpin our advice.\nFor instance, we encouraged community members to identify and protect drinking water catchments – WCS has some evidence that typhoid outbreaks after cyclones are worsened in logged catchments.\nCommunity members also proposed a large ridge-line protected area that linked the land in between many of their districts. We encouraged them to identify at least one intact catchment per district for protection. That way the protected area might also protect connections among the regions ecosystems. For instance, Fiji’s unique fish species that migrate from the ocean to freshwater streams.\nOur discussions also emphasised the importance of riparian buffers and suitable road construction to avoid sediment run-off. These principles are already captured in Fiji’s forest practices act, but logging companies often disregard them. A key outcome from our workshop was identifying the need for paid forest wardens, who could ensure forest practices were followed.\n\n\n\nOne of Fiji’s iconic migratory fish (a goby) is represented on their $10 note.\n\n\nAt the end of the workshop I was exhausted but inspired to see our group’s science getting applied to a real planning process.\nWhether real change will happen in the region, we are yet to see. We discovered at this workshop that past protected areas that had been agreed to are now under logging concessions. At the end of the day, it will be WCS and the local people who will play the long-game in finding funding and community support for the new management plan.\n\n\n\nScenes from around Bua Province, Vanua Levu. Clockwise from left: coconut palms, Bua’s administrative centre, cleared land and forest adjacent to coral reefs.\n\n\nI was heartened though, by the closing speech from the local administrator. He said ‘we will not be the guilty generation.’ The Fijian people have lived sustainably in these catchments for generations. He was hoping that his generation could continue to coexist with nature into the future.\nNow, I am back in Fiji’s capital city and a brief encounter convinced me the Fijian people have the right mentality to protect their environment for future generations. Last night I was leaving a restaurant in the rain with a colleague and her baby. A group of burly men approached us in the dark car-park. “Baby Jack!” one of them greeted the baby warmly. The others swooned over him, tickling his toes to make him laugh.\nIn that moment, I saw a country bound by a sense of community and a people who clearly love the next generation. A desire to help the next generation, and the knowledge for how to do so should be all a community needs to make decisions that protect the future of their environment."
  },
  {
    "objectID": "posts/2016-12-03-r2r-workshop/index.html#protecting-forests-in-fiji-to-give-coral-reefs-a-future",
    "href": "posts/2016-12-03-r2r-workshop/index.html#protecting-forests-in-fiji-to-give-coral-reefs-a-future",
    "title": "Protecting forests in Fiji to give coral reefs a future",
    "section": "",
    "text": "Many of us become scientists because we have witnessed the degradation of the environment and want to ‘make a difference’. I was therefore excited when the Wildlife Conservation Society (WCS) asked me for advice on developing a plan for managing the catchments and coral reefs of Vanua Levu, Fiji.\n\n\n\nLeft: Fish like this Giant Trevally prefer to hunt in clear water. Right: Streams draining forested catchments will carry clean water to the ocean.\n\n\nWCS is working with communities on island of Vanua Levu, to designate protected areas that will conserve local forests, protect clean water sources and avoid excessive runoff of soils and pollutants that may degrade coastal reefs.\nMy own career was inspired by growing up on an estuary where I couldn’t eat the local fish I caught, because of heavy metal pollution. The pollution was decades old, a result of decisions made by my parent’s and grandparent’s generations. I wanted to make sure my generation didn’t make the same mistakes.\nYet, the daily life of a university scientist often feels distant from any real change. Most early career researchers are so busy trying to chase funding and write ‘high-impact’ academic publications that they don’t have time to think about ‘real impact’ – science that helps people and ecosystems. The daily struggle to keep a job has often left me wondering if a career in science is really the most effective way to help the environment.\n\n\n\nFrom top: A Bauxite mine in Vanua Levu, the Bauxite is extracted from the top-soil. In the process much soil is released that flows into streams and out to the ocean. A river on Vanua Levu carrying sediment from mining and logging to the ocean. A reef smothered to death by sediment from on land.\n\n\nThe opportunity to advise on management plans for Fiji came through with a Science for Nature and People Partnership working group I have helped to lead (along with Carissa Klein and Hugh Possingham). These working groups are funded by the Wildlife Conservation Society (WCS) and The Nature Conservancy. They bring together scientists with the conservation staff who are working on the ground with environmental issues.\nMajor environmental issues and development pressures face the island of Vanua Levu. In particular, agriculture, logging, mining and road development cause soils and pollutants to run-off into streams where they can flow out to coral reef ecosystems. Run-off may be a major threat to coastal reefs and the local fisheries they support.\nThe hard work done by our WCS colleagues, particularly Sangeeta Mangubhai and Stacy Jupiter, to develop collaborations with communities and government on Vanua Levu meant our working group had a direct avenue to contribute toward environmental planning.\nWhen the Wildlife Conservation Society invited my colleague (Amelia Wenger from Uni of Queensland) and I to present our group’s science at a workshop with community and government leaders, I was both thrilled and daunted. Thrilled, because my science had a chance to inform environmental change. Daunted, because I had never visited Vanua Levu, but would be presenting to local people who live from the land and ocean resources I had studied remotely.\nMy own contribution before the workshop in Fiji was largely analytical. Our group brought together data from WCS to measure how land-use change has affected stream fish, coral reefs and coral reef fish (e.g. see this recent talk and paper). If I needed local advice on our work I could easily call up my WCS colleagues.\nTravelling to Nabouwalu a government centre on Vanua Levu where the workshop would be held, was something of a pilgrimage for me. On the way I finally got to see the ecosystems I had studied extensively on maps and satellite images. We flew over extensive reefs, pristine forest. We also saw the environmental damage the region will face more of in the future – Bauxite mining, logging, pine plantations, agriculture and murky rivers.\n\n\n\nDiscussing proposals for protected areas with local community members, WCS and Dr. Wenger from UQ.\n\n\nThe objective for the workshop was to bring together leaders from government departments representing these different industries and local leaders from different districts to build a spatial plan across the Bua Province (south-west portion of Vanua Levu). It is also an opportunity for communities to voice their concerns about environmental protection to central government.\nOur role was to present our work that showed the connections between land-use change and reef fish. In particular, we highlighted how actions in one district can affect the catchments and marine areas of neighbouring districts.\nAfter our initial presentations we had almost two days for community leaders and government members to discuss proposals for a new spatial plan.\nWe then developed the spatial and management plans with community members. The plans were designed through discussion around maps with marker pens. All my training in the sophisticated quantitative and computerised methods used spatial planning went out the window.\n\n\n\nClockwise from top-left: Fire threatens a pine plantation on Vanua Levu. A truck carries logs to a nearby sawmill. Farmland on Vanua Levu. Dr Wenger speaks at the workshop about balancing nature conservation with logging, farming and development.\n\n\nYou can run the computer programs for spatial planning interactively. However, in this case we had to ask the locals about where the pristine catchments were and where their drinking water came from. We also learned that one large catchment with intact forest on the south-coast had now been largely sold into logging concessions. Sadly, my earlier modelling had identified this very catchment as a major contributor to sedimentation to reefs if it was logged.\nDespite the technical simplicity of our approach, we did use many of the principles for conservation planning I had learned over the years. So science did underpin our advice.\nFor instance, we encouraged community members to identify and protect drinking water catchments – WCS has some evidence that typhoid outbreaks after cyclones are worsened in logged catchments.\nCommunity members also proposed a large ridge-line protected area that linked the land in between many of their districts. We encouraged them to identify at least one intact catchment per district for protection. That way the protected area might also protect connections among the regions ecosystems. For instance, Fiji’s unique fish species that migrate from the ocean to freshwater streams.\nOur discussions also emphasised the importance of riparian buffers and suitable road construction to avoid sediment run-off. These principles are already captured in Fiji’s forest practices act, but logging companies often disregard them. A key outcome from our workshop was identifying the need for paid forest wardens, who could ensure forest practices were followed.\n\n\n\nOne of Fiji’s iconic migratory fish (a goby) is represented on their $10 note.\n\n\nAt the end of the workshop I was exhausted but inspired to see our group’s science getting applied to a real planning process.\nWhether real change will happen in the region, we are yet to see. We discovered at this workshop that past protected areas that had been agreed to are now under logging concessions. At the end of the day, it will be WCS and the local people who will play the long-game in finding funding and community support for the new management plan.\n\n\n\nScenes from around Bua Province, Vanua Levu. Clockwise from left: coconut palms, Bua’s administrative centre, cleared land and forest adjacent to coral reefs.\n\n\nI was heartened though, by the closing speech from the local administrator. He said ‘we will not be the guilty generation.’ The Fijian people have lived sustainably in these catchments for generations. He was hoping that his generation could continue to coexist with nature into the future.\nNow, I am back in Fiji’s capital city and a brief encounter convinced me the Fijian people have the right mentality to protect their environment for future generations. Last night I was leaving a restaurant in the rain with a colleague and her baby. A group of burly men approached us in the dark car-park. “Baby Jack!” one of them greeted the baby warmly. The others swooned over him, tickling his toes to make him laugh.\nIn that moment, I saw a country bound by a sense of community and a people who clearly love the next generation. A desire to help the next generation, and the knowledge for how to do so should be all a community needs to make decisions that protect the future of their environment."
  },
  {
    "objectID": "posts/2025-07-07-writing-papers-with-quarto/index.html",
    "href": "posts/2025-07-07-writing-papers-with-quarto/index.html",
    "title": "Writing scientific papers with quarto and AI assistants",
    "section": "",
    "text": "Below is my suggested workflow for using quarto to write scientific papers. I’m currently switching to doing as many projects as possible with quarto rather than word for a few reasons:\n\nEasier to manage document style\nEasier to manage references\nWorkflows that auto-update figures/tables when R code is re-run\nGenerative AI integration that is customizable.\n\nPoint 3 is great, no more cut and pasting figures into word documents!\nPoint 4 is the big one. I’m developing my own ‘writing mentor’ scripts for large language models. Using quarto lets me implement writing advice specific to science direclty into my manuscripts.\nQuarto is ‘What You See is What you Make’, meaning that you write special syntax for formatting. Once you are used to it, this is way easier way to manage styles than word.\nThe downside is getting your (non-coding) collaborators to edit files in quarto. This is the biggest bottleneck to my use of quarto/markdown. Currently I send them word documents then have to manually integrate the feedback. Or I work in quarto until the near final stages, accepting comments only, then get them to edit the final manuscript.\nFor instance, I wrote most of this paper in markdown but had to go to word editing towards the end so I could get edits from my collaborator. Once you’ve progressed it in word, its hard to go back to markdown.\nInstructions below are high level. There are quite a few pieces of software you need to do this, so I’ve linked to tutorials for each below.\n\n1. Download and install an IDE\nDownload and install VScode.\nInstructions online e.g. here\nI’m using VScode because of its AI assistant integration. But you could also use positron if you have issues with VScode or want to use a Posit product rather than a Microsoft product.\n\n\n2. Get git and github\nInstall git on your computer. Optionally, get a github account and connect to that. Git does version control. Github lets you share that online. If your collaborators are github users then you can also share edits on documents this way.\nGit is also essential if you are using AI assistants. Sometimes they majorly stuff up your documents. So keeping back ups with git is essential.\n\n\n3. VScode extensions\nInstall these VScode extentions (or equivalents if you are using positron, note that many vscode extensions are also compatable with Positron)\n\nQuarto extension.\n\nOpen VSCode and click the four boxes ‘extension’ icon on the LHS then search and install the Quarto extension.\nOptional extensions: - R language extension (can be tricky to connect VScode to R, suggest you web search for advice)\n\n\n4. Optional steps for AI integration\nThis next step is optional. If you are using quarto or markdown its possible to get large-language models to help with many paper writing tasks (including the writing). This is a specialized area though and I’ve only given basic technical instructions here. Actually getting it to work well is another topic altogether and something I’m still developing…\nGet an API key with an LLM provider (e.g. OpenRouter, OpenAI, Anthropic). You’ll need a developer account to get an API key, rather than a regular account that you may already have (see links in last sentence). Buy some credits so you can use your API.\nMake sure you save your API key somewhere safe! You can usually only view them once on creation. You’ll need it for the workshop.\nGet the Roo Code extension for vscode/positron.\nRead the documents/watch the tutorials and learn how to use Roo Code\nYou can now create a custom mode, e.g. a ‘scientific writing mode’ in Roo code. As of writing this requires clicking the mode selection button at the bottom of the Roo Code Pane, then click the Cog, then the + button to make a new mode. Then you need to write a ‘Role Definition’ and ‘Custom instructions’. For tools I just use ‘Read Files’, ‘Edit Files’ and unclick the others (will save you money and tokens).\nThis is the hard part that needs a lot of thought:\nIn the custom instructions you should write detailed instructions on how to help an author with scientific writing. For instance, you might want to put some very strong instructions about not making up references. You might also put instructions about your particular writing style preferences. I’m working on a template, but am not yet ready to share it.\nSee Roo code documentation for more advice on custom modes.\n\n\n5. Using quarto\nTake a tutorial and learn how to use Quarto.\nFor academic paper writing the key things to understand from the Quarto tutorial are:\n\nHow to knit as word or pdf (pdf requires extra software installations)\nFormatting, headings, bold etc…\nYAML frontmatter for styles, linking a bibliography and bibliography style\nHow to insert images and/or code.\n\nNote on AI integration once you are using quarto and Roo Code you can simply ask Roo Code to do things in your document (like outline a paper template) by referencing the file (e.g. @myfile.qmd) in the prompt box.\nWhether this works well for you is another questions. Prompting well requires a lot of thought and practice. Its not simply going to write a paper for you. You have to give the AI assistant detailed, specific, instructions and lots of context.\n\n\n6. YAML front matter\nThe YAML controls how your qmd document is rendered. Here’s an example of mine:\n---\ntitle: \"The paper's title\"\nformat: docx\neditor: visual\nbibliography: mybib.bib\ncsl: myjournal.csl\nexecute: \n  echo: false\n  message: false\n  warning: false\n---\nThis goes at the top of your document. A few key points.\nformat controls document type to render this as, here a word doc.\neditor controls how it is viewed in vscode. Options are editor: visual and editor: source. Visual looks more like a word doc, source looks more like markdown. You’ll have to save and re-open the document for this to change.\nbibliography links to a bibtex file where your references are stored.\ncsl links to a style guide for the bibliography.\nMore on styles and references below.\nexecute is controlling how R code is run and if the R code appears in the document.\n\n\n7. Rendering as a document\nUse the short-cut key ‘cmd-shift-K’/‘cntrl-shft-k’ (mac/windows) to preview your document. It will also create a rendered version in your current directory.\nIts helpful to set: format: html when you are writing the document, then you get a live preview in vscode. Use format: docx when you want a word document.\nIts worth also learning the short-cut `cmd-shft-p’/‘cntrl-shft-p’, this brings up searchable actions for all extensions in vscode. The one you want is ‘Quarto: preview’ which does the same as the shortcut above.\nI tend to have minimal R code in my quarto manuscript. Or none at all (just reference .png files for figures). This keeps rendering quick. Also your document can get unweildy if there is a lot of text mixed in with R code.\n\n\n8. Word counts\nThere are various word count extensions for vscode qmd and md documents.\n\n\n9. Document styles\nGetting a word document to follow a particular style is a bit fiddly. You need to set-up a template word document with styles the include that as a reference in your YAML.\nSee instructions here.\n\n\n10. Reference manager integration\nQuarto integrates with many different reference managers. There’s a good guide here.\nIn brief you create a .bib file that has your references in it. This is then linked in the YAML. The manual way to manage this is just to create a .bib file and paste bibtext entries directly into it (available on most journal’s pages as a citation format, as well as google scholar).\ne.g. the bibtext for R looks like this:\n@Manual{Rlanguage,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\nThen in quarto you just type @ and a dropdown of all your references will appear. @Manual{Rlanguage, the Rlanguage bit is the CiteKey that will appear in the dropdown. So @Rlanguage will insert that reference into the bibliography and the citation at that place in the document.\nYou can streamline the process of gathering and managing references with a reference manager.\nMy workflow in Zotero is as follows:\n\nOpen Zotero on my computer\nGo to journal webpage for paper\nUse zotero plugin to my browser to grab the citation and save it to a library\nGo to my quarto document in VScode\ntype @ and a drop down of all references in all libraries on zotero appears. Pick the one I want.\nClick the OK button which saves that reference into my local .bib file.\n\nFor some reason (that does not seem to be documented in any quarto tutorials anywhere!) it will find any reference I have anywhere in zotero and then save that bibtex entry to my local .bib file, so it is now accessible for use in my quarto doc. This only works if I have zotero open and use editor: visual in the YAML.\nThere are many other options however.\n\n\n11. Optional AI integration for reference management\nYou can get AI assistants to help with referencing if you keep your notes on papers linked to your references. For instance, you could keep your notes on references in the bibtex field for notes. Alternatively you could create another quarto/markdown document that has a header for each citation tag along with its notes in a structured way:\n## Rlanguage \n\n### What it is\n\nThe R software for scientific computing. \n\n### Usage\n\nCitation for the R software. Use this at least once in every paper where i've used R for statistics\n\n## edgar2023continent\n\n### What is it\n\nKey paper that shows Australia is losing its marine biodiversity. \n\n### Usage\n\nCite this as evidence that Australia is losing coastal marine biodiversity and as evidence that climate change is causing marine biodiversity loss\n\nIt doesn’t matter how you do this, so long as you follow a consistent structure. I’ve used the CiteKey as the main header for each reference entry. Then I’ve put in markdown sections about each paper and why I might wnat to cite it. Then you can get Roo Code to help with inserting references.\nNote that if you are using the .bib directly just be careful not to plagiarise! Roo Code might insert excerpts from the abstracts/titles directly into your written document, which is a no-no for publishing."
  },
  {
    "objectID": "posts/2016-02-09-measuring_range_shifts/index.html",
    "href": "posts/2016-02-09-measuring_range_shifts/index.html",
    "title": "Measuring range shifts",
    "section": "",
    "text": "Why do species vary in how fast they shift their ranges?\nIt is well established that species are moving their ranges towards the poles to keep up with global warming. We would like to know how fast species are likely to move, so we can inform policy responses. For instance, how soon will fisheries be affected?\nSpecies on the move is the topic of a major conference this week (10-Feb-2016). There are many talks about identifying range shifts. This is my take on climate range shifts in marine species, based on our recent paper.\n\n\n\nWe want to know how fast species will move under global warming, so we can inform management policy, such as for fisheries of affected species \n\n\nThere is considerable variation in how fast different species are moving. What we don’t know is why species vary. This variation makes predicting range shifts in the future difficult. We can build realistic models of almost 13 000 ocean species to predict changes in species richness under global warming, but we have very few observations to test those predictions against (see figure below). While the general trends from the models might be correct, it is hard to fine tune them without more information on how individual species will respond.\nWe set out to find out why species vary using an analysis of a global database of observed species range shifts. What we found surprised us. We were expecting ecological traits, like a species’ dispersal ability to predict the rate of range shifts. It turned out that the rate of range shifts we measure was more strongly determined by how scientists measured the range shift, than by the traits of the species they measured.\n\n\n\nWe have the  models  to that can generate predictions for a large number of species, but we have very few  observations of range shifts  to test those models against. Finding generalities in species responses, for instance better dispersers may move faster, can help us fine tune the models to get more accurate predictions.\n\n\nThe data we used came from a literature review, that our team conducted during working groups at the National Center for Ecological Analysis and Synthesis. We updated that review and found 485 observations of range shifts, taken from observations of marine species. As a minimum criteria studies had to have observations in the field with a time-span of at least 19 years, to ensure they captured a long-term climate change response, rather than just short-term variation.\nWe asked two main questions with our database:\n\nDo the ecological attributes of a species explain how fast a species range moves in response to climate change?\nDid the methods used to measure a species’ range affect the rate of the range shift that was observed?\n\nWe found the most important predictors of the rate species moved where the type of data (abundance or occurrence) used to measure ranges and how frequently a species’ range was sampled.\nAbundance vs. occurrence measurements were important because they measure different parts of a population’s range shift. Occurrence based measures measured faster range shifts than abundance based measures. Occurrence based measures are influenced by the movement of single individuals, who may represent the extremes of movement, not the norm. They likely pick up on the early colonisers that are extending a species range, but have not yet established new populations. These colonisers can move very rapidly. Abundance based measures focus more on established populations. Because population establishment is slower, rates measured by abundance are also slower.\n\n\n\nAbundance and occurrence surveys measure different rates of range shift because they measure different ecological processes \n\n\nFrequency of sampling was also important. Yearly sampled ranges were observed to move more slowly than resurveys - studies that repeat a historical survey in the present day. This difference may indicate a publication bias in resurveys unintentionally picking places where change has been more extreme.\nWe did find some differences between species’ movements that were attributable to ecological traits. Most importantly, coastal species moved much more slowly than open-ocean species. This may be because coastal species are more constrained in the habitats they can use (e.g. barnacles on rocky shores), so the rate of their range shifts is limited by availability of new habitat to move into.\nOur study has important implications for how we measure and interpret range shifts. While there is no doubt species are responding to ocean warming, we need to be careful about how we measure that change.\nFirst, we should be encouraging resurveys in places that may not be strongly warming. Doing so will give us a more comprehensive picture of which species are moving (or not moving) in response to warming.\nSecond, meta-analyses should control for the methods of studies they analyse. Previous analyses of smaller regions than ours have been successful at identifying traits that predict how species vary. Our analysis encompassed a much broader area (the whole globe), and therefore a broader range of methods for measuring range shifts. If we are going to uncover traits that matter, we need to control for differences in the methods for measuring range shifts in global analyses.\nFinally, the design of new time-series to detect range shifts can be informed by our findings. For instance, surveys of species occurrence may be useful as an ‘early warning’ of new colonisers, by picking up the extreme individuals that respond to warming first. In contrast, surveys of abundance may be most useful for picking up on ecologically significant change, to identify when a new species has established populations in an area."
  },
  {
    "objectID": "posts/2016-11-23-mapping-abundance-photos/index.html",
    "href": "posts/2016-11-23-mapping-abundance-photos/index.html",
    "title": "Create an interactive web map with geotagged photos",
    "section": "",
    "text": "See the other parts in this series of blog posts.\nIn part 1 we looked at making a simple interactive map using locations from our geotagged photos. Those photos have data associated with them (counts of oysters in quadrats) so here we will look at how you would plot that data on your map.\nWe will also add a geo-referenced pop-up that includes a picture of our study site. You might be familiar with this idea from Google Earth or Flickr.\nTo get you excited, here is the end result (try clicking on things! If you are viewing this blog on Rbloggers you will need to go here to see the map):\n\nIt no map appears here your browser doesn’t support iframes. Try &lt;a href = \"/data/hornby_oysters_map.html\"&gt;this link instead&lt;/a&gt;\n\n\n\nFirst up here are the packages we need:\nlibrary(leaflet)\nlibrary(exifr)\nThe next list of packages is optional, but I will use here to make data processing more convenient:\nlibrary(readr) #for the updated version of read.csv\nlibrary(dplyr) #for data wrangling\nlibrary(stringr) #for wrangling strings\nFinally to get some nice colours:\nlibrary(RColorBrewer)\nNow load in the data. I am using read_csv which comes from the readr package. read_csv is a modernised version of the base R function read.csv. Here are the exif data and associated oyster counts if you are repeating this example.\nedat &lt;- read_csv('Exifdata.csv')\nodat &lt;- read_csv('oyster_data.csv')\nn &lt;- nrow(odat) #number of sample sites\n\n\n\nNow we need to join our counts of oysters to the locations. We can match by the picture numbers, which I recorded when I counted the oysters. First up we need to clean the clean the picture numbers in the exif data to remove the leading ‘DSC_’. We do this using mutate from dplyr and str_extract from stringr to extract the first four digits in each picture’s filename.\nedat&lt;- edat %&gt;%\nmutate(pic_num =\n    as.numeric(\n        str_extract(SourceFile, '\\\\d\\\\d\\\\d\\\\d')\n        ))\nIf you haven’t seen %&gt;% before it is a ‘pipe’ that puts the dataframe edat into the first argument of our mutate function. The pipe isn’t essential, but I think it improves readibility. See Wickham’s book for more guidance on pipes.\nNow we can join the oysters and pictures. left_join will automatically identify pic_num to join on, because it is the common column across both dataframes:\ndat &lt;- odat %&gt;%\n  left_join(edat)\n\n\n\nTo create the pop-up you need to know a little bit of html. There is plenty of helpful guides on the web and html is pretty simple to learn. The pop-up will show an image and some text. Here we define the content:\ncontent &lt;- paste(sep = \"&lt;br/&gt;\",\n\"&lt;img src='http://www.seascapemodels.orgintertidal_scene.JPG'\n   style='width:230px;height:300px;'&gt;\",\n\"The intertidal zone\",\n\"at Hornby Island\"\n)\nIn short we have specified a string with some html code. &lt;br/&gt; html tag creates a new line. The &lt;img&gt; tag specifies the insertion of an image (the link gives the image’s location, which is on my webpage. We have also set the width and height of the image. Then we follow with a bit of text explaining the image.\nWe will use content in our leaflet map below.\nWe also need the location to place our pop-up. Because the image is gps tagged, we can extract the gps locations from the exif data. Download the image from the link given above in content then you can get the exif data associated with it like this:\nscene &lt;- exifr('intertidal_scene.JPG')\nx1 &lt;- scene $GPSLongitude\ny1 &lt;- scene $GPSLatitude\nWe now have coordinates for the pop-up in our leaflet map.\n\n\n\nleaflet has some pretty convenient functions for creating nice colour scales. Here we use colourBin to bin counts of oysters into different categories that we set in the vector brks:\nbrks &lt;- c(0,1, 5, 10, 20, 70)\nncol &lt;- length(brks)-1\noystercols &lt;- c('grey20',brewer.pal(ncol-1, 'Purples'))\npal &lt;- colorBin(oystercols, dat$oysters_live, bins = brks)\nThe end result pal is a function that will generate a colour palette when some data is input to it.\n\n\n\nWe have prepped our data, so now the fun bit, making the map! This map is somewhat complicated so I will step you through it using our friend ‘pipes’ (the full code without text breaks is below if you want to cut and paste). Note how convenient pipes are here, we can essentially just layer up our map data in an intuitive fashion.\nmapout &lt;- leaflet(dat) %&gt;%\nWe name our map and set the dataframe it will use.\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\nSet’s the background (a satellite image)\nsetView(lng = x1, lat = y1, zoom = 16) %&gt;%\nChooses the region to zoom into. leaflet will guess the zoom level from our dataframe, but we have more control if we choose the region. The default here was a higher zoom level where the tiles don’t render when using Esri.WorldImagery.\nNow we add some markers, setting their colours using oyster counts:\naddCircleMarkers(~ GPSLongitude, ~ GPSLatitude,\n    color = 'white',opacity =1, weight = 1,\nfillColor = ~pal(oysters_live),\npopup = as.character(dat$oysters_live),\nfillOpacity = 0.8,\nradius = 6) %&gt;%\nThis looks complex, but it is just a series of simple commands. ~ GPSLongitude tells leaflet which column of dat is longitude. color, opacity and weight refer to the colour, opacity (solid in this case) and width of the border of the markers. fillColor uses our palette and applies it the oysters_live column of dat. popup creates a popup when we click on a marker, which gives the number of oysters. fillOpacity and radius refer to the fill of the markers.\nWe will also add a marker for the picture I took of the shoreline. Note that this time I use our content data created above to specify what goes in the popup. We also change some of the marker options with options. See ?markerOptions for other options:\naddMarkers(x1, y1, popup = content,\n    options = markerOptions(opacity = 0.9, draggable=T)) %&gt;%\nNow let’s add a legend:\naddLegend(\"topright\", pal = pal,\nvalues = brks,\ntitle = \"Number of live oysters\n&lt;a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'&gt; (Crassostrea gigas)&lt;/a&gt;\",\nopacity = 1)\nthe nice thing about using colorBin to create our colours is we can map them directly to the legend, as we have done here with pal = pal (telling leaflet to use our pal function for the colour palette). Note in the title command we have also inserted some more html, this time a &lt;a&gt; tag which just turns the species name ‘(Crassostrea gigas)’ into a link to the Wikipedia page for the Pacific oyster.\nAnd that’s it. Run that code then type mapout to print your map. Next up we will look at how to build a spatial model using this data.\n\n\n\nIf you want to save the map to use on your own page, you can do so with the htmlwidgets package, like this:\nhtmlwidgets::saveWidget(mapout, file = 'hornby_oysters_map.html', selfcontained = F, libdir = \"leafletmap_files\")\nI recommend using `so that the javascript files that create the map are stored in a seperate folder, rather than the javascript code being included within the html page (which makes it a very long file).  Also, if you set thelibdircommand the javascript files will be saved in the folder specified bylibdir` and all the references to the javascript on the html page (which has the map) will be to that folder (in this case: ‘/leafletmap_files/’). If you are putting multiple maps on your webpage, always use the same libdir and you will only need one copy of the leaflet javascript (under libdir) in your page’s data files. Poke around in github.com/cbrown5/cbrown5.github.io if you want to see how this works. I keep the leaflat maps in the data folder.\n\n\n\nmapout &lt;- leaflet(dat) %&gt;%\n#Use satellite image as base\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\nsetView(lng = x1, lat = y1, zoom = 16) %&gt;%\n#Add markers for oyster quadrats\naddCircleMarkers(~ GPSLongitude, ~ GPSLatitude,\n    color = 'white',opacity =1, weight = 1,\n    fillColor = ~pal(oysters_live),\n    popup = as.character(dat$oysters_live),\n    fillOpacity = 0.8,\n    radius = 6) %&gt;% # add a popup for number of oysters\n    #Add marker showing a picture of the survey site\n    addMarkers(x1, y1, popup = content,\n        options = markerOptions(opacity = 0.9, draggable=T)) %&gt;%\n        #Add a legend\n        addLegend(\"topright\", pal = pal,\n        values = brks,\n        title = \"Number of live oysters\n        &lt;a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'&gt; (Crassostrea gigas)&lt;/a&gt;\",\n        opacity = 1)"
  },
  {
    "objectID": "posts/2016-11-23-mapping-abundance-photos/index.html#part-2-create-an-interactive-web-map-with-geotagged-photos",
    "href": "posts/2016-11-23-mapping-abundance-photos/index.html#part-2-create-an-interactive-web-map-with-geotagged-photos",
    "title": "Create an interactive web map with geotagged photos",
    "section": "",
    "text": "See the other parts in this series of blog posts.\nIn part 1 we looked at making a simple interactive map using locations from our geotagged photos. Those photos have data associated with them (counts of oysters in quadrats) so here we will look at how you would plot that data on your map.\nWe will also add a geo-referenced pop-up that includes a picture of our study site. You might be familiar with this idea from Google Earth or Flickr.\nTo get you excited, here is the end result (try clicking on things! If you are viewing this blog on Rbloggers you will need to go here to see the map):\n\nIt no map appears here your browser doesn’t support iframes. Try &lt;a href = \"/data/hornby_oysters_map.html\"&gt;this link instead&lt;/a&gt;\n\n\n\nFirst up here are the packages we need:\nlibrary(leaflet)\nlibrary(exifr)\nThe next list of packages is optional, but I will use here to make data processing more convenient:\nlibrary(readr) #for the updated version of read.csv\nlibrary(dplyr) #for data wrangling\nlibrary(stringr) #for wrangling strings\nFinally to get some nice colours:\nlibrary(RColorBrewer)\nNow load in the data. I am using read_csv which comes from the readr package. read_csv is a modernised version of the base R function read.csv. Here are the exif data and associated oyster counts if you are repeating this example.\nedat &lt;- read_csv('Exifdata.csv')\nodat &lt;- read_csv('oyster_data.csv')\nn &lt;- nrow(odat) #number of sample sites\n\n\n\nNow we need to join our counts of oysters to the locations. We can match by the picture numbers, which I recorded when I counted the oysters. First up we need to clean the clean the picture numbers in the exif data to remove the leading ‘DSC_’. We do this using mutate from dplyr and str_extract from stringr to extract the first four digits in each picture’s filename.\nedat&lt;- edat %&gt;%\nmutate(pic_num =\n    as.numeric(\n        str_extract(SourceFile, '\\\\d\\\\d\\\\d\\\\d')\n        ))\nIf you haven’t seen %&gt;% before it is a ‘pipe’ that puts the dataframe edat into the first argument of our mutate function. The pipe isn’t essential, but I think it improves readibility. See Wickham’s book for more guidance on pipes.\nNow we can join the oysters and pictures. left_join will automatically identify pic_num to join on, because it is the common column across both dataframes:\ndat &lt;- odat %&gt;%\n  left_join(edat)\n\n\n\nTo create the pop-up you need to know a little bit of html. There is plenty of helpful guides on the web and html is pretty simple to learn. The pop-up will show an image and some text. Here we define the content:\ncontent &lt;- paste(sep = \"&lt;br/&gt;\",\n\"&lt;img src='http://www.seascapemodels.orgintertidal_scene.JPG'\n   style='width:230px;height:300px;'&gt;\",\n\"The intertidal zone\",\n\"at Hornby Island\"\n)\nIn short we have specified a string with some html code. &lt;br/&gt; html tag creates a new line. The &lt;img&gt; tag specifies the insertion of an image (the link gives the image’s location, which is on my webpage. We have also set the width and height of the image. Then we follow with a bit of text explaining the image.\nWe will use content in our leaflet map below.\nWe also need the location to place our pop-up. Because the image is gps tagged, we can extract the gps locations from the exif data. Download the image from the link given above in content then you can get the exif data associated with it like this:\nscene &lt;- exifr('intertidal_scene.JPG')\nx1 &lt;- scene $GPSLongitude\ny1 &lt;- scene $GPSLatitude\nWe now have coordinates for the pop-up in our leaflet map.\n\n\n\nleaflet has some pretty convenient functions for creating nice colour scales. Here we use colourBin to bin counts of oysters into different categories that we set in the vector brks:\nbrks &lt;- c(0,1, 5, 10, 20, 70)\nncol &lt;- length(brks)-1\noystercols &lt;- c('grey20',brewer.pal(ncol-1, 'Purples'))\npal &lt;- colorBin(oystercols, dat$oysters_live, bins = brks)\nThe end result pal is a function that will generate a colour palette when some data is input to it.\n\n\n\nWe have prepped our data, so now the fun bit, making the map! This map is somewhat complicated so I will step you through it using our friend ‘pipes’ (the full code without text breaks is below if you want to cut and paste). Note how convenient pipes are here, we can essentially just layer up our map data in an intuitive fashion.\nmapout &lt;- leaflet(dat) %&gt;%\nWe name our map and set the dataframe it will use.\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\nSet’s the background (a satellite image)\nsetView(lng = x1, lat = y1, zoom = 16) %&gt;%\nChooses the region to zoom into. leaflet will guess the zoom level from our dataframe, but we have more control if we choose the region. The default here was a higher zoom level where the tiles don’t render when using Esri.WorldImagery.\nNow we add some markers, setting their colours using oyster counts:\naddCircleMarkers(~ GPSLongitude, ~ GPSLatitude,\n    color = 'white',opacity =1, weight = 1,\nfillColor = ~pal(oysters_live),\npopup = as.character(dat$oysters_live),\nfillOpacity = 0.8,\nradius = 6) %&gt;%\nThis looks complex, but it is just a series of simple commands. ~ GPSLongitude tells leaflet which column of dat is longitude. color, opacity and weight refer to the colour, opacity (solid in this case) and width of the border of the markers. fillColor uses our palette and applies it the oysters_live column of dat. popup creates a popup when we click on a marker, which gives the number of oysters. fillOpacity and radius refer to the fill of the markers.\nWe will also add a marker for the picture I took of the shoreline. Note that this time I use our content data created above to specify what goes in the popup. We also change some of the marker options with options. See ?markerOptions for other options:\naddMarkers(x1, y1, popup = content,\n    options = markerOptions(opacity = 0.9, draggable=T)) %&gt;%\nNow let’s add a legend:\naddLegend(\"topright\", pal = pal,\nvalues = brks,\ntitle = \"Number of live oysters\n&lt;a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'&gt; (Crassostrea gigas)&lt;/a&gt;\",\nopacity = 1)\nthe nice thing about using colorBin to create our colours is we can map them directly to the legend, as we have done here with pal = pal (telling leaflet to use our pal function for the colour palette). Note in the title command we have also inserted some more html, this time a &lt;a&gt; tag which just turns the species name ‘(Crassostrea gigas)’ into a link to the Wikipedia page for the Pacific oyster.\nAnd that’s it. Run that code then type mapout to print your map. Next up we will look at how to build a spatial model using this data.\n\n\n\nIf you want to save the map to use on your own page, you can do so with the htmlwidgets package, like this:\nhtmlwidgets::saveWidget(mapout, file = 'hornby_oysters_map.html', selfcontained = F, libdir = \"leafletmap_files\")\nI recommend using `so that the javascript files that create the map are stored in a seperate folder, rather than the javascript code being included within the html page (which makes it a very long file).  Also, if you set thelibdircommand the javascript files will be saved in the folder specified bylibdir` and all the references to the javascript on the html page (which has the map) will be to that folder (in this case: ‘/leafletmap_files/’). If you are putting multiple maps on your webpage, always use the same libdir and you will only need one copy of the leaflet javascript (under libdir) in your page’s data files. Poke around in github.com/cbrown5/cbrown5.github.io if you want to see how this works. I keep the leaflat maps in the data folder.\n\n\n\nmapout &lt;- leaflet(dat) %&gt;%\n#Use satellite image as base\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\nsetView(lng = x1, lat = y1, zoom = 16) %&gt;%\n#Add markers for oyster quadrats\naddCircleMarkers(~ GPSLongitude, ~ GPSLatitude,\n    color = 'white',opacity =1, weight = 1,\n    fillColor = ~pal(oysters_live),\n    popup = as.character(dat$oysters_live),\n    fillOpacity = 0.8,\n    radius = 6) %&gt;% # add a popup for number of oysters\n    #Add marker showing a picture of the survey site\n    addMarkers(x1, y1, popup = content,\n        options = markerOptions(opacity = 0.9, draggable=T)) %&gt;%\n        #Add a legend\n        addLegend(\"topright\", pal = pal,\n        values = brks,\n        title = \"Number of live oysters\n        &lt;a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'&gt; (Crassostrea gigas)&lt;/a&gt;\",\n        opacity = 1)"
  },
  {
    "objectID": "posts/2023-07-09-AMSA-interviews/index.html",
    "href": "posts/2023-07-09-AMSA-interviews/index.html",
    "title": "What do Australian marine scientists like about their jobs?",
    "section": "",
    "text": "I caught up with some marine scientists (and a couple of other marine professionals) to ask them what they love about their jobs, what they find hard and for their advice to aspiring marine scientists. The rapid-fire interivews are on my youtube channel or watch below"
  },
  {
    "objectID": "posts/2023-07-09-AMSA-interviews/index.html#what-do-you-love-about-your-career",
    "href": "posts/2023-07-09-AMSA-interviews/index.html#what-do-you-love-about-your-career",
    "title": "What do Australian marine scientists like about their jobs?",
    "section": "What do you love about your career?",
    "text": "What do you love about your career?"
  },
  {
    "objectID": "posts/2023-07-09-AMSA-interviews/index.html#what-do-you-find-hard-about-your-career",
    "href": "posts/2023-07-09-AMSA-interviews/index.html#what-do-you-find-hard-about-your-career",
    "title": "What do Australian marine scientists like about their jobs?",
    "section": "What do you find hard about your career?",
    "text": "What do you find hard about your career?"
  },
  {
    "objectID": "posts/2023-07-09-AMSA-interviews/index.html#whats-your-advice-to-aspiring-marine-scientists",
    "href": "posts/2023-07-09-AMSA-interviews/index.html#whats-your-advice-to-aspiring-marine-scientists",
    "title": "What do Australian marine scientists like about their jobs?",
    "section": "What’s your advice to aspiring marine scientists?",
    "text": "What’s your advice to aspiring marine scientists?"
  },
  {
    "objectID": "posts/2017-02-22-spatial-statistics-photos/index.html",
    "href": "posts/2017-02-22-spatial-statistics-photos/index.html",
    "title": "Part 3: Spatial analysis of geotagged data",
    "section": "",
    "text": "See the other parts in this series of blog posts.\nIn parts 1 and 2 we extracted spatial coordinates from our photos and then made an interactive web map that included data associate with those photos. Here I describe how we can build a spatial statistical model to interpolate to unmeasured locations.\nHere we will build an interactive map showing model results from an interpolation of oyster presence/absence.\n\nIt no map appears here your browser doesn’t support iframes. Try &lt;a href = \"/data/hornby_oysters_interp.html\"&gt;this link instead&lt;/a&gt;\n\nGet the data for this blog.\nFirst up we need to load some packages:\n    library(readr)\n    library(INLA)\n    library(sp)\n    library(raster)\n    library(rgeos)\nThe INLA package is crucial here. We will be using INLA to perform Bayesian Kriging. In non-technical terms, Bayesian kriging lets us fill in the gaps between observations of oyster counts by interpolating from nearby points.\n\n\nBefore we begin, we need to load in the data, and transform it into a spatial data frame. In particular, it is important to transform the lon-lats into UTM coordinates. UTM coordinates allow us to measure distances between sample sites in metres:\n    dat &lt;- read_csv('Oysters_merged.csv')\n    n &lt;- nrow(dat)\n    utmproj &lt;- \"+proj=utm +zone=10 +north +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n    spdat &lt;- dat\n    coordinates(spdat) &lt;- ~ GPSLongitude + GPSLatitude\n    proj4string(spdat) &lt;- \"+init=epsg:4326\"\n    spdf &lt;- spTransform(spdat, CRS(utmproj))\n    plot(spdf)\n\nYou should get a plot like that pictured just of the sample sites.\n\n\n\nNow we can start building the specialised data-structures we need for the INLA package. First up, we have to build what’s called a mesh. This is a Delaunay triangulation built around our data points. It accounts for the non-regular spatial structuring of the sampling (I wandered around on the rocky shore throwing the quadrat at intervals).\nBelow we provide two parameters to max.edge, this will enable us to buffer the edges to obtain a slightly larger spatial domain. Buffering is important because the INLA approximation will be inaccurate at the edges of the spatial domain.\nmax.edge.length &lt;- c(25, 40)\nloc1 &lt;- as.matrix(coordinates(spdf))\nmesh &lt;- inla.mesh.2d(loc=loc1, max.edge = max.edge.length, offset=1, cutoff = 5)\n\nplot(mesh)\nplot(spdf, add = T, col = 'red')\n\nYou can play with the edge length, offset and cutoff parameters to vary how the triangulations turns out.\nYou can get more guidance at the INLA page and in this tutorial. This tutorial also has some very excellent advice about how to choose boundaries and triangle sizes.\nAn important step is to check that our maximum edge lengths are less than the estimated range (otherwise it is pointless including a spatial effect!). We will check the range size below, once we have fitted the model.\nYou might like to think of a-prior reasons for modelling a certain range, for instance, what is a reasonable spatial scale for ‘clumping’ of oyster patches?\n\n\n\nNow we need to implement a few more steps to build up an appropriate ‘stack’ of data for INLA. Note that I have transformed oyster count to presence - absence. We will build a binomial model and just predict whether any given quadrat had oysters or not.\nA.data &lt;- inla.spde.make.A(mesh, loc1)\nspde &lt;- inla.spde2.matern(mesh, alpha = 2)\nspdf$presabs &lt;- ifelse(spdf$oysters_live&gt;0, 1, 0)\n\nstk &lt;- inla.stack(data=list(y=spdf$presabs), A=list(A.data, 1),\n                  effects=list(s=1:mesh$n, intercept=rep(1, n)),\n                  remove.unused = FALSE, tag = \"est\")\nNote that here we choose alpha = 2. alpha is a smoothness parameter that must be 0 &lt;= alpha &lt;= 2, higher values will give smoother spatial interpolation. Also note that we have to specify ‘data’ for the intercept (just a vector of 1).\nThe tag = \"est\" argument just gives a name tag to the fitted model. In other applications you might want to join the estimation stack with a prediction stack (just build two stacks, one where the response is NA and then join them by applying inla.stack to both).\n\n\n\nNow, after that considerable amount of prepartion, we get to fitting the model. We use standard R syntax for the formula, with the addition of the f() term to specify a function.\n    formula &lt;- y ~ 0 + intercept + f(s, model=spde)\n\n    mod &lt;- inla(formula, data=inla.stack.data(stk),\n            control.predictor=list(A = inla.stack.A(stk)),\n            family = 'binomial')\nWe can extract summary parameters using summary. Here we are interested in the random effects, it is good practice to call inla.hyperpar before we inspect random effects:\nhyper &lt;- inla.hyperpar(mod)\nsummary(hyper)\n\n\n\nWe should also check that our triangle edge length is less than the estimated spatial range. We can do that by making a call to the model object, asking for the estimated parameters for the random field\nrf &lt;- inla.spde.result(inla = mod, name = \"s\",\nspde = spde, do.transf = TRUE)\n\nplot(rf$marginals.range[[1]], type = \"l\",\n xlab = \"Range (m)\", ylab = \"Density\",\n xlim = c(0, 1000))\nabline(v = max.edge.length[2], col = 'red')\n\nThe red line shows our edge length parameter, which is well less than the model estimate for the spatial range.\n\n\n\nNow we have a fitted model, we can extracted predicted probabilities of oyster presence and map them. First transform the predictions from the logit scale back to probabilities:\n  ypred &lt;- exp(mod$summary.random$s$mean)/(1 + exp(mod$summary.random$s$mean))\nThen we can turn our predictions into a spatial points, that can be be mapped onto a raster for plotting:\nextent &lt;- extent(spdf)\nxdims &lt;- 100; ydims &lt;- 200\nxlim &lt;- c(extent[1], extent[2]); ylim = c(extent[3], extent[4]);\nproj &lt;- inla.mesh.projector(mesh, xlim = xlim, ylim = ylim, dims=c(xdims, ydims))\nfield.proj &lt;- inla.mesh.project(proj, ypred)\n\ndatpred &lt;- data.frame(x = rep(proj$x, ydims), y = rep(proj$y, each = xdims), pred = as.numeric(field.proj))\ncoordinates(datpred) &lt;- ~x + y\n\nproj4string(datpred) &lt;- utmproj\ndat3 &lt;- spTransform(datpred, crs(\"+init=epsg:4326\"))\n\nr &lt;- raster(extent(dat3), ncols = xdims, nrows= ydims, crs = crs(spdat))\nicell &lt;- cellFromXY(r, dat3)\n\nr[icell] &lt;- as.numeric(dat3$pred)\nAnd finally the plot:\ncols &lt;- RColorBrewer::brewer.pal(9, \"Purples\")\nplot(r, col = cols)\npoints(spdf)\nplot(spdat, add = TRUE)\n\nSo it looks like oysters are more common in the southern portion of the survey area.\nBut note that our predictions extent a fair way from the points. We might want to redo these with a more constrained spatial region. In general we should’t trust predictions that are more than ~ 1 spatial range from the edge of our sampled region (in this case the posterior estimate for range was 181m).\nFinally, we could map this onto a satellite image to get a better view of what is going on. I am just using the same leaflet code as before:\nlibrary(leaflet)\nbrks &lt;- seq(0,1, by = 0.1)\nncol &lt;- length(brks)\noystercols &lt;- RColorBrewer::brewer.pal(min(ncol, 9), 'Reds')\npal &lt;- colorBin(oystercols, dat$oysters_live, bins = brks)\n\nx1 &lt;- -124.5997\ny1 &lt;- 49.52848\n\nleaflet() %&gt;%\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\n\nsetView(lng = x1, lat = y1, zoom = 16) %&gt;%\naddRasterImage(r, opacity = 0.8, color = pal) %&gt;%\naddCircleMarkers(lng = dat$GPSLongitude, lat = spdat$GPSLatitude,\nradius = 4) %&gt;%\naddLegend(\"topright\", pal = pal,\nvalues = brks,\ntitle = \"Chance of live oysters &lt;a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'&gt; (Crassostrea gigas)&lt;/a&gt;\",\nopacity = 1)"
  },
  {
    "objectID": "posts/2017-02-22-spatial-statistics-photos/index.html#part-3-spatial-analysis-of-geotagged-data",
    "href": "posts/2017-02-22-spatial-statistics-photos/index.html#part-3-spatial-analysis-of-geotagged-data",
    "title": "Part 3: Spatial analysis of geotagged data",
    "section": "",
    "text": "See the other parts in this series of blog posts.\nIn parts 1 and 2 we extracted spatial coordinates from our photos and then made an interactive web map that included data associate with those photos. Here I describe how we can build a spatial statistical model to interpolate to unmeasured locations.\nHere we will build an interactive map showing model results from an interpolation of oyster presence/absence.\n\nIt no map appears here your browser doesn’t support iframes. Try &lt;a href = \"/data/hornby_oysters_interp.html\"&gt;this link instead&lt;/a&gt;\n\nGet the data for this blog.\nFirst up we need to load some packages:\n    library(readr)\n    library(INLA)\n    library(sp)\n    library(raster)\n    library(rgeos)\nThe INLA package is crucial here. We will be using INLA to perform Bayesian Kriging. In non-technical terms, Bayesian kriging lets us fill in the gaps between observations of oyster counts by interpolating from nearby points.\n\n\nBefore we begin, we need to load in the data, and transform it into a spatial data frame. In particular, it is important to transform the lon-lats into UTM coordinates. UTM coordinates allow us to measure distances between sample sites in metres:\n    dat &lt;- read_csv('Oysters_merged.csv')\n    n &lt;- nrow(dat)\n    utmproj &lt;- \"+proj=utm +zone=10 +north +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n    spdat &lt;- dat\n    coordinates(spdat) &lt;- ~ GPSLongitude + GPSLatitude\n    proj4string(spdat) &lt;- \"+init=epsg:4326\"\n    spdf &lt;- spTransform(spdat, CRS(utmproj))\n    plot(spdf)\n\nYou should get a plot like that pictured just of the sample sites.\n\n\n\nNow we can start building the specialised data-structures we need for the INLA package. First up, we have to build what’s called a mesh. This is a Delaunay triangulation built around our data points. It accounts for the non-regular spatial structuring of the sampling (I wandered around on the rocky shore throwing the quadrat at intervals).\nBelow we provide two parameters to max.edge, this will enable us to buffer the edges to obtain a slightly larger spatial domain. Buffering is important because the INLA approximation will be inaccurate at the edges of the spatial domain.\nmax.edge.length &lt;- c(25, 40)\nloc1 &lt;- as.matrix(coordinates(spdf))\nmesh &lt;- inla.mesh.2d(loc=loc1, max.edge = max.edge.length, offset=1, cutoff = 5)\n\nplot(mesh)\nplot(spdf, add = T, col = 'red')\n\nYou can play with the edge length, offset and cutoff parameters to vary how the triangulations turns out.\nYou can get more guidance at the INLA page and in this tutorial. This tutorial also has some very excellent advice about how to choose boundaries and triangle sizes.\nAn important step is to check that our maximum edge lengths are less than the estimated range (otherwise it is pointless including a spatial effect!). We will check the range size below, once we have fitted the model.\nYou might like to think of a-prior reasons for modelling a certain range, for instance, what is a reasonable spatial scale for ‘clumping’ of oyster patches?\n\n\n\nNow we need to implement a few more steps to build up an appropriate ‘stack’ of data for INLA. Note that I have transformed oyster count to presence - absence. We will build a binomial model and just predict whether any given quadrat had oysters or not.\nA.data &lt;- inla.spde.make.A(mesh, loc1)\nspde &lt;- inla.spde2.matern(mesh, alpha = 2)\nspdf$presabs &lt;- ifelse(spdf$oysters_live&gt;0, 1, 0)\n\nstk &lt;- inla.stack(data=list(y=spdf$presabs), A=list(A.data, 1),\n                  effects=list(s=1:mesh$n, intercept=rep(1, n)),\n                  remove.unused = FALSE, tag = \"est\")\nNote that here we choose alpha = 2. alpha is a smoothness parameter that must be 0 &lt;= alpha &lt;= 2, higher values will give smoother spatial interpolation. Also note that we have to specify ‘data’ for the intercept (just a vector of 1).\nThe tag = \"est\" argument just gives a name tag to the fitted model. In other applications you might want to join the estimation stack with a prediction stack (just build two stacks, one where the response is NA and then join them by applying inla.stack to both).\n\n\n\nNow, after that considerable amount of prepartion, we get to fitting the model. We use standard R syntax for the formula, with the addition of the f() term to specify a function.\n    formula &lt;- y ~ 0 + intercept + f(s, model=spde)\n\n    mod &lt;- inla(formula, data=inla.stack.data(stk),\n            control.predictor=list(A = inla.stack.A(stk)),\n            family = 'binomial')\nWe can extract summary parameters using summary. Here we are interested in the random effects, it is good practice to call inla.hyperpar before we inspect random effects:\nhyper &lt;- inla.hyperpar(mod)\nsummary(hyper)\n\n\n\nWe should also check that our triangle edge length is less than the estimated spatial range. We can do that by making a call to the model object, asking for the estimated parameters for the random field\nrf &lt;- inla.spde.result(inla = mod, name = \"s\",\nspde = spde, do.transf = TRUE)\n\nplot(rf$marginals.range[[1]], type = \"l\",\n xlab = \"Range (m)\", ylab = \"Density\",\n xlim = c(0, 1000))\nabline(v = max.edge.length[2], col = 'red')\n\nThe red line shows our edge length parameter, which is well less than the model estimate for the spatial range.\n\n\n\nNow we have a fitted model, we can extracted predicted probabilities of oyster presence and map them. First transform the predictions from the logit scale back to probabilities:\n  ypred &lt;- exp(mod$summary.random$s$mean)/(1 + exp(mod$summary.random$s$mean))\nThen we can turn our predictions into a spatial points, that can be be mapped onto a raster for plotting:\nextent &lt;- extent(spdf)\nxdims &lt;- 100; ydims &lt;- 200\nxlim &lt;- c(extent[1], extent[2]); ylim = c(extent[3], extent[4]);\nproj &lt;- inla.mesh.projector(mesh, xlim = xlim, ylim = ylim, dims=c(xdims, ydims))\nfield.proj &lt;- inla.mesh.project(proj, ypred)\n\ndatpred &lt;- data.frame(x = rep(proj$x, ydims), y = rep(proj$y, each = xdims), pred = as.numeric(field.proj))\ncoordinates(datpred) &lt;- ~x + y\n\nproj4string(datpred) &lt;- utmproj\ndat3 &lt;- spTransform(datpred, crs(\"+init=epsg:4326\"))\n\nr &lt;- raster(extent(dat3), ncols = xdims, nrows= ydims, crs = crs(spdat))\nicell &lt;- cellFromXY(r, dat3)\n\nr[icell] &lt;- as.numeric(dat3$pred)\nAnd finally the plot:\ncols &lt;- RColorBrewer::brewer.pal(9, \"Purples\")\nplot(r, col = cols)\npoints(spdf)\nplot(spdat, add = TRUE)\n\nSo it looks like oysters are more common in the southern portion of the survey area.\nBut note that our predictions extent a fair way from the points. We might want to redo these with a more constrained spatial region. In general we should’t trust predictions that are more than ~ 1 spatial range from the edge of our sampled region (in this case the posterior estimate for range was 181m).\nFinally, we could map this onto a satellite image to get a better view of what is going on. I am just using the same leaflet code as before:\nlibrary(leaflet)\nbrks &lt;- seq(0,1, by = 0.1)\nncol &lt;- length(brks)\noystercols &lt;- RColorBrewer::brewer.pal(min(ncol, 9), 'Reds')\npal &lt;- colorBin(oystercols, dat$oysters_live, bins = brks)\n\nx1 &lt;- -124.5997\ny1 &lt;- 49.52848\n\nleaflet() %&gt;%\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\n\nsetView(lng = x1, lat = y1, zoom = 16) %&gt;%\naddRasterImage(r, opacity = 0.8, color = pal) %&gt;%\naddCircleMarkers(lng = dat$GPSLongitude, lat = spdat$GPSLatitude,\nradius = 4) %&gt;%\naddLegend(\"topright\", pal = pal,\nvalues = brks,\ntitle = \"Chance of live oysters &lt;a href = 'https://en.wikipedia.org/wiki/Pacific_oyster' target = '_blank'&gt; (Crassostrea gigas)&lt;/a&gt;\",\nopacity = 1)"
  },
  {
    "objectID": "posts/2015-09-16-think-like-an-economist/index.html",
    "href": "posts/2015-09-16-think-like-an-economist/index.html",
    "title": "How to think like an economist",
    "section": "",
    "text": "How conservation biologists can think like economists\nIf you are a conservation biologist at some point you will probably find yourself writing about $. Conservation often requires restricting people’s use of natural resources, at least in the short-term. Also funds for conservation are limited. So as we recently wrote about* if we can find ways to make conservation cheaper, we can often protect more species, habitats and ecosystems. Writing about money is difficult, particularly if like me, you were trained in the biological sciences. This is where I have got into trouble. I have had several papers (fairly) rejected when they were reviewed by economists. I went back to the drawing board and have eventually had some of them published. So, to avoid this happening to you, (and to remind myself for next time) here are a few tips for how to think like an economist if you are a biological scientist.\n\n\nTip 1 Consult a professional\nThis goes without saying. The problem is often finding an economist who has time or is willing to help. I have been lucky in a few collaborations. In general though, when I have managed to corner an economist, they have been quite keen to talk, given what they see as a disappointing ignorance of economics amongst biological scientists.\n\n\nTip 2 Get your terms right\nConsult a basic economics text book before you use words like ‘cost’, ‘value’ and ‘profit’. We use these everyday in colloquial language, but economists attach special meanings to these words. I won’t try to define them here in case I get it wrong, but as an example, value might be what you make from harvesting a resource before you factor in costs, whereas profit = value - costs. Also, don’t forget that values can be also be non-monetary, like cultural values. If you use the word ‘value’ in an ecological context, like the value of prey nutrition to predator growth, be sure you make the context clear. Otherwise, you will set alarm bells ringing when it appears you have mis-used an economic term.\n\n\nTip 3 Ask yourself how costs, profits might vary\nAs biologists, we are interested in the causes of variability in nature. For instance, in my models of fish populations I ask how interactions among predators and prey vary over space and time. Sometimes, this variability has implications for the profitability of fisheries, for instance when we overfish prey species to the detriment of their predators. Well you might have done something very clever with modelling predators and prey that an ecologist would be imminently impressed with, but if you article is reviewed by an economist they probably won’t care much.\nThe economist might criticize you for not having the price of the predator fish change with supply of that fish or, ask why fishing industries should care about a benefit 50 years down the road when they only think about next year’s profits (high discount rates). Obviously, economists are interested in the causes of variability in economies. So your work in conservation biology should address the main causes of variability in economies, like how supply and demand affect prices. It is often enough to approach economic dynamics in a relatively simplistic way, as long as you think about it. After all, your main focus is probably still the biological dynamics.\n\n\nTip 4 Optimise something\nEconomists, especially the mathematically minded theoretical type, like to know what the optimal solution is. For instance, what arrangement of protected areas protects the most species for the cheapest cost? Of course, the optimal solution should also account for some important economic dynamics, like changes in the value of resources over time (e.g. discount rates).\n\n\nTip 5 Have a hard look at the economics literature\nWhatever you are doing, you will probably find out that some economist has analysed the same problem 20 years ago (I did when I wrote about the economics of restoring fisheries using marine reserves). Of course, it is hard to search the economics literature when they often use very different terms than biologists. I have been lucky so far in having helpful economists review my papers and point out those references I couldn’t have possibly found myself. Of course, the best thing to do would be to work with an economist that knows that literature from the outset.\n\n\nConservation biologists should think more about the cost of conservation for society\nAs conservation biologists we need to think more about the cost of conservation on society than we currently do. Often doing so results in new insights and findings that can help make conservation science more realistic to real world situations. We might even find ways that conservation can be profitable to society. So go forth, collaborate with economists and good luck!\n\nthe irony of writing about how my articles have been rejected in the face of criticism on their economics, a month after we published an article criticizing others for ignoring economics, is not lost on me. We can all do better."
  },
  {
    "objectID": "posts/2019-08-12-r-course-2020/index.html",
    "href": "posts/2019-08-12-r-course-2020/index.html",
    "title": "R courses February 2020",
    "section": "",
    "text": "Our next series of R courses for professionals and graduate students is now open for registration. The courses include 1-day for absolute beginners, 3 days for intermediate material and 3 days for the advanced class. They are run 11-14 February 2020 at the University of Queensland in St Lucia, Queensland.\nThe courses are taught by a team of experienced R teachers and programmers. Including Prof David Schoeman (Uni of Sunshine Coast), Prof Anthony Richardson, Dr Bill Venables and myself.\nIn the beginner and intermediate classes we cover topics from getting start in R, graphing, data wrangling, mapping, multivariate analysis, linear models and GLMs.\nThe advanced class is run by Bill Venables of MASS package fame and covers topics from advanced modelling to package development and documenting code with markdown.\nLast year I taught a new 1-day course on data wrangling and spatial analysis. We will run this again this year. The notes are freely available online, but always better to come and learn them in person. Also I am going to update them with the latest in spatial mapping and analysis.\nMore information and registration is here"
  },
  {
    "objectID": "posts/2016-02-11-SoTM_day2_synopsis/index.html",
    "href": "posts/2016-02-11-SoTM_day2_synopsis/index.html",
    "title": "Species on the Move - Day 2 synopsis",
    "section": "",
    "text": "Species on the Move Conference: A synopsis of day 2\n11 February 2016, Hobart Tasmania\nHere’s my day two synopsis, covering a few big themes and some of the talks I saw. If you want to know more, check out the book of abstracts. See also my Day 1 synopsis and Michelle Neil’s - storify of the tweets. Here is a summary of some key things I learnt.\nTwo big themes emerging today were\nConservation decisions in a changing climate - We want to know when to translocate species, if we even should translocate them, where we should be putting protected areas to capture climate migrants, how to assist climate-driven migrations, and what to do about other human impacts like harvesting; and\nUnderstanding of range shifts in communities of species - scientists are grappling with how communities of species have changed, how their ecological interactions regulate community change and whether we can predict community change in the future.\n\n\n\nAnother day of extensive media interest directed at Species on the Move. Commendations are deserved for Gretta Pecl and the team for getting the important messages from this conference out there.\n\n\nOne talk stood out for making use cutting edge techniques to address a really big picture question: Julia Blanchard is charting the future of sustainability of human food using (ecological) community modelling approaches. Blanchard’s work is far ahead of the curve. Miguel Arajuo, leading SDMer, posed a rhetorical question - how would you assess the future of biodiversity? He spent the last 16 years of his career answering this question with species distribution models. He ended his talk by suggesting if he were to do it all again he would start with a community/ecosystem approach. Well, Blanchard is making great strides in achieving global ecosystem models that are relevant to a no smaller problem than the future of sustainability of the human species.\nI won’t say more, because the work is unpublished, but check out some of her earlier work on community modelling. This is certainly a space to watch.\nConservation decisions in a changing climate\nHugh Possingham led the keynotes, reminding us to remember the big picture of conservation- it is not just climate change we need to model/predict/adapt to, there is widespread Change in society, economies and the environment.\nTracy Rout’s work was a great introduction to decision science for assisted colonisation of range shifting species. She showed how a transparent decision framework (open access pub) can help us work out what we need to know if we are translocating a species. Her work highlights that decision science can be most useful when we don’t know much about a system, because it helps us identify the uncertainties that matter for conservation.\n\n\n\nA decision tree for deciding whether to move species threatened by climate change from Rout’s paper in PLoS One (open access).\n\n\nJoshua Lawler showed how we can include climate change in cost-effective priorities for reserve placement. I won’t say more because I think this work is yet to be published.\nOn a positive note Timothy Bonebrake predicts that the recovery of an endangered spoonbill will be slowed, but not reversed, by climate change. His talked also weaved in his work on plant conservation under climate change.\nFishing industries will also be affected by range shifts. Eva Plaganyi argued that models can help us ‘re-write history’ so we can learn more from past management mistakes. She showed an application to range in lobster in South Africa, which is impacting on valuable abalone fisheries. Her protege Viv Tulloch, is working on similar models to predict the future recovery (or not) of southern ocean whales.\nGuilia Porra, Aus Fishery Management Authority biologist and foodie of Love at Every Bite fame talked about management approaches the Aus Government is taking for fisheries to address range shifting species.\nUnderstanding and predicting change in whole communities was another major theme\n‘Species’ distribution modelling is moving from species to community focussed approaches. We saw several novel ways of modelling change in whole communities of species.\nThe community thermal index was highlighted in several talks. Rick Stuart-Smith and team have shown that there are thermal biases in fish communities, with most fish species being either associated with temperate or tropical conditions. This has implications for the effects of ocean warming on whole communities, as communities near their thermal limits may suffer large species losses. Mike Burrows is also using the community index to investigate sensitivity of different communities to warming.\n\n\n\n\nSpecies on the Move is helping turn the tide on gender inequity in science by promoting #WomeninSTEM on what is the “International Women in STEM day”: we saw an all female panel during an organised discussion.\n\n\nClimate-driven changes in one ecosystem can affect whole other ecosystems. Megan Saunders showed that responses of tropical seagrass ecosystems to sea level rise depend on whether coral reefs grow upwards, coral reefs shelter seagrass from waves so if corals don’t keep up with sea level rise lagoons may loose seagrass too. A key management policy response is helping coastal ecosystems migrate inland through ‘planned retreat’ of coastal infrastructure. See her video for a fun introduction to this topic\nSome exciting work on modelling community responses is coming out of CSIRO. Bruce Webber introduced a new tool for distinguishing novel environmental ranges from contemporary, to help avoid extrapolation in models. The tool ExDet is available via his blog. Weber also highlighted that the global datasets for temperature and rainfall we use for model fitting have considerable uncertainties. Be careful!\nA keynote from Simon Ferrier, also of CSIRO, highlighted that species composition doesn’t change linearly across environmental gradients, like we often assume it does. Karel Mokany presented meta-community modelling as a solution to this problem, and showed its application to dynamic models of climate-driven shifts in whole plant communities.\n\n\n\nSea-urchin invasion in Tasmania is now a major threatening process - Scott Ling explained.\n\n\nDigging deeper into ecological interactions Miguel Arajuo is finding there are general rules that can explain the assembly of species interactions which can be exploited for distribution modelling of ecological webs.\nA regional example of the importance of species interactions comes from Tasmania, where barrens formed by the climate migration of a sea-urchin have driven a phase-shift from productive kelp habitats to rock, changing the entire reef community: check out the video. Martin Marzloff is modelling the dynamics of this system to understand where the tipping point occurs for canopy loss. Scott Ling talked about how evidence for the regime shift and the implications it has for further climate-driven invasions. His latest work is in progress, but check out his latest multi-system study on regime shifts caused by sea-urchins (thanks to Suzanna Mason @SuzannaCMason for being my eyes on the ground for this talk).\nSome taxonomic gaps in understanding of range shifts are being filled in\nSeveral talks have highlighted the large number of taxonomic and temporal gaps in our sampling of range shifting species. Several talks today worked to fill the gaps. Julian Olden is looking at range shifts in freshwater fish, an under-studied group. I won’t say more because the work is cutting edge and in prep.\nFinally, to wrap up on a bit of fun, Robert Colwell presented in prep work on a new model of niche evolution. I won’t give away his exciting results, except to say he made use of exceptional animations. Several commentators on twitter suggested there should be an award for best animations in a talk and that Colwell would win it."
  },
  {
    "objectID": "posts/2016-02-11-SoTM_day3_synopsis/index.html",
    "href": "posts/2016-02-11-SoTM_day3_synopsis/index.html",
    "title": "Species on the Move - Day 3 synopsis",
    "section": "",
    "text": "Species on the Move Conference: A synopsis of day 3\n12 February 2016, Hobart Tasmania\nYou can read the complete day 1 and day 2 synopses.\nAlso a summary of some key things I learnt.\n\n\n\nThanks to the Species on the Move Sponsors for supporting an exceptional conference. Via Gretta Pecl\n\n\nClimate change impacts are happening now and managers are already responding to them\nTalks today and throughout the conference have highlighted that climate-driven change in ecosystems is already under way. Today we saw a few talks showing that while many scientists debate the suitability of management actions like assisted migration, managers are already making decisions.\nA series of recent fires in southern Australia is driving a state change away from ash forests explained David Bowman. This “Interval squeeze” of more frequent fires combined with slower plant growth will squeeze out some tree species and they can’t just migrate up mountains. Bowman gave an example of two rapid fires were two successional fires caused a state change, because after the second fire there are no seeds for forest recovery and the Eucalyptus species has poor seed dispersal. The third fire locked in a state change. Managers had to make a rapid decision in just a few weeks over what to do. They spread seed during the key season and have probably reversed the state shift.\nTrain managers to respond to short-term climate variability, to improve their skill in decision making for long-term change argued Alistair Hobday. His work on identifying seasonal tuna habitat to help avoid bycatch species, has been used in an increasingly sophisticated way over time as managers become more familiar with the model predictions (note: I have just included one pub, but I recommend contacting Hobday if you want the full story as it covers multiple papers).\nThe prolific tweeter David Watson spoke on linking efforts to improve landscape connectivity for conservation and ecological monitoring two large areas of ecological research that are rarely linked. ‘Connectivity conservation’ is becoming increasingly popular with stakeholders and scientists need to be providing rigorous methods for monitoring their effectiveness.\nMultiple stressors and climate change\nNathalie Pettorelli answered Possingham’s challenge from day 2 that we need to study and model Change, not just Climate Change. She is using satellites to detect human impacts that may interact with climate change to drive biodiversity loss, for instance in desert ecosystems.\nTim Clark is doing exciting experimental work to look at the interactive effect of warming and fishing on coral trout mortality. I won’t say more because I think it is in prep, but a hint, he has his finger on the pulse of fish to learn how they respond to warming and fishing stress.\nThe right models and data are of course key when predicting the effects of multipe-stressors. Lucy Robinson is modelling the distribution of toothfish, to better understand fishing impacts. She found correcting for sampling bias in distribution modelling of toothfish makes a big difference to the predicted effect of temperature on the species.\nA range shift in a lobster on the south-west coast of Africa is driving an ecosystem regime shift on rocky reefs and impacting abalone fisheries, Laura Blamey explained. But an interaction with historical overfishing is also contributing. She used models to ‘rewrite history’ (Plaganyi day 2) and found if predatory fish hadn’t been overfished, this regmie shift may not have happened.\nThe impacts to fisheries are interesting to me, because years ago I naively thought (and others too) the impacts of range shifts on fisheries would not be that significant. If a species moved, fisheries should be mobile enough to follow. That is not how it is playing out. In South Africa Blamey explains, expansion of lobster is creating new opportunities for small scale fisheries but having a large economic impact on the valuable abalone fishery - a different industry that can’t just switch its target species. The transition to a new ecosystem regime can be very painful for society, even if the end point looks ok.\nBlamey’s work illustrates that multiple stressors, including climate change, can drive changes in the functioning of ecosytems, which is the next major theme.\nChanges in the composition and function of ecosystems\nAt a global scale, Jorge Molinos used the velocity of climate change to predict the future redistribution of ocean biodiversity. A strength of using this simple model is that it can encompass many species, here Molinos used the aquamaps data to model almost 13 000 species. Importantly, he took a current marine ecoregionalisation and looked at change in species composition in these regions. Ecoregions won’t be the same in the future. He is working on regional testing of the models, for instance in the work presented on day one by Jan Hiddink.\nIvana Matejickova from Czech Republic is finding a native freshwater fish changes to herbivory at high temperatures. Big implications here - the ecological function of species, which we often assume is constant, changes with warming.\nClimate-driven invasions can lead to the creation of novel ecosystems, the topic of Richard Hobbs’ talk. The invaders can bring new ecological interactions to an ecosystem, changing the way it functions. Research on climate invasions can learn much from invasion ecology. We don’t have a manual yet for managing novel ecosystems and species translocation, but Hobb’s talk highlights that the status-quo isn’t an option.\nThat’s it for today. Check out some lessons we learned by way of a conference summary from me.\n…"
  },
  {
    "objectID": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html",
    "href": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html",
    "title": "Translating data and models into benefits for coastal ecosystems",
    "section": "",
    "text": "If you attended the seminar here’s the links to all the papers and resources. Feel free to email me (c.j.brown@utas.edu.au) if you need access to any pdfs."
  },
  {
    "objectID": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html#understanding-the-impact-of-quantitative-ecology",
    "href": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html#understanding-the-impact-of-quantitative-ecology",
    "title": "Translating data and models into benefits for coastal ecosystems",
    "section": "Understanding the Impact of Quantitative Ecology",
    "text": "Understanding the Impact of Quantitative Ecology\nA fundamental question all conservation scientists should ask themselves is: Is our science making a positive difference to the environment? While quantitative ecology has the potential to influence policy and conservation efforts, it can also be misused or misinterpreted. For instance, the value of coastal wetlands for carbon storage has been so well-promoted that it has led to the proliferation of questionable carbon offset projects."
  },
  {
    "objectID": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html#the-theory-of-change",
    "href": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html#the-theory-of-change",
    "title": "Translating data and models into benefits for coastal ecosystems",
    "section": "The Theory of Change",
    "text": "The Theory of Change\nIts important that we understand the theory of change behind our scientific work. Scientists should think critically about how their research can lead to tangible environmental benefits. Below I’ll share some examples from the Global Wetlands Project (GLOW), which has been running for over seven years, to illustrate different theories of change in action."
  },
  {
    "objectID": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html#three-models-to-make-science-more-relevant-to-conservation",
    "href": "posts/2025-03-28-models-to-impact-coastal-conservation/index.html#three-models-to-make-science-more-relevant-to-conservation",
    "title": "Translating data and models into benefits for coastal ecosystems",
    "section": "Three Models to Make Science More Relevant to Conservation",
    "text": "Three Models to Make Science More Relevant to Conservation\nI proposed three models to enhance the relevance of scientific research to conservation efforts:\n\nCommunication and Advocacy: Effective communication of research findings is crucial. For example, studies on animal responses to coastal wetland restoration have shown significant increases in abundance and diversity compared to degraded sites. By publishing these findings and engaging with media and apps, scientists can motivate restoration efforts.\nOpen Data: Making data freely available allows for its reuse in various contexts. For instance, biodiversity credit markets can use open data to finance conservation projects. This approach aligns with global goals to halt biodiversity loss, double protection efforts, and restore ecosystems.\nCodevelop Science with Policy Makers: Collaborating with policy makers and NGOs ensures that scientific research informs and supports policy decisions. The Global Wetlands Project work has been used by organizations like WWF to develop strategies for mangrove conservation and community engagement.\n\nEngaging with NGOs and Policy Makers\nIts important to engage with NGOs and policy makers to ensure that scientific research translates into actionable policies. By providing data and tools, such as the Blue Forests Data Explorer, scientists can help NGOs like WWF make informed decisions about conservation strategies.\nAdvice for Students and Professors\nFor students, I recommend planning a theory of change for your projects, seeking internships, and valuing applied science. For professors, I advise becoming advisors, looking for policy relevance in their research, and actively engaging with policy makers and the community."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "The terra package looks designed to replace an old favourite raster. It is made by a similar team. The terra documentation states “can do more, is simpler to use, and it is faster.”\nSo should you make the switch to terra? I’ll answer that here.\nTLDR: terra is simpler and faster than raster and will be easy for existing raster users to learn. Compatibility with other packages can be an issue, but conversion back to raster objects is easy. Verdict: make the switch.\nThere are a few important considerations when changing packages:\n\nHow long will it take me to learn the new syntax?\nHow much help is available online?\nIs it faster than what I used to use?\nWill it be compatible with other packages I use?\n\nI will test each in turn.\n\n\nWe’ll use this data from one of my courses.\n\n\n\nFirst, let’s take a look at some basic syntax and compare it with raster\nYou can read in data much the same way, with the command rast():\nlibrary(terra)\nr &lt;- rast(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nplot(r)\n\next(r)\n\n## SpatExtent : 82.5, 181.25, -72.25, -9.75 (xmin, xmax, ymin, ymax)\nNow let’s crop and reproject it:\n#create an extent object\next2 &lt;- ext(r)\n\n#constrain it in x direction\next2[1] &lt;- 120\next2[2] &lt;- 170\n\nr2 &lt;- crop(r, ext2)\n\nr3 &lt;- project(r2, \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\")\n\nplot(r3)\n\nSo much of the syntax is familiar (or identical), if slightly different. It took me about 10 minutes to translate what I know from raster to terra syntax.\nNote there are some important caveats with terra when it comes to cluster computations and saving data see ?terra for more information.\n\n\n\nIt’s early days yet. But the terra package documentation is outstanding, as good as it was for raster. This was one reason raster became so popular.\n?terra provides a very helpful description, a menu of functions and at the very end a translation of function names from raster to terra (many are the same)\nSo users will be once again grateful to Robert Hijmans and the authorship team for the effort tney put into package documentation\nThere are a few courses/ blogs online if you google it and some limited posts on stackexchange sites.\nNo vignette with the package as yet.\nSo the verdict is that the documentation of the package and functions is excellent. Currently, there is limited existing documentation of troubleshooting errors and bugs online. So you might have to ask yourself. But online content will grow as the package becomes more popular.\n\n\n\nI take the author’s word that its faster, but let’s see how much faster:\nlibrary(microbenchmark)\n\nr_raster &lt;- raster::raster(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nrobin_proj &lt;- \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\"\n\ntout &lt;- microbenchmark(\nproject(r, robin_proj),\nraster::projectRaster(r_raster, crs = robin_proj),\ntimes = 10\n)\ntout\n\n## Unit: milliseconds\n##                                               expr     median    \n##                             project(r, robin_proj)      76.3  \n##  raster::projectRaster(r_raster, crs = robin_proj)     529.6\nSo something like 7 times faster for the computationally demanding task of reprojecting a raster.\n\n\n\nThe answer here obviously depends on what packages you want to use. A key one for me is tmap for mapping. This now works with terra.\nBut at the time of first writing this post my version of tmap was out of date and terra wasn’t compatible with tmap. So if you package isn’t compatible, the next question, how onerous is it to convert a terra raster to a raster raster?\nHere’s a demonstration (with my out of date version of tmap)\nlibrary(tmap)\n\nr_raster &lt;- raster::raster(r)\n\ntm_shape(r_raster) +\n  tm_raster()\n\nThe multi-tool function raster() does the job, so I’ll take that for now.\n\n\n\nterra looks set to replace raster. It is faster and just as easy to use as raster. Making the switch to terra isn’t as hard as it may seem, its use will seem very familiar to raster users.\nThere are probably common errors and bugs with particular data types for the R community to find and there isn’t help online for those yet. There will be challenges in compatibility with other packages. But conversion back to raster objects is easy.\nThere are also new features in terra, to handle vector data and manage very large datasets. So plenty more to explore."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html#the-data",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html#the-data",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "We’ll use this data from one of my courses."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html#how-long-will-it-take-to-learn-terras-syntax",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html#how-long-will-it-take-to-learn-terras-syntax",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "First, let’s take a look at some basic syntax and compare it with raster\nYou can read in data much the same way, with the command rast():\nlibrary(terra)\nr &lt;- rast(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nplot(r)\n\next(r)\n\n## SpatExtent : 82.5, 181.25, -72.25, -9.75 (xmin, xmax, ymin, ymax)\nNow let’s crop and reproject it:\n#create an extent object\next2 &lt;- ext(r)\n\n#constrain it in x direction\next2[1] &lt;- 120\next2[2] &lt;- 170\n\nr2 &lt;- crop(r, ext2)\n\nr3 &lt;- project(r2, \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\")\n\nplot(r3)\n\nSo much of the syntax is familiar (or identical), if slightly different. It took me about 10 minutes to translate what I know from raster to terra syntax.\nNote there are some important caveats with terra when it comes to cluster computations and saving data see ?terra for more information."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html#how-much-help-is-available-online",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html#how-much-help-is-available-online",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "It’s early days yet. But the terra package documentation is outstanding, as good as it was for raster. This was one reason raster became so popular.\n?terra provides a very helpful description, a menu of functions and at the very end a translation of function names from raster to terra (many are the same)\nSo users will be once again grateful to Robert Hijmans and the authorship team for the effort tney put into package documentation\nThere are a few courses/ blogs online if you google it and some limited posts on stackexchange sites.\nNo vignette with the package as yet.\nSo the verdict is that the documentation of the package and functions is excellent. Currently, there is limited existing documentation of troubleshooting errors and bugs online. So you might have to ask yourself. But online content will grow as the package becomes more popular."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html#is-terra-faster-than-raster",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html#is-terra-faster-than-raster",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "I take the author’s word that its faster, but let’s see how much faster:\nlibrary(microbenchmark)\n\nr_raster &lt;- raster::raster(\"data-for-course/spatial-data/MeanAVHRRSST.grd\")\nrobin_proj &lt;- \"+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0\"\n\ntout &lt;- microbenchmark(\nproject(r, robin_proj),\nraster::projectRaster(r_raster, crs = robin_proj),\ntimes = 10\n)\ntout\n\n## Unit: milliseconds\n##                                               expr     median    \n##                             project(r, robin_proj)      76.3  \n##  raster::projectRaster(r_raster, crs = robin_proj)     529.6\nSo something like 7 times faster for the computationally demanding task of reprojecting a raster."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html#will-terra-be-compatible-with-other-packages-i-use",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html#will-terra-be-compatible-with-other-packages-i-use",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "The answer here obviously depends on what packages you want to use. A key one for me is tmap for mapping. This now works with terra.\nBut at the time of first writing this post my version of tmap was out of date and terra wasn’t compatible with tmap. So if you package isn’t compatible, the next question, how onerous is it to convert a terra raster to a raster raster?\nHere’s a demonstration (with my out of date version of tmap)\nlibrary(tmap)\n\nr_raster &lt;- raster::raster(r)\n\ntm_shape(r_raster) +\n  tm_raster()\n\nThe multi-tool function raster() does the job, so I’ll take that for now."
  },
  {
    "objectID": "posts/2021-05-28-terra-raster-comparison/index.html#summary",
    "href": "posts/2021-05-28-terra-raster-comparison/index.html#summary",
    "title": "A comparison of terra and raster packages",
    "section": "",
    "text": "terra looks set to replace raster. It is faster and just as easy to use as raster. Making the switch to terra isn’t as hard as it may seem, its use will seem very familiar to raster users.\nThere are probably common errors and bugs with particular data types for the R community to find and there isn’t help online for those yet. There will be challenges in compatibility with other packages. But conversion back to raster objects is easy.\nThere are also new features in terra, to handle vector data and manage very large datasets. So plenty more to explore."
  },
  {
    "objectID": "posts/2021-03-27-common-GAM-problems/index.html",
    "href": "posts/2021-03-27-common-GAM-problems/index.html",
    "title": "How to solve common problems with GAMs",
    "section": "",
    "text": "Here I’ll provide solutions to some common problems I run into when fitting GAMs with mgcv and ways to solve them. The four problems are:\n\nCategorical variables should be input to gam() as factors\nAdjust the degrees of freedom parameter to avoid an error involving the maximum degrees of freedom\nUse the REML method for smaller datasets to avoid overfitting\nHow to deal with crazy extrapolations of a response when there are large gaps in measurements of a covariate\n\n\n\nCategorical variables can cause errors if they are not specified in dataframes as character vectors, and not as factors. But mgcv is inconsistent about whether you need to use factors or not. It is always safest to declare categorical variables as factors if using mgcv.\nOddly character vectors are fine if the categorical variable is included as an additive term like:\nfit1 &lt;- gam(y ~ s(x) + group)\nBut if you want the spline to vary by groups like this:\nfit1 &lt;- gam(y ~ group + s(x, by = group))\nYou will get this obscure error message:\nError in smoothCon(split$smooth.spec[[i]], data, knots, absorb.cons,\n    scale.penalty = scale.penalty, : Can't find by variable\nThe solution is as simple as changing the class of your categorical variable before using the GAM: dat$group &lt;- factor(dat$group).\nThe new version of R (&gt;4.0) defaults to reading in external dataframes with stringsAsFactors == FALSE meaning it reads categorical variables in as characters, not factors. Factors can be more annoying than characters for many other reasons, so watch out for this error with your GAM code.\n\n\n\nThe default settings for GAM smooths is to try and estimate the degrees of freedom (which controls the ‘wiggliness’) from the data. But this routine can fail if you many more replicates than levels in the smooth. Consider this data:\n\nWe have ten replicates for each of 5 levels of x. Data like this is common in experimental settings.\nIf I try fit a GAM with default settings I get an error:\nfit1 &lt;- gam(y ~ s(x), data = dat)\n\n## Error in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots):\nA term has fewer unique covariate combinations than specified maximum\ndegrees of freedom\nMeaning the GAM has failed to construct a sensible smooth term for this data. You can limit the maximum df of the smooth using the k parameter:\nfit1 &lt;- gam(y ~ s(x, k = 3), data = dat)\nNow my GAM fits fine and looks like this:\n\nIf you want to force the GAM to fit a smooth a specific degrees of freedom, then use fx = TRUE:\nfit1 &lt;- gam(y ~ s(x, k = 4, fx = TRUE), data = dat)\nThe above code forces the GAM to fit with exactly k-1 = 2 an effective DF (EDF in the gam summary).\n\n\n\nThe default method for fitting in GAM tends to overfit smaller datasets. Overfitting tends to look like spline fits that are too wiggly.\nHow you define ‘small’ depends on variation and effect sizes in your data WRT the model, but generally sample sizes smaller than 100s to 1000s of samples ‘small’ in this context.\nThis overfitting is well known and is described in Simon Wood’s classic book on GAMs and mgcv.\nHere’s an example. Say you were expecting a smooth, but non-linear, fit to your data. You use a GAM, but you get a result like this:\n\nThose bumps in the curve cannot be justified by any a-priori knowledge you have about the system you are studying. So you’d like a smoother fit.\nThe default method uses generalized cross validation to fit the smooth. The restricted maximum likelihood method is more robust for small sample sizes. So generally in ecological datasets you’ll want to specify the “REML” method like this:\nfit1 &lt;- gam(y~ s(x), method = \"REML\", data = dat)\nNow our smooth looks like a more reasonable linear slope:\n\nNow your trend reduces to a straight linear trend.\n\n\n\nGaps in an x-covariate, such as gaps in a time-series, can be a problem to deal with if you are trying to interpolate to those gaps.\nQuoting from my last blog about interpolation with GAMs:\nThere is no straightforward answer to solving this problem. The answer depends on what you know about the system from prior work. Ideally you’d collect more data for the intermediate x values, but if that is not possible here are some options:\n\nKeep the result above if you have reason to believe there truly is a hump shaped relationship between x and y. If you that x causes y and have a theory for that relationship then the extrapolation of y values might be reasonable. This theory would be based on prior work on the two variables.\nKeep the fit above but refrain from making predictions in the gap.\nTry a different basis spline for the smooth. See ?smooth.terms for options\nModify the smoother to have a first order, not second order, penalty. See my last blog for an explanation. You’ll get a fit like this:\n\n\n\n\n\nHere is a detailed explanation.\nFirst generate some data. We’ll simulate some data y that is related to x via a polynomial (hump shaped) function. Then there will be three groups in variable grp. Each group has a different mean, which will just offset the three groups on the y-axis from each other. See plot below:\nlibrary(ggplot2)\nlibrary(mgcv)\n\nn &lt;- 99\ndat &lt;- data.frame(x = runif(n),\n                  grp = rep(letters[1:3], each = n/3),\n                  cat_effect = rep(c(0, 1, -1), each = n/3))\n\ndat$y &lt;- with(dat, 7*x -8*x^2 + cat_effect + rnorm(n, sd = 0.1))\n\nggplot(dat) +\n  aes(x = x, y = y, color = grp)  +\n  geom_point() +\n  theme_classic()\n First we’ll fit a model that fits a spline to x and estimates a constant additive term for each group. This should provide a good fit to the original data, because it is very close to the model we used to generate the data. The splines should be able to approximate the polynomial and the categorical will allow for each group’s curve to be offset.\nfit1 &lt;- gam(y ~s(x) + grp, data = dat)\nThis fits fine, I like to use visreg to view GAM results. This plot will show x on the x-axis, fits on the y-axis and have one panel for each group.\nlibrary(visreg)\nvisreg(fit1, xvar = \"x\",\n       by = \"grp\", data = dat,\n       method = \"REML\")\n\nNow say we want to compare this to a model where we allow splines to vary for each group. This model looks like this:\nfit2 &lt;- gam(y ~s(x, by = grp) + grp, data = dat)\n\n## Error in smoothCon(split$smooth.spec[[i]], data, knots, absorb.cons,\n    scale.penalty = scale.penalty, : Can't find by variable\nWhich gives us an error. So simply convert the group to a factor (here I declare a new variable) and use that:\ndat$grp2 &lt;- factor(dat$grp)\nfit2 &lt;- gam(y ~s(x, by = grp2) + grp2, data = dat)\nvisreg(fit2, xvar = \"x\",\n       by = \"grp\", data = dat,\n       method = \"REML\")\n\nNow it works fine."
  },
  {
    "objectID": "posts/2021-03-27-common-GAM-problems/index.html#categorical-variables-should-be-factors",
    "href": "posts/2021-03-27-common-GAM-problems/index.html#categorical-variables-should-be-factors",
    "title": "How to solve common problems with GAMs",
    "section": "",
    "text": "Categorical variables can cause errors if they are not specified in dataframes as character vectors, and not as factors. But mgcv is inconsistent about whether you need to use factors or not. It is always safest to declare categorical variables as factors if using mgcv.\nOddly character vectors are fine if the categorical variable is included as an additive term like:\nfit1 &lt;- gam(y ~ s(x) + group)\nBut if you want the spline to vary by groups like this:\nfit1 &lt;- gam(y ~ group + s(x, by = group))\nYou will get this obscure error message:\nError in smoothCon(split$smooth.spec[[i]], data, knots, absorb.cons,\n    scale.penalty = scale.penalty, : Can't find by variable\nThe solution is as simple as changing the class of your categorical variable before using the GAM: dat$group &lt;- factor(dat$group).\nThe new version of R (&gt;4.0) defaults to reading in external dataframes with stringsAsFactors == FALSE meaning it reads categorical variables in as characters, not factors. Factors can be more annoying than characters for many other reasons, so watch out for this error with your GAM code."
  },
  {
    "objectID": "posts/2021-03-27-common-GAM-problems/index.html#insufficient-degrees-of-freedom",
    "href": "posts/2021-03-27-common-GAM-problems/index.html#insufficient-degrees-of-freedom",
    "title": "How to solve common problems with GAMs",
    "section": "",
    "text": "The default settings for GAM smooths is to try and estimate the degrees of freedom (which controls the ‘wiggliness’) from the data. But this routine can fail if you many more replicates than levels in the smooth. Consider this data:\n\nWe have ten replicates for each of 5 levels of x. Data like this is common in experimental settings.\nIf I try fit a GAM with default settings I get an error:\nfit1 &lt;- gam(y ~ s(x), data = dat)\n\n## Error in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots):\nA term has fewer unique covariate combinations than specified maximum\ndegrees of freedom\nMeaning the GAM has failed to construct a sensible smooth term for this data. You can limit the maximum df of the smooth using the k parameter:\nfit1 &lt;- gam(y ~ s(x, k = 3), data = dat)\nNow my GAM fits fine and looks like this:\n\nIf you want to force the GAM to fit a smooth a specific degrees of freedom, then use fx = TRUE:\nfit1 &lt;- gam(y ~ s(x, k = 4, fx = TRUE), data = dat)\nThe above code forces the GAM to fit with exactly k-1 = 2 an effective DF (EDF in the gam summary)."
  },
  {
    "objectID": "posts/2021-03-27-common-GAM-problems/index.html#overfitting",
    "href": "posts/2021-03-27-common-GAM-problems/index.html#overfitting",
    "title": "How to solve common problems with GAMs",
    "section": "",
    "text": "The default method for fitting in GAM tends to overfit smaller datasets. Overfitting tends to look like spline fits that are too wiggly.\nHow you define ‘small’ depends on variation and effect sizes in your data WRT the model, but generally sample sizes smaller than 100s to 1000s of samples ‘small’ in this context.\nThis overfitting is well known and is described in Simon Wood’s classic book on GAMs and mgcv.\nHere’s an example. Say you were expecting a smooth, but non-linear, fit to your data. You use a GAM, but you get a result like this:\n\nThose bumps in the curve cannot be justified by any a-priori knowledge you have about the system you are studying. So you’d like a smoother fit.\nThe default method uses generalized cross validation to fit the smooth. The restricted maximum likelihood method is more robust for small sample sizes. So generally in ecological datasets you’ll want to specify the “REML” method like this:\nfit1 &lt;- gam(y~ s(x), method = \"REML\", data = dat)\nNow our smooth looks like a more reasonable linear slope:\n\nNow your trend reduces to a straight linear trend."
  },
  {
    "objectID": "posts/2021-03-27-common-GAM-problems/index.html#extreme-interoplated-values",
    "href": "posts/2021-03-27-common-GAM-problems/index.html#extreme-interoplated-values",
    "title": "How to solve common problems with GAMs",
    "section": "",
    "text": "Gaps in an x-covariate, such as gaps in a time-series, can be a problem to deal with if you are trying to interpolate to those gaps.\nQuoting from my last blog about interpolation with GAMs:\nThere is no straightforward answer to solving this problem. The answer depends on what you know about the system from prior work. Ideally you’d collect more data for the intermediate x values, but if that is not possible here are some options:\n\nKeep the result above if you have reason to believe there truly is a hump shaped relationship between x and y. If you that x causes y and have a theory for that relationship then the extrapolation of y values might be reasonable. This theory would be based on prior work on the two variables.\nKeep the fit above but refrain from making predictions in the gap.\nTry a different basis spline for the smooth. See ?smooth.terms for options\nModify the smoother to have a first order, not second order, penalty. See my last blog for an explanation. You’ll get a fit like this:"
  },
  {
    "objectID": "posts/2021-03-27-common-GAM-problems/index.html#detailed-example-of-fitting-gam-with-categorical-variable",
    "href": "posts/2021-03-27-common-GAM-problems/index.html#detailed-example-of-fitting-gam-with-categorical-variable",
    "title": "How to solve common problems with GAMs",
    "section": "",
    "text": "Here is a detailed explanation.\nFirst generate some data. We’ll simulate some data y that is related to x via a polynomial (hump shaped) function. Then there will be three groups in variable grp. Each group has a different mean, which will just offset the three groups on the y-axis from each other. See plot below:\nlibrary(ggplot2)\nlibrary(mgcv)\n\nn &lt;- 99\ndat &lt;- data.frame(x = runif(n),\n                  grp = rep(letters[1:3], each = n/3),\n                  cat_effect = rep(c(0, 1, -1), each = n/3))\n\ndat$y &lt;- with(dat, 7*x -8*x^2 + cat_effect + rnorm(n, sd = 0.1))\n\nggplot(dat) +\n  aes(x = x, y = y, color = grp)  +\n  geom_point() +\n  theme_classic()\n First we’ll fit a model that fits a spline to x and estimates a constant additive term for each group. This should provide a good fit to the original data, because it is very close to the model we used to generate the data. The splines should be able to approximate the polynomial and the categorical will allow for each group’s curve to be offset.\nfit1 &lt;- gam(y ~s(x) + grp, data = dat)\nThis fits fine, I like to use visreg to view GAM results. This plot will show x on the x-axis, fits on the y-axis and have one panel for each group.\nlibrary(visreg)\nvisreg(fit1, xvar = \"x\",\n       by = \"grp\", data = dat,\n       method = \"REML\")\n\nNow say we want to compare this to a model where we allow splines to vary for each group. This model looks like this:\nfit2 &lt;- gam(y ~s(x, by = grp) + grp, data = dat)\n\n## Error in smoothCon(split$smooth.spec[[i]], data, knots, absorb.cons,\n    scale.penalty = scale.penalty, : Can't find by variable\nWhich gives us an error. So simply convert the group to a factor (here I declare a new variable) and use that:\ndat$grp2 &lt;- factor(dat$grp)\nfit2 &lt;- gam(y ~s(x, by = grp2) + grp2, data = dat)\nvisreg(fit2, xvar = \"x\",\n       by = \"grp\", data = dat,\n       method = \"REML\")\n\nNow it works fine."
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "",
    "text": "Several AI model providers have integrated web search capabilities into their large language models. I was attempting to use these features via the R ellmer package. However, changes to LLMs are so frequent that ellmer isn’t keeping up.\nI did get ellmer to run Perplexity’s Sonar web search model, however it didn’t provide me with the references, which are essential.\nIf you haven’t seen these AI generated reports, check out the example at the end. Its a useful way to get a quick literature summary or create bespoke R tutorials.\nHere I’m just announcing a couple of simple R scripts that you can use to do web search powered AI reports with the Openrouter service.\nI just created two functions (with the help of copilot of course), one that makes a call to the Openrouter API to send a question to a model. The second function processes the output (which is in JSON format) to make a nice qmd, with the references hyper-linked (be sure to check the url before clicking them, who knows what the AI will turn up!). From there you can render the qmd to get a pdf/word/html report.\nThe functions are easy to use. First download or copy these functions from my github.\nNote never trust any code from someone else that sends requests to LLMs! It could potentially include harmful prompts. I recommend reading all code that is sending prompts to LLMs just to be sure you know what it does."
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#using-the-code-to-do-a-search-and-create-a-report",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#using-the-code-to-do-a-search-and-create-a-report",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "Using the code to do a search and create a report",
    "text": "Using the code to do a search and create a report\nOnce you have my two functions, you will need to set-up your Openrouter API key and save the key somewhere (e.g. you can use usethis::edit_r_environ() and save it in there like OPENROUTER_API_KEY=“MY-KEY-HERE”)\nHere’s an example of using the function:\nlibrary(httr)\nlibrary(jsonlite)\n\nsource(\"perplexity-search-functions.R\")\n\nopenrouter_api_key &lt;- Sys.getenv(\"OPENROUTER_API_KEY\")\n\nuser_message &lt;- \"I want to learn how to use the NIMBLE package to fit autoregressive time-series models\"\n\nsystem_message &lt;- \"You are a helpful AI agent who creates statistical analysis tutorials in R. \n        Rules: \n        1. Include text and examples of code in your responses. \n        2. Produce reports that are less than 10000 words.\"\n\n#Send response to openrouter \nresponse &lt;- call_openrouter_api(\n  openrouter_api_key,\n  model = \"perplexity/sonar-deep-research\",\n  system_message = system_message,\n  user_message,\n  search_context_size = \"medium\"\n  #Options \"low\"  \"medium\", \"high\"\n)\n\n#Save the response as a qmd\nsave_response_as_qmd(response, \"results/AR-models-in-NIMBLE.qmd\")"
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#inputs-for-the-llm",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#inputs-for-the-llm",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "Inputs for the LLM",
    "text": "Inputs for the LLM\nThe user message is your prompt for searching. The system message sets the scope of how the report is created. Note that anything for the web search goes in the user message, not the system message see the Perplexity guidelines for more info on prompting advice, its different than for regular LLMs.\nFor example, another idea for a system prompt could be:\nsystem_message &lt;- \"You are a helpful AI agent who creates summary reports of the scientific literature. \n        Rules: \n        1. Produce reports that are less than 2000 words.\n        2. Include a Summary section that summarizes key research trends. \"\n\nuser_message &lt;- \"What are the impacts of climate warming on fish physiology documented in the peer-reviewed academic literature\"\n\nsearch_context_size is meant to control how much effort it puts in, its hard to tell if that impacts results or not, see openrouter’s docs for more information.."
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#model-choices",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#model-choices",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "Model choices",
    "text": "Model choices\nOther models to try are:\n\nperplexity/sonar for a simpler, cheaper searches, including citations.\nperplexity/sonar-deep-research For deeper, more expensive searches with citations and reasoning.\nopenai/o4-mini Is another option, but doesn’t return citations.\n\nExplore the openrouter site for other web search enabled LLMs."
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#customization",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#customization",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "Customization",
    "text": "Customization\nThe call_openrouter_api.R function is a template for customizing, not a comprehensive framework for using the openrouter API. Its actually very easy to connect to LLMs from R (even though most examples online are in python or typscript). Here’s a basic template:\nlibrary(httr)\nlibrary(jsonlite)\nresponse &lt;- POST(\n    url = \"https://openrouter.ai/api/v1/chat/completions\",\n    add_headers(\n      \"Content-Type\" = \"application/json\",\n      \"Authorization\" = paste(\"Bearer\", openrouter_api_key)\n    ),\n    body = toJSON(list(\n      model = model,\n      messages = list(\n        list(\n          role = \"system\",\n          content = system_message\n        ),  \n        list(\n          role = \"user\",\n          content = user_message\n        )\n      )\n    ), auto_unbox = TRUE),\n    encode = \"raw\"\n  )\n\nThe trick then is formatting the output, which is what my other function does (focussing on perplexities models)."
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#report-example",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#report-example",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "Report example",
    "text": "Report example\nBelow is an example, I asked used the system prompt above and asked about fitting surplus production models (for fisheries science) with the NIMBLE package. Note one minor bug with my function converting qmd references to hyperlinks, it potential does R sub-scripts as well. Something to work on later. Here’s the AI report:\nYou can fit a surplus production model (SPM) with the NIMBLE package in R by writing the model in BUGS language, which NIMBLE extends, specifying your likelihood (e.g., production dynamics with catch data) and priors for the parameters, and then using NIMBLE’s MCMC capabilities to estimate the posterior distributions of parameters. Here is a stepwise approach with example code snippets:\n\nSpecify the surplus production model in BUGS code:\nThis typically models biomass ( B_t ) evolving over time according to a logistic growth or Gompertz model minus catches ( C_t ). For example, the Schaefer model can be expressed as:\n[ B_{t+1} = B_t + r B_t (1 - ) - C_t + _t ]\nwhere (r) is intrinsic growth rate, (K) is carrying capacity, and (_t) models process noise.\n\nlibrary(nimble)\n\n# Define the model in BUGS syntax\nspm_code &lt;- nimbleCode({\n  # Priors for parameters\n  r ~ dunif(0, 2)           # growth rate\n  K ~ dunif(maxCatch, 10 * maxCatch)  # carrying capacity (expand as appropriate)\n  sigma ~ dunif(0, 5)       # process noise SD\n\n  B&lt;a href=\"https://oliviergimenez.github.io/banana-book/intronimble.html\" target=\"_blank\"&gt;[1]&lt;/a&gt; ~ dunif(0, K)       # initial biomass\n  \n  for(t in 1:(nYears - 1)) {\n    mu[t] &lt;- B[t] + r * B[t] * (1 - B[t]/K) - catch[t]\n    B[t + 1] ~ dnorm(mu[t], sd = sigma)\n  }\n})\n\nPrepare the data and constants (e.g., catch time series, number of years):\n\ndata &lt;- list(catch = catch_vector)  # your observed catches\nconstants &lt;- list(nYears = length(catch_vector), maxCatch = max(catch_vector))\ninits &lt;- list(r = 0.5, K = max(catch_vector)*5, sigma = 0.1, B = rep(NA, length(catch_vector)))\n\nBuild, compile, and run the model with NIMBLE:\n\n# Create model object\nspm_model &lt;- nimbleModel(spm_code, data = data, inits = inits, constants = constants)\n\n# Compile model\nc_spm_model &lt;- compileNimble(spm_model)\n\n# Configure MCMC; default samplers are usually fine\nmcmc_conf &lt;- configureMCMC(spm_model)\nmcmc &lt;- buildMCMC(mcmc_conf)\nc_mcmc &lt;- compileNimble(mcmc, project = spm_model)\n\n# Run MCMC sampling\nsamples &lt;- runMCMC(c_mcmc, niter = 10000, nburnin = 2000, thin = 5)\n\n# Check results\nsummary(samples)\nThis workflow follows NIMBLE’s general approach of specifying hierarchical Bayesian models in an extended BUGS language, compiling fast C++ versions of the model and samplers, and running MCMC algorithms[1][2][4][7]. The key is coding the surplus production dynamics explicitly as state-transition equations with observation or process error as needed.\nFor clarity, here is a minimal example with dummy catch data:\nlibrary(nimble)\n\ncatch_vector &lt;- c(100, 120, 90, 80, 110) # example catches\n\nspm_code &lt;- nimbleCode({\n  r ~ dunif(0, 2)\n  K ~ dunif(maxCatch, 10 * maxCatch)\n  sigma ~ dunif(0, 5)\n\n  B&lt;a href=\"https://oliviergimenez.github.io/banana-book/intronimble.html\" target=\"_blank\"&gt;[1]&lt;/a&gt; ~ dunif(0, K)\n  for(t in 1:(nYears - 1)) {\n    mu[t] &lt;- B[t] + r * B[t] * (1 - B[t]/K) - catch[t]\n    B[t+1] ~ dnorm(mu[t], sd = sigma)\n  }\n})\n\ndata &lt;- list(catch = catch_vector)\nconstants &lt;- list(nYears = length(catch_vector), maxCatch = max(catch_vector))\ninits &lt;- list(r = 0.5, K = max(catch_vector)*5, sigma = 0.1, B = rep(NA, length(catch_vector)))\n\nspm_model &lt;- nimbleModel(spm_code, data = data, inits = inits, constants = constants)\nc_spm_model &lt;- compileNimble(spm_model)\n\nmcmc_conf &lt;- configureMCMC(spm_model)\nmcmc &lt;- buildMCMC(mcmc_conf)\nc_mcmc &lt;- compileNimble(mcmc, project = spm_model)\n\nsamples &lt;- runMCMC(c_mcmc, niter = 5000, nburnin = 1000, thin = 2)\nprint(summary(samples))\nThis example models biomass dynamics probabilistically and fits parameters (r), (K), and noise level () given known catches using Bayesian inference.\nIf you want to implement maximum likelihood fitting instead, NIMBLE lets you compile the model and evaluate likelihoods for optimization, as demonstrated in tutorials[1].\nIn summary: - Write your surplus production model in NIMBLE’s BUGS language, defining biomass dynamics, catch, and priors for parameters\n- Compile and run MCMC sampling in NIMBLE to fit the model\n- Extract posterior samples for growth rate, carrying capacity, biomass, and uncertainty\n- Optionally, use likelihood-based methods by compiling the model and defining your own optimizer function in R[1][7].\nThis approach leverages NIMBLE’s power for hierarchical, Bayesian time-series stock assessment models including surplus production models."
  },
  {
    "objectID": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#references",
    "href": "posts/2025-09-02-AI-reports-with-openrouter-and-r/index.html#references",
    "title": "Create reports and tutorials with generative AI from R",
    "section": "References",
    "text": "References\n\noliviergimenez.github.io/banana-book/intronimble.html\nwww.rdocumentation.org/packages/nimble/versions/1.3.0/topics/nimble-package\nwww.youtube.com/watch\ngithub.com/nimble-dev/nimble\nr-nimble.org/documentation-2\ncran.r-project.org/web/packages/nimbleCarbon/vignettes/nimble_carbon_vignette.html\nr-nimble.org/\nwww.rdocumentation.org/packages/nimble/versions/1.3.0"
  },
  {
    "objectID": "posts/2017-06-13-enhancing-research-network-social-media/index.html",
    "href": "posts/2017-06-13-enhancing-research-network-social-media/index.html",
    "title": "Enhancing your research network with social media",
    "section": "",
    "text": "What defines a (successful) scientist? Their collaborations, their Phd supervisor, their students, who they cite, conferences they attend and citations of their publications all indicate what field they belong to and how successfull they are as a researcher.\nIn short, scientists are defined by their network of influence.\nThis is a brief blog on using social media to enhance your research network. It has been prepared as a supplement for my talk at the Australian Rivers Institute (13/6/2017), but should also stand alone.\n\n\nA great way to learn how to blog is to start copying the style of other people who are good at it.\nA good way to find out who is good at blogging (or social media in general) is to look at the Altmetrics for your favourite papers.\nAltmetrics will give you a influence score for a paper that includes among other things the number of blogs it could detect. You can get a handy link for your browser to quickly view Altmetrics here.\nIf you have your own blog, make sure you get it registered with Altmetrics. Also make sure you include a link or DOI for your paper in the blog.\nThere are many ways to write a blog, but to start I recommend writing about one of your research articles. You want to hit the points below, making sure that the significance of the work, the problem you addressed and the solutions are given priority. Methods and detailed lit review are less important (or don’t even include them!):\nA reasonable blog structure:\n\nThe significance of the work to a generalist audience.\nWhat you did that was new to address the problem\nWhat you found\nWhat are the implications of your finding for science or humanity\nAny actions people should take as a result of your research\n\nAnother key style point is to write for a generalist audience (e.g. avoid jargon), even if you are aiming it at specialists. Making it easier for specialists to read will mean they are more likely to read it too.\n\n\n\nNaturally you are already on my blog, so check out some of my blogs about research papers e.g. here.\nI also recommend you check out the Australian Rivers Institute Catchment to Coast blog.\nIn my talk I will give the example of Tyson Martin’s post as a recent one that emphasises signficance in an easily accessible way.\nI love The Conversation. Their blogs are written for a generalist audience - their style guide encourages readibility for highschool students, but even discpline experts can learn a lot from this blog.\nIn fact, I reckon one of their main followings is probably scientists. So you will reach your colleagues as well as a broader audience if you write for The Conversation.\nThe Conversation does blogs about research papers, but also emphasises topical issues too.\nFor instance, a couple examples I give in my seminar are The 2017 budget has axed research to help Australia adapt to climate change and Could ‘nitrogen trading’ help the Great Barrier Reef?.\nIf you want to learn how to blog about opinons or interesting facts as an individual, try these two entertaining ecologically orientated blogs:\nScientist Sees Squirrel\nDynamic Ecology\nA few other professional blogs for places to get ideas are: Cool Green Science and Hakai Magazine.\nHakai Magazine, Cool Green Science and The Conversation all accept ‘pitches’ for new articles from scientists. So if you have a great paper coming out, or have an informed opinion on a topical event, write them and ask to blog about it.\n\n\n\nTo get started, here are a couple of good examples of tweets about research papers: 1. This one by Simon Linke 2. This one by Gretta Pecl\nSome things to notice are:\n\nThey link to their research papers\nBoth have engaging images, which will increase the number of people interacting with your tweet\nBoth have short catchy statements\nSimon Linke’s is notable because he has just started on Twitter and has less than 100 followers, but the tweet has been pretty successful (18 retweets when I last checked) for a research paper tweet.\nBoth share promote their own science, but are not boastful about it and people like that.\nGretta’s post also ties into a hashtag (#BillMeetScienceTwitter) that was trending at the time.\n\nThere are a number of different Twitter strategies you can take, or mix and match. Here are some examples:\n\nSharing personal views e.g. here\nNews and topic expert e.g. here\nScience commmunicator e.g. here\nThe conversationalist e.g. here\nNew publications (often anonymous)e.g. here\nTweet and blog combo That’s what I do!\n\n\n\n\nIn addition to thinking about how social media can benefit your research career, its worth thinking about potential costs.\nWhile it can be time-efficient and cost-effective (i.e. no plane flights required to talk with overseas colleagues) way to communicate, it can also be a time sink. Personally, I only leave myself a limited amount of time for social media. e.g. I tweet on the bus and I usually set myself an hour max to finish a blog.\nI think writing fast blogs can actually help make you a more efficient scientific writer.\nAnother potential risk in expanding your communication network is that some people might not like what you have to say. While some people thrive of confrontation, other people (like me!) feel uncomfortable about it.\nCommonsense prevails here. Be courteous, acknowledge other people’s views and keep your conversations professional.\nAn extreme case of confrontation is getting trolled. That is someone who deliberately sets out to demean your views and offend you. Good general advice is to ignore trolls, they are looking for a response. Most social media platforms offer a blocking option too, so you might want to use that.\nI will say that people’s experiences of social media will vary depending on context. In particular, I know a lot of female colleagues are concerned about sexism. Unfortunately, if you are a women and become a well known communicator you will almost certainly recieve sexist comments.\nI don’t want to put women off of using social media. Instead, I hope we can encourage a more forthright conversation about appropriate behaviour on social media. I also want to highlight this issue to men, who may not be aware of it. Guys, be supportive of your female colleagues engaging with public communication. You can help by calling people out on sexist comments (here’s one recent example).\nSocial media can also be a force for good when it comes to sexism. For instance, two women who submitted an article to PLoS One recieved a review saying the paper would be better with a male coauthor.\nTweets about their experience resulted in public outrage from the scientific community. Ultimately the handing editor was asked to step down from PLoS and PLoS has reviewed its reviewing practices.\nSo social media can help level the playing field and improve equity in scientific careers.\n\n\n\nHope you found this helpful. Have fun out there on the network.\nIn short, successful scientists are good networkers. Social media helps us enhance our networks."
  },
  {
    "objectID": "posts/2017-06-13-enhancing-research-network-social-media/index.html#enhancing-your-research-network-with-social-media",
    "href": "posts/2017-06-13-enhancing-research-network-social-media/index.html#enhancing-your-research-network-with-social-media",
    "title": "Enhancing your research network with social media",
    "section": "",
    "text": "What defines a (successful) scientist? Their collaborations, their Phd supervisor, their students, who they cite, conferences they attend and citations of their publications all indicate what field they belong to and how successfull they are as a researcher.\nIn short, scientists are defined by their network of influence.\nThis is a brief blog on using social media to enhance your research network. It has been prepared as a supplement for my talk at the Australian Rivers Institute (13/6/2017), but should also stand alone.\n\n\nA great way to learn how to blog is to start copying the style of other people who are good at it.\nA good way to find out who is good at blogging (or social media in general) is to look at the Altmetrics for your favourite papers.\nAltmetrics will give you a influence score for a paper that includes among other things the number of blogs it could detect. You can get a handy link for your browser to quickly view Altmetrics here.\nIf you have your own blog, make sure you get it registered with Altmetrics. Also make sure you include a link or DOI for your paper in the blog.\nThere are many ways to write a blog, but to start I recommend writing about one of your research articles. You want to hit the points below, making sure that the significance of the work, the problem you addressed and the solutions are given priority. Methods and detailed lit review are less important (or don’t even include them!):\nA reasonable blog structure:\n\nThe significance of the work to a generalist audience.\nWhat you did that was new to address the problem\nWhat you found\nWhat are the implications of your finding for science or humanity\nAny actions people should take as a result of your research\n\nAnother key style point is to write for a generalist audience (e.g. avoid jargon), even if you are aiming it at specialists. Making it easier for specialists to read will mean they are more likely to read it too.\n\n\n\nNaturally you are already on my blog, so check out some of my blogs about research papers e.g. here.\nI also recommend you check out the Australian Rivers Institute Catchment to Coast blog.\nIn my talk I will give the example of Tyson Martin’s post as a recent one that emphasises signficance in an easily accessible way.\nI love The Conversation. Their blogs are written for a generalist audience - their style guide encourages readibility for highschool students, but even discpline experts can learn a lot from this blog.\nIn fact, I reckon one of their main followings is probably scientists. So you will reach your colleagues as well as a broader audience if you write for The Conversation.\nThe Conversation does blogs about research papers, but also emphasises topical issues too.\nFor instance, a couple examples I give in my seminar are The 2017 budget has axed research to help Australia adapt to climate change and Could ‘nitrogen trading’ help the Great Barrier Reef?.\nIf you want to learn how to blog about opinons or interesting facts as an individual, try these two entertaining ecologically orientated blogs:\nScientist Sees Squirrel\nDynamic Ecology\nA few other professional blogs for places to get ideas are: Cool Green Science and Hakai Magazine.\nHakai Magazine, Cool Green Science and The Conversation all accept ‘pitches’ for new articles from scientists. So if you have a great paper coming out, or have an informed opinion on a topical event, write them and ask to blog about it.\n\n\n\nTo get started, here are a couple of good examples of tweets about research papers: 1. This one by Simon Linke 2. This one by Gretta Pecl\nSome things to notice are:\n\nThey link to their research papers\nBoth have engaging images, which will increase the number of people interacting with your tweet\nBoth have short catchy statements\nSimon Linke’s is notable because he has just started on Twitter and has less than 100 followers, but the tweet has been pretty successful (18 retweets when I last checked) for a research paper tweet.\nBoth share promote their own science, but are not boastful about it and people like that.\nGretta’s post also ties into a hashtag (#BillMeetScienceTwitter) that was trending at the time.\n\nThere are a number of different Twitter strategies you can take, or mix and match. Here are some examples:\n\nSharing personal views e.g. here\nNews and topic expert e.g. here\nScience commmunicator e.g. here\nThe conversationalist e.g. here\nNew publications (often anonymous)e.g. here\nTweet and blog combo That’s what I do!\n\n\n\n\nIn addition to thinking about how social media can benefit your research career, its worth thinking about potential costs.\nWhile it can be time-efficient and cost-effective (i.e. no plane flights required to talk with overseas colleagues) way to communicate, it can also be a time sink. Personally, I only leave myself a limited amount of time for social media. e.g. I tweet on the bus and I usually set myself an hour max to finish a blog.\nI think writing fast blogs can actually help make you a more efficient scientific writer.\nAnother potential risk in expanding your communication network is that some people might not like what you have to say. While some people thrive of confrontation, other people (like me!) feel uncomfortable about it.\nCommonsense prevails here. Be courteous, acknowledge other people’s views and keep your conversations professional.\nAn extreme case of confrontation is getting trolled. That is someone who deliberately sets out to demean your views and offend you. Good general advice is to ignore trolls, they are looking for a response. Most social media platforms offer a blocking option too, so you might want to use that.\nI will say that people’s experiences of social media will vary depending on context. In particular, I know a lot of female colleagues are concerned about sexism. Unfortunately, if you are a women and become a well known communicator you will almost certainly recieve sexist comments.\nI don’t want to put women off of using social media. Instead, I hope we can encourage a more forthright conversation about appropriate behaviour on social media. I also want to highlight this issue to men, who may not be aware of it. Guys, be supportive of your female colleagues engaging with public communication. You can help by calling people out on sexist comments (here’s one recent example).\nSocial media can also be a force for good when it comes to sexism. For instance, two women who submitted an article to PLoS One recieved a review saying the paper would be better with a male coauthor.\nTweets about their experience resulted in public outrage from the scientific community. Ultimately the handing editor was asked to step down from PLoS and PLoS has reviewed its reviewing practices.\nSo social media can help level the playing field and improve equity in scientific careers.\n\n\n\nHope you found this helpful. Have fun out there on the network.\nIn short, successful scientists are good networkers. Social media helps us enhance our networks."
  },
  {
    "objectID": "posts/2021-02-22-forecasting-project-outcomes/index.html",
    "href": "posts/2021-02-22-forecasting-project-outcomes/index.html",
    "title": "How do you forecast whether your next research project will be successful?",
    "section": "",
    "text": "Making reliable forecasts the outcomes of a project is important for project planning. A typical example for a researcher is planning a collaborative new research paper. You have to coordinate a team and resources to pull it off. You will need to estimate how long different tasks will take, so you can coordinate the team. You will also need to forecast the likely outcome.\nYou would also typically make forecasts about the benefits on completion, such as the likely journal or other benefits such as policy decisions it will influence.\nBelow is a set of questions to test how you think about forecasting the outcomes of projects. First take the quiz. Then below is some discussion of how this self-evaluation can help you improve your decision making when planning projects.\nThe test is based on the Foxes and Hedgehogs dichotomy proposed by Philip Tetlock, a psychologist who specialises in how humans make forecasts.\nA nice write up of the framework is in Nate Silver’s book Signal and the Noise."
  },
  {
    "objectID": "posts/2021-02-22-forecasting-project-outcomes/index.html#test",
    "href": "posts/2021-02-22-forecasting-project-outcomes/index.html#test",
    "title": "How do you forecast whether your next research project will be successful?",
    "section": "Test",
    "text": "Test\nThink about a project you planned and have been working on recently, such a research paper you are leading. Now scores yourself on each one of the below questions. Each question is in two parts. Score yourself 1-5, where 1 is a strong yes to the first question in each pair and 5 is a strong yes to the second question in each pair.\nTry to be as honest with yourself as possible, don’t score yourself for what you think you should have done, score yourself for what you did (even better, get someone who knows you to score for you). The evaluation will be more useful that way.\n\nQ1\n\nI sought multiple perspectives and opinions on my project plan\n\nI sought the perspective of one expert or relied on my own expert judgement\n\n\n\nQ2\n\nI sought perspectives from people from different disciplines, with different backgrounds, or different viewpoints\n\nI sought perspectives only from experts in a relevant field\n\n\n\nQ3\n\nOnce the project began, I adapted the plan as new information came to light\n\nI stuck to my plan\n\n\n\nQ4\n\nI acknowledged mistakes in the project planning when something didn’t got to plan\nWhen something didn’t got to plan it was typically the result of bad luck\n\n\n\nQ5\n\nI planned for multiple possible contingencies in project outcomes\n\nI made a detailed plan for a single forecasted outcome.\n\n\n\nQ6\n\nI used data from past project outcomes to forecast the likely outcomes of this project\nI used my expert judgement to forecast the likely outcomes\n\n\n\nQ7\n\nI estimated the chances that different contingencies would eventuate.\nI only considered one outcome or, if I considered multiple outcomes I didn’t compare their chances of success.\n\n\n\nQ8\n\nI used data from past projects to estimate how long this project would take\nI estimated how long the project would take based on my expert judgement"
  },
  {
    "objectID": "posts/2021-02-22-forecasting-project-outcomes/index.html#your-score",
    "href": "posts/2021-02-22-forecasting-project-outcomes/index.html#your-score",
    "title": "How do you forecast whether your next research project will be successful?",
    "section": "Your score",
    "text": "Your score\nNow add up your scores. If you scored less than 20 you are more like a ‘fox’ in your forecasts. If you scored more than 20 you are more like a hedgehog."
  },
  {
    "objectID": "posts/2021-02-22-forecasting-project-outcomes/index.html#fox-vs-hedgehogs-in-project-planning",
    "href": "posts/2021-02-22-forecasting-project-outcomes/index.html#fox-vs-hedgehogs-in-project-planning",
    "title": "How do you forecast whether your next research project will be successful?",
    "section": "Fox vs hedgehogs in project planning",
    "text": "Fox vs hedgehogs in project planning\nFoxes tend to be better at forecasting than hedgehogs. Foxes take into account diverse viewpoints, consider the complexity of the world, hold multiple theories for how the world operates at the same time, use data to inform forecasts, estimate the chances of different outcomes, adapt to changes in outcomes and acknowledge their mistakes.\nHedgehogs based their forecasts on a single theory, have simple theories for how the world works, use theories or their ‘gut’ to forecast outcomes, don’t draw on different perspectives, stick to a single plan or theory and tend to attribute forecasting mistakes to bad luck.\nIf you want to know more read the book or watch the video above.\nIt is not that hard to think more like a fox when you are planning and implementing your next research project. Just scan the questions above and try to action the first in each pair (e.g. seek multiple perspectives on your plan, use data from past projects to estimate how long this one will take).\n\nThe test is not at all scientific, I’m sure it could be designed by a real psychologist to be much less biased."
  },
  {
    "objectID": "posts/2019-03-18-bayesian-timeline/index.html",
    "href": "posts/2019-03-18-bayesian-timeline/index.html",
    "title": "Diary of becoming a Bayesian",
    "section": "",
    "text": "Here’s my timeline of going from frequentist stats to becoming a fully fledged Bayesian eco-statistician.\n2006 Read the Ecological Detective (Hilborn and Walters), the philosophy makes sense, the history of falsification is interesting but, geez, the maths looks hard.\n2008 Started PhD. Peers also interested in Bayes stats. But, Prof gets angry and tells us “ANOVA is all you will ever need”. We rebel and start a study group on that James Clark book with elephants on the front cover.\nGeez, the maths is even harder than I remember.\nNot comfortable with the ‘subjective’ aspect of Bayesian stats (AKA prior information). But its ok to use Bayesian stats for its flexibility, so long as we use ‘uninformed’ priors.\n2009 Read that Mick McCarthy book with the frog on the front cover. Start playing around in WinBUGS. BUGS code breaks the mystery of mixed effects models in R. MM models are just a series of equations, and much more than the obscure magic of R model formulas.\n2014 Actually published a paper on something with statistics in it (first time ever). (Interdependency of tropical marine ecosystems in response to climate change). Used frequentist GLM. Leant heavily on advice in that Zuur, Ieno et al. book that has penguins on the front cover.\n2017 Publish first Bayesian eco-stats paper. Went all out with a bespoke non-linear latent variable GLM in JAGS. Use broad ‘uninformed priors’, because the data should speak for themselves.\nBrian Fry convinces me there is a big problem with the ‘uninformed priors’ used in isotope mixing models. Studies with poor data seem to unwittingly present the priors as their result.\nWinter 2017 Obsessed with the apparent simplicity and speed of the INLA rstats package for Bayesian GLMMs. Frequently taking hot baths in evening to read the latest INLA pre-print. Wife thinks I’m odd.\n(Later on realize INLA is very fast, but not so simple)\nAutumn 2018 Started paper about sharks with regular frequentist GAMs. Finished paper with Bayesian GAMMs, because lead author wants random effects in a time-series model AND “confidence intervals” plotted on the graph. Too hard with GAMs, easy with Bayes.\nPublish paper on how Bayesian isotope mixing models can’t have uninformed priors. It is just mathematically impossible. Now convinced we should always use prior information to develop Bayesian priors.\nObsession with reading INLA in the bath vindicated by publishing in Nature.\nRead about ‘penalized complexity priors’. Now convinced priors for random effects should be ‘informed’ (or regularizing), even if we don’t have prior information.\n2018/2019 Read that red and black MacElreath’s book about rethinking. Agree that using informed priors for fixed effects is actually more conservative than broad priors.\nIn fact, realize now that no prior is truly ‘uninformed’, some are just broader than others. Broad priors can’t be ‘uninformed’, because they are just are informed about a high variance.\n2019 Used INLA and Bayesian stats to do a simple linear regression. Never going back to frequentist.\nConvinced postdoc to use informed priors for his model. Model works much better, results make much more sense.\n2020 Insist PhD students have to use Bayesian stats over frequentist stats too?"
  },
  {
    "objectID": "posts/2025-09-04-making-the-jump-from-phd-to-postdoc copy/index.html",
    "href": "posts/2025-09-04-making-the-jump-from-phd-to-postdoc copy/index.html",
    "title": "Making the jump from PhD to Postdoc",
    "section": "",
    "text": "In our lab meeting today we talked about the transition from a PhD to a postdoc position. I’m sharing some of the advice from the lab and reflections here.\nThanks to Seascapes and CAT labs for sharing their thoughts.\nThis can be a hard transition to make. At the end of your PhD you should have a good level of confidence about your topic. But then a postdoc position will often be on a different research problem, use different methods or be working with a different network of scientists and stakeholders."
  },
  {
    "objectID": "posts/2025-09-04-making-the-jump-from-phd-to-postdoc copy/index.html#tips-to-prepare-for-the-transition",
    "href": "posts/2025-09-04-making-the-jump-from-phd-to-postdoc copy/index.html#tips-to-prepare-for-the-transition",
    "title": "Making the jump from PhD to Postdoc",
    "section": "Tips to prepare for the transition",
    "text": "Tips to prepare for the transition\n\nMetacognition\nMetacognition is self-awareness of your thought processes. For research its particularly key to be self-aware of how you learn.\nUnderstanding how to learn means you can quickly and more smoothly immerse yourself in a new research topic.\nFor PhD supervisors you can point out or ask questions to help students understand their own thought and learning processes. For example, one metacognition trick is to use analogy to solve a new problem.\nAnother skill is understanding how you learn (is it by reading, by talking to people, by doing? ). This can help you come to grips with a new topic if you know how to research it and what is the best way that works for you.\nExposing yourself to a broad range of different types of research (for me this meant attending the biology department seminar which ranged from fisheries topics I was interested in to neuroscience which could still be interesting though not directly relevant to my studies then).\n\n\nPresentation and communication skills\nThese skills are just broadly useful in many careers and easy to translate.\nIts one reason we practice facilitation of meetings in our lab meetings.\nThings to think about in your PhD (or for supervisors) are to get a broad view on different ways of presenting information. Sometimes our PhDs are too focused on academic publications and presentations.\nRead, and look for opportunities to write in other formats such as blogs, media releases, reports and policy briefs.\nFor presentations, take comms training (if its available), do media interviews if you get the chance. Participating in 3 Minute Thesis (3MT) is another great way to learn presentation skills that are highly transferable.\n\n\nLearning who matters and who you report to\nLots of people will give you advice or directions. Some of it is good, some not so good. Understanding of reporting structures in your organisation is important so you can understand your role and what the organisation wants you to achieve.\nIn your postdoc you may have less autonomy in some ways than you had in your PhD. Often postdocs are funded to deliver a certain project outcome (e.g. for industry partner or government).\nThis will mean you need to better differentiate between helpful advisors and project leaders that you need to deliver too.\nAs you progress in your career its also important to learn more about institutional structures. For instance, your supervisor might report to a head of school who reports to a dean who reports to a pro-vice chancellor. You probably didn’t pay too much attention to this stuff in your PhD.\nIt becomes increasingly important to understand as you move up in your career."
  },
  {
    "objectID": "posts/2025-09-04-making-the-jump-from-phd-to-postdoc copy/index.html#making-the-transition",
    "href": "posts/2025-09-04-making-the-jump-from-phd-to-postdoc copy/index.html#making-the-transition",
    "title": "Making the jump from PhD to Postdoc",
    "section": "Making the transition",
    "text": "Making the transition\n\nKnowing how to get help\nFind out how to get help and don’t be afraid to ask. Your new colleagues are usually happy to help.\nIt could be small things like how to navigate the financial reimbursement system. This can save you a lot of time.\nMost universities and large institutions will also provide access to counselling and mental health care services, so take make the most of those if you are finding the new job or move stressful.\n\n\nLearning a new topic\nDon’t underestimate the time it takes to learn a new topic. So give yourself (and talk to your supervisor about having) time to learn the new topic.\nAnd the final tip from the team was to read old papers, from 10, 20, 30 years ago or more. When you are approaching a new topic these papers give you essential background and are often simpler to understand. Its also helpful to understand the historical context of a new research field."
  },
  {
    "objectID": "posts/2025-09-13-projecting-uncertainty-ecosystem-persistence/index.html",
    "href": "posts/2025-09-13-projecting-uncertainty-ecosystem-persistence/index.html",
    "title": "Projecting Uncertainty in Ecosystem Persistence Under Climate Change",
    "section": "",
    "text": "Rising seas and coastal development are pushing mangrove forests beyond their historical limits. Predicting how these critical ecosystems will respond is much harder when environmental change is unprecedented. In a new study, Projecting Uncertainty in Ecosystem Persistence Under Climate Change, we show how to estimate the probability that mangrove forests will persist or decline worldwide.\nGood predictions of ecosystem persistence are essential for coastal protection, fisheries management and climate-smart conservation planning.\nThe problem is that traditional models require detailed data that doesn’t exist at global scales. Models predict best when conditions match their training data. Climate change means mangroves face conditions they’ve never experienced before.\nWe developed a method to estimate probabilities of mangrove persistence using global datasets. This gives us a way to measure uncertainty that we can use to guide conservation decisions, e.g. to identify where management actions are most likely to succeed.\nWe used network models to analyze how mangroves respond at both their seaward and landward edges. Mangroves are unique because they can potentially expand in both directions as seas rise - landward into newly flooded areas, or seaward by trapping sediment.\nHere’s the projections for persistence of mangroves as they are currently (ie without any loss) from one of our scenarios:\n\nOur models reveal a stark contrast. 77% of mangrove units worldwide are likely to lose area at their seaward edge by 2040-2060. Rising seas, storms and reduced sediment from dammed rivers create multiple pressures. In contrast, about 30% of units could gain area landward, but coastal development often blocks this migration.\nPerhaps most importantly, uncertainty is high. In more than half the mangrove units studied, we couldn’t predict whether forests would persist or decline - the probability was essentially random. This isn’t a model failure. It tells us where better data and management actions could tip the balance.\nGeographic patterns emerged clearly. Mangroves in areas with small tidal ranges face greater risks. Sediment-rich coastlines offer better prospects for seaward expansion. Areas with dense coastal populations face the greatest landward migration challenges.\nWe tested conservation scenarios and found encouraging results. With targeted management - sediment addition, barrier removal, and assisted migration - the number of units likely to persist could nearly double.\nThis research shows a new way to handle ecosystem predictions under climate change. Instead of false precision, we quantify what we know and don’t know. For mangroves, significant seaward losses are likely, but landward gains and conservation success remain possible with strategic action. Read more here."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html",
    "href": "posts/2023-03-10-organising-r-scripts/index.html",
    "title": "Organising R scripts",
    "section": "",
    "text": "Here’s some key points for how I organise my R projects."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#folder-structure",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#folder-structure",
    "title": "Organising R scripts",
    "section": "Folder structure",
    "text": "Folder structure\nI use the folder structure from the excellent collaboration framework Max Campbell created for my lab. This allows us to share and backup code via github, as well connect and share large datasets on cloud repos (e.g. onedrive or dropbox).\nIn short we have a project folder in which would sit your Rproj file. I always start a new project by creating an R project in RStudio.\nWithin this folder you at least a scripts folder to keep R scripts and a data folder (which might be on the cloud) for sharing everything else.\nCreate subdirectories in /data folder for input data, output data (e.g. .rda and .csv files), documents. This will help you keep the data organised.\nNote if using github: make sure your include /data in your gitignore, so that the folder doesn’t sync to github. WIthin the data folder put everything that doesn’t go on github including .rda files, images and documents (exception being .md or .Rmd files). The reason for this is that version control on these types of files consumes a lot of memory (git will save a new file copy each time you make a change, rather than just saving the change)."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#structuring-the-data-folder",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#structuring-the-data-folder",
    "title": "Organising R scripts",
    "section": "Structuring the data folder",
    "text": "Structuring the data folder\nUse subfolders within your data folder if you have a lot of data. For instance if using the aweful shapefile format keep each layer in its own folder, so you don’t lose all those files."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#readme-file",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#readme-file",
    "title": "Organising R scripts",
    "section": "readme file",
    "text": "readme file\nMake a readme.md (markdown) file at the top directory. Github will automatically show this when people look at your repo. Use it to give an overview of the project, keep track of the latest updates, keep track of ToDos and document meta-data (e.g. what scripts are, what different data files are for)."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#organising-scripts",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#organising-scripts",
    "title": "Organising R scripts",
    "section": "Organising scripts",
    "text": "Organising scripts\nI like to number my scripts like: 1_data-processing.R, 2_glm-analysis.R, 3-plots.R. This helps me keep track of the order they should be run in. If you like coping and creating new versions then make an /archive folder to keep the old ones in. Or just use git."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#how-much-r-code-to-put-in-each-script",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#how-much-r-code-to-put-in-each-script",
    "title": "Organising R scripts",
    "section": "How much R code to put in each script?",
    "text": "How much R code to put in each script?\nI hate looking at other people’s mega-scripts, or even worse, my own mega-scripts months after I first created them. So chunk your workflow into sensible steps and save the output at the end of each step. I usually have 1-3 data processing and cleaning scripts (e.g. extracting data from GIS layers, merging different data frames and then cleaning errors or inconsistencies).\nThen its easy to just jump back in where you left off, without having to run 5 minutes of data cleaning computations every time you open the project.\nIt’s also easier for other people, and future-you, to navigate."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#how-much-commenting-to-do",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#how-much-commenting-to-do",
    "title": "Organising R scripts",
    "section": "How much commenting to do?",
    "text": "How much commenting to do?\nI comment more now than I used to. Again, it will help other people and future-you to navigate the scripts. The downsides to commenting are it takes time and gets out of date. I find that I forget to update my comments when I update the code, so there are mis-matches.\nAt the very least give a brief description of the task at the start of each script and put your name and date, so we know who created this and when."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#have-a-script-just-for-figures",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#have-a-script-just-for-figures",
    "title": "Organising R scripts",
    "section": "Have a script just for figures",
    "text": "Have a script just for figures\nIts a good idea to have a script (or scripts) who’s only task is to make the final publication quality figures.\nTo do this save the final dataframes used to make figures, as well as perhaps the figure objects themselves (if using ggplot). You can save data as .csv or just save figure objects and data all together as a .rda file.\nHaving a script that just loads the final clean data will save you a lot of pain down the track when the figures need small changes. You can quickly load the data and make the changes, without having to re-run analyses.\nLikewise if figures are made from the results of an analysis, save the final results before they are plotted and have a script just for the plotting.\nI usually have code for plots at the end of my analysis script, which are there for checking. The pub quality plots go in their own script."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#learn-to-use-version-control",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#learn-to-use-version-control",
    "title": "Organising R scripts",
    "section": "Learn to use version control",
    "text": "Learn to use version control\nThis is something I wished I invested in doing ages ago. I used to just copy scripts to make new versions, now I use version control via github desktop. There are other ways to do it (e.g. git via Rstudio) so find what works for you and do that."
  },
  {
    "objectID": "posts/2023-03-10-organising-r-scripts/index.html#share-your-code-online",
    "href": "posts/2023-03-10-organising-r-scripts/index.html#share-your-code-online",
    "title": "Organising R scripts",
    "section": "Share your code online",
    "text": "Share your code online\nFinally, I try to put everything I do on github now. This is as much for myself as for other people. I’ve found that over the years I often want to come back to code but its on some old hard-drive or computer. Much easier if I can just look up the repo online."
  },
  {
    "objectID": "posts/2021-10-02-grant-writing-strategies/index.html",
    "href": "posts/2021-10-02-grant-writing-strategies/index.html",
    "title": "Time for grants: grant writing strategies",
    "section": "",
    "text": "Writing major grant applications (like ARC grants) takes time, this requires planning. In a previous post I looked at how much time it takes me. Here I’ll discuss some of the strategies I when planning the application.\n\nI start writing the proposal early. Starting writing months ahead gets me thinking seriously about the ideas and gives me plenty of thinking time (not reported above). I always have a notebook on hand, once I start I never know when an idea will pop into my head.\nWriting, writing, writing. This helps get the ideas flowing and irons out the logical inconsistencies. I only use half of less of what I write, but I need to write to sift out and refine the good quality ideas (I understand that’s not the same for everyone).\nI start early on both the project design and the ‘investigator’ sections. I find I need as much time to think about the ‘investigator’ sections as the project design section. Good ideas for how to clearly communicate your career track record and past work are invaluable (though I find the part where I’m writing about myself the most painful part of grant writing).\nRecruiting collaborators. Starting early also gives me time to recruit co-investigators on the grant, and get their input.\nRecruit experienced reviewers. You need to start early to give them time to have input\nUnderstand the rules and jargon. Grant assessment criteria are full of jargon and words with very specific meanings. e.g. ‘Benefit’ and ‘Significance’ mean two different things to the ARC, if you mix them up you will probably score badly. Experienced grant writers can help point you in the write direction.\nI don’t spend much time on the budget until the final month. I have a rough idea of the limit and what I need to do what’s in the grant, but its one of the last things I refine, since the details are very dependent on what ends up in the project design.\n\nFinally, if its your first time writing a grant prepare to be overwhelmed by the number of different sections. In an ARC there’s project design, institutional support statement, budget, budget justification, industry partner support letters (for some grants), investigator career opportunities, investigator career impact, investigator citation metrics, … it goes on and on.\nStarting early will give you time to focus on getting your head around what needs to go into each individual section."
  },
  {
    "objectID": "posts/2025-06-25-why-frequentist-statistics-makes-no-sense/index.html",
    "href": "posts/2025-06-25-why-frequentist-statistics-makes-no-sense/index.html",
    "title": "Why we are all naturally Bayesians not frequentists",
    "section": "",
    "text": "I don’t see why its a thing to say “I’m a Bayesian”. Being a Bayesian is normal for any rational person as we will prove in a couple of sentences. Being a frequentist is what is what is strange. It would make more sense to assume we are all Bayesians, and have frequentists have to declare “I’m a frequentist”.\nThe statistical quantity of interest for frequentist is the likelihood, which is defined as the probability of your observations given your hypothesis. It can be written:\nPr(D | H)\n(Equivalently probability of data given we assume a particular model). For a Bayesian the statistical quantity is the probability of the hypothesis given the observations or:\nPr(H | D)\n(Equivalently probability of a model given the data).\nSay we’re out on a boat and we catch a glance of a human sized animal dive underwater with a flap of its tail. Was it a mermaid or a dugong? (A dugong is type of seagrass eating mammal).\nA frequentist would think in likelihoods and conclude, correctly, that both mermaid and dugong are equally likely: Both creatures, if assumed to exist, could result in a similar observation of us seeing a body and a tail diving underwater, i.e.\nPr(seeing a tail | dugongs exist) ≈ Pr(seeing a tail | mermaid exist)\nA rational person would guess the observed tail was a dugong. That is because a rational person is thinking in terms of probabilities. The probability we observed a mermaid is vanishingly small, because all prior evidence leads us to believe that mermaids don’t exist (though I do know some less than rational people who believe in ‘merfolk’).\nIn logic, Bayes theorem says we can calculate the probability of the hypothesis\nPr(H | D) = Pr(D | H)*Pr(H) / Pr(D)\nSo for the hypothesis “it’s a dugong”:\nPr(dugong | seeing tail) = Pr(seeing tail | dugong)*Pr(dugong) / Pr(seeing tail)\nFor our two models, dugong versus mermaid, we have similar values for the likelihoods and pr(D), but the ‘prior probabilities’, the Pr(H), are very different. Pr(dugong) is plausible (even though is rare to see them), whereas Pr(mermaid) is zero, because merfolk don’t exist.\nWe multiply the likelihood by the prior in the Bayes equation, so the Pr(mermaid | seeing tail) becomes zero.\n(If you are new to Bayes theory, you might be wondering what does Pr(D) mean? How do we calculate the probability of the data? For the above example it doesn’t matter as Pr(seeing a tail) is the same for both hypotheses. One of the challenges of Bayes stats in practice is calculating this Pr(D). A lot of statistical computing is devoted to figuring out how to do that. In brief though Pr(D) can be calculated as the sum of Pr(seeing tail | dugong)*Pr(H) for all discrete hypotheses or the integral if our hypotheses are continuous (like parameter values)).\nDoes this philosophical distinction it matter in the practice of statistics?\nThe Bayesian version of an analysis, such as a GLM, will often given similar or the same predictions as the frequentist version. Yet ecology, like many other disciplines, has a problem with repeatability and p-hacking.\nPart of the problem stems from the methods we use. For large sample sizes frequentist and Bayesian results often do converge.\nBut small sample sizes are common in many fields, including ecology. A small sample size study that finds an extreme and significant result is attractive to publish. This issue has been particularly problematic in behavioural ecology. These false positives come to dominate the literature.\nAppropriate use of Bayesian priors would penalize these extreme effect sizes. This is equivalent to setting a low probability on mermaid sightings above. Some have called for stronger priors to be the default choice\nSo in every day life we think of probabilities Bayesians, not like frequentists. There are numerous Bayesian R packages now and they are increasingly easy to use. So it makes sense that we should apply the same logic to our statistical analyses as well.\nThanks to Sober’s ‘Philosophy of Biology’ for this example (he uses gremlins in the roof rather than mermaids) and Coralina for telling me about the book!"
  },
  {
    "objectID": "posts/2016-08-03-spurious-doomsday/index.html",
    "href": "posts/2016-08-03-spurious-doomsday/index.html",
    "title": "Spurious doomsday scenarios?",
    "section": "",
    "text": "Are we facing doomsday, or is the evidence spurious?\nThis recent article in The Guardian caught my eye (thanks to Euan Ritchie for posting it on twitter).\nThe authors argue that human society is approaching the limits of growth that our planet can sustain and we will soon be faced with the collapse of civilization as we know it.\nA group called the Club of Rome developed a very sophisticated model over 40 years ago. The model predicts global collapse in 2015-2030. Turner and Alexander argue that the the model has done a great job predicting the last 40 years, so we should be very concerned about what will happen in the next 15 years or so.\nWhile I am certainly concerned about environmental sustainability of human endeavours (they aren’t at a global scale!), I do not agree that the historical data presented in the new study is convincing evidence that this Club of Rome model’s predictions will unfold.\nWhy? Well the co-trending of data since 1970 and the model’s predictions could well be spurious.\nAs an aside I saw this excellent post on my same twitter feed. I recommend reading it, it’s an example of how human bias can lead to spurious findings.\nScroll down through The Guardian article or the author’s original report and you will see plots of several variables over time, as collated from global datasources and predicted by the 1970 model.\nThe variables include global population size, birth rate, death rate, among others, plotted on a ‘common scale’. The use of a ‘common scale (normalized values)’ is the first hint that something may be wrong. Often it is very hard to model actual values, but models can do a better job of predicting relative change. So we normalize model’s output to give a picture of relative change.\nPlotting ‘normalized’ model predictions and ‘normalized’ data can be misleading, because it gives the impression the model is predicting exactly what is happening. In reality it is only predicting the trend.\nThe second, bigger issue, relates to the reliability of predicting trends.\nThe key predictions of the 1970s model all revolve around a series of ‘turning points’: points in time where food production and services output suddenly drop and consequently birth rate drops, death rate increases and global population peaks then declines.\nWell none of the data show a drop yet. So the validation of the model is only done on increasing or decreasing trends. It is pretty easy to build a model that predicts a continuing trend and such correlations of trends over time are often spurious.\nThe real test of the model will be when the drop happens, or doesn’t.\nTo the authors credit they do spend some time looking at whether the processes underlying the model are likely to be realised, like peak oil. It is having plausible evidence for processes that distinguishes causation from correlation.\nHowever, the main argument for the 1970s model prediction of collapse is the historic comparison of model predictions to data.\nWe have learned a lot since the 1970s. There is still widespread belief among scientists that we are approaching global planetary boundaries, and some may have already been exceeded. However, it is not clear whether all of them have been reached yet and if they lead to the collapse of civilization. Not least because of the highly unpredictable role of technology in human change.\nSo while we should be concerned about doomsday scenarios, we should not give up hope for a better future on the basis of spurious correlations."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "",
    "text": "An Introduction to Spatial Analysis in R"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#why-you-should-learn-r",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#why-you-should-learn-r",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nWhy you should learn R\n",
    "text": "Why you should learn R\n\n\nIt takes a lot of effort to learn to program and to use R. So why bother? Here are some very good reasons1. R is free2. R programs are repeatable. You can share them with others, or look up your old programs when a reviewer asks you months later to repeat that figure, but just include the outliers this time.3. R is relatively fast at performing analyses (compared to many GIS programs)4. You can combine state of the art statistics with great visualisations, including maps. Many leader’s in their fields develop packages for R, so if you can use R, you can get access to these pacakges.5. You can iterate repetitive tasks. This saves you time and means you can do things like model the distributions of 100s of species instead of just one, or make predictions at a global scale, instead of only local scales.  6. I think it’s a lot of fun."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#who-i-am",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#who-i-am",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nWho I am\n",
    "text": "Who I am\n\n\nI come from a background of field ecology and ecological modelling. I realised that being able to perform your own quantitative analyses is incredibly useful (and helps with getting jobs). Hence, over the past few years I have been teaching myself how to use R. I use spatial analyses in R for a wide-variety of purposes, including producing publication quality maps and for analysing the extent of climate change impacts on ocean ecosystems.\n\n\nI designed this course not to comprehensively cover all the spatial tools in R (there are far too many to cover in one course), but rather to teach you skills for getting started. You can build on the basics we teach you today to perform a wide variety of spatial analyses. Every new project comes with its own problems and questions and I have found having some knowledge of spatial tools in R allows me to develop new tools and analyses as they are needed."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#workshop-aims",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#workshop-aims",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nWorkshop aims\n",
    "text": "Workshop aims\n\n\n\nGet an overview of spatial data manipulation in R\n\n\nLearn how to get help with R\n\n\nLearn some basic spatial analysis in R\n\n\nHow to navigate R objects, particularly spatial objects\n\n\nLearn how to combine different types of spatial object\n\n\nLearn about map projections and how to change them\n\n\nMake a nice map\n\n\nPlan a complex workflow\n\n\nSpatial modelling (Advanced users)\n\n\n\nThe course has been designed to be accessible for people with a range of R abilities. At the very least you should have some limited experience in R, like reading in datasets and calculating summary statistics. I hope regular R users will also find something useful here. I have also included tasks for “Advanced users”. If you are going well with the basic tasks, feel free to tackle these. If you are struggling with the basic tasks, then I recommend skipping the tasks labelled “Advanced users” today, you can always come back to them later."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#writing-the-code",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#writing-the-code",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nWriting the code\n",
    "text": "Writing the code\n\n\nI provide most of the code for completing the tasks in the workshop below. However, this code may not work if you paste directly from the manual (because docs can contain hidden formatting). I encourage you to type the code out from the notes. This will also help you learn and understand the tasks. Even better, try changing the code to change analyses or figures."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#acknowledgements",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#acknowledgements",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nAcknowledgements\n",
    "text": "Acknowledgements\n\n\nThanks to Ross Dwyer for inspiring the original idea for the course. Also thanks to Ben Gilby and Chris Roelfsema for helping me out with some data-sets.\n\n\n\nUse and feedback\n\n\nThe course is designed to be freely shared with anyone. Please point people to my webpage if they want to download a copy. I can be reached on email at chris.brown@griffith.edu.au if you have comments. Feedback is appreciated. I would not recommend using any of these data-sets for real analyses, as they have been modified from the originals. If you want to get your hands on the original datasets, please see the references below.\n\n\n\nDownload a pdf of the course notes and the datasets"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#obtaining-r",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#obtaining-r",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nObtaining R\n",
    "text": "Obtaining R\n\n\nR can be downloaded from http://cran.r-project.org/Using a script editor, such as “RStudio”, can also be helpful. RStudio can be downloaded from http://www.rstudio.com/. If I am starting to confuse you with terms like ‘script’ please send the glossary in the appendix."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#starting-rstudio",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#starting-rstudio",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nStarting RStudio\n",
    "text": "Starting RStudio\n\n\nClick the RStudio icon to open R Studio. The interface is divided into several panels (clockwise from top left):1. The source code (supporting tabs)2. The currently active objects/history3. A file browser/plot window/package install window/R help window (tabbed)4. The R consoleThe source code editor (top left) is where you type, edit and save your R code. The editor supports text highlighting, autocompletes common functions and parentheses, and allows the user to select R code and run it in the R console (bottom right) with a keyboard shortcut (Ctrl R on windows, command-enter on macs) From now on, I will be providing you with code that you can type in yourself. Code will appear in this font: plot(x,y)"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#installing-packages",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#installing-packages",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nInstalling packages\n",
    "text": "Installing packages\n\n\nR offers a variety of functions for importing, editing, manipulating, analysing and exporting spatial data. Most of these functions rely on add-on packages that can be loaded to an R session using the library(packagename) command. If you close R and restart you will have to load the packages again.If your required package is not already installed on your computer, it can be simply installed by implementing the following command (you must be connected to the internet):\n\ninstall.packages(\"sp\")\n\nMultiple packages can be loaded at the same time by listing the required spatial packages in a vector. Here we will install all the packages you will require for today’s course:\n\ninstall.packages(c(\"sp\",\"rgeos\",\"rgdal\",\"maptools\",\"raster\"))\n\nAlso a couple of optional packages:\n\ninstall.packages(c(\"RColorBrewer\",\"wesanderson\",\"dplyr\"))\n\nSelect the local CRAN mirror (e.g. Canberra).If you are using a mac, rgdal and rgeos cannot be installed in this way. See my webpage for more help (https://sites.google.com/site/seascapemodelling/installing-rgdal-on-osx)"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#starting-a-new-script",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#starting-a-new-script",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nStarting a new script\n",
    "text": "Starting a new script\n\n\nLet’s open a new script and save it to your harddrive.When writing R scripts, use numerous # comments throughout your R scripts, you will thank yourself when you go back to the analysis later! Similarly at the start of your code, put some meta-information, such as:\n\n# who wrote the code?\n# what does the code do?\n# when did you write the code? etc. \n\nThe more projects you work on and analyses you do, the more important it is to have this meta-information at the beginning of your code."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#spatial-packages",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#spatial-packages",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nSpatial packages\n",
    "text": "Spatial packages\n\n\nWe usually start a script by loading the neccessary packages with the library() function:\n\nlibrary(sp)\nlibrary(rgdal)\nlibrary(rgeos)\nlibrary(maptools)\n\nThese add-on packages contain everything we need for the first half of today. sp defines spatial objects and has a few functions for spatial data. rgdal is R’s interpretation of the Geospatial Data Abstraction Library. It contains definitions and translations for projections for rasters and vectors (data types we will come to later), and is something like the babel fish for projections. rgeos is an open source geometry engine, and is used for manipulation of vector data. For instance, if we want know the land area of an island with lakes on it, rgeos will do the job. maptools has additional useful spatial tools. Take a moment and reflect how awesome it is that large teams of people have made all these packages open-source and that others have written packages to use them in R.\n\n\nMoving on. You will need to set your working directory to the folder where the datafiles for this course are: setwd(‘mypath/Datasets’) You can get the path by using your folder browser and right clicking and selecting properties, then just copy the path. Note on windows paths will be copied with &lt;/code&gt; rather than / so make sure you turn them around in your script."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#read-in-points",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#read-in-points",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nRead in points\n",
    "text": "Read in points\n\n\nOur first data-set is a typical dataframe, a format you should have worked with in your past R experience (if you haven’t I recommend doing a begginer course and getting some more experience before you take this course). The only difference with this dataframe is that samples also have coordinates. Let’s read it in:\n\ndat &lt;- read.csv('Video monitoring sites.csv', header=T)\nclass(dat)\nhead(dat)\nstr(dat)\n\nThese commands tell us ‘dat’ is of Class data frame (see glossary in appendix), we can look at the top six rows and see that each row is a single site. str(dat) let’s us look at how our objects are structured."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-list-objects",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-list-objects",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nA primer on list objects\n",
    "text": "A primer on list objects\n\n\nWe will break to discuss list objects in R"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#exploring-the-dataframe",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#exploring-the-dataframe",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nExploring the dataframe\n",
    "text": "Exploring the dataframe\n\n\nThere are a few ways to extract specific variables or sites from the data frame. The \\(&lt;/code&gt; is used to indicate specific variables. Try typing &lt;code&gt;dat\\)s.gilbii. You can also call variables by their column numbers dat[,3] gets the third column. dat[5,] will give you all variables for the 5th site. We might also want to find out the number of sites, and having a nsites variable will be useful later on.\n\nnsites &lt;- nrow(dat)\nnsites\n\nWe might want to explore some summaries of the data too. For instance make a table of occurences of our two species\n\ntable(dat$s.gilbii, dat$s.stevenii)\n\nWe could do a simple plot of the sites by: plot(dat\\(x, dat\\)y). Another useful skill is subsetting dataframes, for instance, so we can just plot sites where S. gilbii occurs. We can do this using the which() function.\n\nisgilbi &lt;- which(dat$s.gilbii==1)\nisgilbi\n\nwhich() finds rows where s.gilbii equals 1 (note the double =, which means ‘does it equal 1’, rather than ‘make it equal 1’), which is where the species was recorded on the cameras. We could use our new variable of row numbers to plot the points where S. gilbii is present in red.\n\nplot(dat$x, dat$y)\npoints(dat$x[isgilbi], dat$y[isgilbi], col ='red')\n\nIf you are struggling to make sense of the which() command, don’t worry. Indexing is a basic programming skill, but it can take some time to get your head around. If you are looking for a book to help you can’t go past The Art of R Programming by Norman Matloff. It’s a great read and I read it cover to cover in almost a single sitting. Once you master simple programming tricks like indexing a whole new world of data manipulation and analysis (not just spatial!) will open up to you."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#turning-points-into-a-spatial-object",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#turning-points-into-a-spatial-object",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nTurning points into a spatial object\n",
    "text": "Turning points into a spatial object\n\n\nSo far we have been treating the dataframe as a normal, non-spatial dataframe. We want to convert to a Spatial Points Data Frame (that’s a new Class). This will enable R to uniquely recognise the coordinates for what they are and let us match up this dataset with others we will use later.To create a Spatial Points Data Frame, tell R which columns of dat are coordinates\n\ncoordinates(dat) &lt;- ~x +y\nclass(dat)\nstr(dat)\ndat\ncoordinates(dat)\ndat@data\n\nNote the class and structure has changed now. The coordinates are now stored separately. Site coordinates are matched to the original data, which can now be found in the ‘data’ slot. More on slots later."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#plotting-with-different-colours-for-animals",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#plotting-with-different-colours-for-animals",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nPlotting with different colours for animals\n",
    "text": "Plotting with different colours for animals\n\n\nCreating a Spatial Points Data Frame opens up new possiblities for visualising the data. One useful feature of R is that many functions will have different ‘methods’ which mean you can use the same function on different types of data. Try the below plotting functions one at a time and see what happens.\n\nplot(dat)\nlines(coordinates(dat))\nspplot(dat)\n\nspplot() comes with the sp package and is a useful tool for quickly viewing data."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#finding-help",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#finding-help",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nFinding help\n",
    "text": "Finding help\n\n\nKnowing how to find help for yourself is a critical part of using R. Anytime I code something, I spend a lot of time in the R helpfiles that can be accessed: ?spplot. I encourage you to make heavy use of ?, anytime we introduce a new function. You can also look at the cran webpage for help, or search sites like Stack Overflow to see if someone else has had a similar problem. Another useful resource are vignettes, which are distributed with some packages try typing browseVignettes()."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#coordinate-reference-systems---assigning-a-crs",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#coordinate-reference-systems---assigning-a-crs",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nCoordinate reference systems - assigning a CRS\n",
    "text": "Coordinate reference systems - assigning a CRS\n\n\nBack to our spatial data. At this point, we haven’t defined a projection for our dataframe. To see this type proj4string(dat). This data is unprojected longitude latitude coordinates, we can tell R this by assigning to the proj4string.\n\nproj4string(dat) &lt;- \"+proj=longlat +ellps=GRS80 +no_defs\"\n\nYou can search for CRSs on spatialreference.org."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-map-projections-and-coordinate-reference-systems",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-map-projections-and-coordinate-reference-systems",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nA primer on map projections and coordinate reference systems\n",
    "text": "A primer on map projections and coordinate reference systems\n\n\nWe will break to discuss map projections."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#transform-to-local-projection",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#transform-to-local-projection",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nTransform to local projection\n",
    "text": "Transform to local projection\n\n\nOk, so for Moreton bay our UTM zone is 56, so let’s define a proj4string for that zone.\n\nmyproj &lt;- \"+proj=utm +zone=56 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\nBe warned. The naive user might think we can update the projection of our data just by re-assigning the projection string. For instance: proj4string(dat) &lt;- myproj. Notice the warning that throws. Also, look at the coordinates by typing coordinates(dat), they have not changed. Please return your dataframe to the correct projection by typing\n\nproj4string(dat) &lt;- \"+proj=longlat +ellps=GRS80 +no_defs\"\n\nand we will do a proper transform of the coordinates.The proper way uses the power rgdal to convert unprojected coordinates to our local UTM:\n\ndatutm &lt;- spTransform(dat, CRS(myproj))\ncoordinates(datutm)\n\nNotice the coordinates have changed. The units are now metres, rather than decimal degrees."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#calculate-distances-between-sites",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#calculate-distances-between-sites",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nCalculate distances between sites\n",
    "text": "Calculate distances between sites\n\n\nSo we have tranformed our points to a local UTM, which is a distance preserving projection. Now is a good time to try an rgeos for calculating distances:\n\ndistmatrix &lt;- gDistance(datutm, byid=TRUE)\ndistmatrix\n\ngDistance calculates the distance (as the crow flies) between every point, if we specify byid=TRUE. We now have a matrix of distances between all sites, like those tables you see in driving maps.See the reference manual at: http://cran.r-project.org/web/packages/rgeos for other useful geometry functions"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#subsetting-data-frame",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#subsetting-data-frame",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nSubsetting data frame\n",
    "text": "Subsetting data frame\n\n\nWe might also want to know distances just between sites where S. gilbii was seen. To do this we can subset the Spatial Points Data Frame, just like we would subset a normal dataframe\n\ndat.gilbii &lt;- subset(datutm, datutm@data$s.gilbii==1)\nclass(dat.gilbii)\n\nThe subset() function creates a new dataframe from an existing one (datutm), subject to a rule. Our rule in this case is any observation where S. gilbii occurs. Now try calculating the distance matrix on this data frame."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#importing-gis-shapefiles",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#importing-gis-shapefiles",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nImporting GIS shapefiles\n",
    "text": "Importing GIS shapefiles\n\n\nWe will now look at the shape files for Moreton Bay’s marine protected areas (MPAs or ‘Green zones’). First, set your working directory to the ‘MoretonBayGreenZones’ folder inside of the datasets folder I gave you.\n\n\nrgdal has a function for interpreting shapefiles:\n\ngreenzones &lt;- readOGR('.', 'MB_Green_Zones_2009_100805_region')\nclass(greenzones)\n\nSee that it has read in a Spatial Polygons Data Frame. Try these functions for exploring the data\n\nplot(greenzones)\nspplot(greenzones)\ngreenzones\nstr(greenzones)\nnMPAs &lt;- length(greenzones)\nnMPAs\ngreenzones@data\ngreenzones@data$ID\ngreenzones@bbox\nslot(greenzones, 'bbox')\nslot(greenzones, 'proj4string')\n\nSpatial data have a different sort of list structure to normal dataframes. Intead of the \\(&lt;/code&gt; for accessing variables, we have slots, accessed using &lt;code&gt;@&lt;/code&gt; or the &lt;code&gt;slot()&lt;/code&gt; function. The dataframe embedded in the ‘data' slot still has the usual structure, and you access variables in the normal way with &lt;code&gt;\\).Note in general it is fine to access data using the slots, but you should not modify data directly in the slots because other attributes for that spatial object won’t be updated. To modify spatial data, it is best to use specific functions for that purpose."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-nesting-in-spatial-objects",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-nesting-in-spatial-objects",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nA primer on nesting in spatial objects\n",
    "text": "A primer on nesting in spatial objects\n\n\nWe will break to discuss lines, polygons and how these data are stored\n\n\n\nLets change our polygon dataframe to make ID more informative\n\ngreenzones@data$ID &lt;- 1:nMPAs\nspplot(greenzones, zcol ='ID', col.regions = rainbow(nMPAs))\n\nThe call to the rainbow() function just makes 33 different colours along the rainbow, so we can colour code our 33 MPAs."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#nested-structure-of-spatial-polygons-and-lines",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#nested-structure-of-spatial-polygons-and-lines",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nNested structure of spatial polygons (and lines)\n",
    "text": "Nested structure of spatial polygons (and lines)\n\n\nSpatial polygon dataframes can contain numerous polygons and polygons within polygons. As I explained in the primer, nesting is a useful way to store this data. The following code might look complex, but it is really just a long string of nested commands. To get an overview of the nesting try\n\nstr(greenzones)\n\nWe can access the polygons for MPA 29 (the pink one) by typing\n\ngreenzones@polygons[[29]]@Polygons\n\nTry also\n\nstr(greenzones@polygons[[29]]@Polygons)\n\nMPA 29 is a bit unusual, in that it has two polygons. Why is this? Notice that there is a ‘hole’ slot within each polygon\n\ngreenzones@polygons[[29]]@Polygons[[1]]@hole\ngreenzones@polygons[[29]]@Polygons[[2]]@hole\n\nPolygon 1 has hole=F ALSE, polygon 2 has hole = TRUE. Have a look at the last map and see if you can figure out what the hole represents.\n\n\nYes indeed, it is an island. If we had a lake on the island with another island in that lake, we would have another polygon within the hole, with a hole in that! To get an idea of how polygon data are stored, you can extract the coordinates and just plot them (note, for type=‘l’ that is a letter ‘l’ for laugh, not the number ‘1’ or letter ‘i’)\n\nplot(greenzones@polygons[[29]]@Polygons[[1]]@coords, type='l', col='blue') #The MPA\nlines(greenzones@polygons[[29]]@Polygons[[2]]@coords, col='red') #The island in the MPA\n\nYou should see a blue box (the MPA) with a red island in it.\n\n\nYou might also notice there is an area slot for each polygon\n\ngreenzones@polygons[[23]]@area\n\nConvenient, but unfortunately it does not subtract the holes."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#calculate-areas-of-greenzones",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#calculate-areas-of-greenzones",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nCalculate areas of greenzones\n",
    "text": "Calculate areas of greenzones\n\n\nThe rgeos package can account for holes in polygons. To use rgeos to calculate a polgyon’s area:\n\ngz.areas &lt;- gArea(greenzones, byid=TRUE)\n\nLet’s attach this more accurate area measurement to our spatial polgyons data frame, so we can plot it in spplot. By the way, if you wanted a really accurate area measurement, you should transform to an equal area projection, particularly if you are calculating areas over a much larger region.\n\ngreenzones@data$areas &lt;- gz.areas\nspplot(greenzones)\n\n\nSo now we get two plots, one colour coded by ID, the other by MPA area. Try changing the colours using rainbow (hint look at ?spplot to find what argument to change). We could also use subset on our spatial polygons to get the biggest MPAs and plot them.\n\ngreensub &lt;- subset(greenzones, greenzones@data$areas &gt; 2e+07)\nspplot(greensub) #our new spatial polygons\n\nYou might also want to do an ordered plot of the distribution of MPA areas\n\ngzsort &lt;- sort(gz.areas, decreasing=T)\nbarplot(gzsort, ylab='Area m^2',xlab='ID')\n\nIt seems there are many small reserves and just a few big ones.\n\n\n\n\nAdvanced users: loops\n\n\nIf you ever need to do a repetitive task on some data, loops are your friend. Writing loops is a very basic programming skill, but they can take some time to figure out initially. In this advanced task, we will use a loop to compare the rgeos area estimates with the areas in the ‘area’ slot. We can easily access the rgeos areas from the dataframe we made. However, the areas slot is hidden away deep in the polygon’s nested structure. We will use a loop to get the value for each polygon.First step, we have to make an empty variable to hold results. We can use rep() to do just replicate some NAs 33 times (33 is the number of MPAs).\n\npolyareas &lt;- rep(NA, nMPAs)\n\nThen follows our loop.\n\nfor (i in 1:nMPAs){\n  polyareas[i] &lt;- greenzones@polygons[[i]]@area\n}\n\nThis is a for loop. There are other types of loops too, but we will just look at for loops today. ‘For’ loops make a counting variable, in this case I called it i and they iterate the counter over multiple values, in this case from 1 to nMPAs. Try typing 1:nMPAs. These are the values i will take in each turn of the loop.The section in {} is where the looping actually happens. So for the first iteration i=1. We access polygon number 1 at its area slot. Then we assign it to polyareas[1]. The loop then steps to i=2 and repeats. And so on until i=33. Now look at polyareas. Before it was NAs, now it has the area numbers for each polygon, conveniently ordered in the same order as the polygons are stored. Our loop was pretty simple, but you can make them as complex as you like, including putting loops inside loops. We can compare the two area calculations to see where the areas slot is off and by how much it is off:\n\narea.diff &lt;- round(polyareas - gz.areas)\narea.diff\n\nCan you think of a way to plot the MPAs again, but this time with a different colour to warn us which ones have incorrect areas?"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#overlay-and-extract-polygon-data-green-zone-status-at-points",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#overlay-and-extract-polygon-data-green-zone-status-at-points",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nOverlay and extract polygon data (Green zone status) at points\n",
    "text": "Overlay and extract polygon data (Green zone status) at points\n\n\nSo far we have just manipulated spatial objects on their own. To reach our goal of analysing S. gilbii’s distribution, we need to combine spatial objects.Let’s find out whether each of our video sites is inside or outside greenzones. We can do this using over().\n\nMPAover &lt;- over(datutm, greenzones)\nMPAover\n\nover() returns a dataframe telling us the attributes of greenzones at each video site. When sites are outside MPAs, there is no polygon data, so we just get an NA (representing no data). We can use the NAs to define a new variable of whether sites are inside or outside MPAs.\n\nMPA &lt;- rep('Open', nsites)\niMPA &lt;- which(MPAover[,1]&gt;0)\nMPA[iMPA] &lt;- 'MPA'\n\nFirst we create a variable of 70 ‘Open’. Then we find sites that don’t have MPAs and assign them ‘MPA’. Then we should add our MPA variable to our points dataframe\n\ndatutm$MPA &lt;- factor(MPA)"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#saving-what-we-have-done-so-far",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#saving-what-we-have-done-so-far",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nSaving what we have done so far\n",
    "text": "Saving what we have done so far\n\n\nAlmost time for a break, so let’s save what we have done so far. Set your working directory to a location where you want to save the new data to.\n\n\nWe can write the points dataframe using write.csv(datutm, ‘mydata.csv’, row.names=F). However, this won’t export the projection info. To export all the extra spatial info, use writePointsShape(datutm, ‘Video points.shp’). Then if we reload this points.shp file, the proj4string will be included.We can also export the greenzones shapefile\n\nwriteOGR(greenzones, dsn ='green zones',layer='greenzone', driver ='ESRI Shapefile')\n\nFor fun, let’s also export the sites as a .kml file. You can view .kml files in Google Earth. Note that the projection must be in lon-lat coordinates for kmls.\n\nkmlPoints(dat, 'Site points.kml', name ='video sites',icon=\"http://maps.google.com/mapfiles/kml/shapes/fishing.png\")\n\nlonglatproj &lt;- proj 4string(dat)\ngreenzoneslonglat &lt;- spTransform(greenzones, CRS(longlatproj))\n\nkmlPolygons(greenzoneslonglat, 'greenzones.kml', lwd=5)\n\nA link if you want to look at different icons: http://kml4earth.appspot.com/icons.html. You can also export polygons, or even import google maps into R using the dismo package."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#read-in-raster-grids-seagrasses-and-distance-from-rivers",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#read-in-raster-grids-seagrasses-and-distance-from-rivers",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nRead in raster grids (seagrasses and distance from rivers)\n",
    "text": "Read in raster grids (seagrasses and distance from rivers)\n\n\nLet’s read in some raster grids. First reset your working directory to the datasets folder I gave you.\n\n\nRaster grids are easy to read in now\n\nr.sgutm &lt;- raster('raster seagrass.grd')\nr.riverdists &lt;- raster('Raster river distances.grd')\n\nWe have two rasters. One is seagrass presence/absence in UTM coordinates, the other is the shortest distance to two major rivers in Moreton Bay."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#plotting-rasters",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#plotting-rasters",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nPlotting rasters\n",
    "text": "Plotting rasters\n\n\nRemember I said many functions have multiple methods, so they work on all kinds of data. Try plot() to view the data\n\nplot(r.sgutm)\nplot(r.riverdists)\n\nVery nice you might say, but what if you are red-green colour blind and can’t see this pretty rainbow scale properly. Two of my favourite packages for different colour schemes are\n\nlibrary(RColorBrewer)\nlibrary(wesanderson)\n\nTo get help on brewer palettes ?brewer.pal and click the weblink. There is even a check box for red-green colour blind safe. We can then save a colour palette as a variable for later use\n\nmycols &lt;- brewer.pal(8, 'OrRd')\n\nI will let you explore the wesanderson package for yourself. Try looking it up on the cran webpage. Personally, I am a fan of ‘A Life Aquatic’, so I will go for the ‘Zissou’ palette for the rest of today.\n\ndisplay.wes.palette(5, 'Zissou')\n?wes.palette\npal &lt;- wes.palette(name = \"Zissou\", type = \"continuous\")\nmycols &lt;- rev(pal(120))[30:120]\nzissou.cap &lt;- wes.palette(5, name = 'Zissou')[5]\n\nNow lets look at river distances again with our new colours.\n\nplot(r.riverdists, col =mycols)"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#interacting-with-plots",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#interacting-with-plots",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nInteracting with plots\n",
    "text": "Interacting with plots\n\n\nraster package is getting better all the time. One of the newer additions is many functions that allow you to interact with plots. For instance, try these (one at a time). You will need to click on the map to implement each function. Hit escape to exit the interactive mode.\n\nplot(r.riverdists, col =mycols)\nzoom(r.riverdists)\nplot(r.riverdists, col =mycols)\nclick(r.riverdists,n=3)\nplot(r.riverdists, col =mycols)\nnewpoly &lt;- drawPoly() #hit escape to finish your polygon\nplot(newpoly)\n\nAnother useful function for interacting with any type of plot is locator(n), it returns the n coordinates for where you haved clicked on a plot."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#look-at-characteristics-of-the-rasters",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#look-at-characteristics-of-the-rasters",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nLook at characteristics of the rasters\n",
    "text": "Look at characteristics of the rasters\n\n\nIt is good to explore you data, to understand rasters better. Try these\n\nr.riverdists\nprojection(r.riverdists)\nprojection(r.sgutm)\nres(r.riverdists) #resolution in x and y\nnrow(r.riverdists) #number of rows\nhist(r.riverdists[]) #histogram of distances"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#project-distances-to-local-projections",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#project-distances-to-local-projections",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nProject distances to local projections\n",
    "text": "Project distances to local projections\n\n\nYou might have noticed that the river distances are not in our local UTM coordinates. We can reproject rasters, using projectRaster and specify the projection to be the same as the seagrass raster.\n\nr.riverutm &lt;- projectRaster(r.riverdists,r.sgutm)\n\nLet’s open two figure windows and compare our projections\n\npar(mfrow =c(1,2)) #set the number of figure subwindows, 1 row, 2 columns.\nplot(r.riverdists, col =mycols)\nplot(r.riverutm, col =mycols)\n\nNotice that the UTM raster has also been cropped and resampled to the same extent and resolution as the seagrass raster. You can do cropping, aggregation and resampling yourself if you like.We can draw a new extent on our new raster and crop to it\n\nplot(r.riverutm, col =mycols)\ndrawExtent()\n\nBelow I have pasted in some coordinates (around the Brisbane river) I drew myself to the extent() function. The extent() function creates a new extent object that is basically just a box with a proj4string.\n\nnewext &lt;- extent(510877.8, 524824.3, 6962493, 6979397)\nr.bris &lt;- crop(r.riverutm, newext)\nplot(r.bris, col =mycols)\n\nYou might also want to aggregate cells in a raster. For instance, very large rasters with very fine resolution can be very slow to handle and use a lot of memory. Just use the aggregate() function to make them smaller. The fact=8 argument aggregates 8 cells in each direction.\n\nncell(r.riverutm)\nr.riveragg &lt;- aggregate(r.riverutm, fact=8)\nncell(r.riveragg)\npar(mfrow =c(1,2))\nplot(r.riverutm, col =mycols)\nplot(r.riveragg, col =mycols)\n\nNotice that if we increase the fact, the plot gets very blocky. We are trading off memory size for lower resolution. A similar function, that we won’t cover today is resample(). Resampling changes the origin and resolution of a raster. In general you have to be a bit careful with all of these functions (reprojecting, aggregating, resampling), because they can sometimes distort the data in ways you don’t intend, for instance the smaller islands dissappear when we aggregate."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#rasterizing",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#rasterizing",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nRasterizing\n",
    "text": "Rasterizing\n\n\nRasterizing is turning spatial shapes (e.g. polygons) into rasters, it has nothing to do with Bob Marley. Let’s rasterize our greenzones data onto the seagrass raster, so we can compare it to river distances.\n\n?rasterize\nr.greenzone &lt;- rasterize(greenzones, r.sgutm, field='ID')\n\nThis copies the seagrass raster, clears its values and pastes in the values from the ‘ID’ column of the spatial polygons data frame. Recall that ‘ID’ was just a unique number for each MPA.\n\nplot(r.greenzone)\n\nNow we have two rasters with exactly the same coordinates, resolution, cells etc… We can combine them in many different ways. For instance, we can get mean distances to rivers for each MPA.\n\nzonal(r.riverutm, r.greenzone, fun='mean')\n\nThis returns an array with zone numbers (the MPAs) and the mean distance of that MPA to rivers. You could also rasterize the greenzones by the areas field.\n\nr.greenzone &lt;- rasterize(greenzones, r.sgutm,field ='areas')\nplot(r.greenzone)\n\nShows us a plot of MPAs, where colours represent their areas (minus the holes)."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#write-raster",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#write-raster",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nWrite raster\n",
    "text": "Write raster\n\n\nLet’s save where we are up to. Set your working directory to an appropriate folder then try\n\nwriteRaster(r.riverutm, 'River distances in utm.grd')\n\nSaves our raster (as a .grd. file). This can be reused later in R or imported into ARC GIS for instance, though I hope by now you are considering deleting all GIS programs from your computer and using only R from now on. Again, we can also write rasters for Google Earth. But remember it must be a raster with long-lat coordinates.\n\nKML(r.riverdists, file='RiverDists.kml')"
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#extracting-data",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#extracting-data",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nExtracting data\n",
    "text": "Extracting data\n\n\nTo truly master spatial data manipulation in R, we need to know how to combine rasters and vectors. Luckily, this is pretty easy. For instance, let’s find out whether seagrass is present or not at our video sites\n\n?extract\nsgpres &lt;- extract(r.sgutm, datutm)\ndatutm$SG &lt;- sgpres\n\nextract() takes a raster (here our seagrass layer) and gets the values at a locations of spatial points (here our video sites). Again, the projections must be identical. Then we attached the results to our points dataframe. Now we are making some progress on matching our environmental variables with our video sites.We can do the same for river distances\n\nsitedist &lt;- extract(r.riverutm, datutm)\ndatutm$rdist &lt;- sitedist\ndatutm@data\n\nGreat! Now our points dataframe has presences of two species, presence of seagrass, MPAs and the distance to the nearest river, our proxy for water quality.\n\n\nWe can do some preliminary analyses to investigate how our species responsd to these variables. Let’s do boxplots of river distances against our two species. You can try some other boxplots for yourself too\n\nboxplot(datutm$rdist~ datutm$s.gilbii)\nboxplot(datutm$rdist~ datutm$s.stevenii)\n\nWe can also calculate the proportion of sites where S. gilbii occurs on seagrass.\n\ntapply(datutm$SG, datutm$s.gilbii,mean)\n\ntapply() takes a vector (SG), and applies a function (mean) by groups (S. gilbii presence/absence)."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-workflows",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#a-primer-on-workflows",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nA primer on workflows\n",
    "text": "A primer on workflows\n\n\nSo far we have mainly been following my script without any view as to where we are going. With your own data, you will need to take more time to plan the workflow. Otherwise you can waste a lot of time and end up running down dead-ends, like eventually finding you have cropped a raster down too far. As an example of a simple workflow, let’s revist the tasks we just did, but with the specific aim of extracting river distances and seagrass presence at video sites in mind.So we have our sites data and we know it is in UTM coordinates: proj4String(datutm). We want to match our other layers to this layer. We might start a new script, and add comments as placeholders for tasks later, like this:\n\n# Script to extract seagrass and river distances for video sites\n# CJ Brown 27 Nov 2014\n### Outline\n#Load packages\n#Load points data\n# Load rivers and seagrass\n# Check projections for rivers and seagrass\n# Reproject rivers and seagrass if neccessary\n# Extract points\n# Add points to sites dataframe\n# Save new sites dataframe for analysis later\n\nThen we would start filling this in. For instance, assume you have read the data in already and are looking at rivers:\n\n#...\n#Check projection\nproj4string(r.riverdists)\n#It's not the correct UTM, reproject it\nr.riverutm &lt;- projectRaster(r.riverdists,r.sgutm)\n#Plot to check it looks right\nplot(r.riverutm, col =mycols)\n#Extract points\nsitedist &lt;- extract(r.riverutm, datutm)\n# And so on...\n\nPlanning like this can really help you save a lot of time by keeping you on track.\n\n\nFor Advanced users the dplyr package offers a cool function called pipes, which can make workflows easier to follow. Pipes look like this %&gt;% and basically take the output of one function and port it into the first argument of the next function. As an example, lets reproject rivers then extract site values in one line.\n\nlibrary(dplyr)\nsitedist &lt;- r.riverdists %&gt;% projectRaster(r.sgutm) %&gt;% extract(datutm)\n\nYou could read this code as: ’take r.riverdists, project it using the CRS of r.sgutm then extract the values at the coordinates of datutm."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#advanced-users---plotting-our-model-predictions",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#advanced-users---plotting-our-model-predictions",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nAdvanced users - plotting our model predictions\n",
    "text": "Advanced users - plotting our model predictions\n\n\nWe can use the same ideas from mapping above, combined with the predictions for the probability of S. gilbii occurrences, to make a plot of were we expect to find S. gilbii.\n\n\n\nPredict in space\n\n\nWe use predict again, but this time instead of predicting with a sequence of river distance values for seagrass, we predict with the rasters for river distance and seagrass presence.To make our predictor variables, we just extract every value of the relevant raster layers\n\nr.sgutm[]\npredsg &lt;- r.sgutm[]\npredrdist &lt;- r.riverutm[]\n\nthe [] gives us every value in the raster, starting at the top left and reading along rows. The two predictors will be extracted in the same order, so we can be confident they align properly, provided the resolution, extents are identical (which they are).Now put the predictors into a dataframe and predict to that dataframe using model 2.\n\npred.df &lt;- data.frame(SG = predsg, rdist= predrdist)\npred.df$fit &lt;- predict(mod2, newdata = pred.df, type='response')\n\nWe just added a column of model fits to the dataframe. The model fits are the predicted probability of occurrence for S. gilbii. We can return those model fits to a raster for plotting, first create an empty raster that is the same as the original predictors (seagrass and river distances)\n\nr.probs &lt;- raster(r.sgutm)\n\nThen just assign the fitted values to that raster\n\nr.probs[] &lt;- pred.df$fit\n\nYou will notice we used the [] again, but this time on the left hand side of the assign symbol (‘&lt;-’). This changes the values in that raster\n\n\n\n\nPlot predictions\n\n\nPlotting the predictions basically follows the same procedure as the last plot we made. The main change is that I decided to use a different colour palette. Here is my code for you to follow or customise to make the plot the way you want it.\n\nprobcols &lt;- brewer.pal(9,'PuBu') #new colour palette\nplot(r.probs, col = probcols, xaxt='n', yaxt='n') #plots predicted probabilities\nplot(coastutm ,add=T, border='grey30')\n#Add seagrass, as polygons\nplot(poly.sgutm, add=T, border='DarkGreen', lwd=0.3)\nplot(greenzones, add=T, border='grey30')\n#Add an arrow\narrows(513150, 6934468, 513150, 6949293, len=0.3)\ntext(513150, 6953621, 'N', cex=1.5)\n#Scale bar\nscale.len &lt;- 10000 #10 km\nlines(c(518282, 518282+scale.len), c(6931187, 6931187), cex=3)\ntext(524025, 6929337, '10 km', cex=1)\n\nWe can see now that our species is unlikely to be near rivers, unless there is some seagrass there."
  },
  {
    "objectID": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#glossary",
    "href": "posts/2015-06-22-R_Spatial_course/2015-06-22-R_Spatial_course.html#glossary",
    "title": "An Introduction to Spatial Analysis in R",
    "section": "\nGlossary\n",
    "text": "Glossary\n\n\nThere are a few important programming terms to know. Here is a quick glossary, please refer back to it if you see a term you don’t understand. I will explain more of these terms during the course.* AssignThe assign symbol &lt;- (AKA =) assigns a variable, like a number to a name like x. For instance to save the number one with the name x: x&lt;-1, which you can read as ‘assign x the number 1’.* ClassA class defines a structure for an object. That way, when I give some data and say it is of ‘raster’ class, you will know how it is structured and where to find different variables For instance, the extent.* ConsoleThe window where R code is executed.* FunctionA function takes some objects and changes them, returning a new object. For instance if we had a variable x of 20 numbers, we could calculate its mean using the mean() function like this mean(x). Some functions also have additional arguments. For instance, if we wanted to calculated the mean of ‘x’ removing missing values (coded as ‘NA’) we could write: mean(x, na.rm=T). Multiple arguments are separated by commas with a function’s brackets.* ListLists are nested objects of more objects.* ObjectAnything you store in R’s memory is an object. For instance, a string like this spatial a number like this 42 or even a new function. We will be working a lot with spatial objects today. See the entry for ‘lists’.* MethodA single function can have multiple methods, if it works with different classes of objects. For instance plot(x) works with many types of data including arrays of numbers, rasters, polygons and even model objects. See the entries for functions, objects and classes.* PackagesPackages are collections of functions that someone has written for distribution to others. The folder a package of functions is kept in is called its ‘library’, hence we load packages to an R session using the library() function.* ScriptA window with a text editor where we write code so we can save it for later. The code in your script won’t be exectued until you send it to the console (e.g. by clicking on a line and pressing Ctrl-R). The script should be written in the order you want the code to be executed. We’re not writing Pulp Fiction so keep your code in chronological order.* VariableI use this term loosely here, in general a variable is just a number or sequence of numbers you have saved in R’s memory.* Working directoryThe directory where R will look for any files you try to load. You can find out what it is by typing getwd() in the console. You can change it by typing setwd(’/Users/cbrown/Databases), where the path is the path I use for databases. You need to change it to your own."
  },
  {
    "objectID": "posts/2017-10-06-qqplot-non-normal-glm/index.html",
    "href": "posts/2017-10-06-qqplot-non-normal-glm/index.html",
    "title": "Checking residual distributions for non-normal GLMs",
    "section": "",
    "text": "If you are fitting a linear regression with Gaussian (normally distributed) errors, then one of the standard checks is to make sure the residuals are approximately normally distributed.\nIt is a good idea to do these checks for non-normal GLMs too, to make sure your residuals approximate the model’s assumption.\nHere I explain how to create quantile-quantile plots for non-normal data, using an example of fitting a GLM using Student-t distributed errors. Such models can be appropriate when the residuals are overdispersed.\nFirst let’s create some data. We will make a linear predictor (ie the true regression line) eta and then simulate some data by adding residuals. We will simulate two data-sets that have the same linear predictor, but the first will have normally distributed errors and the second will have t distributed errors:\nn &lt;- 100\nphi &lt;- 0.85\nmu &lt;- 0.5\nset.seed(23)\nx &lt;- rnorm(n)\n\neta &lt;- mu + phi * x\nnu &lt;- 2.5\ntau &lt;- 3\n\ny_tdist &lt;- eta + (rt(n, df=nu)/sqrt(tau))\ny_normdist &lt;- eta + rnorm(n, sd = 1/sqrt(tau))\n\nplot(x, y_tdist)\npoints(x, y_normdist, col = \"red\", pch = 16, cex = 0.8)\nlegend(\"topleft\", legend = c(\"t-distributed errors\", \"normally distributed errors\"), pch = c(1,16), col = c(\"black\", \"red\"))\n\nNotice how the t-distributed data are more spread out. The df parameter, here named nu=2.5 controls how dispersed the data are. Lower values will give data that are more dispersed, large values approach a normal distribution.\nNow let’s fit a Gaussian glm (just a linear regression really) to both these data-sets\nm1_norm &lt;- glm(y_normdist ~ x)\nm1_t &lt;-  glm(y_tdist ~ x)\nWe should check whether the two models meet the normal assumption, using the standard ‘qq’ (quantile-quantile) plot:\npar(mfrow = c(1,2))\nplot(m1_norm, 2)\nplot(m1_t, 2)\n\nThese plots compare the theoretical quantiles to the actual quantiles of the residuals. If the points fall on the straight line then the theoretical and realised are very similar, and the assumption is met. Clearly this is not the case for the second model, which is overdispersed.\nWe know it is overdispersed because the theoretical quantiles are much smaller than the actual at the tails (notice how the ends down then up).\nThe p-values (or CIs if you use them) for m1_t are therefore likely biased and too narrow, leading potentially to type I errors (us saying that x affects y, which in fact it does not). In this case we know we haven’t made a type I error, because we made up the data. However, if you were using real data you wouldn’t be so sure.\n\n\n\nTo better understand the QQ plot it helps to generate it yourself, rather than using R’s automatic checks.\nFirst we calculate the model residuals (in plot(m1_t) R did this internally):\nm1_t_resid &lt;- y_tdist - predict(m1_t)\nThen we can plot the quantiles for the residuals against theoretical quantiles generated using qnorm. Below we also plot the original QQ plot from above, so you can see that our version is the same as R’s automatic one:\npar(mfrow = c(1,2))\nqqplot(qnorm(ppoints(n), sd = 1), m1_t_resid)\nqqline(m1_t_resid, lty = 2)\n\nplot(m1_t,2)\n\nI added the qqline for comparative purposes. It just puts a line through the 25th and 75th quantiles.\n\n\n\nNow we have learned how to write our own custom for a QQ plot, we can use it to check other types of non-normal data.\nHere we will fit a GLM to the y_tdist data using student-t distributed errors. I do this using the Bayesian package INLA.\nlibrary(INLA)\n\ndata &lt;- list(y=y_tdist, x = x)\nmod_tdist &lt;- inla(y ~ x, family=\"T\", data=data,\n    control.predictor = list(compute = TRUE),\n    control.family =\n        list(\n            hyper =\n                list(prec = list(prior=\"loggamma\",param=c(1,0.5)),\n                    dof = list(prior=\"loggamma\",param=c(1,0.5))\n                    )\n            )\n        )\nThe family =\"T\" command tells INLA to use the t-distribution, rather than the Normal distribution. Note also I have specified the priors using the control.family command. This is best practice. We need a prior for the precision (1/variance) and a prior for the dof (= degrees of freedom, which has to be &gt;2 in INLA).\nIt is sometimes help to visualise the priors, so we can check too see they look sensible. Here we visualise the prior for the dof, (which in INLA has a min value of 2):\nxgamma &lt;- seq(0.01, 10, length.out = 100)\nplot(xgamma+2, dgamma(xgamma, 1, 0.5), type = 'l', xlab = \"Quantile\", ylab = \"Density\")\n\nWe don’t really expect values much greater than 10, so this prior makes sense. If we used an old-school prior that was flat in 2-1000 we might get issues with model fitting.\nNow enough about priors. Let’s look at the estimated coefficients:\nmod_tdist$summary.fixed\n\n##                  mean         sd 0.025quant  0.5quant 0.975quant      mode\n## (Intercept) 0.5324814 0.07927198  0.3773399 0.5321649  0.6891779 0.5315490\n## x           0.7229362 0.08301006  0.5565746 0.7239544  0.8835630 0.7259817\n##                      kld\n## (Intercept) 3.067485e-12\n## x           6.557565e-12\nGood the CIs contain our true values, and the mean is close to our true value too. What about the hyper-parameters (the precision and DF)? We need to get INLA to run some more calucations to get accurate estimates of these:\nh_tdist &lt;- inla.hyperpar(mod_tdist)\nh_tdist$summary.hyperpar[,3:5]\n\n##                                          0.025quant  0.5quant 0.975quant\n## precision for the student-t observations  0.2663364 0.6293265   1.163440\n## degrees of freedom for student-t          2.2404966 2.7396391   4.459057\nThe estimate for the DF might be a ways off the mark. That is ok, we expect that, you need lots of really good data to get accurate estimates of hyper-parameters.\nNow, let’s use our skills in creating QQ plots to make QQ plot using theoretical quantiles from the t distribution.\nFirst step is to extract INLA’s predictions of the data, so we can calculate residuals\npreds &lt;- mod_tdist$summary.fitted.values\nresids &lt;- y_tdist - preds[,4]\nNext step is to extract the marginal estimates of the DF and precision to use when generating our QQ plot (the quantiles will change with the DF):\ntau_est &lt;- h_tdist$summary.hyperpar[1,4]\nnu_est &lt;- h_tdist$summary.hyperpar[2,4]\nNow we can use qt() to generate theoretical quantiles and the residuals for our realised quantiles:\nqqplot(qt(ppoints(n), df = nu_est), resids * sqrt(tau_est),\n    xlab = \"Theoretical quantile\", ylab = \"residuals\")\nqqline(resids * sqrt(tau_est), lty = 2)\n\nNote that I multiply the residuals by the sqrt of the precision estimate. This is how INLA fits a t-distributed GLM. I do the same for the qqline.\nOur residuals are now falling much closer to the line. The model is doing a much better job of fitting the data. You could also calculate the WAIC for this model and a Gaussian one, to compare the fits. The t-distributed GLM should have a lower WAIC (better fit).\nWe can now be confident that our CIs are accurate."
  },
  {
    "objectID": "posts/2017-10-06-qqplot-non-normal-glm/index.html#quantile-quantile-plots",
    "href": "posts/2017-10-06-qqplot-non-normal-glm/index.html#quantile-quantile-plots",
    "title": "Checking residual distributions for non-normal GLMs",
    "section": "",
    "text": "If you are fitting a linear regression with Gaussian (normally distributed) errors, then one of the standard checks is to make sure the residuals are approximately normally distributed.\nIt is a good idea to do these checks for non-normal GLMs too, to make sure your residuals approximate the model’s assumption.\nHere I explain how to create quantile-quantile plots for non-normal data, using an example of fitting a GLM using Student-t distributed errors. Such models can be appropriate when the residuals are overdispersed.\nFirst let’s create some data. We will make a linear predictor (ie the true regression line) eta and then simulate some data by adding residuals. We will simulate two data-sets that have the same linear predictor, but the first will have normally distributed errors and the second will have t distributed errors:\nn &lt;- 100\nphi &lt;- 0.85\nmu &lt;- 0.5\nset.seed(23)\nx &lt;- rnorm(n)\n\neta &lt;- mu + phi * x\nnu &lt;- 2.5\ntau &lt;- 3\n\ny_tdist &lt;- eta + (rt(n, df=nu)/sqrt(tau))\ny_normdist &lt;- eta + rnorm(n, sd = 1/sqrt(tau))\n\nplot(x, y_tdist)\npoints(x, y_normdist, col = \"red\", pch = 16, cex = 0.8)\nlegend(\"topleft\", legend = c(\"t-distributed errors\", \"normally distributed errors\"), pch = c(1,16), col = c(\"black\", \"red\"))\n\nNotice how the t-distributed data are more spread out. The df parameter, here named nu=2.5 controls how dispersed the data are. Lower values will give data that are more dispersed, large values approach a normal distribution.\nNow let’s fit a Gaussian glm (just a linear regression really) to both these data-sets\nm1_norm &lt;- glm(y_normdist ~ x)\nm1_t &lt;-  glm(y_tdist ~ x)\nWe should check whether the two models meet the normal assumption, using the standard ‘qq’ (quantile-quantile) plot:\npar(mfrow = c(1,2))\nplot(m1_norm, 2)\nplot(m1_t, 2)\n\nThese plots compare the theoretical quantiles to the actual quantiles of the residuals. If the points fall on the straight line then the theoretical and realised are very similar, and the assumption is met. Clearly this is not the case for the second model, which is overdispersed.\nWe know it is overdispersed because the theoretical quantiles are much smaller than the actual at the tails (notice how the ends down then up).\nThe p-values (or CIs if you use them) for m1_t are therefore likely biased and too narrow, leading potentially to type I errors (us saying that x affects y, which in fact it does not). In this case we know we haven’t made a type I error, because we made up the data. However, if you were using real data you wouldn’t be so sure."
  },
  {
    "objectID": "posts/2017-10-06-qqplot-non-normal-glm/index.html#doing-our-own-quantile-quantile-plot",
    "href": "posts/2017-10-06-qqplot-non-normal-glm/index.html#doing-our-own-quantile-quantile-plot",
    "title": "Checking residual distributions for non-normal GLMs",
    "section": "",
    "text": "To better understand the QQ plot it helps to generate it yourself, rather than using R’s automatic checks.\nFirst we calculate the model residuals (in plot(m1_t) R did this internally):\nm1_t_resid &lt;- y_tdist - predict(m1_t)\nThen we can plot the quantiles for the residuals against theoretical quantiles generated using qnorm. Below we also plot the original QQ plot from above, so you can see that our version is the same as R’s automatic one:\npar(mfrow = c(1,2))\nqqplot(qnorm(ppoints(n), sd = 1), m1_t_resid)\nqqline(m1_t_resid, lty = 2)\n\nplot(m1_t,2)\n\nI added the qqline for comparative purposes. It just puts a line through the 25th and 75th quantiles."
  },
  {
    "objectID": "posts/2017-10-06-qqplot-non-normal-glm/index.html#qq-plot-for-a-non-normal-glm",
    "href": "posts/2017-10-06-qqplot-non-normal-glm/index.html#qq-plot-for-a-non-normal-glm",
    "title": "Checking residual distributions for non-normal GLMs",
    "section": "",
    "text": "Now we have learned how to write our own custom for a QQ plot, we can use it to check other types of non-normal data.\nHere we will fit a GLM to the y_tdist data using student-t distributed errors. I do this using the Bayesian package INLA.\nlibrary(INLA)\n\ndata &lt;- list(y=y_tdist, x = x)\nmod_tdist &lt;- inla(y ~ x, family=\"T\", data=data,\n    control.predictor = list(compute = TRUE),\n    control.family =\n        list(\n            hyper =\n                list(prec = list(prior=\"loggamma\",param=c(1,0.5)),\n                    dof = list(prior=\"loggamma\",param=c(1,0.5))\n                    )\n            )\n        )\nThe family =\"T\" command tells INLA to use the t-distribution, rather than the Normal distribution. Note also I have specified the priors using the control.family command. This is best practice. We need a prior for the precision (1/variance) and a prior for the dof (= degrees of freedom, which has to be &gt;2 in INLA).\nIt is sometimes help to visualise the priors, so we can check too see they look sensible. Here we visualise the prior for the dof, (which in INLA has a min value of 2):\nxgamma &lt;- seq(0.01, 10, length.out = 100)\nplot(xgamma+2, dgamma(xgamma, 1, 0.5), type = 'l', xlab = \"Quantile\", ylab = \"Density\")\n\nWe don’t really expect values much greater than 10, so this prior makes sense. If we used an old-school prior that was flat in 2-1000 we might get issues with model fitting.\nNow enough about priors. Let’s look at the estimated coefficients:\nmod_tdist$summary.fixed\n\n##                  mean         sd 0.025quant  0.5quant 0.975quant      mode\n## (Intercept) 0.5324814 0.07927198  0.3773399 0.5321649  0.6891779 0.5315490\n## x           0.7229362 0.08301006  0.5565746 0.7239544  0.8835630 0.7259817\n##                      kld\n## (Intercept) 3.067485e-12\n## x           6.557565e-12\nGood the CIs contain our true values, and the mean is close to our true value too. What about the hyper-parameters (the precision and DF)? We need to get INLA to run some more calucations to get accurate estimates of these:\nh_tdist &lt;- inla.hyperpar(mod_tdist)\nh_tdist$summary.hyperpar[,3:5]\n\n##                                          0.025quant  0.5quant 0.975quant\n## precision for the student-t observations  0.2663364 0.6293265   1.163440\n## degrees of freedom for student-t          2.2404966 2.7396391   4.459057\nThe estimate for the DF might be a ways off the mark. That is ok, we expect that, you need lots of really good data to get accurate estimates of hyper-parameters.\nNow, let’s use our skills in creating QQ plots to make QQ plot using theoretical quantiles from the t distribution.\nFirst step is to extract INLA’s predictions of the data, so we can calculate residuals\npreds &lt;- mod_tdist$summary.fitted.values\nresids &lt;- y_tdist - preds[,4]\nNext step is to extract the marginal estimates of the DF and precision to use when generating our QQ plot (the quantiles will change with the DF):\ntau_est &lt;- h_tdist$summary.hyperpar[1,4]\nnu_est &lt;- h_tdist$summary.hyperpar[2,4]\nNow we can use qt() to generate theoretical quantiles and the residuals for our realised quantiles:\nqqplot(qt(ppoints(n), df = nu_est), resids * sqrt(tau_est),\n    xlab = \"Theoretical quantile\", ylab = \"residuals\")\nqqline(resids * sqrt(tau_est), lty = 2)\n\nNote that I multiply the residuals by the sqrt of the precision estimate. This is how INLA fits a t-distributed GLM. I do the same for the qqline.\nOur residuals are now falling much closer to the line. The model is doing a much better job of fitting the data. You could also calculate the WAIC for this model and a Gaussian one, to compare the fits. The t-distributed GLM should have a lower WAIC (better fit).\nWe can now be confident that our CIs are accurate."
  },
  {
    "objectID": "posts/2017-07-30-peoples-choice-dataviz/index.html",
    "href": "posts/2017-07-30-peoples-choice-dataviz/index.html",
    "title": "Memorable dataviz with the R program, talk awarded people’s choice prize",
    "section": "",
    "text": "For the past two years Dr Nick Hamilton has invited me to give a talk on creating data visuals with the R program at the wonderful UQ Winterschool in Bioinformatics.\nThis year I was lucky enough to be awarded a prize for my talk - best speaker from a mid-career presenter, as voted by the audience.\nNick and the UQ Winterschool team have been kind enough to post my talk on Vimeo, so I am sharing it here in the hope that others find it useful. You can also get all the talk notes (and code) on my blog here\nI think it is something of a feat to have a talk win a people’s choice award, when that talk is fundamentally about computer programming. The talk’s success speaks not to my own presentation skill, but the rather the huge interest in the R program.\nI also borrowed a lot of neat ideas from people much more smarter than me (all duly acknowledged), like the Datasaurus (thanks Alberto Cairo). So check out the video, I hope you find it insightful, and a bit entertaining."
  },
  {
    "objectID": "posts/2017-07-30-peoples-choice-dataviz/index.html#memorable-dataviz-with-the-r-program-awarded-peoples-choice-prize",
    "href": "posts/2017-07-30-peoples-choice-dataviz/index.html#memorable-dataviz-with-the-r-program-awarded-peoples-choice-prize",
    "title": "Memorable dataviz with the R program, talk awarded people’s choice prize",
    "section": "",
    "text": "For the past two years Dr Nick Hamilton has invited me to give a talk on creating data visuals with the R program at the wonderful UQ Winterschool in Bioinformatics.\nThis year I was lucky enough to be awarded a prize for my talk - best speaker from a mid-career presenter, as voted by the audience.\nNick and the UQ Winterschool team have been kind enough to post my talk on Vimeo, so I am sharing it here in the hope that others find it useful. You can also get all the talk notes (and code) on my blog here\nI think it is something of a feat to have a talk win a people’s choice award, when that talk is fundamentally about computer programming. The talk’s success speaks not to my own presentation skill, but the rather the huge interest in the R program.\nI also borrowed a lot of neat ideas from people much more smarter than me (all duly acknowledged), like the Datasaurus (thanks Alberto Cairo). So check out the video, I hope you find it insightful, and a bit entertaining."
  },
  {
    "objectID": "posts/2021-08-04-estimating-iucn-declines/index.html",
    "href": "posts/2021-08-04-estimating-iucn-declines/index.html",
    "title": "Predicting global emissions from mangrove deforestation",
    "section": "",
    "text": "In a new study we predict global carbon emissions from different drives of mangrove deforestation. Our predictions can help nations value mangrove conservation, identify what actions are needed to reduce carbon emissions and set targets for reducing carbon emissions.\n\nPredicted emissions to 2090 and primary drivers of emissions\nAnnouncing here that the R code for the study is now available on github.\nWe went an extra step with this model to also produce an interactive web app. So if you want to explore this model, without having to use R, then check out our interactive web app.\nYou can learn more about the study, and watch a tutorial on the web app in this online seminar."
  },
  {
    "objectID": "posts/2015-12-03-marine_reserve_objectives/index.html",
    "href": "posts/2015-12-03-marine_reserve_objectives/index.html",
    "title": "Are more marine protected areas needed to protect ocean biodiversity?",
    "section": "",
    "text": "Are more marine protected areas needed to protect ocean biodiversity?\nMarine protected areas are a primary tool for conserving marine ecosystems, habitats and species. For instance, the Convention on Biological Diversity, has 193 signatory nations committing to protecting more than 10% of marine environments by 2020. We have recently shown that there are a large number of species that are not protected or inadequately protected in the global protected area system. These ‘gaps’ will be challenging to fill by 2020.\n\n\n\nDensity map of species not receiving any protection from protected areas, according to our recent analysis \n\n\nNew marine reserves are always contentious. Marine reserves are areas closed to fishing to protect native plants and animals, so protection often comes at a cost to local fishing industries who will be excluded from fishing in some places. For instance, Australia’s Great Barrier Reef Marine Protected Area was rezoned in 2004 to have 33.3% of its area as no fishing zones. The process was highly contentious and cost the Federal Government ~$250 million in compensation pay-outs to fisheries. Clearly, balancing the needs of conservation with its impacts on fisheries is important for both people and economies.\n\n\n\nConserving ecosystems, like this coral reef, sustaining productive fisheries and restoring depleted fisheries are all potentially competing objectives.\n\n\nOur recently analysis makes a start at quantifying the shortfall in conservation, but does not consider the cost of meeting that shortfall for economies, society and industries, such as fisheries. Balancing multiple ecological and societal objectives is tricky. Doing it properly requires government and stakeholders to be clear about the objectives of management and then applying the right management tools to achieve those objectives.\nMarine protected areas are one tool for balanced management, but not the only tool. In many cases, sustainable management of fisheries may do a better job at both protecting ecosystems, while still allowing profitable fisheries to operate. The reason being that marine reserves on their own (without buy-backs to reduce the number of boats) squeeze fishers into a smaller operating space, and thus, can actually increase the impact of fishing on marine ecosystems.\nIn some cases marine protected areas will be the best tool balancing ecosystem objectives with fisheries. For instance, they may often be easier to enforce than catch limits. Hence their popularity as a management tool in many community based initiatives.\nClearly marine ecosystems are under-protected globally. But in meeting the shortfall we need to be careful that we choose the right tools and consider the costs of conservation to people. In choosing the right tools, we can help to avoid contentious debates on protection. In the long-run protecting ecosystems is not only good for biodiversity, but it can help sustain fisheries too.\nFurther reading\nGet the pdf\nBrown CJ, Mumby PJ. Trade‐offs between fisheries and the conservation of ecosystem function are defined by management strategy. Frontiers in Ecology and the Environment. 2014 Aug 1;12(6):324-9.\nKlein CJ, Brown CJ, Halpern BS, Segan DB, McGowan J, Beger M, Watson JE. Shortfalls in the global protected area network at representing marine biodiversity. Scientific reports. 2015;5.\nMacintosh A, Bonyhady T, Wilkinson D. Dealing with interests displaced by marine protected areas: a case study on the Great Barrier Reef Marine Park Structural Adjustment Package. Ocean & Coastal Management. 2010 Sep 30;53(9):581-8."
  },
  {
    "objectID": "posts/2020-02-08-calculating-distances-in-R/index.html",
    "href": "posts/2020-02-08-calculating-distances-in-R/index.html",
    "title": "Three ways to calculate distances in R",
    "section": "",
    "text": "Calculating a distance on a map sounds straightforward, but it can be confusing how many different ways there are to do this in R.\nThis complexity arises because there are different ways of defining ‘distance’ on the Earth’s surface.\nThe Earth is spherical. So do you want to calculate distances around the sphere (‘great circle distances’) or distances on a map (‘Euclidean distances’).\nThen there are barriers. For example, for distances in the ocean, we often want to know the nearest distance around islands.\nThen there is the added complexity of the different spatial data types. Here we will just look at points, but these same concepts apply to other data types, like shapes.\n\n\nLet’s look at some example data. It is just a series of points across the island of Tasmania. We are going to calculate how far apart these points are from each other.\nWe’ll use sf for spatial data and tmap for mapping.\nHere’s a map:\ntm_shape(stas) +\n  tm_polygons() +\n  tm_graticules(col = \"grey60\") +\n  tm_shape(pts) +\n  tm_symbols(col = \"black\") +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_shape(pts) +\n  tm_text(\"pt\", ymod = -1)\n\nNote I’ve included a scale bar, but of course the distance between longitude lines gets closer at higher latitudes.\n\n\n\nThe first method is to calculate great circle distances, that account for the curvature of the earth. If we use st_distance() with unprojected coordinates (ie in lon-lat) then we get great circle distances (in metres).\nm &lt;- st_distance(pts)\nm/1000\n\n## Units: [m]\n##          [,1]     [,2]      [,3]\n## [1,]    0.000 821.5470 1200.7406\n## [2,]  821.547   0.0000  419.5004\n## [3,] 1200.741 419.5004    0.0000\nThe matrix m gives the distances between points (we divided by 1000 to get distances in KM).\n\n\n\nAnother option is to first project the points to a projection that preserves distances and then calculate the distances. This option is computationally faster, but can be less accurate, as we will see.\nWe will use the local UTM projection. So you can see what this looks like, we will project the land too.\ntas_utm &lt;- st_crs(\"+proj=utm +zone=55 +datum=WGS84 +units=m +no_defs\")\nstas2 &lt;- st_transform(stas, crs = tas_utm)\npts2 &lt;- st_transform(pts, crs = tas_utm)\n\ntm_shape(stas2) +\n  tm_polygons() +\n  tm_graticules(col = \"grey60\") +\n  tm_shape(pts2) +\n  tm_symbols(col = \"black\") +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_shape(pts) +\n  tm_text(\"pt\", ymod = -1)\n\nNote how it now bends the lat/long lines. This happens because we are projecting a sphere onto a flat surface. The UTM will be most accurate at the centre of its zone (we used Zone 55 which is approximately centred on Tasmania).\nIf we were interested in mapping the mainland of Australia accurately, we’d use a different UTM zone.\nNow we can calculate Euclidean distances:\nm2 &lt;- st_distance(pts2)\nm2/1000\n\n## Units: [m]\n##           [,1]     [,2]      [,3]\n## [1,]    0.0000 824.8996 1203.6228\n## [2,]  824.8996   0.0000  419.4163\n## [3,] 1203.6228 419.4163    0.0000\nCompare these to our great circle distances:\nm/1000\n\n## Units: [m]\n##          [,1]     [,2]      [,3]\n## [1,]    0.000 821.5470 1200.7406\n## [2,]  821.547   0.0000  419.5004\n## [3,] 1200.741 419.5004    0.0000\nNote the slight differences, particularly between point 1 and the other points. The first method (great circle) is the more accurate one, but is also a bit slower. The Euclidean distances become a bit inaccurate for point 1, because it is so far outside the zone of the UTM projection.\nPoints 2 & 3 are within the UTM zone, so the distance between these points is almost identical to the great circle calculation.\n\n\n\nThe basic idea here is that we turn the data into a raster grid and then use the gridDistance() function to calculate distances around barriers (land) between points.\nSo first we need to rasterize the land. The package fasterize has a fast way to turn sf polygons into land:\nlibrary(fasterize)\nlibrary(raster)\nlibrary(dplyr)\nr &lt;- raster(extent(stas2), nrows = 50, ncols = 50)\nrtas &lt;- fasterize(summarize(stas2), r)\nI made the raster pretty blocky (50 x 50). You could increase the resolution to improve the accuracy of the distance measurements. Here’s how it looks:\n\nNow we need to identify the raster cell’s where the points fall. We do this by extracting coordinates from pts2 and asking for their unique raster cell numbers:\nrtas_pts &lt;- rtas\nxy &lt;- st_coordinates(pts2)\nicell &lt;- cellFromXY(rtas, xy)\nNow, we set the cells of our raster corresponding to the points to a different number than the rest. I will just use the 3rd point (if we used all points then we get nearest distance around barriers to any point).\nrtas_pts[icell[3]] &lt;- 2\nThis will look like the same raster, but with a spot where the 3rd point fell (note red box):\n\nNow just run gridDistance telling it to calculate distances from the cells with a value of 2 (just one cell in this case) and omit values of 1 (land) when doing the distances:\nd &lt;- gridDistance(rtas_pts, origin = 2,\n                  omit = 1)/1000\nThis will be slow for larger rasters (or very high res). Let’s see how it looks:\n\nColours correspond to distances from point 3 (the location we gave a value of ‘2’ to in the raster).\nNow we can just ask for the distance values at the cells of the other points:\nd[icell]\n\n## [1] 1310.5141  612.1404    0.0000\nSo 612 km around Tasmania from point 3 to 2, as the dolphin swims. It was only 419 km if we could fly straight over Tasmania:\nm[2,3]/1000\n\n## 419.5004 [m]\n(note is says metres, but that is because R hasn’t remembered we’ve divided by 1000)"
  },
  {
    "objectID": "posts/2020-02-08-calculating-distances-in-R/index.html#example-data",
    "href": "posts/2020-02-08-calculating-distances-in-R/index.html#example-data",
    "title": "Three ways to calculate distances in R",
    "section": "",
    "text": "Let’s look at some example data. It is just a series of points across the island of Tasmania. We are going to calculate how far apart these points are from each other.\nWe’ll use sf for spatial data and tmap for mapping.\nHere’s a map:\ntm_shape(stas) +\n  tm_polygons() +\n  tm_graticules(col = \"grey60\") +\n  tm_shape(pts) +\n  tm_symbols(col = \"black\") +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_shape(pts) +\n  tm_text(\"pt\", ymod = -1)\n\nNote I’ve included a scale bar, but of course the distance between longitude lines gets closer at higher latitudes."
  },
  {
    "objectID": "posts/2020-02-08-calculating-distances-in-R/index.html#great-circle-distances",
    "href": "posts/2020-02-08-calculating-distances-in-R/index.html#great-circle-distances",
    "title": "Three ways to calculate distances in R",
    "section": "",
    "text": "The first method is to calculate great circle distances, that account for the curvature of the earth. If we use st_distance() with unprojected coordinates (ie in lon-lat) then we get great circle distances (in metres).\nm &lt;- st_distance(pts)\nm/1000\n\n## Units: [m]\n##          [,1]     [,2]      [,3]\n## [1,]    0.000 821.5470 1200.7406\n## [2,]  821.547   0.0000  419.5004\n## [3,] 1200.741 419.5004    0.0000\nThe matrix m gives the distances between points (we divided by 1000 to get distances in KM)."
  },
  {
    "objectID": "posts/2020-02-08-calculating-distances-in-R/index.html#euclidean-distances",
    "href": "posts/2020-02-08-calculating-distances-in-R/index.html#euclidean-distances",
    "title": "Three ways to calculate distances in R",
    "section": "",
    "text": "Another option is to first project the points to a projection that preserves distances and then calculate the distances. This option is computationally faster, but can be less accurate, as we will see.\nWe will use the local UTM projection. So you can see what this looks like, we will project the land too.\ntas_utm &lt;- st_crs(\"+proj=utm +zone=55 +datum=WGS84 +units=m +no_defs\")\nstas2 &lt;- st_transform(stas, crs = tas_utm)\npts2 &lt;- st_transform(pts, crs = tas_utm)\n\ntm_shape(stas2) +\n  tm_polygons() +\n  tm_graticules(col = \"grey60\") +\n  tm_shape(pts2) +\n  tm_symbols(col = \"black\") +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_shape(pts) +\n  tm_text(\"pt\", ymod = -1)\n\nNote how it now bends the lat/long lines. This happens because we are projecting a sphere onto a flat surface. The UTM will be most accurate at the centre of its zone (we used Zone 55 which is approximately centred on Tasmania).\nIf we were interested in mapping the mainland of Australia accurately, we’d use a different UTM zone.\nNow we can calculate Euclidean distances:\nm2 &lt;- st_distance(pts2)\nm2/1000\n\n## Units: [m]\n##           [,1]     [,2]      [,3]\n## [1,]    0.0000 824.8996 1203.6228\n## [2,]  824.8996   0.0000  419.4163\n## [3,] 1203.6228 419.4163    0.0000\nCompare these to our great circle distances:\nm/1000\n\n## Units: [m]\n##          [,1]     [,2]      [,3]\n## [1,]    0.000 821.5470 1200.7406\n## [2,]  821.547   0.0000  419.5004\n## [3,] 1200.741 419.5004    0.0000\nNote the slight differences, particularly between point 1 and the other points. The first method (great circle) is the more accurate one, but is also a bit slower. The Euclidean distances become a bit inaccurate for point 1, because it is so far outside the zone of the UTM projection.\nPoints 2 & 3 are within the UTM zone, so the distance between these points is almost identical to the great circle calculation."
  },
  {
    "objectID": "posts/2020-02-08-calculating-distances-in-R/index.html#distances-around-a-barrier",
    "href": "posts/2020-02-08-calculating-distances-in-R/index.html#distances-around-a-barrier",
    "title": "Three ways to calculate distances in R",
    "section": "",
    "text": "The basic idea here is that we turn the data into a raster grid and then use the gridDistance() function to calculate distances around barriers (land) between points.\nSo first we need to rasterize the land. The package fasterize has a fast way to turn sf polygons into land:\nlibrary(fasterize)\nlibrary(raster)\nlibrary(dplyr)\nr &lt;- raster(extent(stas2), nrows = 50, ncols = 50)\nrtas &lt;- fasterize(summarize(stas2), r)\nI made the raster pretty blocky (50 x 50). You could increase the resolution to improve the accuracy of the distance measurements. Here’s how it looks:\n\nNow we need to identify the raster cell’s where the points fall. We do this by extracting coordinates from pts2 and asking for their unique raster cell numbers:\nrtas_pts &lt;- rtas\nxy &lt;- st_coordinates(pts2)\nicell &lt;- cellFromXY(rtas, xy)\nNow, we set the cells of our raster corresponding to the points to a different number than the rest. I will just use the 3rd point (if we used all points then we get nearest distance around barriers to any point).\nrtas_pts[icell[3]] &lt;- 2\nThis will look like the same raster, but with a spot where the 3rd point fell (note red box):\n\nNow just run gridDistance telling it to calculate distances from the cells with a value of 2 (just one cell in this case) and omit values of 1 (land) when doing the distances:\nd &lt;- gridDistance(rtas_pts, origin = 2,\n                  omit = 1)/1000\nThis will be slow for larger rasters (or very high res). Let’s see how it looks:\n\nColours correspond to distances from point 3 (the location we gave a value of ‘2’ to in the raster).\nNow we can just ask for the distance values at the cells of the other points:\nd[icell]\n\n## [1] 1310.5141  612.1404    0.0000\nSo 612 km around Tasmania from point 3 to 2, as the dolphin swims. It was only 419 km if we could fly straight over Tasmania:\nm[2,3]/1000\n\n## 419.5004 [m]\n(note is says metres, but that is because R hasn’t remembered we’ve divided by 1000)"
  },
  {
    "objectID": "posts/2015-08-20-tall-poppy-award/2015-08-20-tall-poppy-award.html",
    "href": "posts/2015-08-20-tall-poppy-award/2015-08-20-tall-poppy-award.html",
    "title": "Tall Young Poppy Award",
    "section": "",
    "text": "Award for Scientific Research and Communcation\n\n\nI am honoured to receive a Young Tall Poppy Award for Scientific Research and Communication.\n\n\nYou can read our press release below.\n\n\n\n\n\nResearch seeking solutions to the potential conflict between marine ecosystems and commercial and recreational fishing last night (August 20) earned Griffith University’s Dr Chris Brown a 2015 Young Tall Poppy Science Award.\n\n\nAn initiative of the Australian Institute of Policy and Science, awards were presented to young researchers by Queensland’s Minister for Science and Innovation, The Honourable Ms Leeanne Enoch MP, as part of the annual Science in Parliament program in Brisbane.\n\n\nA member of Griffith’s world-leading Australian Rivers Institute, Dr Brown’s research is focused on striking a sustainable balance between the importance of fish for human consumption as well as to the health of marine ecosystems.\n\n\n“Fishing is regulated in two ways: marine parks, which protect ecosystems; and catch limits, which protect fisheries,” says Dr Brown.\n\n\n“For instance, on the Great Barrier Reef, which supports about 63,000 jobs, there are numerous areas where fishing is banned and there are bag limits for recreational fishers.”\n\n\n“Parks are great because they ensure a few places are near pristine and have many fish. However, the problem is that parks don’t protect fish outside their boundaries.”\n\n\n“In fact, extensive parks can actually concentrate fishing in outside areas.”\n\n\nDr Brown’s research shows that bag limits are just as important for healthy ecosystems because they apply everywhere and thus help protect the health of ecosystems outside park boundaries.\n\n\n“The right combination of bag limits and parks can ensure we have healthy ecosystems and fish to catch,” says Dr Brown.\n\n\n“In other words, ecosystems can have their fish, and we can eat them too.”\n\n\nMore details on the research:\n\n\nTrade-offs between fisheries and the conservation of ecosystem function are defined by management strategy (2014) Brown, Mumby Frontiers in Ecology and the Environment"
  },
  {
    "objectID": "posts/2016-07-07-SCBO2016-talk/index.html",
    "href": "posts/2016-07-07-SCBO2016-talk/index.html",
    "title": "More info on my talk at SCBO 2016",
    "section": "",
    "text": "8 July 2016\nThis week I’m at the Society for Conservation Biology, Oceanea Conference in Brisbane.\nI am speaking about our working group on Ridge to Reef planning for fisheries, in particular the work we are doing in Fiji.\nOne of the major challenges in ridge to reef planning is figuring out where the pollution you observe impacting a reef comes from. This problem is often solved using lots of data and modelling. However, such approaches are expensive and time consuming. We didn’t have the luxury of a lot of time or money, because in Fiji there is a limited window of opportunity to inform a major planning process.\nThe time-constraint forced us to come up with a new solution to the problem of tracing sediment to its sources. In brief we used a Bayesian model to uncover latent connections between ridge and reef ecosystems. The model relies on freely available satellite data and some commonly available local data for validation. It lets us link impacts in the ocean to their causes on land. We can use this model to help inform planning about where to protect forest, or restore forest on land.\nI have submitted the ms, but am keen to get feed-back. You can read the pre-review ms here.\nI also mentioned some work about conservation-fisheries trade-offs in my talk. The open access article on that can be found here."
  },
  {
    "objectID": "posts/2016-07-07-SCBO2016-talk/index.html#more-info-on-my-talk-at-scbo-2016",
    "href": "posts/2016-07-07-SCBO2016-talk/index.html#more-info-on-my-talk-at-scbo-2016",
    "title": "More info on my talk at SCBO 2016",
    "section": "",
    "text": "8 July 2016\nThis week I’m at the Society for Conservation Biology, Oceanea Conference in Brisbane.\nI am speaking about our working group on Ridge to Reef planning for fisheries, in particular the work we are doing in Fiji.\nOne of the major challenges in ridge to reef planning is figuring out where the pollution you observe impacting a reef comes from. This problem is often solved using lots of data and modelling. However, such approaches are expensive and time consuming. We didn’t have the luxury of a lot of time or money, because in Fiji there is a limited window of opportunity to inform a major planning process.\nThe time-constraint forced us to come up with a new solution to the problem of tracing sediment to its sources. In brief we used a Bayesian model to uncover latent connections between ridge and reef ecosystems. The model relies on freely available satellite data and some commonly available local data for validation. It lets us link impacts in the ocean to their causes on land. We can use this model to help inform planning about where to protect forest, or restore forest on land.\nI have submitted the ms, but am keen to get feed-back. You can read the pre-review ms here.\nI also mentioned some work about conservation-fisheries trade-offs in my talk. The open access article on that can be found here."
  },
  {
    "objectID": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html",
    "href": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html",
    "title": "The ellmer package for using LLMs with R is a game changer for scientists",
    "section": "",
    "text": "In this tutorial we’ll look at how we can access LLM agents through API calls. We’ll use this skill for created structued data from documents.\nWe’ll use the R ellmer package (launched 25th Feb 2025).\nThere are a few package options (I was also using tidychatmodels before).\nellmer is a game changer for scientists using R: It supports tool use and has functions for structured data.\nBefore ellmer you had to know other languages and data-structures, like JSON. Ellmer means that many powerful LLM uses are now easily accessible to R users.\nTool use means the LLM can run commands on your computer to retrieve information. You know how LLMs can be bad at simple math like 2+2 or today’s date? Well, with a tool, the LLM would know to use R to calculate this to return the correct answer. Tools can also connect to web APIs, means they can also be used to retrieve information or databases from the web.\nThe functions that assist you in creating structured data from text are also important.\nFor instance, by combining tool use with structured data extraction, ellmer could be used to synthesize literature for a quantitative review. We’ll cover that here.\nAnother application would be to interpret the meta-data from an online database, download the data, write and run the analysis and then write up the findings.\nBecause you are running the LLM from R it means you can batch process prompts, such as asking it to summarize many documents.\nThey are a couple of obvious uses. I’m sure people will soon come up with many more.\nLet’s see how to use ellmer to extract structured data from peer reviewed studies.\n\n\nFirst, you need to get an API key from the provider. Login to the provider’s website and follow the instructions.\nThen, you need to add the key to your .Renviron file:\nusethis::edit_r_environ()\nThen type in your key like this:\nANTHROPIC_API_KEY=\"xxxxxx\"\nThen restart R. ellmer will automatically find your key so long as you use the recommended envirment variable names. See ?ellmer::chat_claude (or chat_xxx where xxx is whatever provider you are using).\n\n\n\nPosit recommends this is useful for experimenting with new prompts. It can also be better than the web interfaces you might be used to. This is because, unlike on the web interfaces, you can control the system message.\nMost models have three types of messages:\nsystem, which sets the context, role and is given higher priority than the user message\nuser, which is your conversation or request\nassistant, which is the model’s response\nMany prompt engineer pros have found better results if you put as much of your request in the system as possible. So I recommend you play around with this yourself.\nlibrary(ellmer)\n\nchat &lt;- chat_claude(\n  system_prompt = \"You are a friendly but terse assistant.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nlive_console(chat)\nlive_browser(chat)\nTry this but experiment with a new system prompts. Here’s an example, try your own.\nchat &lt;- chat_claude(\n  system_prompt = \"You are a mystic with expertise in R programming. You answer questions indirectly in riddles and rhymes.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nlive_console(chat)\n\nUser: how can I do a glm in R?\n\n*Swirling mystical orb glows with R code*\n\nThrough statistics' mist, a model takes flight,\nglm() is the spell to cast in the night,\nWith family bound, be it gaussian or more,\nAnd formula written as y ~ x4.\n...\n\n\n\nSo once you’ve tested your prompt you can run it like this:\nchat &lt;- chat_claude(\n  system_prompt = \"You are a surfy dude who likes waves, tubes and statistics.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nchat$chat(\"How can I do a glm in R?\")\n‘… Hang ten with your statistical surfboard! 🏄‍♂️📊’\nThat was cheesy…"
  },
  {
    "objectID": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#why-is-ellmer-a-game-changer-for-scientists",
    "href": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#why-is-ellmer-a-game-changer-for-scientists",
    "title": "The ellmer package for using LLMs with R is a game changer for scientists",
    "section": "",
    "text": "In this tutorial we’ll look at how we can access LLM agents through API calls. We’ll use this skill for created structued data from documents.\nWe’ll use the R ellmer package (launched 25th Feb 2025).\nThere are a few package options (I was also using tidychatmodels before).\nellmer is a game changer for scientists using R: It supports tool use and has functions for structured data.\nBefore ellmer you had to know other languages and data-structures, like JSON. Ellmer means that many powerful LLM uses are now easily accessible to R users.\nTool use means the LLM can run commands on your computer to retrieve information. You know how LLMs can be bad at simple math like 2+2 or today’s date? Well, with a tool, the LLM would know to use R to calculate this to return the correct answer. Tools can also connect to web APIs, means they can also be used to retrieve information or databases from the web.\nThe functions that assist you in creating structured data from text are also important.\nFor instance, by combining tool use with structured data extraction, ellmer could be used to synthesize literature for a quantitative review. We’ll cover that here.\nAnother application would be to interpret the meta-data from an online database, download the data, write and run the analysis and then write up the findings.\nBecause you are running the LLM from R it means you can batch process prompts, such as asking it to summarize many documents.\nThey are a couple of obvious uses. I’m sure people will soon come up with many more.\nLet’s see how to use ellmer to extract structured data from peer reviewed studies.\n\n\nFirst, you need to get an API key from the provider. Login to the provider’s website and follow the instructions.\nThen, you need to add the key to your .Renviron file:\nusethis::edit_r_environ()\nThen type in your key like this:\nANTHROPIC_API_KEY=\"xxxxxx\"\nThen restart R. ellmer will automatically find your key so long as you use the recommended envirment variable names. See ?ellmer::chat_claude (or chat_xxx where xxx is whatever provider you are using).\n\n\n\nPosit recommends this is useful for experimenting with new prompts. It can also be better than the web interfaces you might be used to. This is because, unlike on the web interfaces, you can control the system message.\nMost models have three types of messages:\nsystem, which sets the context, role and is given higher priority than the user message\nuser, which is your conversation or request\nassistant, which is the model’s response\nMany prompt engineer pros have found better results if you put as much of your request in the system as possible. So I recommend you play around with this yourself.\nlibrary(ellmer)\n\nchat &lt;- chat_claude(\n  system_prompt = \"You are a friendly but terse assistant.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nlive_console(chat)\nlive_browser(chat)\nTry this but experiment with a new system prompts. Here’s an example, try your own.\nchat &lt;- chat_claude(\n  system_prompt = \"You are a mystic with expertise in R programming. You answer questions indirectly in riddles and rhymes.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nlive_console(chat)\n\nUser: how can I do a glm in R?\n\n*Swirling mystical orb glows with R code*\n\nThrough statistics' mist, a model takes flight,\nglm() is the spell to cast in the night,\nWith family bound, be it gaussian or more,\nAnd formula written as y ~ x4.\n...\n\n\n\nSo once you’ve tested your prompt you can run it like this:\nchat &lt;- chat_claude(\n  system_prompt = \"You are a surfy dude who likes waves, tubes and statistics.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nchat$chat(\"How can I do a glm in R?\")\n‘… Hang ten with your statistical surfboard! 🏄‍♂️📊’\nThat was cheesy…"
  },
  {
    "objectID": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#example-clean-pdfs-text-and-summarize",
    "href": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#example-clean-pdfs-text-and-summarize",
    "title": "The ellmer package for using LLMs with R is a game changer for scientists",
    "section": "Example: Clean pdfs text and summarize",
    "text": "Example: Clean pdfs text and summarize\nNow let’s see if we can use ellmer to clean up some text from a pdf and summarize it. ellmer has some handy functions for processing pdfs to text, so they can then be fed into prompts.\nI’m going to attempt to summarize my recent paper on turtle fishing.\nx &lt;- content_pdf_url(\"https://conbio.onlinelibrary.wiley.com/doi/epdf/10.1111/conl.13056\")\nThis fails with a 403 error. This means the server is blocking the request, it probably guesses (correctly) that I’m calling the pdf programmatically: it thinks I’m a bot (which this tutorial kind of is creating).\nWe can also try with a file on our hard drive, we just have to manually download the pdf.\nmypdf &lt;- content_pdf_file(\"pdf-examples/Brown_etal2024 national scale turtle mortality.pdf\")\nThat works, now let’s use it within a chat. First set-up our chat:\nchat &lt;- chat_claude(\n  system_prompt = \"You are a research assistant who specializes in extracting structured data from scientific papers.\",\n  model = \"claude-3-5-haiku-20241022\", \n  max_tokens = 1000\n)\n\nNow, we can use ellmer’s functions for specifying structured data. Many LLMs can be used to generate data in the JSON format (they were specifically trained with that in mind).\nellmer handles the conversion from JSON to R objects that are easier for us R users to understand.\nYou use the type_object then type_number, type_string etc.. to specify the types of data. Read more in the ellmer package vignettes\npaper_stats &lt;- type_object(\n  sample_size = type_number(\"Sample size of the study\"),\n  year_of_study = type_number(\"Year data was collected\"),\n  method = type_string(\"Summary of statistical method, one paragraph max\")\n)\n\nFinally, we send the request for a summary to the provider:\nturtle_study &lt;- chat$extract_data(mypdf, type = paper_stats)\nThe turtle_study object will contain the structured data from the pdf. I think (the ellmer documentation is a bit sparse on implementation details) ellmer is converting a JSON that comes from the LLM to a friendly R list.\nclass(turtle_study)\n#list\nAnd:\nturtle_study$sample_size\n#11935\nturtle_study$year_of_study\n#2018\nturtle_study$method\n#The study estimated national-scale turtle catches for two fisheries in the Solomon Islands \n#- a small-scale reef fishery and a tuna longline fishery - using community surveys and \n#electronic monitoring. The researchers used nonparametric bootstrapping to scale up \n#catch data and calculate national-level estimates with confidence intervals.\nIt works, but like any structured lit review you need to be careful what questions you ask. Even more so with an LLM as you are not reading the paper and understanding the context.\nIn this case the sample size its given us is the estimated number of turtles caught. This was a model output, not a sample size. In fact this paper has several methods with different sample sizes. So some work would be needed to fine-tune the prompt, especially if you are batch processing many papers.\nYou should also experiment with models, I used Claude haiku because its cheap, but Claude sonnet would probably be more accurate."
  },
  {
    "objectID": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#batch-processing-prompts",
    "href": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#batch-processing-prompts",
    "title": "The ellmer package for using LLMs with R is a game changer for scientists",
    "section": "Batch processing prompts",
    "text": "Batch processing prompts\nLet’s try this with a batch of papers (here I’ll just use two). For this example I’ll just use two abstracts, which I’ve obtained as plain text. The first is from another study on turtle catch in Madagascar. The second is from my study above.\nWhat we’ll do is create a function that reads in the text, then passes it to the LLM, using the request for structured data from above.\n  process_abstract &lt;- function(file_path, chat) {\n  # Read in the text file\n  abstract_text &lt;- readLines(file_path, warn = FALSE)\n  \n  # Extract data from the abstract\n  result &lt;- chat$extract_data(abstract_text, type = paper_stats)\n  \n  return(result)\n}\nNow set-up our chat and data request\n# Create chat object if not already created\nchat &lt;- chat_claude(\n      system_prompt = \"You are a research assistant who specializes in extracting structured data from scientific papers.\",\n      model = \"claude-3-5-haiku-20241022\", \n      max_tokens = 1000\n)\nThere’s a risk that the LLM will hallucinate data if it can’t find an answer. To try to prevent this we can set an option , required = FALSE. Then the LLM should return ‘NULL’ if it can’t find the data.\n# Define the structured data format\npaper_stats &lt;- type_object(\n    sample_size = type_number(\"Number of surveys conducted to estimate turtle catch\", required = FALSE),\n    turtles_caught = type_number(\"Estimate for number of turtles caught\", required = FALSE),\n    year_of_study = type_number(\"Year data was collected\", required = FALSE),\n    region = type_string(\"Country or geographic region of the study\", required = FALSE)\n  )\nNow we can batch process the abstracts and get the structured data\n\nabstract_files &lt;- list.files(path = \"pdf-examples\", pattern = \"\\\\.txt$\", full.names = TRUE)\nresults &lt;- lapply(abstract_files, function(file) process_abstract(file, chat))\nnames(results) &lt;- basename(abstract_files)\n\n# Display results\nprint(results)\nIn my first take without the required = FALSE I got some fake results. It hallucinated that the Humber study was conducted in 2023 (it was published in 2010!) and that there were 2 villages surveyed in my study. The problem was that you can’t get that data from the abstracts. So the model is hallucinating a response.\nUnfortunately, with required = FALSE it still hallucinated answers. I then tried Claude sonnet (a more powerful reasoning model) and it correctly put NULL for my study’s sample size, but still got the year wrong for the Humber study.\nI think this could work, but some work on the prompts would be needed."
  },
  {
    "objectID": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#reflections",
    "href": "posts/2025-03-15-LMs-in-R-with-ellmer/index.html#reflections",
    "title": "The ellmer package for using LLMs with R is a game changer for scientists",
    "section": "Reflections",
    "text": "Reflections\nThe ellmer package solves some of the challenges I outlined in my last blog on LLM access from R. But others are deeper conceptual challengs and remain. I’ll repeat those here\n\nCost uncertainty\nThis should be cheap. It cost &lt;1c to make this post with all the testing. So in theory you could do 100s of methods sections for &lt;100USD. However, if you are testing back and forwards a lot or using full papers the cost could add up. It will be hard to estimate this until people get more experience.\n\n\nObtaining the papers and dealing with unstructued text in PDFs or HTML\nA big challenge will be getting the text into a format that the LLM can use. Then there are issues like obtaining the text. Downloading pdfs is time consuming and data intensive. Trying to read text data from webpages can also be hard, due to paywalls and rate limits (you might get blocked for making reqeat requests).\nFor instance, in a past study we did where we did simple ‘bag of words analysis’ we either downloaded the pdfs manually, or set timers to delay web hits and avoid getting blocked.\nHTML format would be ideal, because the tags mean the sections of the paper, and the figures already semi-structured.\nThe ellmer pdf utility function seems to work ok for getting text from pdfs. I’m guessing it could be improved though, e.g. to remove wastefull (=$) text like page headers.\n\n\nPrompting\nNeed to experiment with this to get it right. It might also be good to repeat prompt the same text to triangulate accurate results.\n\n\nValidation\nYou’ll definitely want to manually check the output and report accuracy statistics in your study. So maybe your review has 1000 papers, you’ll want to manually check 100 of them to see how accurate the LLM was.\n\n\nYou’ll still need to read a lot of papers to write a good lit review\nA lit review is more than the systematic data. I still believe you need to read a lot of papers in order to understand the literature and make a useful synthesis. If you just use AI you’re vulnerable to the ‘illusion of understanding’.\n\n\nConclusion\nThis tool will be best for well defined tasks and consistently written papers. For instance, an ideal use case would be reviewing 500 ocean acidification papers that all used similar experimental designs and terminology. You’ll then be able to get consistent answers to prompts about sample size etc…\nAnother good use case would be to extract model types from species distribution model papers.\nHarder tasks will be where the papers are from diverse disciplines, or use inconsistent terminology, or methods. My study was a good example of that, there were about 5 different sample sizes reported. So in this example we’d need first to think clearly about what sample size you wanted to extract before writing the prompt."
  },
  {
    "objectID": "posts/2017-02-26-speeding-up-sims/index.html",
    "href": "posts/2017-02-26-speeding-up-sims/index.html",
    "title": "Make your R simulation models 20 times faster",
    "section": "",
    "text": "R can be frustratingly slow if you use its loops. However, you can speed it up significantly (e.g. 20 times!) using the Rcpp package. That could turn a day long simulation into an hour long simulation.\nI had heard years ago about the Rcpp package. Rcpp lets you write and use C ++ functions in R. However, I had never bothered to learn how to write C ++. Instead if my simulation model got too slow, I just redefined the problem (e.g. by using smaller spatial domain) so I could continue with R.\n\n\n\nOutput of one of the population model runs showing solutions from an R function and an Rcpp function. The C++ version gives an identical results and was up to 20 times faster.\n\n\nI persisted with R, rather than use another language, because of its powerful graphics and the convenience of using a functional language like R to perform sensitivity analyses. More on this later.\nThe other day I was browsing Wickhams Advanced R book and realised it is actually pretty easy to write basic C++ loops.\nThen I wondered if it would still be faster if you had to make repeated calls to the same C++ function, for instance if you wanted to run a sensitivity analysis, varying some model parameters. I like to use R for this task because the purrr package makes it incredibly easy to run arbitrary combinations of parameters through a function. Then it is straightforward to summarize and plot the results with ggplot2.\nTurns out you can get a massive improvement, even for repeated calls to the same function. Here is a test.\n\n\nFirst up, let’s write a simple model for simulating population size over time, according to the logistic function. The below function just takes your standard r (intrinsic population growth) and k (carrying capacity) parameters and simulates population size starting at yinit over t years.\nFurther, to I have included a stochastic process, whose variation is controlled by thetasd, to illustrate Rcpp random number generator.\nlogmodr &lt;- function(t, yinit, r, k, thetasd){\n    y &lt;- numeric(t)\n    y[1] &lt;- yinit\n    theta &lt;- rnorm(t, 0, thetasd)\n    for(i in 2:t){\n        y[i] &lt;- y[i-1]*(r - r*(y[i-1]/k)) * exp(theta[i])\n    }\n    return(y)\n}\nNote that I also ran these models without the stochastic component. The speedup was even greater when you compared C++ to R without the stochastic step (about 20 times).\n\n\n\nNow let’s write the equivalent C++ function. You will need to install the Rcpp package. Note that it has some other software dependencies, so I recommend you read the guide on CRAN.\nWe write the function definition as a string and pass it to cppFunction from Rcpp:\nlibrary(Rcpp)\n    cppFunction(\"NumericVector logmodc(int t, double yinit, double r,\ndouble k, double thetasd){\n            NumericVector y(t);\n            y[0] = yinit;\n      NumericVector theta = rnorm(t, 0, thetasd);\n            for (int i = 1; i &lt; t; ++i){\n                y[i] = y[i-1]*(r - r*(y[i-1] / k)) * exp(theta[i]);\n                }\n            return y;\n    }\n    \")\nHopefully you can understand this, even if you are not familiar with C++. The syntax is reasonably similar to R. If you learned to program in R you may notice a few discrepencies.\nFirst, C++ requires that you specify the type of each variable when its created. You can’t just create new variables without assigning them a type, and you can’t just change the type. This makes C++ more efficient than R, because the computer knows exactly how much memory to allocate a variable and doesn’t have to watch for changes.\nSecond, notice I start the iterator at time-step 1, whereas in the R code we started at time-step 2. In C++ vectors are indexed starting at 0.\nFinally, don’t forget to end lines with ; (you can use ; to end lines in R, but it is not essential).\n\n\n\nFirst up, we need to define the model parameters:\nt &lt;- 100\nyinit &lt;- 1\nk &lt;- 20\nthetasd &lt;- 0.1\nr &lt;- 0.2\nNow we can run our model. I am just going to plug the models straight into microbenchmark, so I can compare their times.\nlibrary(microbenchmark)\nmb1 &lt;- microbenchmark(\n    logmodc(t, yinit, 1.4, k, thetasd),\n    logmodr(t, yinit, 1.4, k, thetasd)\n)\nmb1\n\n## Unit: microseconds\n##                                expr     min       lq      mean   median\n##  logmodc(t, yinit, 1.4, k, thetasd)  10.051  11.1100  12.70373  11.7415\n##  logmodr(t, yinit, 1.4, k, thetasd) 179.053 198.8315 251.48889 224.3450\n##        uq      max neval cld\n##   12.8825   67.801   100  a\n##  296.1400 1098.436   100   b\nSo the C++ version is 19 times faster.\n\n\n\nSo C++ is faster for a single call to a function (that contains a loop). No surprises there. What if we want to make repeated calls to the same function, is C++ still faster than R? We might want to make repeated calls if we want to run different values of r through our model to do a sensitivty analysis.\nWe could increase the scope of the C++ code to include a loop over different values of r. However, then we would lose some of the convenience of R, which is good at manipulating data. We also wouldn’t be able to use purrr package to make sensitivity analysis easy.\nFirst, up let’s create a sequence of r values:\nrseq &lt;- seq(1.1, 2.2, length.out = 10)\nNow we can run our two models. I will use purrr::map (the :: just means map is in the package purrr and avoids another call to library()). We will also use set.seed() to make sure both algorithms generate the same series of random numbers, that way we can check whether the results are identical.\nset.seed(42)\nyc &lt;- purrr::map(rseq, ~logmodc(t, yinit, .x, k, thetasd))\nset.seed(42)\nyr &lt;- purrr::map(rseq, ~logmodr(t, yinit, .x, k, thetasd))\nmap iteratively steps through rseq replacing the .x in the function call with each value of r in turn. Note that we also have to turn the function call into a formula (with ~) to iterate in this way.\nmap returns a list, where each element is a time-series of population sizes for a given value of r.\nLet’s plot the result, for the second value of r:\nplot(yr[[2]], type = \"l\", col = \"DarkBlue\", lwd = 2)\npoints(yc[[2]], pch = 16, col = \"Tomato\", cex = 0.8)\nlegend('topleft', legend = c(\"R solution\",\"C solution\"),\n       pch = 16, col = c(\"DarkBlue\", \"Tomato\"))\n\nThey look identical, excellent.\nNow, let’s compare the time. Remember I had wondered whether repeated calls to a C++ function might lose some of the performance gain:\nmb2 &lt;- microbenchmark(\n    purrr::map(rseq, ~logmodc(t, yinit, .x, k, thetasd)),\n    purrr::map(rseq, ~logmodr(t, yinit, .x, k, thetasd))\n)\nmb2\n\n## Unit: microseconds\n##                                                  expr      min        lq\n##  purrr::map(rseq, ~logmodc(t, yinit, .x, k, thetasd))  151.421  166.4165\n##  purrr::map(rseq, ~logmodr(t, yinit, .x, k, thetasd)) 1890.549 2047.6310\n##       mean    median       uq      max neval cld\n##   199.9101  179.5795  221.885  371.192   100  a\n##  2543.3459 2233.7455 2534.350 9173.440   100   b\nTurns out we still gain a 12 times improvement when using C++.\nI don’t believe I have been wasting so many hours waiting for simulations to run all these years. Learning a bit of C++ is well worth the investment."
  },
  {
    "objectID": "posts/2017-02-26-speeding-up-sims/index.html#make-your-r-simulation-models-20-times-faster",
    "href": "posts/2017-02-26-speeding-up-sims/index.html#make-your-r-simulation-models-20-times-faster",
    "title": "Make your R simulation models 20 times faster",
    "section": "",
    "text": "R can be frustratingly slow if you use its loops. However, you can speed it up significantly (e.g. 20 times!) using the Rcpp package. That could turn a day long simulation into an hour long simulation.\nI had heard years ago about the Rcpp package. Rcpp lets you write and use C ++ functions in R. However, I had never bothered to learn how to write C ++. Instead if my simulation model got too slow, I just redefined the problem (e.g. by using smaller spatial domain) so I could continue with R.\n\n\n\nOutput of one of the population model runs showing solutions from an R function and an Rcpp function. The C++ version gives an identical results and was up to 20 times faster.\n\n\nI persisted with R, rather than use another language, because of its powerful graphics and the convenience of using a functional language like R to perform sensitivity analyses. More on this later.\nThe other day I was browsing Wickhams Advanced R book and realised it is actually pretty easy to write basic C++ loops.\nThen I wondered if it would still be faster if you had to make repeated calls to the same C++ function, for instance if you wanted to run a sensitivity analysis, varying some model parameters. I like to use R for this task because the purrr package makes it incredibly easy to run arbitrary combinations of parameters through a function. Then it is straightforward to summarize and plot the results with ggplot2.\nTurns out you can get a massive improvement, even for repeated calls to the same function. Here is a test.\n\n\nFirst up, let’s write a simple model for simulating population size over time, according to the logistic function. The below function just takes your standard r (intrinsic population growth) and k (carrying capacity) parameters and simulates population size starting at yinit over t years.\nFurther, to I have included a stochastic process, whose variation is controlled by thetasd, to illustrate Rcpp random number generator.\nlogmodr &lt;- function(t, yinit, r, k, thetasd){\n    y &lt;- numeric(t)\n    y[1] &lt;- yinit\n    theta &lt;- rnorm(t, 0, thetasd)\n    for(i in 2:t){\n        y[i] &lt;- y[i-1]*(r - r*(y[i-1]/k)) * exp(theta[i])\n    }\n    return(y)\n}\nNote that I also ran these models without the stochastic component. The speedup was even greater when you compared C++ to R without the stochastic step (about 20 times).\n\n\n\nNow let’s write the equivalent C++ function. You will need to install the Rcpp package. Note that it has some other software dependencies, so I recommend you read the guide on CRAN.\nWe write the function definition as a string and pass it to cppFunction from Rcpp:\nlibrary(Rcpp)\n    cppFunction(\"NumericVector logmodc(int t, double yinit, double r,\ndouble k, double thetasd){\n            NumericVector y(t);\n            y[0] = yinit;\n      NumericVector theta = rnorm(t, 0, thetasd);\n            for (int i = 1; i &lt; t; ++i){\n                y[i] = y[i-1]*(r - r*(y[i-1] / k)) * exp(theta[i]);\n                }\n            return y;\n    }\n    \")\nHopefully you can understand this, even if you are not familiar with C++. The syntax is reasonably similar to R. If you learned to program in R you may notice a few discrepencies.\nFirst, C++ requires that you specify the type of each variable when its created. You can’t just create new variables without assigning them a type, and you can’t just change the type. This makes C++ more efficient than R, because the computer knows exactly how much memory to allocate a variable and doesn’t have to watch for changes.\nSecond, notice I start the iterator at time-step 1, whereas in the R code we started at time-step 2. In C++ vectors are indexed starting at 0.\nFinally, don’t forget to end lines with ; (you can use ; to end lines in R, but it is not essential).\n\n\n\nFirst up, we need to define the model parameters:\nt &lt;- 100\nyinit &lt;- 1\nk &lt;- 20\nthetasd &lt;- 0.1\nr &lt;- 0.2\nNow we can run our model. I am just going to plug the models straight into microbenchmark, so I can compare their times.\nlibrary(microbenchmark)\nmb1 &lt;- microbenchmark(\n    logmodc(t, yinit, 1.4, k, thetasd),\n    logmodr(t, yinit, 1.4, k, thetasd)\n)\nmb1\n\n## Unit: microseconds\n##                                expr     min       lq      mean   median\n##  logmodc(t, yinit, 1.4, k, thetasd)  10.051  11.1100  12.70373  11.7415\n##  logmodr(t, yinit, 1.4, k, thetasd) 179.053 198.8315 251.48889 224.3450\n##        uq      max neval cld\n##   12.8825   67.801   100  a\n##  296.1400 1098.436   100   b\nSo the C++ version is 19 times faster.\n\n\n\nSo C++ is faster for a single call to a function (that contains a loop). No surprises there. What if we want to make repeated calls to the same function, is C++ still faster than R? We might want to make repeated calls if we want to run different values of r through our model to do a sensitivty analysis.\nWe could increase the scope of the C++ code to include a loop over different values of r. However, then we would lose some of the convenience of R, which is good at manipulating data. We also wouldn’t be able to use purrr package to make sensitivity analysis easy.\nFirst, up let’s create a sequence of r values:\nrseq &lt;- seq(1.1, 2.2, length.out = 10)\nNow we can run our two models. I will use purrr::map (the :: just means map is in the package purrr and avoids another call to library()). We will also use set.seed() to make sure both algorithms generate the same series of random numbers, that way we can check whether the results are identical.\nset.seed(42)\nyc &lt;- purrr::map(rseq, ~logmodc(t, yinit, .x, k, thetasd))\nset.seed(42)\nyr &lt;- purrr::map(rseq, ~logmodr(t, yinit, .x, k, thetasd))\nmap iteratively steps through rseq replacing the .x in the function call with each value of r in turn. Note that we also have to turn the function call into a formula (with ~) to iterate in this way.\nmap returns a list, where each element is a time-series of population sizes for a given value of r.\nLet’s plot the result, for the second value of r:\nplot(yr[[2]], type = \"l\", col = \"DarkBlue\", lwd = 2)\npoints(yc[[2]], pch = 16, col = \"Tomato\", cex = 0.8)\nlegend('topleft', legend = c(\"R solution\",\"C solution\"),\n       pch = 16, col = c(\"DarkBlue\", \"Tomato\"))\n\nThey look identical, excellent.\nNow, let’s compare the time. Remember I had wondered whether repeated calls to a C++ function might lose some of the performance gain:\nmb2 &lt;- microbenchmark(\n    purrr::map(rseq, ~logmodc(t, yinit, .x, k, thetasd)),\n    purrr::map(rseq, ~logmodr(t, yinit, .x, k, thetasd))\n)\nmb2\n\n## Unit: microseconds\n##                                                  expr      min        lq\n##  purrr::map(rseq, ~logmodc(t, yinit, .x, k, thetasd))  151.421  166.4165\n##  purrr::map(rseq, ~logmodr(t, yinit, .x, k, thetasd)) 1890.549 2047.6310\n##       mean    median       uq      max neval cld\n##   199.9101  179.5795  221.885  371.192   100  a\n##  2543.3459 2233.7455 2534.350 9173.440   100   b\nTurns out we still gain a 12 times improvement when using C++.\nI don’t believe I have been wasting so many hours waiting for simulations to run all these years. Learning a bit of C++ is well worth the investment."
  },
  {
    "objectID": "posts/2018-01-29-can-I-do-bayesian-r-squared/index.html",
    "href": "posts/2018-01-29-can-I-do-bayesian-r-squared/index.html",
    "title": "How do I calculate the R squared metric for a Bayesian model",
    "section": "",
    "text": "A good friend I met on a field trip long ago, Dominique Roche, recently emailed me to ask about evaluation of Bayesian models.\nHe has been delving into generalized linear models, using Bayesian methods, and needed to decide what criteria he should use for model simplification (the process of removing ‘insignificant’ covariates) and/or deciding which covariates had the strongest effect on his response of interest.\nI wanted to share an abridged version of our conversation, as I think it is enlightening. Specifically we discussed whether one can (and should) calculate the R-squared metric for Bayesian models (the short answer being that the ‘traditional’ R-squared metric doesn’t make much sense for Bayesian models, but there are alternatives).\nAs an aside, I have received a lot of helpful feedback on this post since first putting it up, so I am updating it as this comes in. I’m particularly grateful to Ben Stewart-Koster and Aki Vehtari (who’s work I cited in the original post) for pointing me towards some important literature I missed in the earlier post (which changed the advice below somewhat).\nModel selection is a fast moving area of statistical theory, so it is worth keeping updated. Many of the methods I used a few years ago are now out-of-date."
  },
  {
    "objectID": "posts/2018-01-29-can-I-do-bayesian-r-squared/index.html#the-email-chain",
    "href": "posts/2018-01-29-can-I-do-bayesian-r-squared/index.html#the-email-chain",
    "title": "How do I calculate the R squared metric for a Bayesian model",
    "section": "The email chain",
    "text": "The email chain\nDom: What criteria can I use to make inferences with my Bayesian model? I’ve used MCMCglmm (an R package) previously to run multivariate models. However, in addition to doing this in our study, we want to test the effect of multiple predictors (covariates) on our response variables. I’m curious to know if you recommend one in particular (parameter estimate and 95% CI, Bayes factor, ).\nCB: You can test the covariates very much like you did for your correlated response model. Your covariate model will estimate a slope very much like a linear regression. So if you want a criteria for inferences, you can just use the 95% CI on that slope estimate. If it overlaps zero then you could say that the effect of that covariate is ‘insignificant’.\nYou can just then compare the median and 95% CI values for your covariates to see which ones have the largest effect.\nAnother option is to use the WAIC which some stats packages will calculate for you. It is very much like the AIC and can be used to select the most parsimonious model. i.e. your compare models with different sets of covariates and pick the one (or several) with the lowest WAIC.\nNow be careful with the WAIC. You don’t want to use it for comparing a large number of candidate models, such as you would if you were fitting an model with many covariates and just simplifying it one by one (thanks to Aki Vehtari for pointing this out, here’s the details for the technically minded).\nUsing the WAIC too liberally can lead to over-fitting and poor predictive importance - by which I mean you model may fit the data at hand very well, but contains spurious effects and therefore would perform poorly if asked to explain a new dataset.\nVehtari and colleagues provide an alternative statistic for covariate selection, but unfortunately an ‘easy way’ to calculate it is only available for a few Bayesian rstats packages. If you are not using those, you will have to program it yourself or wait.\nThe WAIC will perform better with large sample sizes or for comparing a limited subset of candidate models. I would encourage you to think about writing each model as an alternative hypothesis, then just testing your set of alternatives. This will result in fewer overall tests than testing all possible combinations of covariates with the WAIC.\nAnother thing to watch out for is that your model may give funky results if the covariates are strongly correlated with each other. A good reference is the book about eco stats by Zuur that has penguins on the front cover. (One of my favourite reference books, I can never remember the title though).\nAki Vehtari added (via email) that strongly correlated covariates will throw out your marginal CIs (they will be too wide), effectively increasing the chances of a Type II error (you miss an effect that is real). He has written more about this here.\nDom: Your answer confirms what I was thinking - that reporting an estimate and 95% CI is fine for hypothesis testing, which is great.\nI’m familiar with model selection and AIC but I usually also report a pseudo R2 (usually using r.squaredGLMM) to indicate how much of the variance my model explains. There is a short paper on that topic which came out recently in JAP (which is worth reading). Do you know if there a way to compute a pseudo R2 for a multivariate Bayesian model?\nCB: In short by asking about an R2 for a Bayesian model you are opening a can of worms.\nBayesians prefer WAIC or LOO (leave one out cross validation) for evaluating models because they integrate across the full posterior probability. The R2 is a point estimate, it is just an evaluation of the mean prediction of the model (so doesn’t account for the uncertainty in parameter estimates).\nPoint estimates don’t sit well with the Bayesian philosophy. A Bayesian assumes all parameters are random variables. Whereas, a frequentist would use the R-squared because they are assuming there are actually fixed true values we are trying to observe.\nI do see the appeal of the R2 though, because it is absolute in the sense that 0.9 is 10% better than 0.8. WAIC is not like this.\nBut, a Bayesian isn’t too bothered by R2 because they spend more time looking at the posterior credibility or predictive intervals. You can predict your data using the fitted model and then just see how wide the CIs or PIs are. Wide CIs would be analogous to a poor R2.\nSome model specifications might have narrower CIs than others (for instance if you include more informative covariates).\nAfter I published this post Ben Stewart-Koster wrote to tell me that their is a way to calculate a Baysian R-squared and Gelman and colleagues have written about it here if your interested to learn more. The Bayesian R2 integrates across the modelled uncertainty, so is not a point estimate.\nAnother aside, if you are still using the DIC (similar to the WAIC) to evaluate Bayesian models you shouldn’t be. It is a point estimate, so makes the same philosophical fallacy that the R2) makes for a Bayesian model. It is also now understood to be quite unreliable.\nDom: Also, from a quick look at the paper you sent me (the one about WAIC) and the vignette for the loo package, it seems like implementing model selection for a Bayesian model using WAIC is analogous to carrying out likelihood ratio tests to compare nested models.\nHow would one go about comparing models with four predictors (no interactions)? Would you compute the WAIC by hand for all possible models and retain the ‘best’ model(s)? I’ve used the dredge function in MuMln to automate the process in the past.\nCB: The WAIC is just like the AIC, so it is a relative measure of model merit. You would jsut run all candidate models (e.g. with 4, 3, 2, 1 covariates) and compare them.\nSo far you will have to do this by hand, I’m not aware of anything that automates it like MuMln. Could be very time consuming, because bayes models tend to be slow to run.\nRather than just dredging for the best AIC, I prefer a hypothesis driven approach. So specify the subset of models that each addresses a specific hypotheses, then test and compare with WAIC to find which hypotheses have the greatest support.\nDom: RE issues of multi-collinearity - yep, I’m aware that this is something to check for. I’ve used vifs in the past for frequentist models. Is there something analogous for Bayesian models or does one simply look at correlations?\nCB: Good question wrt multicollinearity. I’m not actually sure how to formally test for it, never had this probelm in any bayesian models I’ve run. Something to research. It will be an issue in Bayesian models just as it is in frequentist models, the only exception being that you could get around the issue with strongly informative priors, but that’s another story (look at this paper if you’re interested)."
  },
  {
    "objectID": "posts/2018-01-29-can-I-do-bayesian-r-squared/index.html#going-forward-a-bayesian",
    "href": "posts/2018-01-29-can-I-do-bayesian-r-squared/index.html#going-forward-a-bayesian",
    "title": "How do I calculate the R squared metric for a Bayesian model",
    "section": "Going forward a Bayesian",
    "text": "Going forward a Bayesian\nHit me up on Twitter if you want to join in on the conversation (or don’t agree with something I said).\nFinally, if you are thinking of going Bayesian for your next GLM, here’s a short review of ways you can do it in R."
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "Pipes (%&gt;%) are great for improving readibility of lengthy data processing scripts, but I’m beggining to learn they have some weaknesses when it comes to large and complex data processing.\nWe are running a number of projects at the moment that require managing and wrangling large and complex datasets. We have numerous scripts we use to document our workflow and the data wrangling steps. This has turned out to be very helpful, because when we identify bugs in the end product, we can go back and fix them.\nBut I’m starting to see a pattern. Most of the really insidious bugs occur in sections of code that use dplyr tools and pipes. These are always the types of bugs that don’t throw an error, so you get a result, it just turns out to be wrong. They are the worst kind of bugs. And hard to detect and fix.\nSo we are now moving away from using pipes in complex scripts. For simple scripts I intend to keep using them, they are so fast and easy. Here’s what we’re trying instead.\n\n\nSo here’s some made up data that mimics the kind of fish survey data we often have:\nsites &lt;- data.frame(site = letters[1:5],\n                    temp = rnorm(5, 25, 2), stringsAsFactors = FALSE)\ndat &lt;- expand.grid(site = letters[1:5],\n                   transect = 1:4)\ndat$abundance &lt;- rpois(20, 11)\nSo we have site level data with a covariate, temp and transect level data with fish counts.\nNow say we have an error and one of our sites has capitals, instead of lower case, so lets introduce that bug:\nsites$site[1] &lt;- \"A\"\nNow if I join and summarize them, I will lose one of the sites\nlibrary(dplyr)\n\n## Warning: package 'dplyr' was built under R version 3.6.3\n\n##\n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n##\n##     filter, lag\n\n## The following objects are masked from 'package:base':\n##\n##     intersect, setdiff, setequal, union\n\ndat %&gt;%\n  inner_join(sites) %&gt;%\n  group_by(site) %&gt;%\n  summarize(mean(abundance))\n\n## Joining, by = \"site\"\n\n## Warning: Column `site` joining factor and character vector, coercing into\n## character vector\n\n## # A tibble: 4 x 2\n##   site  `mean(abundance)`\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 b                   9  \n## 2 c                  12  \n## 3 d                  12.5\n## 4 e                  12.8\nObvious enough here, but issues like that are much harder to detect in very large datasets.\n\n\n\nThe solution of course is to code in ‘unit tests’ to make sure each operations are doing what you expect. For small data you can just look, but for big datasets its not so easy.\nFor long pipes with multiple steps we’d usually do this debugging and testing interactively. So I’d write the first line (the join) save the output to a new variable, check it worked ok, then move on to write the next step of the pipe.\nNow here’s the catch. In complex project its common to change the data that goes into your pipe (in this case dat or sites dataframes). For instance, in our current project new data comes in all the time.\nNew data presents new issues. So a pipe that worked the first time may no longer work the second time.\nThis is why it is crucial to have unit tests built into your code.\nThere are lots of sophisticated R packages for unit testing, including ones that work with pipes. But given many of us are just learning tools like dplyr its not wise to add extra tools. So here I’ll show some simple unit tests with base R.\n\n\n\nJoins often case problems, due to mis-matching (e.g. if site names are spelt differently in different datasets, which is a very common human data entry error!).\nSo its wise to check the join has worked. Here’s some examples:\ndat2 &lt;- inner_join(dat, sites)\n\n## Joining, by = \"site\"\n\n## Warning: Column `site` joining factor and character vector, coercing into\n## character vector\nNow compare number of rows:\nnrow(dat2)\n\n## [1] 16\n\nnrow(dat)\n\n## [1] 20\nObviously the join has lost data in this case.\nWe can do better though with a complex script. We’d like to have an error if the data length changes. We can do this:\nnrow(dat2) == nrow(dat)\n\n## [1] FALSE\nWhich tells us TRUE/FALSE if the condition is met. To get an error use stopifnot\nstopifnot(nrow(dat2) == nrow(dat))\n\n\n\nLet’s start with the basics. No special packages, just simple logical queries.\nOf the top of my head here are a few of my most commonly used unit tests To check the number of sites has stayed the same, use length(unique(… to get the number of unique cases:\nlength(unique(dat$site))\n\n## [1] 5\n\nlength(unique(dat2$site))\n\n## [1] 4\n\nlength(unique(dat$site)) == length(unique(dat2$site))\n\n## [1] FALSE\nOr if we wanted to compare the site and dat dataframes:\nunique(sites$site) %in% unique(dat$site)\n\n## [1] FALSE  TRUE  TRUE  TRUE  TRUE\nThe %in% just means are the sites names in sites matching the site names in dat? (We can use stopifnot here too, with multiple TRUE/FALSE values).\nHow many don’t match?\nsum(!(unique(sites$site) %in% unique(dat$site)))\n\n## [1] 1\nThe ! is a logical ‘not’ (not FALSE = TRUE, so we are counting non-matches).\nWhich one doesn’t match?\nsites$site[!unique(sites$site) %in% unique(dat$site)]\n\n## [1] \"A\"\nHere’s another insidious bug caused by joins, when our covariate dataframe has duplicate site entries:\nsites &lt;- data.frame(site = c(letters[1:5], \"a\"),\n                    temp = c(rnorm(5, 25, 2), 11), stringsAsFactors = FALSE)\nsites\n\n##   site     temp\n## 1    a 19.30061\n## 2    b 23.76530\n## 3    c 24.89018\n## 4    d 25.16386\n## 5    e 23.83092\n## 6    a 11.00000\nNow we have two sites called a with different values of temp. Check out the join:\ndat2 &lt;- inner_join(dat, sites)\n\n## Joining, by = \"site\"\n\n## Warning: Column `site` joining factor and character vector, coercing into\n## character vector\n\nnrow(dat)\n\n## [1] 20\n\nnrow(dat2)\n\n## [1] 24\nSo its added rows, ie made up data we didn’t have. Why? Well the join duplicated all the site a values for both values of temp:\nfilter(dat2, site == \"a\")\n\n##   site transect abundance     temp\n## 1    a        1        10 19.30061\n## 2    a        1        10 11.00000\n## 3    a        2        11 19.30061\n## 4    a        2        11 11.00000\n## 5    a        3        12 19.30061\n## 6    a        3        12 11.00000\n## 7    a        4         9 19.30061\n## 8    a        4         9 11.00000\nNo watch this, we can really go wrong when we summarize:\ndat2 %&gt;%\n  group_by(site) %&gt;%\n  summarize(sum(abundance))\n\n## # A tibble: 5 x 2\n##   site  `sum(abundance)`\n##   &lt;chr&gt;            &lt;int&gt;\n## 1 a                   84\n## 2 b                   36\n## 3 c                   48\n## 4 d                   50\n## 5 e                   51\nIt looks like site a has twice as many fish as it really does (78, when it should have 39). So imagine you had a site dataframe you were happy worked, then your collaborator sent you a new one to use, but it had duplicate rows. If you didn’t have the unit test to check your join in place, you may never know about this doubling of data error.\nWe could check for this by checking for the number of transects e.g.\ndat_ntrans &lt;- dat2 %&gt;% group_by(site) %&gt;% summarize(n = n())\ndat_ntrans\n\n## # A tibble: 5 x 2\n##   site      n\n##   &lt;chr&gt; &lt;int&gt;\n## 1 a         8\n## 2 b         4\n## 3 c         4\n## 4 d         4\n## 5 e         4\n\ndat_ntrans$n != 4\n\n## [1]  TRUE FALSE FALSE FALSE FALSE\n(Yes I used a pipe this time, but a simple one).\n\n\n\n(This part is new as of 2020-04-22)\nSo once this post got shared I got a lot of feedback. A lot of lovely people wrote me with the timeless rstats cliche “there’s a package for that” (I say that being an abuser of that cliche myself).\nSo if you really want to get into unit testing and take it to the next level here’s a few of the tips I received.\nRoger Bivand wrote to tell me about validate package, which comes with a whole book in pdf format on data cleaning (that is now on my ‘lockdown’ reading list).\nAnother popular one is testthat, which I received many recommendations for (haven’t used it myself).\nEd Hagen shared tidylog that conveniently prints messages telling you what join and so on have done did (pretty low overhead on learning this one!). Might be good package to install if you are just learning dplyr too, as it reveals more about what is going on.\nDavid Aja and others messaged me about point blank, which integrates unit testing into pipes.\nThese are all great solutions for the advanced R user. My challenge with them is that students get worn down by hearing “there’s another R package for that”. So best solution is take it step-by-step, get some simple unit testing going first. If that’s not enough to get the job done, then start using these tools, once you’re comfortable with the basics.\n\n\n\nSo I plan on keeping up pipes, but just for simple things. For complex scripts we’ll break the pipes with more intermediate tests and do more unit testing. It’ll save a lot of pain down the road."
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html#the-problem-with-pipes",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html#the-problem-with-pipes",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "So here’s some made up data that mimics the kind of fish survey data we often have:\nsites &lt;- data.frame(site = letters[1:5],\n                    temp = rnorm(5, 25, 2), stringsAsFactors = FALSE)\ndat &lt;- expand.grid(site = letters[1:5],\n                   transect = 1:4)\ndat$abundance &lt;- rpois(20, 11)\nSo we have site level data with a covariate, temp and transect level data with fish counts.\nNow say we have an error and one of our sites has capitals, instead of lower case, so lets introduce that bug:\nsites$site[1] &lt;- \"A\"\nNow if I join and summarize them, I will lose one of the sites\nlibrary(dplyr)\n\n## Warning: package 'dplyr' was built under R version 3.6.3\n\n##\n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n##\n##     filter, lag\n\n## The following objects are masked from 'package:base':\n##\n##     intersect, setdiff, setequal, union\n\ndat %&gt;%\n  inner_join(sites) %&gt;%\n  group_by(site) %&gt;%\n  summarize(mean(abundance))\n\n## Joining, by = \"site\"\n\n## Warning: Column `site` joining factor and character vector, coercing into\n## character vector\n\n## # A tibble: 4 x 2\n##   site  `mean(abundance)`\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 b                   9  \n## 2 c                  12  \n## 3 d                  12.5\n## 4 e                  12.8\nObvious enough here, but issues like that are much harder to detect in very large datasets."
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html#unit-testing",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html#unit-testing",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "The solution of course is to code in ‘unit tests’ to make sure each operations are doing what you expect. For small data you can just look, but for big datasets its not so easy.\nFor long pipes with multiple steps we’d usually do this debugging and testing interactively. So I’d write the first line (the join) save the output to a new variable, check it worked ok, then move on to write the next step of the pipe.\nNow here’s the catch. In complex project its common to change the data that goes into your pipe (in this case dat or sites dataframes). For instance, in our current project new data comes in all the time.\nNew data presents new issues. So a pipe that worked the first time may no longer work the second time.\nThis is why it is crucial to have unit tests built into your code.\nThere are lots of sophisticated R packages for unit testing, including ones that work with pipes. But given many of us are just learning tools like dplyr its not wise to add extra tools. So here I’ll show some simple unit tests with base R."
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html#unit-testing-an-example",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html#unit-testing-an-example",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "Joins often case problems, due to mis-matching (e.g. if site names are spelt differently in different datasets, which is a very common human data entry error!).\nSo its wise to check the join has worked. Here’s some examples:\ndat2 &lt;- inner_join(dat, sites)\n\n## Joining, by = \"site\"\n\n## Warning: Column `site` joining factor and character vector, coercing into\n## character vector\nNow compare number of rows:\nnrow(dat2)\n\n## [1] 16\n\nnrow(dat)\n\n## [1] 20\nObviously the join has lost data in this case.\nWe can do better though with a complex script. We’d like to have an error if the data length changes. We can do this:\nnrow(dat2) == nrow(dat)\n\n## [1] FALSE\nWhich tells us TRUE/FALSE if the condition is met. To get an error use stopifnot\nstopifnot(nrow(dat2) == nrow(dat))"
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html#common-unit-tests-for-data-wrangling",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html#common-unit-tests-for-data-wrangling",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "Let’s start with the basics. No special packages, just simple logical queries.\nOf the top of my head here are a few of my most commonly used unit tests To check the number of sites has stayed the same, use length(unique(… to get the number of unique cases:\nlength(unique(dat$site))\n\n## [1] 5\n\nlength(unique(dat2$site))\n\n## [1] 4\n\nlength(unique(dat$site)) == length(unique(dat2$site))\n\n## [1] FALSE\nOr if we wanted to compare the site and dat dataframes:\nunique(sites$site) %in% unique(dat$site)\n\n## [1] FALSE  TRUE  TRUE  TRUE  TRUE\nThe %in% just means are the sites names in sites matching the site names in dat? (We can use stopifnot here too, with multiple TRUE/FALSE values).\nHow many don’t match?\nsum(!(unique(sites$site) %in% unique(dat$site)))\n\n## [1] 1\nThe ! is a logical ‘not’ (not FALSE = TRUE, so we are counting non-matches).\nWhich one doesn’t match?\nsites$site[!unique(sites$site) %in% unique(dat$site)]\n\n## [1] \"A\"\nHere’s another insidious bug caused by joins, when our covariate dataframe has duplicate site entries:\nsites &lt;- data.frame(site = c(letters[1:5], \"a\"),\n                    temp = c(rnorm(5, 25, 2), 11), stringsAsFactors = FALSE)\nsites\n\n##   site     temp\n## 1    a 19.30061\n## 2    b 23.76530\n## 3    c 24.89018\n## 4    d 25.16386\n## 5    e 23.83092\n## 6    a 11.00000\nNow we have two sites called a with different values of temp. Check out the join:\ndat2 &lt;- inner_join(dat, sites)\n\n## Joining, by = \"site\"\n\n## Warning: Column `site` joining factor and character vector, coercing into\n## character vector\n\nnrow(dat)\n\n## [1] 20\n\nnrow(dat2)\n\n## [1] 24\nSo its added rows, ie made up data we didn’t have. Why? Well the join duplicated all the site a values for both values of temp:\nfilter(dat2, site == \"a\")\n\n##   site transect abundance     temp\n## 1    a        1        10 19.30061\n## 2    a        1        10 11.00000\n## 3    a        2        11 19.30061\n## 4    a        2        11 11.00000\n## 5    a        3        12 19.30061\n## 6    a        3        12 11.00000\n## 7    a        4         9 19.30061\n## 8    a        4         9 11.00000\nNo watch this, we can really go wrong when we summarize:\ndat2 %&gt;%\n  group_by(site) %&gt;%\n  summarize(sum(abundance))\n\n## # A tibble: 5 x 2\n##   site  `sum(abundance)`\n##   &lt;chr&gt;            &lt;int&gt;\n## 1 a                   84\n## 2 b                   36\n## 3 c                   48\n## 4 d                   50\n## 5 e                   51\nIt looks like site a has twice as many fish as it really does (78, when it should have 39). So imagine you had a site dataframe you were happy worked, then your collaborator sent you a new one to use, but it had duplicate rows. If you didn’t have the unit test to check your join in place, you may never know about this doubling of data error.\nWe could check for this by checking for the number of transects e.g.\ndat_ntrans &lt;- dat2 %&gt;% group_by(site) %&gt;% summarize(n = n())\ndat_ntrans\n\n## # A tibble: 5 x 2\n##   site      n\n##   &lt;chr&gt; &lt;int&gt;\n## 1 a         8\n## 2 b         4\n## 3 c         4\n## 4 d         4\n## 5 e         4\n\ndat_ntrans$n != 4\n\n## [1]  TRUE FALSE FALSE FALSE FALSE\n(Yes I used a pipe this time, but a simple one)."
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html#more-sophisticated-method-for-unit-testing",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html#more-sophisticated-method-for-unit-testing",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "(This part is new as of 2020-04-22)\nSo once this post got shared I got a lot of feedback. A lot of lovely people wrote me with the timeless rstats cliche “there’s a package for that” (I say that being an abuser of that cliche myself).\nSo if you really want to get into unit testing and take it to the next level here’s a few of the tips I received.\nRoger Bivand wrote to tell me about validate package, which comes with a whole book in pdf format on data cleaning (that is now on my ‘lockdown’ reading list).\nAnother popular one is testthat, which I received many recommendations for (haven’t used it myself).\nEd Hagen shared tidylog that conveniently prints messages telling you what join and so on have done did (pretty low overhead on learning this one!). Might be good package to install if you are just learning dplyr too, as it reveals more about what is going on.\nDavid Aja and others messaged me about point blank, which integrates unit testing into pipes.\nThese are all great solutions for the advanced R user. My challenge with them is that students get worn down by hearing “there’s another R package for that”. So best solution is take it step-by-step, get some simple unit testing going first. If that’s not enough to get the job done, then start using these tools, once you’re comfortable with the basics."
  },
  {
    "objectID": "posts/2020-04-21-a-case-against-pipes/index.html#going-forward",
    "href": "posts/2020-04-21-a-case-against-pipes/index.html#going-forward",
    "title": "A case against pipes in R and what to do instead",
    "section": "",
    "text": "So I plan on keeping up pipes, but just for simple things. For complex scripts we’ll break the pipes with more intermediate tests and do more unit testing. It’ll save a lot of pain down the road."
  },
  {
    "objectID": "posts/2017-11-25-chasing-underwater-forests/index.html",
    "href": "posts/2017-11-25-chasing-underwater-forests/index.html",
    "title": "In rememberance of the ecosystems I knew as a child",
    "section": "",
    "text": "Sitting on narrow rock ledge, the cold Tasmanian ocean at my feet, I wondered if ten years of living in the sub-tropics was going to ruin my attempt to dive back into the underwater forests of my youth.\nI was visiting colleagues in my childhood state of Tasmania when someone mentioned a dense kelp forest, crawling with large rock lobster, which had regrown inside one of Tasmania’s marine reserves.\nIt sounded like the mythical ecosystems described in the accounts of early European explorers. That is, in the days before climate change and industrial fishing took their toll on ocean ecosystems across the globe, including in Tasmania.\n\nA Macrocystis forest. Photo: Megan Saunders\nI had only one weekend left in Tasmania, but I made it a priority to dive this kelp forest. I borrowed a mask from a friend, dug up my stiff old wetsuit from my parent’s basement and fixed ageing fins with cable ties.\nWhen growing up near Hobart I often sought out opportunities for diving in kelp forests, specifically the Giant Kelp Macrocystis. Giant kelp is remarkable because its fast growth (up to 35cm a day) can lead to dense forests that grow from ocean floor to surface and shade out sunlight. If you dive in a kelp forest on a bright day you will be bathed in the golden rays of the sun reflecting off the kelp’s fronds.\nThere is another world to explore in a kelp forest. If you free dive, as I typically did as a teen (scuba was too expensive) on every dive you swim down through a multitude of micro-ecosystems, from canopy to holdfast. Caprellid amphipods, which resemble a praying mantis, graze in the fronds near the surface. Mid-water you might encounter schools of Bastard Trumpeter or spiny boarfish. On the bottom, the kelp’s holdfast, a tangled mass of root like structures, hosts an array of zany looking invertebrates.\n\nBastard Trumpeter Latridopsis forsteri, cruise a kelp forest. Photo: Megan Saunders\nKelp forests were so important to my upbringing that they were the topic of the first conversation I had with my now wife (she is also a diver from another other great bastion of kelp forests - the west coast of North America).\nKelp forests became a fast disappearing ecosystem as I grew up and were increasingly difficult to find.\nIt’s estimated that Giant Kelp occupies &lt;1% of its former range in Tasmania’s waters.\nGiant Kelp likes cold nutrient rich water. Warming oceans and the strengthening East Australia Current have brought warm nutrient-poor waters to Tasmania’s coast over the past several decades, all but wiping out the Giant Kelp.\nThe warm waters also carried the larvae of the Purple Sea Urchin (Centrostephanus rodgersii) to Tasmania, where it did not previously occur. It is a voracious consumer of kelp, and rapidly transforms massive forests into rocky barrens. It is so effective that just a few individuals, who can live as long as 30 years, can prevent any kelp re-establishing in an area.\nI was under no illusions that the ecosystems I knew as a teen would be somehow preserved for all time, as some people see the places of their teen adventures. I even participated in a kelp forest restoration project at one stage.\nThough they were few and far between, there were always the good sites with dense kelp forests. These became important fixed points in my experience. Places I could go back to and remember a younger life, like when you catch up with old school friends to talk gossip from 20 years ago.\nYears later in 2016, as an adult, I visited one of the most reliable sites. Somewhat ironically, it had always been good because pollution from a sewage outflow saw that there were always plenty of nutrients in the water. In 2016 all that remained were a few scraggy strands of kelp.\nA casual observers wouldn’t know the loss I felt. To them it would just be another part of the ocean, filled with empty blue water. To me, however, swimming through the empty water was like walking over the ashes of a razed forest.\n2016 was also the first year of what turned into two consecutive years of mass coral bleaching on the Great Barrier Reef, in my now home state of Queensland. While the world watched in horror at the devastation that climate change wrought on the reef, the forests of my childhood quietly faded away.\nSo I found myself this past weekend on my own navigating down a gravel track, on the side of a steep cliff, with a weightbelt slung over my shoulder. I squeezed into my old wetsuit (it still fits remarkably) and slid down into the cold ocean.\nAs I swam out to the kelp forest, I hoped I would recall the techniques of relaxing into a free dive, so I could hold my breath for long enough to descend into the forest. Free diving is the sport of holding one’s breath so that you can swim to depth, unhindered by scuba gear.\nWhile this serene sport may seem innocuous, mistakes can be deadly. Relax for too long and go too deep and you may not make it back to the top. It is best to dive with a partner for safety, though today I was on my own. The day before my dive I had visited the seaside memorial of a school friend who had died, on his own, during a free dive. I felt his absence today, he was once a free diver and prodigiuos hunter of lobster, and would certainly have joined me on this trip.\nI reached the kelp forest, but my first dives were frantic and I quickly bounced back to the surface for air. But then I found my old skills and was able to relax into dives.\nIt was fitting perhaps for that the day was grey and the visibility underwater poor, so like my memories of childhood dives, the kelp forest was hazy. But the feeling of gliding down from canopy to holdfast was the same, and I still wondered at strange animals that lived there.\nI searched for lobster for a while, but found none. The forest is in a reserve, so there should be no fishing, but I wondered if poachers had been there before me.\nThe ongoing presence of lobsters may be crucial to the kelp’s survival. Large lobsters eat purple sea urchins and can push them back before they get a chance to establish and devour the forest.\nIt is indicative that the kelp holds on in a place where lobster fishing is banned.\nThat the kelp still holds on in a few places is a sign of hope that Tasmania’s ecosystems can withstand climate change.\nAs I made my way back up the cliff track after the dive, I thought about the cliche of arguing we do more to save nature, like creating more marine reserves, so our children can visit these places we were fortunate to experience. But our children will develop their own fixed points of experience as they grow and as the baseline for what is natural shifts from generation to generation the old baselines of their forbearers may not give them much concern.\nI hope my children don’t have the same childhood experience of nature as I had. I hope they can expect things to get better and that their shifting baseline is one of degraded ecosystems that are recovering.\nThere are signs of hope. The talk of Hobart that week had been the sighting of a southern right whale just a hundred metres from shore in the Derwent estuary. Right whales were almost driven to extinction by whale hunting, but now they are protected they are making a slow recovery.\n\nA snorkeller watches a southern right whale that visited the Derwent estuary. Photo: Mick Brown\nControversy surrounded this particular whale, which made the front page of the local news paper. Many onlookers reported it seemed distressed as it was mobbed by swimmers, boats and jetskis.\nI see this debate as a good sign. We need more conversations about what helps and what hinders nature’s recovery. The more people start to notice the empty spaces in our ocean, the more hope there is that society will notice its loss and take action to restore nature."
  },
  {
    "objectID": "posts/2023-08-23-new-phd-projects/index.html",
    "href": "posts/2023-08-23-new-phd-projects/index.html",
    "title": "New PhD opportunities in fisheries",
    "section": "",
    "text": "Associate Professor Chris Brown has two new opportunities for PhD projects in fisheries. We place an emphasis on the application of science to real-world management, communication skills and training of quantitative skills. Past PhD graduates from Chris’ lab have worked on a range of areas including, from field, lab, modelling and GIS focussed projects. All were keen to make a difference and had a desire to learn quantitative methods (e.g. R).\nA background with quantitative skills is encouraged, but not essential. Training quantitative skills is something we specialise in.\nRead more about Chris’ lab, research and his team here and the Global Wetlands Project here.\nIf you are interested, please read the project descriptions below and contact Chris for further information. Australian applicants should have at a minimum first class Honours from their degree.\nInternational applicants should have equivalent of first class honours (e.g. research focused masters with GPA 6+) and either of: substantial (3+ years) research relevant work experience and/or first authored publication in an internationally recognised journal. There are fewer international scholarship positions at UTas than domestic scholarships, so scholarships are more competitive than for domestic applicants.\nFurther details are on the UTas site via below links:\nFuture of seafood (modelling focus includes $5000 pa scholarship topup)\nHabitats for fisheries (fieldwork and some modelling)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html",
    "title": "Data visuals notes for my talks in 2017",
    "section": "",
    "text": "Supplementary notes for CJ Brown’s talks on dataviz in 2017 for Griffith University’s honours students and the UQ Winterschool in Bioinformatics."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#data-visuals-2017",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#data-visuals-2017",
    "title": "Data visuals notes for my talks in 2017",
    "section": "",
    "text": "Supplementary notes for CJ Brown’s talks on dataviz in 2017 for Griffith University’s honours students and the UQ Winterschool in Bioinformatics."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#structure-of-this-talk",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#structure-of-this-talk",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Structure of this talk",
    "text": "Structure of this talk\n\nTools for dataviz\nEleven principles for effective dataviz\nBreaking the rules\nChange the world"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#the-ideal-dataviz-tool",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#the-ideal-dataviz-tool",
    "title": "Data visuals notes for my talks in 2017",
    "section": "The ideal dataviz tool",
    "text": "The ideal dataviz tool\n\nSensible and clean defaults\nFast and convenient production of common graphics\nConvenient to plot statistical models\nFlexible enough to realize our creative thoughts"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#r-integrates-different-tools",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#r-integrates-different-tools",
    "title": "Data visuals notes for my talks in 2017",
    "section": "R integrates different tools",
    "text": "R integrates different tools\n\nData merging\nMaps\nGraphs\nAnalysis\nWord processing\nPresentations (including this one)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#r-is-a-flexible-tool",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#r-is-a-flexible-tool",
    "title": "Data visuals notes for my talks in 2017",
    "section": "R is a flexible tool",
    "text": "R is a flexible tool\n\nFlexibility is also it’s weakness\nHard to start\nSo many options to do the same thing\nSteep learning curve\nOften combine with other tools"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#graphics-packages-in-r",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#graphics-packages-in-r",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Graphics packages in R",
    "text": "Graphics packages in R\nThe dominant options are the base graphics R comes shipped with and the ggplot2 package."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#plot-your-data",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#plot-your-data",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Plot your data",
    "text": "Plot your data\n\nMake your own Datasaurus dozen\nThe datasaurus is a great example of why you should view your data, invented by Alberto Cairo. See Steph Locke’s code and package on github for making this in R.\nlibrary(datasauRus)\ndatnames &lt;- rev(unique(datasaurus_dozen$dataset))\nnlevels &lt;- length(datnames)\n\nfor (i in 1:nlevels){\n  i &lt;- which(datasaurus_dozen$dataset == datnames[i])\n  plot(datasaurus_dozen$x[i], datasaurus_dozen$y[i],\n     xlab = \"x\", ylab = \"y\",   las = 1)\n  Sys.sleep(1)\n}\n\nConvince yourself that the mean, sd and correlation is the same in all of these plots:\nlibrary(dplyr)\ndatasaurus_dozen %&gt;% group_by(dataset) %&gt;%\n    summarize(meanx = mean(x), meany = mean(y),\n              sdx = sd(x), sdy = sd(y),\n              corr = cor(x,y))\n\n## # A tibble: 13 × 6\n##       dataset    meanx    meany      sdx      sdy        corr\n##         &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n## 1        away 54.26610 47.83472 16.76982 26.93974 -0.06412835\n## 2    bullseye 54.26873 47.83082 16.76924 26.93573 -0.06858639\n## 3      circle 54.26732 47.83772 16.76001 26.93004 -0.06834336\n## 4        dino 54.26327 47.83225 16.76514 26.93540 -0.06447185\n## 5        dots 54.26030 47.83983 16.76774 26.93019 -0.06034144\n## 6     h_lines 54.26144 47.83025 16.76590 26.93988 -0.06171484\n## 7  high_lines 54.26881 47.83545 16.76670 26.94000 -0.06850422\n## 8  slant_down 54.26785 47.83590 16.76676 26.93610 -0.06897974\n## 9    slant_up 54.26588 47.83150 16.76885 26.93861 -0.06860921\n## 10       star 54.26734 47.83955 16.76896 26.93027 -0.06296110\n## 11    v_lines 54.26993 47.83699 16.76996 26.93768 -0.06944557\n## 12 wide_lines 54.26692 47.83160 16.77000 26.93790 -0.06657523\n## 13    x_shape 54.26015 47.83972 16.76996 26.93000 -0.06558334\nWe can also save these as .png images to make a .gif image (see also here)\nfor (ilvs in 1:nlevels){\n  i &lt;- which(datasaurus_dozen$dataset == datnames[ilvs])\n  thiscol &lt;- ifelse(datnames[ilvs] == \"dino\", \"darkseagreen\", \"grey20\")\n  png(filename = paste0(\"datasaurus/\",datnames[ilvs],\".png\"))\n  plot(datasaurus_dozen$x[i], datasaurus_dozen$y[i],\n     xlab = \"x\", ylab = \"y\",   las = 1,\n      xlim = c(10, 105), ylim = c(-5, 105), col = thiscol, pch = 16)\n  dev.off()\n}"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#clarity-not-simplicity",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#clarity-not-simplicity",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Clarity not simplicity",
    "text": "Clarity not simplicity\nI give the example of the famout ‘hockey stick’ graph of Northern Hemisphere temperatures."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#dataviz-are-models",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#dataviz-are-models",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Dataviz are models",
    "text": "Dataviz are models\n\nAny visualization is a model\n\nAlberto Cairo 2016\n\nDifferent viz models for the same data\n\n\nThree ways of visualising the same x-y data\nEach of these graphs of the same data has a slightly different interpretation.\nx &lt;- 1:100\ny &lt;- x + rnorm(100, sd=30)\nplot(x,y, pch = 16, col = grey(0.5, 0.5))\n\nmod1 &lt;- lm(y ~ x)\nplot(x,y, col = 'white')\nabline(mod1, lwd = 3, col = 'red')\n\nlibrary(MASS)\nfilled.contour(kde2d(x,y), scale = F)\n\nplot(x,y, pch = 16, col = grey(0.5, 0.5))\nabline(mod1, lwd = 3, col = 'red')"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#models-help-clarify-complex-data-sets",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#models-help-clarify-complex-data-sets",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Models help clarify complex data-sets",
    "text": "Models help clarify complex data-sets\n\nEffect size often has to been seen to be understood\nWhen doing confirmatory analysis, we might want to know how strong an effect is. Data viz is very useful for this task. Lets compare two datasets that have similar p-values, but very different effect sizes\nset.seed(42)\nx &lt;- rnorm(1000)\nset.seed(420)\ny &lt;- 5*x + 3 + rnorm(1000, sd = 15)\nset.seed(420)\ny2 &lt;- 5*x + 3 + rnorm(1000, sd = 1)\n\nmod1 &lt;- lm(y ~ x)\nmod2 &lt;- lm(y2 ~ x)\n\n#Compare the pvalues on the slopes\nsummary(mod1)\n\n##\n## Call:\n## lm(formula = y ~ x)\n##\n## Residuals:\n##     Min      1Q  Median      3Q     Max\n## -43.201 -10.330   0.395   9.634  46.694\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)   2.8054     0.4614   6.080 1.71e-09 ***\n## x             4.2096     0.4603   9.145  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 14.59 on 998 degrees of freedom\n## Multiple R-squared:  0.07732,    Adjusted R-squared:  0.07639\n## F-statistic: 83.63 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod2)\n\n##\n## Call:\n## lm(formula = y2 ~ x)\n##\n## Residuals:\n##      Min       1Q   Median       3Q      Max\n## -2.88004 -0.68868  0.02634  0.64229  3.11291\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)  2.98703    0.03076   97.11   &lt;2e-16 ***\n## x            4.94731    0.03069  161.21   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 0.9724 on 998 degrees of freedom\n## Multiple R-squared:  0.963,  Adjusted R-squared:  0.963\n## F-statistic: 2.599e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow = c(1,2))\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\nplot(x,y2, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod2, lwd = 2, col = 'red')"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#superplots",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#superplots",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Superplots",
    "text": "Superplots\nAndrew Gelman coined the term superplots for plotting different models on multiple panels of a graph so you can visually compare them.\nFor instance, say we have several time-series and we want to know if they deviate from each other signficantly. An easy way to compare them is to fit splines to each time-series and then just plot them next to each other, with SEs. Then we can compare visually for ‘signficant’ differences.\nHere’s some code to simulate three made-up series. The first two have the same trend, but different observation errors, the third has a different trend:\ntmax &lt;- 50\ndrift &lt;- c(10, -5)\nsd &lt;- 40\nsdobs &lt;- 200\nset.seed(5)\nyzero &lt;-  cumsum(rnorm(n=tmax, mean=drift[1], sd=sd))\ny1 &lt;- yzero + rnorm(n = tmax, mean = 0, sd = sdobs)\ny2 &lt;- yzero + rnorm(n = tmax, mean = 0, sd = sdobs)\ny3 &lt;- cumsum(rnorm(n=tmax, mean=drift[2], sd=sd)) +\n  rnorm(n = tmax, mean = 0, sd = sdobs)\ndat &lt;- data.frame(ts = rep(letters[1:3], each = tmax), x = rep(1:tmax, 3), y = c(y1, y2, y3))\nWe can easily plot these three series using ggplot2 and automatically add a spline.\nlibrary(ggplot2)\nggplot(dat, aes(x = x, y = y)) +\n    geom_point() +\n    facet_grid(.~ts) +\n    stat_smooth(method = \"loess\", se = TRUE) +\n   theme(axis.text = element_text(size=14),\n        axis.title = element_text(size=16,face=\"bold\"),\n          strip.text.x = element_text(size = 16),\n          panel.background = element_rect(fill = 'white', colour = 'white'))"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#length-is-most-accurate",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#length-is-most-accurate",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Length is most accurate",
    "text": "Length is most accurate\nWays of comparing data in order from most accurate (top) to more generic (bottom)."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#comparing-volume-and-area",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#comparing-volume-and-area",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Comparing volume and area",
    "text": "Comparing volume and area\nCompare these. Note that if we compare circles we should use area, not the radius or diameter to scale their size.\nn &lt;- c(10, 5)\nbarplot(n, col = 'skyblue', xaxt = 'n', yaxt = 'n')\n\nrad1 &lt;- 1\narea1 &lt;- pi*(rad1^2)\narea2 &lt;- area1/2\nrad2 &lt;- sqrt(area2/pi)\n\npar(mfrow = c(1,2), mar = c(0,0,0,0))\npie(1, col = 'skyblue', labels = NA, border = NA, radius = rad1)\npie(1, col = 'skyblue', labels = NA, border = NA, radius = rad2)\n\n\nExploration of data\nLet’s create a point cloud to demonstrate some data exploration techniques\nset.seed(42)\nx &lt;- rnorm(1000)\ny &lt;- 5*x + 3 + rnorm(1000, sd = 15)\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\n\nCan’t see alot here. A linear model might help us explore if there is any trend going on:\nmod1 &lt;- lm(y ~ x)\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\n\nxnew &lt;- seq(min(x), max(x), length.out = 100)\npmod &lt;- predict(mod1, newdata =data.frame(x=xnew),  se = T)\nlines(xnew, pmod$fit + pmod$se.fit, lwd = 2, col = 'red', lty = 2)\nlines(xnew, pmod$fit - pmod$se.fit, lwd = 2, col = 'red', lty = 2)\n\nWhat about identifying extreme points, that may be worth investigating further? We can pick out points that are greater than 2SDs from the trend:\nmodresid &lt;- resid(mod1)\nsd2 &lt;- sd(modresid)*2\nipt &lt;- which(abs(modresid) &gt; sd2)\n\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\npoints(x[ipt], y[ipt], pch = 16, col = rgb(1,0,0, 0.6))\n\n\n\nEffect size\n\n\nDon’t waste digital ink\nPlots with less ‘add-ons’ tend to communicate the key message more clearly. For instance, just like excel plots dont:\nx &lt;- rnorm(100)\ndat &lt;- data.frame(x = x, y = 0.25*x + rnorm(100, sd = 0.2))\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_excel() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\nYou can get additional themes for ggplot2 using this excellent package. A cleaner view:\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_base() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\nOr simply:\nplot(dat$x, dat$y, xlab = \"x\", ylab = \"y\", las = 1)\n\nA good principle is to not use ‘ink’ on figures that isn’t needed to communicate your message. Tufte takes the ‘less ink’ philosophy to the extreme:\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_tufte() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\n\n\nWhen is ggplot2 appropriate, or when should I use base R?\nIn general I think ggplot2 is appropriate for problems of intermediate complexity. Like this:\n Base R is great if you just want to plot a barplot quickly, or do an x-y plot. ggplot2 comes into its own for slight more complex plots, like having multiple panels for different groups or colouring lines by a 3rd factor. But once you move to really complex plots, like overlaying a subplot on a map, ggplot2 becomes very difficult, if not impossible. At that point it is better to move back to Base R. ggplot2 can also get very fiddly if you are very specific about your plots, like having certain colours, or the labels in a certain way.\nAs an example, ggplot2 is great for data like this:\nx1 &lt;- rnorm(30)\ngrps &lt;- letters[c(rep(1, 10), rep(2, 10), rep(3, 10))]\ny1 &lt;- x1 + c(rep(1, 10), rep(-1, 10), rep(2, 10)) + rnorm(30)\ndat &lt;- data.frame(x = x1, grps = grps, y = y1)\nhead(dat)\n\n##             x grps             y\n## 1  0.19033984    a  6.159889e-01\n## 2 -0.07173877    a -7.577584e-05\n## 3 -0.00285171    a -1.003277e+00\n## 4 -1.10821896    a -7.302877e-01\n## 5  0.93519177    a  2.662337e+00\n## 6 -1.48583945    a -1.588023e+00\n\nggplot(dat, aes(x = x1, y = y1, color = grps)) +\n  geom_point() + theme_bw()\n\nIt is also pretty handy for faceting:\nggplot(dat, aes(x = x1, y = y1)) +\n  geom_point() + facet_wrap(~grps)+\n  theme_bw()\n\nThe key with ggplot2 is to have your data in a data-frame.\nIn reality both ggplot2 and base R graphics are worth learning, but I would start with learning the basics of base R graphics and then move onto ggplot2 if you want to quickly plot lots of structured data-sets.\n\n\nPie graphs vs bar graphs\nIn Mariani et al. they plot rates of seafood fraud by several European countries. While its a foundational study that establishes improvement in the accuracy of food labelling, their graphics could be improved in several ways.\nFirst they use perspective pies. This makes it incredibly hard to compare the two groups (fish that are labelled/mislabelled). Humans are very bad at comparing angles and pretty bad at comparing areas. With the perspective you can’t even compare the areas properly. They do provide the raw numbers, but then, why bother with the pies? Note that the % pies misrepresent the data slightly because the % figures are actually odds ratios (mis-labels / correct labels), rather than percent (mis-labeels / total samples). Second the pies are coloured red/green, which will be hard for red-green colourblind people to see. Third, they have coloured land blue on their map, so it appears to be ocean at first look. Fourth, the map is not really neccessary. There are no spatial patterns going on that the authors want to draw attention to. I guess having a map does emphasize that the study is in Europe. Finally, the size of each pie is scaled to the sample size, but the scale bar for the sample size shows a sample of only 30, whereas most of their data are for much larger samples sizes (&gt;200). Do you get the impression from the pies that the UK has 668 samples, whereas Ireland only has 187? Therefore, from this graphic we have no idea what sample size was used in each country.\nIn fact, all the numbers that are difficult to interpret in the figure are very nicely presented in Table 1.\nBelow is a start at improving the presentation. For instance, you could do a simple bar chart, ordering by rate of mislabelling.\ncnames &lt;- c('Ireland' ,'UK','Germany','France','Spain','Portugal')\ncorrlab &lt;- c(180, 647, 145, 146, 267, 178)\nmislab &lt;- c(7, 21, 9, 4, 24, 12)\nmisrate &lt;- 100*signif(mislab / (corrlab + mislab),2)\ncorrrate &lt;- 100 - misrate\nord &lt;- order(misrate, decreasing = T)\ny &lt;- rbind(corrrate, misrate)\n\npar(mar = c(5,4,4,7))\nbarplot(y[,ord], names.arg = cnames[ord], col = c('skyblue','tomato'), ylab = 'Labelling rate (%)', las = 2)\nlegend(x=7.5, y = 90, legend = c(\"Mislabelled\", \"Correctly labelled\"), pch = 16, col = c('tomato','skyblue'), xpd = NA, cex = 0.7)\n\nYou could add another subfigure to this, showing the rate by different species too.\nThe barplot doesn’t communicate the sample size, but then that is probably not the main point. The sample sizes are probably best reported in the table\nIf we felt the map was essential, then putting barcharts on it would be more informative. It is not that easy to add barcharts ontop of an existing map in R, so I would recommend creating the barcharts seperately, then adding them on in Illustrator or Powerpoint.\nWe can make a basic map like this:\nlibrary(maps)\nlibrary(maptools)\nmap('world', xlim = c(-20, 20), ylim = c(35, 60), col = 'grey', fill = T)\n\nThen create some nice barcharts. We write a loop so we get one barchart for each country.\nnc &lt;- length(cnames)\npar(mfrow = c(2,3), oma = c(1,1,1,3))\nfor (i in 1:nc){\n  y &lt;- c(mislab[i], corrlab[i])\n  barplot(y, names.arg = '', las = 2, col = c('tomato','skyblue'), ylim = c(0, corrlab[i]), main = cnames[i], cex.main = 2.4, yaxt = 'n')\n  byy &lt;- signif(max(y),2)/3\n  yat &lt;- c(0, min(y), max(y))\naxis(2, at = yat, las = 2, cex.axis = 2, labels = F)\naxis(2, at = yat[2:3], las = 2, cex.axis = 2, labels = T)\n}\nlegend(x = 2.8, y = 500, legend = c('Fraud', 'Correct'), pch = 15, col = c('tomato','skyblue'), xpd = NA, cex = 2, bty = 'n')\n\n\n\nScaling matters\nIt can be misleading to present % and proportion data on axes that are not scaled 0 - 100%. For instance, compare these three graphs:\ny &lt;- c(70, 72, 68, 73)\nx &lt;- 1:4\nxnams &lt;- LETTERS[1:4]\n\npar(mfrow = c(1,3), oma = c(1,1,1,3), mar = c(5,6,2,2))\nplot(x,y, pch = 3, cex = 2, las = 1, xaxt  = 'n', xlab = '', ylab = 'Percent', cex.axis = 2, cex.lab = 2, tcl = 0.5, xlim = c(0, 5), col = 'red', lwd = 3)\naxis(1, at = x, labels = xnams, cex.axis = 2, tcl = 0.5)\n\nbarplot(y, names.arg = xnams, las = 1, cex.axis = 2, cex.lab = 2, cex.names = 2, ylab = 'Percent')\n\nbarplot(y, names.arg = xnams, las = 1, cex.axis = 2, cex.lab = 2, cex.names = 2, ylab = 'Percent', ylim = c(0, 100))"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#interpreting-rates",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#interpreting-rates",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Interpreting rates",
    "text": "Interpreting rates\nThe units you use affect how people interpret your graph. People are bad at interpreting rates, we just can’t get our heads around accumulation very well. Here is a numerical example. Check out the below figure and ask yourself:\n\nAt what time is the number of people in the shopping centre declining?\n\n\nWould you say it is at point A, B, C or D?\nBefore you proceed with code below, take the poll:\n\nLoading poll…\n\nHere is how we made the figure and generated the data:\npar(mar = c(4,4.5,2,2), mgp = c(3,1,0))\nplot(times, inrate_err, type = 'l', xlab = 'Hour of day', ylab = 'People per 10 minutes', las = 1, cex.axis = 2, lwd = 3, col = 'darkblue', cex.lab = 2, ylim = c(0, 12))\nlines(times, outrate_err, lwd = 3, col = 'tomato')\n\nabline(v = 12, lwd = 2, col = grey(0.5,0.5))\ntext(12, 13, 'A', xpd = NA, cex = 2)\nabline(v = 13.5, lwd = 2, col = grey(0.5,0.5))\ntext(13.5, 13, 'B', xpd = NA, cex = 2)\nabline(v = 14.2, lwd = 2, col = grey(0.5,0.5))\ntext(14.2, 13, 'C', xpd = NA, cex = 2)\nabline(v = 15.8, lwd = 2, col = grey(0.5,0.5))\ntext(15.8, 13, 'D', xpd = NA, cex = 2)\n\nlegend('bottomleft', legend = c('Entering', 'Leaving'), lwd = 2, col = c('darkblue','tomato'), cex = 1.5, xpd = NA)\n\nLet’s plot the total number of people:\npar(mar = c(5,5.5,2,2), mgp = c(4,1,0))\nplot(times, cumsum(inrate_err) - cumsum(outrate_err), type = 'l', xlab = 'Hour of day', ylab = 'People in shopping centre', las = 1, cex.axis = 2, lwd = 3, col = 'darkblue', cex.lab = 2, ylim = c(0, 120))\n\nabline(v = 12, lwd = 2, col = grey(0.5,0.5))\ntext(12, 130, 'A', xpd = NA, cex = 2)\nabline(v = 13.5, lwd = 2, col = grey(0.5,0.5))\ntext(13.5, 130, 'B', xpd = NA, cex = 2)\nabline(v = 14.1, lwd = 2, col = 'tomato')\ntext(14.2, 130, 'C', xpd = NA, cex = 2)\nabline(v = 15.8, lwd = 2, col = grey(0.5,0.5))\ntext(15.8, 130, 'D', xpd = NA, cex = 2)\n\nHopefully the answer is obvious now. So the right scales can help make interpretation much easier."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#choosing-colour-scales",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#choosing-colour-scales",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Choosing colour scales",
    "text": "Choosing colour scales\nAlot of thought should go into choosing colour scales for graphs for instance- will it print ok? will colour blind people be able to see this? does the scale create artificial visual breaks in the data? Luckily there is a package to help you make the right decision for a colour scale, it is called RColorBrewer. Check out colorbrewer.org for a helpful interactive web interface for choosing colours.\nFirst let’s load some sea surface temperature data as a raster:\nlibrary(raster)\nr &lt;- raster(\"MeanAVHRRSST\")\n\nlibrary(RColorBrewer)\npar(mfrow = c(1,2))\nplot(r, col = rev(brewer.pal(11, \"Spectral\")), asp = NA)\nplot(r, col = brewer.pal(11, \"Purples\"), asp = NA)\n\n## Warning in brewer.pal(11, \"Purples\"): n too large, allowed maximum for palette Purples is 9\n## Returning the palette you asked for with that many colors\n\nUsing red-green palettes makes it hard for colour blind people. Also, using a diverging palette makes it look like there is something important about the middle point (yellow). A better palette to use would be one of the sequential ones, “Purples” shown here.\nTo make it easier to understand, let’s look at these again as contour plots. I will use a more appropriate diverging palette to the red-green one though.\nz &lt;- matrix(rep(1:10, 10), nrow = 10)\nfilled.contour(z, col = brewer.pal(9, 'Reds'), nlevels = 10)\n\nfilled.contour(z, col = brewer.pal(9, 'RdBu'), nlevels = 10)\n\nNotice the diverging pallette creates an artificial split at yellow\nOne of the only legitimate uses for pie graphs (I think) is visualising the colour scales. Here is how:\nreds &lt;- brewer.pal(9, 'Reds')\ngreens &lt;- brewer.pal(9, 'Greens')\nblues &lt;- brewer.pal(9, 'Blues')\nrdylgn &lt;- brewer.pal(9, 'RdYlGn')\nrdbu &lt;- brewer.pal(9, 'RdBu')\ndark2 &lt;- brewer.pal(8, 'Dark2')\n\npar(mfrow = c(2,3), mar = c(0,0,0,0), oma = c(0,0,0,0))\npie(rep(1, 9), col = reds)\npie(rep(1, 9), col = greens)\npie(rep(1, 9), col = blues)\npie(rep(1, 9), col = rdylgn)\npie(rep(1, 9), col = rdbu)\npie(rep(1, 9), col = dark2)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#breaking-the-rules---stick-with-convention",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#breaking-the-rules---stick-with-convention",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Breaking the rules - stick with convention",
    "text": "Breaking the rules - stick with convention\nSometimes you might want to bend or break the principles above in order to stick with convention. This is useful if people are used to viewing data in a particular way. For instance, geneticists are fond of pie graphs in haplotype networks. Here is some code I modified from here to make one (note that in this data-set there is no haplotype diversity so we artificially mix them up so you can see the pies here):\nlibrary(pegas)\nlibrary(RColorBrewer)\ndata(woodmouse)\nx &lt;- woodmouse[sample(15, size = 110, replace = TRUE), ]\nh &lt;- haplotype(x)\nnet &lt;- haploNet(h)\nmixed_up &lt;- rep(letters[1:5], each=22)\nind.hap2 &lt;- with(\n    utils::stack(setNames(attr(h, \"index\"), rownames(h))),\n    table(hap=ind, pop = mixed_up[values])\n)\n\nplot(net, size = attr(net, \"freq\"), scale.ratio = 2, cex = 0.8, pie = ind.hap2, bg = brewer.pal(8, \"Dark2\"))\nlegend(\"bottomright\", colnames(ind.hap2), col = brewer.pal(8, \"Dark2\"), pch=20)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#interactive-dataviz",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#interactive-dataviz",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Interactive dataviz",
    "text": "Interactive dataviz\nInteractive visuals can overcome the dilemma of having too much complexity to show, but also wanting the viewer to explore the details. For instance, you could use the dygraphs package to zoom in on a date range.\nWe will get some recent data on wave heights from QLD government. First we load the data directly from the web and process it to correctly label dates and times, then just select the Signficant wave height variable at Tweed Heads.\nwaves &lt;-read.csv(url(\"http://www.ehp.qld.gov.au/data-sets/waves/wave-7dayopdata.csv?timestamp=2017-07-05EST21-06-50\"), skip = 1)\n\nwaves$time &lt;- as.POSIXct(strptime(as.character(waves$DateTime), format = \"%Y-%m-%dT%H:%M:%S\"))\nwaves2 &lt;- subset(waves, (Hsig &gt; 0) & (Site == \"Tweed Heads\"))[,c(\"time\", \"Hsig\")]\nwaves_xts &lt;- xts::xts(waves2$Hsig, order.by = waves2$time)\nNow have our data, we can look at it using the dygraphs package. We will do two things: add a date range selector and also add a ‘roll’ which will smooth over a certain period of our choosing:\nlibrary(dygraphs)\ndygraph(waves_xts, main = \"Significant wave height - Tweed Heads\") %&gt;%\n  dyRoller(rollPeriod = 50) %&gt;%\n  dyRangeSelector()\nI haven’t put it up on the webpage here, but have a go yourself."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#anatomy-of-a-simple-chart",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#anatomy-of-a-simple-chart",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Anatomy of a simple chart",
    "text": "Anatomy of a simple chart\nThe construction of a simple chart in R can be a surprisingly long piece of code. Here is an example to get you started. Don’t be afraid to experiment!\n# Make some data\n# --------------- #\nset.seed(42)\n\nn &lt;- 11\n\nx &lt;- rnorm(n, mean = seq(18, 25, length.out = n))\ny &lt;- rnorm(n, mean =seq(26, 18, length.out = n))\nz &lt;- rnorm(n, mean = 22)\nt &lt;- 2005:(2005+n-1)\n\ndatnames &lt;- c('Almonds', 'Peanuts', 'Hazelnuts')\n\nplot(t, x)\nlines(t, y)\nlines(t, z)\n\nWhich look terrible. Let’s build a better chart.\n# Package for colours\nlibrary(RColorBrewer)\n\n#Set axis limits\nymax &lt;- 30\nylim &lt;- c(15, ymax)\nxlim &lt;- c(min(t), max(t))\n\n#Define colours\ncols &lt;- brewer.pal(3, 'Dark2')\n\n#Parameters for plotting\nlwd &lt;- 2\nxlabels &lt;- seq(min(t), max(t), by = 5)\nylabels &lt;- seq(0, ymax, by = 5)\n\n#Set the window params\npar(mar = c(5,5,4,4))\n\n#Build the plot\nplot(t, x, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n',\n ylim = ylim, xlim = xlim, lwd = lwd, col = cols[1],\n  xaxs = 'i', yaxs = 'i',\n  xlab = 'Time (yrs)',\n  ylab = '',\n  main = 'Changing price of nuts ($/kg)')\n\n#Add more lines\nlines(t, y, lwd = lwd, col = cols[2])\nlines(t, z, lwd = lwd, col = cols[3])\n\n#Add labels to lines\ntext(t[n], x[n], datnames[1], adj = c(0, 0), xpd = NA, col = cols[1])\ntext(t[n], y[n], datnames[2], xpd = NA, adj = c(0, 0), col = cols[2])\ntext(t[n], z[n], datnames[3], xpd = NA, adj = c(0, 0), col = cols[3])\n\n# Add custom axes\naxis(1, col = 'white', col.ticks = 'black', labels = xlabels, at = xlabels)\naxis(1, col = 'white', col.ticks = 'black', labels = NA, at = t)\n\naxis(2, col = 'white', col.ticks = 'black', las =1, labels = ylabels, at = ylabels)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes0/index.html#resources-and-further-reading",
    "href": "posts/2017-06-26-dataviz2017_notes0/index.html#resources-and-further-reading",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Resources and further reading",
    "text": "Resources and further reading\n\nThe Truthful Art by Alberto Cairo\nBooks by Edward Tufte\nTwitter #dataviz\nAn infographic of chart types: Visual vocabulary\nWTF Visualizations, on Twitter"
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html",
    "title": "Its easy to make your own AI agent in R",
    "section": "",
    "text": "There’s a lot of hype about AI agents, but buried under the hype there is a really simple architecture. I got inspired by this post that points out how easy it is to make an AI agent.\nIn this post I’ll show how to make an agent in R with the ellmer package.\nA large language model agent is a large language models that can run tools in a loop.\nTools give the agent powers to take actions like read data, create files, access the internet. Anything software task you can think of really.\nHere’s the cool thing, the ‘run tools in a loop’ architecture is simple. Building an AI (the LLM) is really complex and super expensive (like cost of $100 millions). But you can easily get access to 100s of these LLMs cheaply via API calls. Then its easy to plug them into DIY agents.\nThe simplicity means you can create just about anything you can dream up.\nLet’s make an agent that reads R help files. A chat agent could give better advice if it read the up-to-date R help files first."
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#chatting-with-llms-from-rs",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#chatting-with-llms-from-rs",
    "title": "Its easy to make your own AI agent in R",
    "section": "Chatting with LLMs from Rs",
    "text": "Chatting with LLMs from Rs\nlibrary(ellmer)\nSet up an API key with the provider you choose and you can easily talk to a large language model with ellmer:\nchat &lt;- chat_anthropic(model = \"claude-haiku-4-5\")\nchat$chat(\"How do I do a regression in R?\")\nWe can then string together a chat:\nchat &lt;- chat_anthropic(model = \"claude-haiku-4-5\")\nturn1 &lt;- chat$chat(\"How do I do a regression in R?\")\nturn2 &lt;- chat$chat(\"Show how to verify the model.\")\nThe way a multi-turn conversation works is that on the second turn the entire conversation thread (system message, user message, assistant message, new user message) is sent back to the LLM at each turn.\nRepeated calls to chat just manage that for us.\nThe LLM talking to itself isn’t very useful, we can use tools to inject new information into the conversation."
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#create-a-tool-to-read-a-help-file",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#create-a-tool-to-read-a-help-file",
    "title": "Its easy to make your own AI agent in R",
    "section": "Create a tool to read a help file",
    "text": "Create a tool to read a help file\nThe tool has two parts: (1) An R script that does something; (2) a specification that tells the LLM how to use the tool.\n\nR function\nread_help_as_text &lt;- function(topic, pkg = NULL) {\n    h &lt;- utils::help(topic, package = eval(pkg))\n    if (length(h) == 0) stop(\"Help topic not found\")\n    rd &lt;- utils:::.getHelpFile(h)             \n    txt &lt;- paste(capture.output(tools::Rd2txt(rd)), collapse = \"\\n\")\n    return(txt)\n}\nThis R function takes a topic and optionally a package name. It then returns the relevant help file as a string\nhelp_txt &lt;- read_help_as_text(\"lm\")\ncat(substr(help_txt, 1, 1000), \"\\n\") \n\n\nTool definition\nThe tool definition tells the LLM how to use the read_help_as_text tool.\nread_help_as_text &lt;- tool(\n  read_help_as_text,\n  .description = \"Returns an R help file as a string\",\n  topic = type_string(\n      \"R function or topic\",\n      required = TRUE\n    ),\n    pkg = type_string(\n      \"R package to get help file from\",\n      required = FALSE\n    )\n)\n\n\nUsing the tool\nchat &lt;- chat_anthropic(model = \"claude-haiku-4-5\")\nchat$register_tool(read_help_as_text)\n\nchat$chat(\"Quote the definition of residuals from the lm() function, don't provide any other explanation or code\")\n\"residuals: the residuals, that is response minus fitted values.\"\nThat’s a pointless use case, but it demonstrates the tool is working. You can check the chat log to see when the tool was called:\nchat\n&lt;Chat Anthropic/claude-haiku-4-5 turns=4 tokens=4529/77&gt;\n── user [626] ──────────────────────────────────────────────────────────────────\nQuote the definition of residuals from the lm() function, don't provide any other explanation or code\n── assistant [59] ──────────────────────────────────────────────────────────────\n[tool request (toolu_)]: read_help_as_text(topic = \"lm\")\n── user [3218] ─────────────────────────────────────────────────────────────────\n[tool result  (toolu_)]:\n...\n     \n── assistant [18] ──────────────────────────────────────────────────────────────\n\"residuals: the residuals, that is response minus fitted values.\""
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#getting-advice-on-recent-packages",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#getting-advice-on-recent-packages",
    "title": "Its easy to make your own AI agent in R",
    "section": "Getting advice on recent packages",
    "text": "Getting advice on recent packages\nLLMs often get tripped up if you ask them for code for niche or very recent packages. The FakeDataR package is a recently released package that helps you make synthetic data to use with LLMs (if you are concerned about privacy of your real data).\nIf you ask an LLM to use this package, it will tend to hallucinate a response, that won’t work. With our tool however, it can create accurate code:\nchat &lt;- chat_anthropic(model = \"claude-haiku-4-5\")\nchat$register_tool(read_help_as_text)\n\nchat$chat(\"How to use the generate_fake_data function from the R package FakeDataR to make a synthetic copy of mtcars\")\nI'll help you find information about the `generate_fake_data` function from the\nFakeDataR package.\n\n◯ [tool call] read_help_as_text(pkg = \"FakeDataR\", topic =\n\"generate_fake_data\")\n...\n\nGreat! Now I can show you how to use `generate_fake_data` to create a synthetic\ncopy of the `mtcars` dataset. Here's how to do it:\n\n## Basic Usage\n\n```r\n# Install FakeDataR if you haven't already\n# install.packages(\"FakeDataR\")\n\nlibrary(FakeDataR)\n\n# Generate synthetic data from mtcars\nsynthetic_mtcars &lt;- generate_fake_data(\n  data = mtcars,\n  n = 32  # Create same number of rows as original mtcars\n)\n\n# View the result\nhead(synthetic_mtcars)\n```\n\n## More Detailed Examples\n...\nThe resulting synthetic dataset preserves the structure and statistical \nproperties of the original while being completely artificial!"
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#creating-an-agentic-loop",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#creating-an-agentic-loop",
    "title": "Its easy to make your own AI agent in R",
    "section": "Creating an agentic loop",
    "text": "Creating an agentic loop\nLet’s add one more tool to show how they can be chained together to build the beginnings of an agentic loop.\nThis tool will grab the R code from the markdown code blocks in the LLMs response, then write it to a file.\nFirst the function definition:\nwrite_r_code &lt;- function(rcode){\n    code_blocks &lt;- stringr::str_extract_all(rcode, \"```r\\\\s*([\\\\s\\\\S]*?)```\")[[1]]\n    code &lt;- paste(stringr::str_replace_all(code_blocks, \"```r\\\\s*|```\", \"\"), collapse = \"\\n\")\n    writeLines(code, con = \"temp_code.R\")\n    return(\"R code written to temp_code.R\")\n}\nNow the tool definition:\nwrite_r_code &lt;- tool(\n  write_r_code,\n  .description = \"Writes R code contained in markdown code blocks to a file.\",\n  rcode = type_string(\n      \"Character string containing R code in markdown code blocks\",\n      required = TRUE\n    )\n)\nNow we just register both tools and start our chat.\nchat &lt;- chat_anthropic(model =  \"claude-haiku-4-5\")\nchat$register_tool(read_help_as_text)\nchat$register_tool(write_r_code)\n\nchat_out &lt;- chat$chat(\"Use generate_fake_data function from the R package FakeDataR to make a single synthetic version of mtcars. \")\nellmer will pass the results of the tool call back to the LLM, which can decide to call more tools if needed. Inspecting the chat log shows the sequence of tool calls, with no user intervention required."
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#tuning-agents",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#tuning-agents",
    "title": "Its easy to make your own AI agent in R",
    "section": "Tuning agents",
    "text": "Tuning agents\nThe write R code tool worked, some of the time… Other times it wrote a blank file. There’s a lot of niggly details to making agents work reliably, including choosing a powerful enough LLM, writing a system prompt that guides tool use, and writing powerful but secure tools.\nIf I was going to take this example further I could add a function to run the Rscript that was created, that might require some extra security considerations (see below for one example).\nGoing further I would also work on system prompt that provided guidance to the LLM on its role and how to approach problems. There’s many open source system prompts out there that I could draw on to create this."
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#conclusion",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#conclusion",
    "title": "Its easy to make your own AI agent in R",
    "section": "Conclusion",
    "text": "Conclusion\nIts an exciting time to be working on AI agents. The field is so new you can easily make new discoveries and be ahead of even world leading AI labs.\nAn example. Scott Spillias, a postdoc at CSIRO, developed an agent framework that uses an evolutionary algorithm to write math to create ecosystem models. He showed the agent could create models that are great at predicting out-of-sample data, and that have biologically meaningful parameters.\nA month or so after he published his pre-print, the Google DeepMind team published a similar idea of using LLM evolutionary algorithms to generate scientific models.\nI look forwards to seeing what the R community creates with agents."
  },
  {
    "objectID": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#example-of-using-tool_reject-for-user-permission",
    "href": "posts/2025-11-10-make-an-ai-agent-in-r/index.html#example-of-using-tool_reject-for-user-permission",
    "title": "Its easy to make your own AI agent in R",
    "section": "Example of using tool_reject for user permission",
    "text": "Example of using tool_reject for user permission\nCheck out ?tool_reject for a way to require user permission before tools are used. Here’s how to implement that in our write_r_code function.\nwrite_r_code &lt;- function(rcode){\n  allow_read &lt;- utils::askYesNo(\n    \"Would you like to allow access to write files?\"\n  )\n  if (isTRUE(allow_read)) {\n    code_blocks &lt;- stringr::str_extract_all(rcode, \"```r\\\\s*([\\\\s\\\\S]*?)```\")[[1]]\n    code &lt;- paste(stringr::str_replace_all(code_blocks, \"```r\\\\s*|```\", \"\"), collapse = \"\\n\")\n    writeLines(code, con = \"temp_code.R\")\n    return(\"R code written to temp_code.R\")\n  } else {\n    tool_reject()\n  } \n}"
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html",
    "href": "posts/2020-03-18-facing-uncertainty/index.html",
    "title": "Facing uncertainty",
    "section": "",
    "text": "Here follows a recent email I posted to my research team. Someone said I should share it here.\nDear team,\nWe have never faced as much uncertainty in our lives, careers and economy as we do right now. The decisions we make now, even small decisions, will have repercussions possibly for years to come. But there is no way knowing what the ‘best’ decision is. We are faced with a dilemma, a dilemma that forces us to act without knowing the right way forward.\nI am writing this message to help you respond to this uncertainty. All of you have the abilities to respond and the strength to do so. As a team we also have great strength to face this challenge.\nWe’ll start at the most immediate decisions and then think about how the uncertainty affects longer term decisions."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#the-uncertainty",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#the-uncertainty",
    "title": "Facing uncertainty",
    "section": "The uncertainty",
    "text": "The uncertainty\nHow the pandemic plays out if the world carries on as it has can be very well predicted by epidemiologists. Cases, deaths, recoveries are all known quite well. The outcome doesn’t look good if we continue as we are.\nEpidemiologists can also predict well what will happen with mild, moderate or drastic interventions, such as closing schools or banning face to face interaction.\nRecent modelling for the US suggests that a very strict lockdown of 1.5 years is needed to avoid massive fatalities and a broken health system. This will give time for a vaccine to be developed and a vaccination program implemented. If we go down this path, but change our minds before the vaccine, the pandemic will come back, as bad as ever.\nThe greatest uncertainty is in human behaviour and politics. We don’t know how society and our government will react once they see the hospital system overwhelmed and the deaths mounting. If they do start a lockdown, we don’t know if they can maintain it for long enough.\nWe also don’t know how either scenario would play out for the economy or for science careers."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#stay-home-or-go-out",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#stay-home-or-go-out",
    "title": "Facing uncertainty",
    "section": "Stay home or go out?",
    "text": "Stay home or go out?\nRight now the decision is whether to stay home and isolate, or to continue to participate in outside life to varying degrees. We are all fortunate in our jobs that we can make this decision and we have the option. Many people must face work now, or lose their jobs. For now this decision is yours, until the government makes it for us.\nIf you decide to keep going out, know that the risk of infection is high, very high. Soon it may be impossible to avoid. If you go out regularly, and keep up person to person contact, you are accepting that you will likely get sick. At this point, you likely will not get a chance to change your mind. You will get sick and we hope, recover with a stronger immune system.\nAlso know that this decision does impact others. You will be contributing to the spread of the disease in our community. So please be extreme in your hygiene and stay away from vulnerable groups if this is the decision you make.\nIf you decide to stay home, know that it will be a long haul. Months, maybe more than a year. You can change your mind later about this of course (unless it becomes illegal), so staying home now will keep your options open to some degree.\nIf you are sick or exposed to someone who is, of course, please stay home for at least 2 weeks.\nWhatever you do BE SAFE. Be 1000x more cautious than your normally are. Whether its driving, working around the house, or exercising. We need to take the stress of our health care system so those that really need it can access it."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#your-local-economy",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#your-local-economy",
    "title": "Facing uncertainty",
    "section": "Your local economy",
    "text": "Your local economy\nThe current pandemic will force a massive amount of economic change. Much of it will be hard, possibly a depression. It could be months or years. We cannot know. Be prudent, but generous, with your money. If you’re not good at budgeting, the Barefoot Investor book series is a great resource to help you get started with a budget free, but financially savvy plan.\nMany people, like those in the service industry or casual workers, will be facing a tougher economic time. So be generous too, to people who need it. They do need us to spend our money and they may need our help."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#compassion",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#compassion",
    "title": "Facing uncertainty",
    "section": "Compassion",
    "text": "Compassion\nMany people are oblivious to what is coming. Many can’t move with anxiety. We can’t say what responses individuals will have to this change. But we can help others be calm, and make better decisions, by showing compassion and being calm ourselves."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#making-decisions-when-faced-with-uncertainty",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#making-decisions-when-faced-with-uncertainty",
    "title": "Facing uncertainty",
    "section": "Making decisions when faced with uncertainty",
    "text": "Making decisions when faced with uncertainty\nThere are things you can do to make good decisions, even when the path is unclear.\nStay abreast of the latest information. Keep in touch with locals, information sharing through the local community will be a real asset right now. So invest in your local community.\nBe ready to respond and be adaptive in your decision making. Make plans, but be prepared to change them, at short notice. If you get the chance, gather data to test alternate future decisions. This could be very local, such as finding out which suburbs are safer to shop in. Or it could be longer term, like trying different routines at home for keeping fit.\nAs change is rapid right now, it is easy to get caught in making moment-by-moment decisions. Give yourself time to think one or two steps ahead. For instance, while everyone else is wondering if there will be a lockdown, get prepared for the possibility of being home a lot. While everyone else is wondering if they will lose casual jobs, ask yourself what casual job might have increased demand right now.\nClear communication with those around you is also important. That means your friends, local community and colleagues. Starting to feel sick? Then let your colleagues know you may be out of action for a while, as well as warning those you’ve seen recently."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#your-career",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#your-career",
    "title": "Facing uncertainty",
    "section": "Your career",
    "text": "Your career\nThis event will impact all of our careers. It seems unlikely that science careers will emerge unchanged from the pandemic and the economic change. We don’t know if these changes will be big or small, or what they’ll look like. Some of us may not have the same careers when this is over.\nBut there may also be new opportunities that emerge.\nFor now we continue toward our career goals as we were as much as possible, disrupted by the move to online interaction. But be ready to adapt. Look for these opportunities and be ready to take them, even as we face challenging times.\nLook to the predictors of past success as a guide, but don’t rely on them too heavily, this event is unprecedented.\nSocial connections and your network will absolutely be crucial, no matter how the economy changes. So make sure you keep investing in those. Find ways to make new bonds and strengthen them, even as we no longer talk face to face."
  },
  {
    "objectID": "posts/2020-03-18-facing-uncertainty/index.html#our-strengths",
    "href": "posts/2020-03-18-facing-uncertainty/index.html#our-strengths",
    "title": "Facing uncertainty",
    "section": "Our strengths",
    "text": "Our strengths\nOur team has a real strength in our focus on team work and the support we all get from that network. Also know that as a scientist, you have strengths in our technical expertise, analytical ability, curiosity for experimentation and creativity that many folk in other careers lack. As scientists working on knowledge frontiers we are all too aware of the discomfort of uncertainty and how to cope with that.\nIndividuals in our team also have their own unique strengths, whether that is reliability, a ‘getting things done attitude’, presentation skills or many, many other things. So remember that as a team we can draw on others with unique strengths.\nYour supervisors are also here to support you. We don’t know the best decisions right now, no one does. But we can help you make good decisions and we can help you cope with the uncertainty. So know that we face this uncertainty together.\nYours truly, Chris"
  },
  {
    "objectID": "posts/2015-08-05-conservation-priorities/2015-08-05-conservation-priorities.html",
    "href": "posts/2015-08-05-conservation-priorities/2015-08-05-conservation-priorities.html",
    "title": "What are conservation priorities?",
    "section": "",
    "text": "How should we identify priorities for conservation of biodiversity?\n\n\nMaps of ‘priorities’ for conservation are popular product of conservation research. One approach is to make ‘hotspots’ maps of species richness, or rare species, or rare species that have low protection (e.g. Jenkins et al. 2015 PNAS). However, such approaches defy decades of research on conservation planning.\n\n\n\n\n\nIn a letter to PNAS we explain why hotspots maps of species are not maps of conservation priorities. In fact, calling a map of species richness a map of priorities has been called one of the six common mistakes in conservation planning. Our letter was a reponse to Jenkins et al. 2015, and has created some controversy.\n\n\nFirst, everyone from CEOs to life-coaches knows that a succesful agenda requires clear objectives. Without clear objectives, we waste time and money because efforts are not coordinated. Conservation planning is no different.\n\n\nSecond, conservation plans should prioritise actions. Setting a clear objective helps us to define the actions we need to take to save biodiversity. If actions are not prioritised, mistakes can occur, for instance, different actions like conservation easments and setting up protected areas can have very different costs and feasibilities. The implicit assumption when researchers talk about prioritising places is that they are talking about protected areas as the action. If that is the case, then they should be explicit. There are many other actions that can be taken to protect species.\n\n\nThird, conservation actions are constrained by economic, social and political factors. At least some of these constraints should be accounted for when prioritising actions.\n\n\nFinally, if conservation aims to protect multiple species, we need to consider how those species are represented across different protected areas. An issue that was first discovered way back in 1983. A map of species richness, may identify two hotspots, but if those hotspots contain the same species, then protecting both of them will not represent all the species in a landscape in the protected areas.\n\n\nThe Jenkins et al. paper, which attempts to set conservation priorities for US protected lands provides a good example of why ‘hotspots’ approaches are dangerous. A previous analysis, but other authors, that did consider constraints to conservation found considerably different priorities for protection in the US.\n\n\nA better way\n\n\nWe recently asked how actions to conserve seagrass in the Mediterranean should be prioritised. We considered several actions, including avoiding areas with high coastal development (it was not considered politically feasible to revert coastal development); placing reefs to prevent illegal trawling on seagrass; and placing moorings to prevent yachts from dropping anchor on seagrass. Importantly, we considered the spatial distribution of seagrass, the distribution of threats to seagrass and the cost of stopping the threats. We were able to identify priorities places for each of these actions. It turned out that placing artificial reefs in the best seagrass meadows, despite their high cost, were the most cost-effective way to save seagrass.\n\n\n  \n\n\nWe also compared other approaches to prioritising actions, like prioritising actions in places with the greatest human impacts, and prioritising actions in places that had the most seagrass. We found that not considering the cost of actions could result in plans that were more than two times as costly as the plan that did consider costs (see figure). The plans that used only human impacts or seagrass cover to prioritise actions wasted resources in places that were either not threatened, or should be avoided because of coastal development impacts.\n\nMore details:\n\n\nUsing threat maps for cost-effective prioritization of actions to conserve coastal habitats (2015) S Giakoumi, CJ Brown, S Katsanevakis, M Saunders, HP Possingham Marine Policy 61 \n\n\nEffective conservation requires clear objectives and prioritizing actions, not places or species (in press) CJ Brown, M Bode, O Venter, MD Barnes, J McGowan, CA Runge, JEM Watson, HP Possingham PNAS online early"
  },
  {
    "objectID": "posts/2025-09-12-creating-a-first-draft-in-one-prompt/index.html",
    "href": "posts/2025-09-12-creating-a-first-draft-in-one-prompt/index.html",
    "title": "Paper in a prompt creating a first draft in one prompt",
    "section": "",
    "text": "*It was a rather long prompt\nI wanted to see if I could get an AI agent to complete a first draft of a paper, including data analysis, searching for references on the web and writing a complete draft.\nI used the Roo Code agent software with the Claude Sonnet 4.0 large language model.\nI’ve shared the results from replicate 1 here.\nI’ve also pasted the abstracts from the two replicates below. Both used identical prompts and settings in Roo Code. Its interesting how they are different. For instance the first one frames the study in terms of estimating the scale of human impacts. The second frames the study as being about studying an under-studied mechanism.\nThe experiment started with a project folder with data and detailed instructions for the research question, the data and the analyses I wanted. The instructions included details of R packages and specific statistics, for instance I told the model to use Bray-Curtis distances for the multivariate analysis.\nI also provided basic instructions for how to write the paper. The agent had web search capabilities, allowing it to access , and read real references, and then incorporate insights from those into the paper.\nIt is important to note that this draft has not been edited by a human. Parts of it are not scientifically accurate. It should not be read as anything other than an experiment. It contains errors and some of the references are false.\nFor instance, it made up the following reference, but oddly linked it to a real reference (that is different) using a real DOI:\nHamilton, Richard J et al. 2025. “Modeling Sediment Impacts on Juvenile Reef Fish.” Frontiers in Marine Science. https://doi.org/10.3389/fmars.2025.1513498.\nIt didn’t make much of an effort to create good instructions for writing a paper. I’m sure you could do a lot better than this with more careful prompting.\nAI generated papers should never replace human oversight. If you rely solely on AI, you miss the chance to truly understand your subject. Science is about learning, not just producing content.\nWhat this sort of thing could be useful for is as a way to iterate and quickly explore alternative options for how to write a paper, or explore alternative interpretations.\nIt can also help you get over ‘the fear of the white page’ (writers block), by giving you something to start editing."
  },
  {
    "objectID": "posts/2025-09-12-creating-a-first-draft-in-one-prompt/index.html#abstract-from-replicate-1",
    "href": "posts/2025-09-12-creating-a-first-draft-in-one-prompt/index.html#abstract-from-replicate-1",
    "title": "Paper in a prompt creating a first draft in one prompt",
    "section": "Abstract from replicate 1",
    "text": "Abstract from replicate 1\nTerrestrial logging threatens coral reef ecosystems through increased sedimentation, yet the spatial patterns of impact remain poorly understood. We examined benthic community composition at 49 coral reef sites across a gradient of logging impacts in Kia Province, Solomon Islands, spanning distances from 0.20 to 22.4 km from logging operations. Using multidimensional scaling ordination and permutational multivariate analysis of variance, we tested for differences in community structure between logged and not logged sites and across distance categories. Benthic communities differed significantly between logged and not logged sites (PERMANOVA p = 0.027), with logged sites showing 14.2% lower coral cover (48.0% vs 62.2%). Distance-dependent effects were highly significant (p = 0.002), explaining 11.9% of community variance. Sites within 1 km of logging showed severely degraded conditions (31.2% coral cover, 33.3% sand cover), while sites beyond 10 km maintained high coral cover (62.9%). Recovery followed a clear gradient: moderate distance sites (1-3 km) had 55.3% coral cover, far sites (3-10 km) had 60.4% coral cover. Categorical distance analysis proved more informative than continuous distance relationships, indicating threshold effects rather than linear gradients. These findings demonstrate that logging impacts extend further than previously documented, with measurable effects at sites 1-3 km from operations. Our results provide quantitative evidence for establishing buffer zones around coral reefs and highlight the need for watershed-scale conservation planning that integrates terrestrial forest management with marine protected area design."
  },
  {
    "objectID": "posts/2025-09-12-creating-a-first-draft-in-one-prompt/index.html#abstract-from-replicate-2",
    "href": "posts/2025-09-12-creating-a-first-draft-in-one-prompt/index.html#abstract-from-replicate-2",
    "title": "Paper in a prompt creating a first draft in one prompt",
    "section": "Abstract from replicate 2",
    "text": "Abstract from replicate 2\nCoral reef ecosystems face increasing threats from terrestrial activities, with logging-induced sedimentation representing a significant but understudied impact mechanism. This study examined benthic community structure across 49 coral reef sites in the Solomon Islands using multidimensional scaling (MDS) ordination to test the hypothesis that logging activities influence reef community composition through sedimentation. We applied non-metric multidimensional scaling to benthic cover data using both Euclidean and Bray-Curtis distance metrics, with environmental vector fitting to identify ecological gradients. The analysis revealed strong community gradients primarily associated with coral branching cover (r² = 0.82) and soft coral cover (r² = 0.62), while water clarity showed moderate correlation (r² = 0.24) with community structure. Contrary to expectations, distance to logging showed weak correlation with community patterns (r² = 0.056, p = 0.101), suggesting that logging impacts operate through pathways not captured by simple proximity measures. The results demonstrate MDS ordination effectiveness for revealing coral community structure patterns while highlighting the complexity of logging-reef relationships in tropical marine systems."
  },
  {
    "objectID": "posts/2017-07-21-ICCB2017-what-analysis-program/index.html",
    "href": "posts/2017-07-21-ICCB2017-what-analysis-program/index.html",
    "title": "What analysis program do conservation scientists use?",
    "section": "",
    "text": "With the International Congress for Conservation Biology starting 23rd July I was wondering, what analysis programs are most commonly used by conservation scientists? And, what do they use for spatial analysis and mapping?\nTo find out, if you are a conservation scientist please participate in these interactive polls:\n\n\n\nLoading poll…\n\n\n\n\n\nLoading poll…\n\nPlease share this post with your friends and colleagues. I will follow up with a discussion once we get some results in, hopefully during ICCB2017 in Columbia.\nIf you choose one of the ‘other’ options, let me know via twitter what program you do use."
  },
  {
    "objectID": "posts/2017-07-21-ICCB2017-what-analysis-program/index.html#international-congress-for-conservation-biology-what-analysis-program-do-conservation-scientists-use",
    "href": "posts/2017-07-21-ICCB2017-what-analysis-program/index.html#international-congress-for-conservation-biology-what-analysis-program-do-conservation-scientists-use",
    "title": "What analysis program do conservation scientists use?",
    "section": "",
    "text": "With the International Congress for Conservation Biology starting 23rd July I was wondering, what analysis programs are most commonly used by conservation scientists? And, what do they use for spatial analysis and mapping?\nTo find out, if you are a conservation scientist please participate in these interactive polls:\n\n\n\nLoading poll…\n\n\n\n\n\nLoading poll…\n\nPlease share this post with your friends and colleagues. I will follow up with a discussion once we get some results in, hopefully during ICCB2017 in Columbia.\nIf you choose one of the ‘other’ options, let me know via twitter what program you do use."
  },
  {
    "objectID": "posts/2018-09-29-R-courses-February-2019/index.html",
    "href": "posts/2018-09-29-R-courses-February-2019/index.html",
    "title": "Beginner Intermediate and Advanced R courses February 2019",
    "section": "",
    "text": "This is an announcement of our next set of R courses. There are 4 separate courses taught over 5th-8th February 2019.\nYou can register with the Centre for Applications in Resource Management site at the University of Queensland.\nThese R workshops are especially tailored for biology/ecology, but will be useful for anyone working in sciences. We often have people from other sciences, like medicine attend.\nLocation is at the Centre for Applications in Natural Resource Management, The University of Queensland in Brisbane Australia. Student discounts on course fees are available.\nPresenters: Prof Anthony Richardson (UQ Centre for Applications in Natural Resource Mathematics), Associate Prof David Schoeman (University of the Sunshine Coast), Dr Chris Brown (Griffith University).\nThe Advanced R course is taught by Dr Bill Venables (CSIRO) and long-time R developer.\nWe provide comprehensive workshop notes, including basic theory, applied worked examples in biology/ecology, and solution code. Here is the course schedule for 2019. In 2019 we have added a whole day for graphics, especially the use of the ‘ggplot2’ R package for plotting complex datasets."
  },
  {
    "objectID": "posts/2018-09-29-R-courses-February-2019/index.html#introduction-to-r-1-day",
    "href": "posts/2018-09-29-R-courses-February-2019/index.html#introduction-to-r-1-day",
    "title": "Beginner Intermediate and Advanced R courses February 2019",
    "section": "INTRODUCTION to R (1 day)",
    "text": "INTRODUCTION to R (1 day)\nGetting data into R; Summary Statistics; Simple Graphics; Manipulating Data; Simple statistics"
  },
  {
    "objectID": "posts/2018-09-29-R-courses-February-2019/index.html#intermediate-r-2-days",
    "href": "posts/2018-09-29-R-courses-February-2019/index.html#intermediate-r-2-days",
    "title": "Beginner Intermediate and Advanced R courses February 2019",
    "section": "INTERMEDIATE R (2 days)",
    "text": "INTERMEDIATE R (2 days)\nANOVA; Regression; Model selection; GLMs for binary and count data; GAMS; TREEs; Clustering; Ordination"
  },
  {
    "objectID": "posts/2018-09-29-R-courses-February-2019/index.html#ggplot-and-spatial-analysis-in-r-1-day",
    "href": "posts/2018-09-29-R-courses-February-2019/index.html#ggplot-and-spatial-analysis-in-r-1-day",
    "title": "Beginner Intermediate and Advanced R courses February 2019",
    "section": "ggplot and spatial analysis in R (1 day)",
    "text": "ggplot and spatial analysis in R (1 day)\nData wrangling; ggplot; Mapping; Spatial analysis\nThis workshop is for you if you have been using R and want to improve your skills."
  },
  {
    "objectID": "posts/2018-09-29-R-courses-February-2019/index.html#advanced-r-3-days",
    "href": "posts/2018-09-29-R-courses-February-2019/index.html#advanced-r-3-days",
    "title": "Beginner Intermediate and Advanced R courses February 2019",
    "section": "ADVANCED R (3 days)",
    "text": "ADVANCED R (3 days)\nAdvanced data manipulation and visualisation; Advanced graphics; Statistical model building based on traditional and machine learning techniques; Programming and software development in the R language.\nIf you have used R extensively, then the Advanced R Workshop is for you."
  },
  {
    "objectID": "posts/2018-09-29-R-courses-February-2019/index.html#testimonials",
    "href": "posts/2018-09-29-R-courses-February-2019/index.html#testimonials",
    "title": "Beginner Intermediate and Advanced R courses February 2019",
    "section": "Testimonials",
    "text": "Testimonials\nJune 2017: “I truly enjoyed the whole workshop. Great teachers, really nice real data examples and good notes and background. Everything was very well thought out and explained.”\nJune 2017: “I was really impressed with everything covered and how it was presented. I am feeling a lot better about using stats now than I was previously. I think it should be a compulsory course for all PhDs.”\nContact: carm@maths.uq.edu.au for more information or visit Centre for Applications in Resource Management site at UQ to register."
  },
  {
    "objectID": "posts/2015-09-23-fast-load/index.html",
    "href": "posts/2015-09-23-fast-load/index.html",
    "title": "Quickly load many files into R",
    "section": "",
    "text": "I had the pleasure of attending the Earth to Ocean group’s (at Simon Fraser University) ‘StatsBeers’ yesterday. One question that came up was how to quickly summarise data from multiple .csv files. If you are working with multiple data files, for instance some genetics programs spit out numerous files of sequences, then you want to make sure you are consistent in your variable name. Consistency will make it easy to write programs for post-processing. Another application is processing satellite images. For instance, I recently wanted to summarise water quality measurements for Fiji and had over 100 images over the space of several years.\nHere is a quick how to for summarising data from multiple files using R. Note that R is not the fastest program for these kind of operations. So if you have really big data frames, you might want to investigate using Python.\nFirst, you want to get all you files into one folder. Or at least, into sub-folders of one folder. Then start up R.\nSet the working directory to the folder with our data. For instance on my mac:\nsetwd('Users/chrisbrown/documents/data')\nNow we can find out the names of files that live in this folder:\nfnams &lt;- list.files()\nfnams\nnfiles &lt;- length(fnams)\nWe have stored the names and also used length() to find out how many files there are.\nNow, lets assume we want to know the mean of the nums column of each of the data frames. We can loop through the files, loading each in turn and taking the mean of the nums column. Recall that I said consistency in naming is key. If you want the mean of a different variable in every data frame, you will need to think of a way to identify the respective columns. For instance, you could try grep() to identify patterns in column names. Thus, life will be a lot simpler if our column always has the same name. So here is the loop:\nnums_means &lt;- rep(NA, nfiles)\nfor (ifiles in 1:nfiles){\n    x &lt;- read.csv(fnams[ifiles], header = TRUE)\n    nums_means[files] &lt;- mean(x$nums)\n}\nhist(nums_means)\nA few points here. Before the loop we pre-allocate a variable nums_means with NAs, which will store our means. Preallocation speeds up processing. We then load each csv file in turn and ask for the mean, returning it to sequential places in nums_means. We finished with a histogram, to get an idea of how the means look across datafiles.\nThat is it. Good luck. I hope this saves you some time."
  },
  {
    "objectID": "posts/2015-09-23-fast-load/index.html#how-to-quickly-load-many-files-into-r",
    "href": "posts/2015-09-23-fast-load/index.html#how-to-quickly-load-many-files-into-r",
    "title": "Quickly load many files into R",
    "section": "",
    "text": "I had the pleasure of attending the Earth to Ocean group’s (at Simon Fraser University) ‘StatsBeers’ yesterday. One question that came up was how to quickly summarise data from multiple .csv files. If you are working with multiple data files, for instance some genetics programs spit out numerous files of sequences, then you want to make sure you are consistent in your variable name. Consistency will make it easy to write programs for post-processing. Another application is processing satellite images. For instance, I recently wanted to summarise water quality measurements for Fiji and had over 100 images over the space of several years.\nHere is a quick how to for summarising data from multiple files using R. Note that R is not the fastest program for these kind of operations. So if you have really big data frames, you might want to investigate using Python.\nFirst, you want to get all you files into one folder. Or at least, into sub-folders of one folder. Then start up R.\nSet the working directory to the folder with our data. For instance on my mac:\nsetwd('Users/chrisbrown/documents/data')\nNow we can find out the names of files that live in this folder:\nfnams &lt;- list.files()\nfnams\nnfiles &lt;- length(fnams)\nWe have stored the names and also used length() to find out how many files there are.\nNow, lets assume we want to know the mean of the nums column of each of the data frames. We can loop through the files, loading each in turn and taking the mean of the nums column. Recall that I said consistency in naming is key. If you want the mean of a different variable in every data frame, you will need to think of a way to identify the respective columns. For instance, you could try grep() to identify patterns in column names. Thus, life will be a lot simpler if our column always has the same name. So here is the loop:\nnums_means &lt;- rep(NA, nfiles)\nfor (ifiles in 1:nfiles){\n    x &lt;- read.csv(fnams[ifiles], header = TRUE)\n    nums_means[files] &lt;- mean(x$nums)\n}\nhist(nums_means)\nA few points here. Before the loop we pre-allocate a variable nums_means with NAs, which will store our means. Preallocation speeds up processing. We then load each csv file in turn and ask for the mean, returning it to sequential places in nums_means. We finished with a histogram, to get an idea of how the means look across datafiles.\nThat is it. Good luck. I hope this saves you some time."
  },
  {
    "objectID": "posts/2018-07-02-postdoc-in-aquatic-ecology/index.html",
    "href": "posts/2018-07-02-postdoc-in-aquatic-ecology/index.html",
    "title": "Postdoctoral job in Aquatic Ecology",
    "section": "",
    "text": "We are seeking a postdoctoral fellow in Aquatic Ecology to work on wetlands science and environmental accounting. The person will be supervised by Dr Chris Brown and work with Dr Mark Kennard and Professor Rod Connolly to bring together two projects. This is a 2 year position.\nApplications close 30th July. Apply here, read more below or in the position description (links to a pdf).\nThe Project\nThis project will develop an economic-environmental accounting approach that will provide clear performance metrics to help inform development and conservation of aquatic habitats. The environmental accounts will help inform water planning and management of coastal wetland ecosystems. The project partners two major programs run by the Australian Rivers Institute. The first, is a consortium of organisations, including the Australian Rivers Institute at Griffith University, who are involved in research under the National Environmental Science Program (NESP) Northern Australia Environmental Resources Hub. NESP Hub partners undertake research to support sustainable development of Australia’s unique northern environments. The Australian Rivers Institute is currently running two complementary projects through the NESP Hub on environmental economic accounting and environmental flow management in northern Australia’s river catchments.\nThe second program is the new Global Wetlands research program, which is developing and applying globally applicable assessments of coastal wetland health to drive actions and investments towards protection, restoration and improvement of coastal wetlands worldwide. Coastal wetlands – mangroves, seagrass and saltmarsh – are valuable ecosystems with high biodiversity. However as the world’s coastal cities continue to grow and industrialise, many coastal wetlands are facing major declines in ecosystem health.\nTo accomplish the programs’ objectives, the Institute is recruiting a research fellow who has a productive publication record and is skilled in the synthesis of ecological data. The research fellow will synthesise existing research on environmental flows and wetland health in Northern Australia. They will use the synthesis to inform a new approach to economic-environmental accounting, developed in collaboration with a team that includes economists and social scientists. Finally, they will work with the GIS experts in the Global Wetlands program to apply the accounting approach to report on the health of coastal wetlands at a global scale.\nThis is an opportunity to be part of major initiatives promoting the health of key aquatic ecosystems. The research fellow will work in a supportive team with complementary skills in ecology, economics, social science, statistical modelling, big data, and science communication.\nThe Role\nThe Postdoctoral Research Fellow will work closely with project Chief Investigators and colleagues in the Australian Rivers Institute. The Postdoctoral Research Fellow will be required to actively contribute to these research projects, and as a member of the Australian Rivers Institute, to broader Institute activities. The role will require high level skills in aquatic ecology, data analysis, stakeholder engagement and communications writing. This is a 2 year, full time position, located at either the Gold Coast or Nathan campuses, however flexible working arrangements may be negotiated.\nThe person\nThe successful applicant will have completed a PhD in aquatic ecology, have demonstrated skills in synthesis of ecological data and be enthusiastic about interdisciplinary collaboration. They will have published lead authored papers in esteemed journals, and have excellent interpersonal and communication skills with the ability to work effectively within a team.\nGriffith University values gender equality, diversity and inclusion and is committed to providing equal employment opportunities to persons from all backgrounds and identities. It offers a supportive and flexible working environment with excellent working conditions. For further information follow this link.\nAll applications must be submitted online.\nFor further information about this role, please contact Dr Chris Brown, email chris.brown@griffith.edu.au.\nFor application queries, please contact Ms Kaitlin Reents, HR Officer on +61 (0) 7 3735 6614."
  },
  {
    "objectID": "posts/2024-10-18-papers-with-impact-on-management/index.html",
    "href": "posts/2024-10-18-papers-with-impact-on-management/index.html",
    "title": "How your research paper can have impact on environmental management",
    "section": "",
    "text": "If you ask someone who the intended audience for their paper is they will often say ‘managers’.\nIts often said that managers don’t read papers, but that’s not quite try. Its more helpful to think about a hierarchy of ways our published research can impact management. This will help you tailor the message, and communications outputs to different audiences.\nThree tiers of audience to think about for your paper to reach managers: (1) managers and their (non-scientific) advisors; (2) scientific advisors of managers; (3) scientists working on this topic.\nI’m going to use my recent study on timelines for ecosystem recovery as an example. The intended audience was primarily researchers, but the findings also have implications for management.\nIn brief, found that recovery times for coastal habitats are often longer and more variable than might be expected. This means when you try to restore a coastal habitat, it might not happen right away. However, appropriate management interventions can significantly accelerate the recovery process."
  },
  {
    "objectID": "posts/2024-10-18-papers-with-impact-on-management/index.html#managers-and-their-non-science-advisors",
    "href": "posts/2024-10-18-papers-with-impact-on-management/index.html#managers-and-their-non-science-advisors",
    "title": "How your research paper can have impact on environmental management",
    "section": "1. Managers and their non-science advisors",
    "text": "1. Managers and their non-science advisors\nManagement roles are as diverse as scientific disciplines, so its better to think about the type of manager you’d like to influence, rather than just ‘managers’ as a homogenous category.\nThen think about what decisions the managers are making. The decisions managers make are varied. Conservation research is often about informing where or how managers act. But the there are many smaller decisions that are made along the way.\nFor instance, we looked at what types of restoration actions (planting seagrass vs stabilising sediments to improve seagrass recruitment) are best under what circumstances. The type of restoration is a relatively big decision that maybe can be made once and not easily changed.\nThere are many other smaller decisions. For example, our estimations of recovery times could influence how communication about conservation is made. It is important to set expectations about recovery and not to over-promise.\nLikewise, managers are often making decisions about how to allocate budgets and resources. So for our research, an implication could be how managers allocate resources to improving restoration actions versus identifying the drivers of ecosystem loss.\nManagers are unlikely to read your paper directly (with some exceptions I’ll detail below). So if managers are the audience for your research, you’ll need to do some additional communication. This could be via media releases, or better yet, presenting to managers at management forums, or collaborating directly with them.\nIn my experience many conservation managers are reading papers, and in fact, study science or even used to be scientists. These people may make the time to read your paper if its direclty relevant them and it helps to email it to them as well.\nFor examples, decisions makers in non-governmental organisations often have a scientific background. The types of decisions they might be making are how to allocate project budgets among monitoring, implementation and community engagement and what types of activities to chase funding for.\nIt helps if you are already collaborating with your target audience, so that you can better understand their needs."
  },
  {
    "objectID": "posts/2024-10-18-papers-with-impact-on-management/index.html#scientific-advisors-to-managers",
    "href": "posts/2024-10-18-papers-with-impact-on-management/index.html#scientific-advisors-to-managers",
    "title": "How your research paper can have impact on environmental management",
    "section": "2. Scientific advisors to managers",
    "text": "2. Scientific advisors to managers\nScientific advisors to managers can also be varied. For exmample, they might be academics who collaborate with managers or government scientists.\nThis target audience is more likely to read your paper and understand more of the technical details. In our study on recovery times we suggested the new models we developed could be applied to any coastal habitat restoration to provide estimates of recovery times.\nSo the decisions this group of people might be making include: what should we focus on in our research and what methods should we use? Think about how you can influence those decisions.\nOf course they need to know about your paper first. So the usual things of emailing people, doing social media and presenting at conferences are important.\nThe audience of scientists advising managers is the one most able to be able to implement the technics we described and directly use the research. In my case, that would mean using the R code we provided that implements the methods we described.\nCommunication to this audience is best via direct contact or presenting and meeting people at conferences. You are more likely to meet these types of people if you target discipline and region specific conferences where your research has greatest direct relevance. At big international conferences the audience will be more interested in the general conclusions."
  },
  {
    "objectID": "posts/2024-10-18-papers-with-impact-on-management/index.html#scientists-working-on-your-topic",
    "href": "posts/2024-10-18-papers-with-impact-on-management/index.html#scientists-working-on-your-topic",
    "title": "How your research paper can have impact on environmental management",
    "section": "3. Scientists working on your topic",
    "text": "3. Scientists working on your topic\nThe final audience is scientists working to advance research in your area of interest.\nThis is the traditional audience for research papers. Things this group does (and there may be overlap) will hopefully influence upwards to levels 2 and 3.\nSo by raising the bar on your research topic you may still ultimately be contributing to management. It’s just going to take longer and the specific impact of your paper on management will be harder to trace.\nYou can tailor your communciations about the paper appropriately. For instance, international conferences are a good opportunity to meet researchers and talk about the general implications of your research for the field."
  },
  {
    "objectID": "posts/2024-10-18-papers-with-impact-on-management/index.html#conclusion",
    "href": "posts/2024-10-18-papers-with-impact-on-management/index.html#conclusion",
    "title": "How your research paper can have impact on environmental management",
    "section": "Conclusion",
    "text": "Conclusion\nAll three types of audience are legitimiate audiences for your research. It helps to think about the audience in advance so you can tailor your paper and communications in advance. In particular, it will affect how you write your abstract (do you mention management implications or not) and where you put your effort for communications and engagement.\nSo when you say ‘I want my paper to impact management’ think about who these people are and what decisions they are making. You don’t have to reach managers directly to have an impact on management."
  },
  {
    "objectID": "posts/2016-10-28-simulations-with-purrr/index.html",
    "href": "posts/2016-10-28-simulations-with-purrr/index.html",
    "title": "A ‘tidyverse’ approach to simulation models",
    "section": "",
    "text": "I am a big fan of Rstudio’s (mainly led by Hadley Wickham) series of R packages known as the ‘tidyverse’. The packages modernise much of R’s functionality to better deal with moderately large databases and programming.\nThere is even a tidyverse package now, so you can hit up all their packages with one short library(tidyverse) call.\nIf you are into programming (ie doing more than just running a couple of linear models) then I recommend you check out the purrr package. It provides many functions that basically make looping easier and faster to code.\n\n\n\nA stochastic simulation of population size generated using a loop (black line) and the accumulate function from purrr package (red points)\n\n\nHere I explore how to use purrr to create a simple simulation model where a state variable at a time depends on its past state.\nSkipping to my main point, it turns out that purrr may be more convenient in that it saves typing over writing loops. However, in this case the purrr function required more computational time.\nThe model we will use to test purrr will create stochastic simulations of population size that look something like is pictured.\nFirst load the tidyverse package (or just purrr) and specify some parameters.\nlibrary(tidyverse)\n#timesteps\ntmax &lt;- 100\n#pop growth\nr &lt;- 1.2\n#capacity\nK &lt;- 20\n#abundance sd\nsdev &lt;- 0.1\nNow we generate a series of random numbers. We will multiply these by population size at each time-step to create stochastic variation:\nset.seed(42) #so we get the same numbers\npmult &lt;- exp(rnorm(100, sd = 0.1))\nNow, write a function that specifies how abundance changes at a single time point:\npopmod &lt;- function(N, popmult, r, K){\n    N * (r  - r*(N / K))*popmult\n}\n\n\n\nThe standard for loop was faster than accumulate\n\n\nNow, let’s implement a function that uses a normal for loop to loop over abundances at time.\nf &lt;- function(pmult, tmax){\n    x2 &lt;- rep(NA, tmax)\n    x2[1] &lt;- 0.1\n    for (t in 2:tmax){\n        x2[t] &lt;-\n        popmod(x2[t-1],pmult[t-1],  r, K)\n   }\n  x2\n}\nWe can use our new function to simulate abundance at time like this (as pictured):\nloopabund &lt;- f(pmult, tmax)\nNow for the purrr equivalent. We will use the accumulate function, which loops over a vector sequentially. Basically we would want to do this because it saves typing. Arguably it can also make your code more readable (assuming your reader knows how accumulate works).\naccumulate(pmult, ~popmod(.x, .y, r = r, K = K), .init = 0.1)\nAccumulate works by taking a vector then iteratively applying a function. In this case we have applied it to our popmod function.\nWe have written our function as a formula, proceeded by ~, for convenience. This means we can use the .x and .y arguments. The .x is the argument that will be accumulated (ie population size). The .y is the argument that we iteratively apply the function to - in this case the vector of multiples.\nWe can wrap our use of accumulate in a function, for convenience later.\nf2 &lt;- function(pmult) {\n    x3 &lt;- accumulate(pmult, ~popmod(.x, .y, r = r, K = K), .init = 0.1)\nx3\n}\nNow we have two ways to simulate the same population. Let’s check they are the same by plotting them over each other:\naccabund &lt;- f2(pmult)[1:tmax]\n\n#put our two simulations in a data.frame\ndatp &lt;- data.frame(time = 1:tmax,\n    loopabund = loopabund,\n    accabund = accabund)\n\n    ggplot(datp, aes(x = time, y = loopabund)) +\n    geom_line() +\n    geom_point(aes(y = accabund), color = 'red') +\n    ylab(\"Abundance\")\nSo they are equivalent. We can then test for computational efficiency using the microbenchmark package:\nmicrobenchmark::microbenchmark(f(pmult, tmax), f2(pmult)) %&gt;% boxplot()\nIt turns out that the for loop was about 40% faster than the purrr function aggregate. Such a time difference is trivial for this problem, but for more complex models could make a big difference. So the choice between purrr and base R comes down to whether you want to emphasise ease of coding and readability or speed of computation."
  },
  {
    "objectID": "posts/2016-10-28-simulations-with-purrr/index.html#a-tidyverse-approach-to-simulation-models",
    "href": "posts/2016-10-28-simulations-with-purrr/index.html#a-tidyverse-approach-to-simulation-models",
    "title": "A ‘tidyverse’ approach to simulation models",
    "section": "",
    "text": "I am a big fan of Rstudio’s (mainly led by Hadley Wickham) series of R packages known as the ‘tidyverse’. The packages modernise much of R’s functionality to better deal with moderately large databases and programming.\nThere is even a tidyverse package now, so you can hit up all their packages with one short library(tidyverse) call.\nIf you are into programming (ie doing more than just running a couple of linear models) then I recommend you check out the purrr package. It provides many functions that basically make looping easier and faster to code.\n\n\n\nA stochastic simulation of population size generated using a loop (black line) and the accumulate function from purrr package (red points)\n\n\nHere I explore how to use purrr to create a simple simulation model where a state variable at a time depends on its past state.\nSkipping to my main point, it turns out that purrr may be more convenient in that it saves typing over writing loops. However, in this case the purrr function required more computational time.\nThe model we will use to test purrr will create stochastic simulations of population size that look something like is pictured.\nFirst load the tidyverse package (or just purrr) and specify some parameters.\nlibrary(tidyverse)\n#timesteps\ntmax &lt;- 100\n#pop growth\nr &lt;- 1.2\n#capacity\nK &lt;- 20\n#abundance sd\nsdev &lt;- 0.1\nNow we generate a series of random numbers. We will multiply these by population size at each time-step to create stochastic variation:\nset.seed(42) #so we get the same numbers\npmult &lt;- exp(rnorm(100, sd = 0.1))\nNow, write a function that specifies how abundance changes at a single time point:\npopmod &lt;- function(N, popmult, r, K){\n    N * (r  - r*(N / K))*popmult\n}\n\n\n\nThe standard for loop was faster than accumulate\n\n\nNow, let’s implement a function that uses a normal for loop to loop over abundances at time.\nf &lt;- function(pmult, tmax){\n    x2 &lt;- rep(NA, tmax)\n    x2[1] &lt;- 0.1\n    for (t in 2:tmax){\n        x2[t] &lt;-\n        popmod(x2[t-1],pmult[t-1],  r, K)\n   }\n  x2\n}\nWe can use our new function to simulate abundance at time like this (as pictured):\nloopabund &lt;- f(pmult, tmax)\nNow for the purrr equivalent. We will use the accumulate function, which loops over a vector sequentially. Basically we would want to do this because it saves typing. Arguably it can also make your code more readable (assuming your reader knows how accumulate works).\naccumulate(pmult, ~popmod(.x, .y, r = r, K = K), .init = 0.1)\nAccumulate works by taking a vector then iteratively applying a function. In this case we have applied it to our popmod function.\nWe have written our function as a formula, proceeded by ~, for convenience. This means we can use the .x and .y arguments. The .x is the argument that will be accumulated (ie population size). The .y is the argument that we iteratively apply the function to - in this case the vector of multiples.\nWe can wrap our use of accumulate in a function, for convenience later.\nf2 &lt;- function(pmult) {\n    x3 &lt;- accumulate(pmult, ~popmod(.x, .y, r = r, K = K), .init = 0.1)\nx3\n}\nNow we have two ways to simulate the same population. Let’s check they are the same by plotting them over each other:\naccabund &lt;- f2(pmult)[1:tmax]\n\n#put our two simulations in a data.frame\ndatp &lt;- data.frame(time = 1:tmax,\n    loopabund = loopabund,\n    accabund = accabund)\n\n    ggplot(datp, aes(x = time, y = loopabund)) +\n    geom_line() +\n    geom_point(aes(y = accabund), color = 'red') +\n    ylab(\"Abundance\")\nSo they are equivalent. We can then test for computational efficiency using the microbenchmark package:\nmicrobenchmark::microbenchmark(f(pmult, tmax), f2(pmult)) %&gt;% boxplot()\nIt turns out that the for loop was about 40% faster than the purrr function aggregate. Such a time difference is trivial for this problem, but for more complex models could make a big difference. So the choice between purrr and base R comes down to whether you want to emphasise ease of coding and readability or speed of computation."
  },
  {
    "objectID": "posts/2020-10-26-R-courses-Brisbane/index.html",
    "href": "posts/2020-10-26-R-courses-Brisbane/index.html",
    "title": "R workshops in Brisbane 2021",
    "section": "",
    "text": "Our annual series of popular R workshops is on again between 22nd and 25th June 2021, in person at the University of Queensland. Over the past 5 years we’ve successfully trained hundreds of academics, researchers and students. There are 4 courses that range from introductory to advanced and focus on applications of R to common types of biological data analysis. We also teach GIS and spatial analysis in R.\nThe courses are taught by analysis experts and experienced teachers Ant Richardson (University of Queensland), Rhetta Chappel (Griffith University), Bill Venables (CSIRO), Chris Brown (Griffith University) and Dave Schoeman (University of Sunshine Coast).\nWe hope to meet in person! But refunds will be available if meeting/travel restrictions prevent that.\nMORE DETAILS AND REGISTRATION HERE"
  },
  {
    "objectID": "research/2024-01-01-Global-Wetlands-Project.html",
    "href": "research/2024-01-01-Global-Wetlands-Project.html",
    "title": "Global Wetlands Project",
    "section": "",
    "text": "Science to conserve the world’s seagrass meadows, mangrove forests and saltmarshes. This international collaboration lead by Dr Chris Brown, Professor Rod Connolly and Dr Fernanda Adame. It includes a team of more than 15 researchers and students\nThe Global Wetlands Project (GLOW) elevates scientific understanding of critical issues facing coastal wetlands and builds online tools to help address these issues.\nThe world needs coastal wetlands. Seagrass meadows, mangrove forests and saltmarshes are high-value habitats providing many benefits to humanity.\nUnfortunately, we’re losing these important places. There’s an urgent need to limit and reverse the loss of coastal wetlands in order to arrest biodiversity decline, protect communities and tackle climate change.\nThis group uses the lastest science and monitoring technology to address coastal wetland conservation issues. There most recent research looks at ways of automating monitoring to inform coastal wetland management."
  },
  {
    "objectID": "research/2021-12-01-future-fellowship.html",
    "href": "research/2021-12-01-future-fellowship.html",
    "title": "Fellowship on predicting coastal ecological futures in an era of unprecedented change",
    "section": "",
    "text": "We are entering a new era of science where ecosystem change is uncharted by historical observations. Observations are the foundation of predictive science, so how can we predict a future that is outside the envelope of the environments?\nThis is the question I am addressing in a 2021 Australian Research Council Future Fellowship. Specifically, I will focus on predicting future change for coastal ecosystems.\nWatch a short video about the project\nHumanities footprint on the Earth’s ecosystem has never been greater. The oceans are increasingly being industrialized, and climate change means the footprint of human economic activity now touches even the most remote ecosystems.\nCoastal ecosystems exemplify the challenges we face in managing our growing impacts. Coasts are the thin line on which much of the human population and industrial activity rests. This human activity places great pressures on coastal ecosystems, and in many places, we are losing species and the ecological functions that support human livelihoods, like fisheries.\nPredicting the future for coastal ecosystems is critical for creating a sustainable relationship between humanity and nature. Ecological predictions allow us to identify options for better managing our relationship with ecosystems.\nReliable predictions are needed to help managers mitigate the risks of future human activities to the environment. In this four-year fellowship I will lead a team to develop new modelling techniques for predicting ecosystem change.\nA couple of key research outputs to date\nIn Assessing predictive accuracy of species abundance models in dynamic systems (published Methods in Ecology and Evolution 2025) I show how we can validate models in rapidly changing environments. We show that the quality of prediction degrades (and errors get bigger) in ecosystems that are undergoing rapid change from climate change and invasive species.\nIn Projecting Uncertainty in Ecosystem Persistence Under Climate Change (published Global Change Biology 2025) we developed a method to estimate probabilities of mangrove persistence using global datasets. This gives us a way to measure uncertainty that we can use to guide conservation decisions, e.g. to identify where management actions are most likely to succeed. This research shows a new way to handle ecosystem predictions under climate change. Instead of false precision, we quantify what we know and don’t know. For mangroves, significant seaward losses are likely, but landward gains and conservation success remain possible with strategic action. R"
  },
  {
    "objectID": "research/sea-turtle-conservation-pacific.html",
    "href": "research/sea-turtle-conservation-pacific.html",
    "title": "Sea turtle conservation in the Pacific",
    "section": "",
    "text": "We are working the The Nature Conservancy on marine conservation across the Pacific. One example is our work in the Solomon Islands.\nThe Solomon Islands is home to globally significant sea turtle populations, yet faces mounting conservation challenges. Our research has provided insights into how fisheries and protected areas impact these endangered species—and how science can drive real-world change.\nA national-scale study estimated that ~12,000 turtles were caught in 2018, with up to 97% harvested in small-scale coastal fisheries (SSFs). Surprisingly, these fisheries, often overlooked by international funders, pose a greater threat to turtles than industrial scale longline bycatch. Community-based monitoring revealed that green turtles dominate catches, and harvest rates have declined 4.9-fold over 30 years—clear signs of overfishing. These findings shaped the Solomon Islands’ National Plan of Action for Marine Turtles (2023–2027), emphasizing sustainable SSF management.\nIn parallel, satellite tracking of 30 nesting hawksbill turtles in the Arnavon Community Marine Park (ACMP) showed that 98.5% remained within park boundaries during nesting, validating its design. Early results led to ACMP’s declaration as the nation’s first national park in 2017. Tracking also exposed poaching hotspots, prompting the establishment of a new ranger station.\n\n\n\nImage: Migration of hawksbill turtles from the Solomon Islands\n\n\nTogether, these projects highlight how community-driven science can rapidly inform policy and protect endangered wildlife."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html",
    "title": "Data visuals notes for my talks in 2017",
    "section": "",
    "text": "Supplementary notes for CJ Brown’s talks on dataviz in 2017 for Griffith University’s honours students and the UQ Winterschool in Bioinformatics."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#data-visuals-2017",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#data-visuals-2017",
    "title": "Data visuals notes for my talks in 2017",
    "section": "",
    "text": "Supplementary notes for CJ Brown’s talks on dataviz in 2017 for Griffith University’s honours students and the UQ Winterschool in Bioinformatics."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#structure-of-this-talk",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#structure-of-this-talk",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Structure of this talk",
    "text": "Structure of this talk\n\nTools for dataviz\nEleven principles for effective dataviz\nBreaking the rules\nChange the world"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#the-ideal-dataviz-tool",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#the-ideal-dataviz-tool",
    "title": "Data visuals notes for my talks in 2017",
    "section": "The ideal dataviz tool",
    "text": "The ideal dataviz tool\n\nSensible and clean defaults\nFast and convenient production of common graphics\nConvenient to plot statistical models\nFlexible enough to realize our creative thoughts"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#r-integrates-different-tools",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#r-integrates-different-tools",
    "title": "Data visuals notes for my talks in 2017",
    "section": "R integrates different tools",
    "text": "R integrates different tools\n\nData merging\nMaps\nGraphs\nAnalysis\nWord processing\nPresentations (including this one)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#r-is-a-flexible-tool",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#r-is-a-flexible-tool",
    "title": "Data visuals notes for my talks in 2017",
    "section": "R is a flexible tool",
    "text": "R is a flexible tool\n\nFlexibility is also it’s weakness\nHard to start\nSo many options to do the same thing\nSteep learning curve\nOften combine with other tools"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#graphics-packages-in-r",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#graphics-packages-in-r",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Graphics packages in R",
    "text": "Graphics packages in R\nThe dominant options are the base graphics R comes shipped with and the ggplot2 package."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#plot-your-data",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#plot-your-data",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Plot your data",
    "text": "Plot your data\n\nMake your own Datasaurus dozen\nThe datasaurus is a great example of why you should view your data, invented by Alberto Cairo. See Steph Locke’s code and package on github for making this in R.\nlibrary(datasauRus)\ndatnames &lt;- rev(unique(datasaurus_dozen$dataset))\nnlevels &lt;- length(datnames)\n\nfor (i in 1:nlevels){\n  i &lt;- which(datasaurus_dozen$dataset == datnames[i])\n  plot(datasaurus_dozen$x[i], datasaurus_dozen$y[i],\n     xlab = \"x\", ylab = \"y\",   las = 1)\n  Sys.sleep(1)\n}\n\nConvince yourself that the mean, sd and correlation is the same in all of these plots:\nlibrary(dplyr)\ndatasaurus_dozen %&gt;% group_by(dataset) %&gt;%\n    summarize(meanx = mean(x), meany = mean(y),\n              sdx = sd(x), sdy = sd(y),\n              corr = cor(x,y))\n\n## # A tibble: 13 × 6\n##       dataset    meanx    meany      sdx      sdy        corr\n##         &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n## 1        away 54.26610 47.83472 16.76982 26.93974 -0.06412835\n## 2    bullseye 54.26873 47.83082 16.76924 26.93573 -0.06858639\n## 3      circle 54.26732 47.83772 16.76001 26.93004 -0.06834336\n## 4        dino 54.26327 47.83225 16.76514 26.93540 -0.06447185\n## 5        dots 54.26030 47.83983 16.76774 26.93019 -0.06034144\n## 6     h_lines 54.26144 47.83025 16.76590 26.93988 -0.06171484\n## 7  high_lines 54.26881 47.83545 16.76670 26.94000 -0.06850422\n## 8  slant_down 54.26785 47.83590 16.76676 26.93610 -0.06897974\n## 9    slant_up 54.26588 47.83150 16.76885 26.93861 -0.06860921\n## 10       star 54.26734 47.83955 16.76896 26.93027 -0.06296110\n## 11    v_lines 54.26993 47.83699 16.76996 26.93768 -0.06944557\n## 12 wide_lines 54.26692 47.83160 16.77000 26.93790 -0.06657523\n## 13    x_shape 54.26015 47.83972 16.76996 26.93000 -0.06558334\nWe can also save these as .png images to make a .gif image (see also here)\nfor (ilvs in 1:nlevels){\n  i &lt;- which(datasaurus_dozen$dataset == datnames[ilvs])\n  thiscol &lt;- ifelse(datnames[ilvs] == \"dino\", \"darkseagreen\", \"grey20\")\n  png(filename = paste0(\"datasaurus/\",datnames[ilvs],\".png\"))\n  plot(datasaurus_dozen$x[i], datasaurus_dozen$y[i],\n     xlab = \"x\", ylab = \"y\",   las = 1,\n      xlim = c(10, 105), ylim = c(-5, 105), col = thiscol, pch = 16)\n  dev.off()\n}"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#clarity-not-simplicity",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#clarity-not-simplicity",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Clarity not simplicity",
    "text": "Clarity not simplicity\nI give the example of the famout ‘hockey stick’ graph of Northern Hemisphere temperatures."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#dataviz-are-models",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#dataviz-are-models",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Dataviz are models",
    "text": "Dataviz are models\n\nAny visualization is a model\n\nAlberto Cairo 2016\n\nDifferent viz models for the same data\n\n\nThree ways of visualising the same x-y data\nEach of these graphs of the same data has a slightly different interpretation.\nx &lt;- 1:100\ny &lt;- x + rnorm(100, sd=30)\nplot(x,y, pch = 16, col = grey(0.5, 0.5))\n\nmod1 &lt;- lm(y ~ x)\nplot(x,y, col = 'white')\nabline(mod1, lwd = 3, col = 'red')\n\nlibrary(MASS)\nfilled.contour(kde2d(x,y), scale = F)\n\nplot(x,y, pch = 16, col = grey(0.5, 0.5))\nabline(mod1, lwd = 3, col = 'red')"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#models-help-clarify-complex-data-sets",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#models-help-clarify-complex-data-sets",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Models help clarify complex data-sets",
    "text": "Models help clarify complex data-sets\n\nEffect size often has to been seen to be understood\nWhen doing confirmatory analysis, we might want to know how strong an effect is. Data viz is very useful for this task. Lets compare two datasets that have similar p-values, but very different effect sizes\nset.seed(42)\nx &lt;- rnorm(1000)\nset.seed(420)\ny &lt;- 5*x + 3 + rnorm(1000, sd = 15)\nset.seed(420)\ny2 &lt;- 5*x + 3 + rnorm(1000, sd = 1)\n\nmod1 &lt;- lm(y ~ x)\nmod2 &lt;- lm(y2 ~ x)\n\n#Compare the pvalues on the slopes\nsummary(mod1)\n\n##\n## Call:\n## lm(formula = y ~ x)\n##\n## Residuals:\n##     Min      1Q  Median      3Q     Max\n## -43.201 -10.330   0.395   9.634  46.694\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)   2.8054     0.4614   6.080 1.71e-09 ***\n## x             4.2096     0.4603   9.145  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 14.59 on 998 degrees of freedom\n## Multiple R-squared:  0.07732,    Adjusted R-squared:  0.07639\n## F-statistic: 83.63 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod2)\n\n##\n## Call:\n## lm(formula = y2 ~ x)\n##\n## Residuals:\n##      Min       1Q   Median       3Q      Max\n## -2.88004 -0.68868  0.02634  0.64229  3.11291\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)  2.98703    0.03076   97.11   &lt;2e-16 ***\n## x            4.94731    0.03069  161.21   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 0.9724 on 998 degrees of freedom\n## Multiple R-squared:  0.963,  Adjusted R-squared:  0.963\n## F-statistic: 2.599e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow = c(1,2))\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\nplot(x,y2, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod2, lwd = 2, col = 'red')"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#superplots",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#superplots",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Superplots",
    "text": "Superplots\nAndrew Gelman coined the term superplots for plotting different models on multiple panels of a graph so you can visually compare them.\nFor instance, say we have several time-series and we want to know if they deviate from each other signficantly. An easy way to compare them is to fit splines to each time-series and then just plot them next to each other, with SEs. Then we can compare visually for ‘signficant’ differences.\nHere’s some code to simulate three made-up series. The first two have the same trend, but different observation errors, the third has a different trend:\ntmax &lt;- 50\ndrift &lt;- c(10, -5)\nsd &lt;- 40\nsdobs &lt;- 200\nset.seed(5)\nyzero &lt;-  cumsum(rnorm(n=tmax, mean=drift[1], sd=sd))\ny1 &lt;- yzero + rnorm(n = tmax, mean = 0, sd = sdobs)\ny2 &lt;- yzero + rnorm(n = tmax, mean = 0, sd = sdobs)\ny3 &lt;- cumsum(rnorm(n=tmax, mean=drift[2], sd=sd)) +\n  rnorm(n = tmax, mean = 0, sd = sdobs)\ndat &lt;- data.frame(ts = rep(letters[1:3], each = tmax), x = rep(1:tmax, 3), y = c(y1, y2, y3))\nWe can easily plot these three series using ggplot2 and automatically add a spline.\nlibrary(ggplot2)\nggplot(dat, aes(x = x, y = y)) +\n    geom_point() +\n    facet_grid(.~ts) +\n    stat_smooth(method = \"loess\", se = TRUE) +\n   theme(axis.text = element_text(size=14),\n        axis.title = element_text(size=16,face=\"bold\"),\n          strip.text.x = element_text(size = 16),\n          panel.background = element_rect(fill = 'white', colour = 'white'))"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#length-is-most-accurate",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#length-is-most-accurate",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Length is most accurate",
    "text": "Length is most accurate\nWays of comparing data in order from most accurate (top) to more generic (bottom)."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#comparing-volume-and-area",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#comparing-volume-and-area",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Comparing volume and area",
    "text": "Comparing volume and area\nCompare these. Note that if we compare circles we should use area, not the radius or diameter to scale their size.\nn &lt;- c(10, 5)\nbarplot(n, col = 'skyblue', xaxt = 'n', yaxt = 'n')\n\nrad1 &lt;- 1\narea1 &lt;- pi*(rad1^2)\narea2 &lt;- area1/2\nrad2 &lt;- sqrt(area2/pi)\n\npar(mfrow = c(1,2), mar = c(0,0,0,0))\npie(1, col = 'skyblue', labels = NA, border = NA, radius = rad1)\npie(1, col = 'skyblue', labels = NA, border = NA, radius = rad2)\n\n\nExploration of data\nLet’s create a point cloud to demonstrate some data exploration techniques\nset.seed(42)\nx &lt;- rnorm(1000)\ny &lt;- 5*x + 3 + rnorm(1000, sd = 15)\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\n\nCan’t see alot here. A linear model might help us explore if there is any trend going on:\nmod1 &lt;- lm(y ~ x)\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\n\nxnew &lt;- seq(min(x), max(x), length.out = 100)\npmod &lt;- predict(mod1, newdata =data.frame(x=xnew),  se = T)\nlines(xnew, pmod$fit + pmod$se.fit, lwd = 2, col = 'red', lty = 2)\nlines(xnew, pmod$fit - pmod$se.fit, lwd = 2, col = 'red', lty = 2)\n\nWhat about identifying extreme points, that may be worth investigating further? We can pick out points that are greater than 2SDs from the trend:\nmodresid &lt;- resid(mod1)\nsd2 &lt;- sd(modresid)*2\nipt &lt;- which(abs(modresid) &gt; sd2)\n\nplot(x,y, pch = 16, col = grey(0.5,0.5), las = 1)\nabline(mod1, lwd = 2, col = 'red')\npoints(x[ipt], y[ipt], pch = 16, col = rgb(1,0,0, 0.6))\n\n\n\nEffect size\n\n\nDon’t waste digital ink\nPlots with less ‘add-ons’ tend to communicate the key message more clearly. For instance, just like excel plots dont:\nx &lt;- rnorm(100)\ndat &lt;- data.frame(x = x, y = 0.25*x + rnorm(100, sd = 0.2))\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_excel() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\nYou can get additional themes for ggplot2 using this excellent package. A cleaner view:\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_base() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\nOr simply:\nplot(dat$x, dat$y, xlab = \"x\", ylab = \"y\", las = 1)\n\nA good principle is to not use ‘ink’ on figures that isn’t needed to communicate your message. Tufte takes the ‘less ink’ philosophy to the extreme:\nggplot(dat, aes(x = x, y = y)) + geom_point() +\n    theme_tufte() + theme(axis.text=element_text(size=20),\n        axis.title=element_text(size=20))\n\n\n\nWhen is ggplot2 appropriate, or when should I use base R?\nIn general I think ggplot2 is appropriate for problems of intermediate complexity. Like this:\n Base R is great if you just want to plot a barplot quickly, or do an x-y plot. ggplot2 comes into its own for slight more complex plots, like having multiple panels for different groups or colouring lines by a 3rd factor. But once you move to really complex plots, like overlaying a subplot on a map, ggplot2 becomes very difficult, if not impossible. At that point it is better to move back to Base R. ggplot2 can also get very fiddly if you are very specific about your plots, like having certain colours, or the labels in a certain way.\nAs an example, ggplot2 is great for data like this:\nx1 &lt;- rnorm(30)\ngrps &lt;- letters[c(rep(1, 10), rep(2, 10), rep(3, 10))]\ny1 &lt;- x1 + c(rep(1, 10), rep(-1, 10), rep(2, 10)) + rnorm(30)\ndat &lt;- data.frame(x = x1, grps = grps, y = y1)\nhead(dat)\n\n##             x grps             y\n## 1  0.19033984    a  6.159889e-01\n## 2 -0.07173877    a -7.577584e-05\n## 3 -0.00285171    a -1.003277e+00\n## 4 -1.10821896    a -7.302877e-01\n## 5  0.93519177    a  2.662337e+00\n## 6 -1.48583945    a -1.588023e+00\n\nggplot(dat, aes(x = x1, y = y1, color = grps)) +\n  geom_point() + theme_bw()\n\nIt is also pretty handy for faceting:\nggplot(dat, aes(x = x1, y = y1)) +\n  geom_point() + facet_wrap(~grps)+\n  theme_bw()\n\nThe key with ggplot2 is to have your data in a data-frame.\nIn reality both ggplot2 and base R graphics are worth learning, but I would start with learning the basics of base R graphics and then move onto ggplot2 if you want to quickly plot lots of structured data-sets.\n\n\nPie graphs vs bar graphs\nIn Mariani et al. they plot rates of seafood fraud by several European countries. While its a foundational study that establishes improvement in the accuracy of food labelling, their graphics could be improved in several ways.\nFirst they use perspective pies. This makes it incredibly hard to compare the two groups (fish that are labelled/mislabelled). Humans are very bad at comparing angles and pretty bad at comparing areas. With the perspective you can’t even compare the areas properly. They do provide the raw numbers, but then, why bother with the pies? Note that the % pies misrepresent the data slightly because the % figures are actually odds ratios (mis-labels / correct labels), rather than percent (mis-labeels / total samples). Second the pies are coloured red/green, which will be hard for red-green colourblind people to see. Third, they have coloured land blue on their map, so it appears to be ocean at first look. Fourth, the map is not really neccessary. There are no spatial patterns going on that the authors want to draw attention to. I guess having a map does emphasize that the study is in Europe. Finally, the size of each pie is scaled to the sample size, but the scale bar for the sample size shows a sample of only 30, whereas most of their data are for much larger samples sizes (&gt;200). Do you get the impression from the pies that the UK has 668 samples, whereas Ireland only has 187? Therefore, from this graphic we have no idea what sample size was used in each country.\nIn fact, all the numbers that are difficult to interpret in the figure are very nicely presented in Table 1.\nBelow is a start at improving the presentation. For instance, you could do a simple bar chart, ordering by rate of mislabelling.\ncnames &lt;- c('Ireland' ,'UK','Germany','France','Spain','Portugal')\ncorrlab &lt;- c(180, 647, 145, 146, 267, 178)\nmislab &lt;- c(7, 21, 9, 4, 24, 12)\nmisrate &lt;- 100*signif(mislab / (corrlab + mislab),2)\ncorrrate &lt;- 100 - misrate\nord &lt;- order(misrate, decreasing = T)\ny &lt;- rbind(corrrate, misrate)\n\npar(mar = c(5,4,4,7))\nbarplot(y[,ord], names.arg = cnames[ord], col = c('skyblue','tomato'), ylab = 'Labelling rate (%)', las = 2)\nlegend(x=7.5, y = 90, legend = c(\"Mislabelled\", \"Correctly labelled\"), pch = 16, col = c('tomato','skyblue'), xpd = NA, cex = 0.7)\n\nYou could add another subfigure to this, showing the rate by different species too.\nThe barplot doesn’t communicate the sample size, but then that is probably not the main point. The sample sizes are probably best reported in the table\nIf we felt the map was essential, then putting barcharts on it would be more informative. It is not that easy to add barcharts ontop of an existing map in R, so I would recommend creating the barcharts seperately, then adding them on in Illustrator or Powerpoint.\nWe can make a basic map like this:\nlibrary(maps)\nlibrary(maptools)\nmap('world', xlim = c(-20, 20), ylim = c(35, 60), col = 'grey', fill = T)\n\nThen create some nice barcharts. We write a loop so we get one barchart for each country.\nnc &lt;- length(cnames)\npar(mfrow = c(2,3), oma = c(1,1,1,3))\nfor (i in 1:nc){\n  y &lt;- c(mislab[i], corrlab[i])\n  barplot(y, names.arg = '', las = 2, col = c('tomato','skyblue'), ylim = c(0, corrlab[i]), main = cnames[i], cex.main = 2.4, yaxt = 'n')\n  byy &lt;- signif(max(y),2)/3\n  yat &lt;- c(0, min(y), max(y))\naxis(2, at = yat, las = 2, cex.axis = 2, labels = F)\naxis(2, at = yat[2:3], las = 2, cex.axis = 2, labels = T)\n}\nlegend(x = 2.8, y = 500, legend = c('Fraud', 'Correct'), pch = 15, col = c('tomato','skyblue'), xpd = NA, cex = 2, bty = 'n')\n\n\n\nScaling matters\nIt can be misleading to present % and proportion data on axes that are not scaled 0 - 100%. For instance, compare these three graphs:\ny &lt;- c(70, 72, 68, 73)\nx &lt;- 1:4\nxnams &lt;- LETTERS[1:4]\n\npar(mfrow = c(1,3), oma = c(1,1,1,3), mar = c(5,6,2,2))\nplot(x,y, pch = 3, cex = 2, las = 1, xaxt  = 'n', xlab = '', ylab = 'Percent', cex.axis = 2, cex.lab = 2, tcl = 0.5, xlim = c(0, 5), col = 'red', lwd = 3)\naxis(1, at = x, labels = xnams, cex.axis = 2, tcl = 0.5)\n\nbarplot(y, names.arg = xnams, las = 1, cex.axis = 2, cex.lab = 2, cex.names = 2, ylab = 'Percent')\n\nbarplot(y, names.arg = xnams, las = 1, cex.axis = 2, cex.lab = 2, cex.names = 2, ylab = 'Percent', ylim = c(0, 100))"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#interpreting-rates",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#interpreting-rates",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Interpreting rates",
    "text": "Interpreting rates\nThe units you use affect how people interpret your graph. People are bad at interpreting rates, we just can’t get our heads around accumulation very well. Here is a numerical example. Check out the below figure and ask yourself:\n\nAt what time is the number of people in the shopping centre declining?\n\n\nWould you say it is at point A, B, C or D?\nBefore you proceed with code below, take the poll:\n\nLoading poll…\n\nHere is how we made the figure and generated the data:\npar(mar = c(4,4.5,2,2), mgp = c(3,1,0))\nplot(times, inrate_err, type = 'l', xlab = 'Hour of day', ylab = 'People per 10 minutes', las = 1, cex.axis = 2, lwd = 3, col = 'darkblue', cex.lab = 2, ylim = c(0, 12))\nlines(times, outrate_err, lwd = 3, col = 'tomato')\n\nabline(v = 12, lwd = 2, col = grey(0.5,0.5))\ntext(12, 13, 'A', xpd = NA, cex = 2)\nabline(v = 13.5, lwd = 2, col = grey(0.5,0.5))\ntext(13.5, 13, 'B', xpd = NA, cex = 2)\nabline(v = 14.2, lwd = 2, col = grey(0.5,0.5))\ntext(14.2, 13, 'C', xpd = NA, cex = 2)\nabline(v = 15.8, lwd = 2, col = grey(0.5,0.5))\ntext(15.8, 13, 'D', xpd = NA, cex = 2)\n\nlegend('bottomleft', legend = c('Entering', 'Leaving'), lwd = 2, col = c('darkblue','tomato'), cex = 1.5, xpd = NA)\n\nLet’s plot the total number of people:\npar(mar = c(5,5.5,2,2), mgp = c(4,1,0))\nplot(times, cumsum(inrate_err) - cumsum(outrate_err), type = 'l', xlab = 'Hour of day', ylab = 'People in shopping centre', las = 1, cex.axis = 2, lwd = 3, col = 'darkblue', cex.lab = 2, ylim = c(0, 120))\n\nabline(v = 12, lwd = 2, col = grey(0.5,0.5))\ntext(12, 130, 'A', xpd = NA, cex = 2)\nabline(v = 13.5, lwd = 2, col = grey(0.5,0.5))\ntext(13.5, 130, 'B', xpd = NA, cex = 2)\nabline(v = 14.1, lwd = 2, col = 'tomato')\ntext(14.2, 130, 'C', xpd = NA, cex = 2)\nabline(v = 15.8, lwd = 2, col = grey(0.5,0.5))\ntext(15.8, 130, 'D', xpd = NA, cex = 2)\n\nHopefully the answer is obvious now. So the right scales can help make interpretation much easier."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#choosing-colour-scales",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#choosing-colour-scales",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Choosing colour scales",
    "text": "Choosing colour scales\nAlot of thought should go into choosing colour scales for graphs for instance- will it print ok? will colour blind people be able to see this? does the scale create artificial visual breaks in the data? Luckily there is a package to help you make the right decision for a colour scale, it is called RColorBrewer. Check out colorbrewer.org for a helpful interactive web interface for choosing colours.\nFirst let’s load some sea surface temperature data as a raster:\nlibrary(raster)\nr &lt;- raster(\"MeanAVHRRSST\")\n\nlibrary(RColorBrewer)\npar(mfrow = c(1,2))\nplot(r, col = rev(brewer.pal(11, \"Spectral\")), asp = NA)\nplot(r, col = brewer.pal(11, \"Purples\"), asp = NA)\n\n## Warning in brewer.pal(11, \"Purples\"): n too large, allowed maximum for palette Purples is 9\n## Returning the palette you asked for with that many colors\n\nUsing red-green palettes makes it hard for colour blind people. Also, using a diverging palette makes it look like there is something important about the middle point (yellow). A better palette to use would be one of the sequential ones, “Purples” shown here.\nTo make it easier to understand, let’s look at these again as contour plots. I will use a more appropriate diverging palette to the red-green one though.\nz &lt;- matrix(rep(1:10, 10), nrow = 10)\nfilled.contour(z, col = brewer.pal(9, 'Reds'), nlevels = 10)\n\nfilled.contour(z, col = brewer.pal(9, 'RdBu'), nlevels = 10)\n\nNotice the diverging pallette creates an artificial split at yellow\nOne of the only legitimate uses for pie graphs (I think) is visualising the colour scales. Here is how:\nreds &lt;- brewer.pal(9, 'Reds')\ngreens &lt;- brewer.pal(9, 'Greens')\nblues &lt;- brewer.pal(9, 'Blues')\nrdylgn &lt;- brewer.pal(9, 'RdYlGn')\nrdbu &lt;- brewer.pal(9, 'RdBu')\ndark2 &lt;- brewer.pal(8, 'Dark2')\n\npar(mfrow = c(2,3), mar = c(0,0,0,0), oma = c(0,0,0,0))\npie(rep(1, 9), col = reds)\npie(rep(1, 9), col = greens)\npie(rep(1, 9), col = blues)\npie(rep(1, 9), col = rdylgn)\npie(rep(1, 9), col = rdbu)\npie(rep(1, 9), col = dark2)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#breaking-the-rules---stick-with-convention",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#breaking-the-rules---stick-with-convention",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Breaking the rules - stick with convention",
    "text": "Breaking the rules - stick with convention\nSometimes you might want to bend or break the principles above in order to stick with convention. This is useful if people are used to viewing data in a particular way. For instance, geneticists are fond of pie graphs in haplotype networks. Here is some code I modified from here to make one (note that in this data-set there is no haplotype diversity so we artificially mix them up so you can see the pies here):\nlibrary(pegas)\nlibrary(RColorBrewer)\ndata(woodmouse)\nx &lt;- woodmouse[sample(15, size = 110, replace = TRUE), ]\nh &lt;- haplotype(x)\nnet &lt;- haploNet(h)\nmixed_up &lt;- rep(letters[1:5], each=22)\nind.hap2 &lt;- with(\n    utils::stack(setNames(attr(h, \"index\"), rownames(h))),\n    table(hap=ind, pop = mixed_up[values])\n)\n\nplot(net, size = attr(net, \"freq\"), scale.ratio = 2, cex = 0.8, pie = ind.hap2, bg = brewer.pal(8, \"Dark2\"))\nlegend(\"bottomright\", colnames(ind.hap2), col = brewer.pal(8, \"Dark2\"), pch=20)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#interactive-dataviz",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#interactive-dataviz",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Interactive dataviz",
    "text": "Interactive dataviz\nInteractive visuals can overcome the dilemma of having too much complexity to show, but also wanting the viewer to explore the details. For instance, you could use the dygraphs package to zoom in on a date range.\nWe will get some recent data on wave heights from QLD government. First we load the data directly from the web and process it to correctly label dates and times, then just select the Signficant wave height variable at Tweed Heads.\nwaves &lt;-read.csv(url(\"http://www.ehp.qld.gov.au/data-sets/waves/wave-7dayopdata.csv?timestamp=2017-07-05EST21-06-50\"), skip = 1)\n\nwaves$time &lt;- as.POSIXct(strptime(as.character(waves$DateTime), format = \"%Y-%m-%dT%H:%M:%S\"))\nwaves2 &lt;- subset(waves, (Hsig &gt; 0) & (Site == \"Tweed Heads\"))[,c(\"time\", \"Hsig\")]\nwaves_xts &lt;- xts::xts(waves2$Hsig, order.by = waves2$time)\nNow have our data, we can look at it using the dygraphs package. We will do two things: add a date range selector and also add a ‘roll’ which will smooth over a certain period of our choosing:\nlibrary(dygraphs)\ndygraph(waves_xts, main = \"Significant wave height - Tweed Heads\") %&gt;%\n  dyRoller(rollPeriod = 50) %&gt;%\n  dyRangeSelector()\nI haven’t put it up on the webpage here, but have a go yourself."
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#anatomy-of-a-simple-chart",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#anatomy-of-a-simple-chart",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Anatomy of a simple chart",
    "text": "Anatomy of a simple chart\nThe construction of a simple chart in R can be a surprisingly long piece of code. Here is an example to get you started. Don’t be afraid to experiment!\n# Make some data\n# --------------- #\nset.seed(42)\n\nn &lt;- 11\n\nx &lt;- rnorm(n, mean = seq(18, 25, length.out = n))\ny &lt;- rnorm(n, mean =seq(26, 18, length.out = n))\nz &lt;- rnorm(n, mean = 22)\nt &lt;- 2005:(2005+n-1)\n\ndatnames &lt;- c('Almonds', 'Peanuts', 'Hazelnuts')\n\nplot(t, x)\nlines(t, y)\nlines(t, z)\n\nWhich look terrible. Let’s build a better chart.\n# Package for colours\nlibrary(RColorBrewer)\n\n#Set axis limits\nymax &lt;- 30\nylim &lt;- c(15, ymax)\nxlim &lt;- c(min(t), max(t))\n\n#Define colours\ncols &lt;- brewer.pal(3, 'Dark2')\n\n#Parameters for plotting\nlwd &lt;- 2\nxlabels &lt;- seq(min(t), max(t), by = 5)\nylabels &lt;- seq(0, ymax, by = 5)\n\n#Set the window params\npar(mar = c(5,5,4,4))\n\n#Build the plot\nplot(t, x, type = 'l', bty = 'n', xaxt = 'n', yaxt = 'n',\n ylim = ylim, xlim = xlim, lwd = lwd, col = cols[1],\n  xaxs = 'i', yaxs = 'i',\n  xlab = 'Time (yrs)',\n  ylab = '',\n  main = 'Changing price of nuts ($/kg)')\n\n#Add more lines\nlines(t, y, lwd = lwd, col = cols[2])\nlines(t, z, lwd = lwd, col = cols[3])\n\n#Add labels to lines\ntext(t[n], x[n], datnames[1], adj = c(0, 0), xpd = NA, col = cols[1])\ntext(t[n], y[n], datnames[2], xpd = NA, adj = c(0, 0), col = cols[2])\ntext(t[n], z[n], datnames[3], xpd = NA, adj = c(0, 0), col = cols[3])\n\n# Add custom axes\naxis(1, col = 'white', col.ticks = 'black', labels = xlabels, at = xlabels)\naxis(1, col = 'white', col.ticks = 'black', labels = NA, at = t)\n\naxis(2, col = 'white', col.ticks = 'black', las =1, labels = ylabels, at = ylabels)"
  },
  {
    "objectID": "posts/2017-06-26-dataviz2017_notes/index.html#resources-and-further-reading",
    "href": "posts/2017-06-26-dataviz2017_notes/index.html#resources-and-further-reading",
    "title": "Data visuals notes for my talks in 2017",
    "section": "Resources and further reading",
    "text": "Resources and further reading\n\nThe Truthful Art by Alberto Cairo\nBooks by Edward Tufte\nTwitter #dataviz\nAn infographic of chart types: Visual vocabulary\nWTF Visualizations, on Twitter"
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html",
    "title": "Growing your international scientific career as a parent",
    "section": "",
    "text": "An international reputation is considered by many to be an important part of having a successful scientific career.\nTraditionally, keeping or developing an international reputation has required a lot of travelling. Visiting conferences, going on sabbatical and doing speaking tours of overseas universities are effective ways to engage international researchers with your science.\nAll this travelling can be very difficult if you are a parent though, particularly if you are primary carer (i.e. usually a mother).\nSo does that mean parents should give up on having a successful international career? Unfortunately, a lot the advice I have recieved for growing an international reputation has revolved around travelling.\nBut, I think there are some ways to grow an international reputation without travelling. For myself I don’t plan on travelling much for a while while my children are so young (even though I’m not primary carer).\nSo here’s some activities I think parents can do."
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#social-media",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#social-media",
    "title": "Growing your international scientific career as a parent",
    "section": "Social media",
    "text": "Social media\nDeveloping a solid social media network, e.g. on Twitter is a very time effective way to engage with scientists across the globe.\nThe key is that you don’t need a ‘big’ following, but you need a network of key people - those people that are most likely to be interested in your research.\nA good way to engage people on Twitter is to listen to them and ask questions about their own work (just like in real conversations!)."
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#write-people",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#write-people",
    "title": "Growing your international scientific career as a parent",
    "section": "Write people",
    "text": "Write people\nRead an interesting paper? Write to the lead author telling you liked it and why. Maybe even share some of your own relevant research.\nThe problem with e-media (email and twitter) is that it does tend to go in one ear and out the other. Your interactions there are quite ‘forgettable’.\nI haven’t done so myself, but if you start a conversation with a researcher online, why not suggest you follow up with a video call to discuss it further? Face to face interactions are much more memorable."
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#conference-sessions-and-talks",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#conference-sessions-and-talks",
    "title": "Growing your international scientific career as a parent",
    "section": "Conference sessions and talks",
    "text": "Conference sessions and talks\nFirst up, submit abstracts to conferences you would go to, but can’t make. Then get a trusted colleague to present your work, take queries and suggest people follow up with you on email.\nOk this next tip is going to be a bit devious and could subvert the system a little (haven’t done it myself). But also remember if you are a primary carer, the system is subverting you, by largely not recognizing the time you are taking away from your career.\nSo the tip is: apply to host session at a conference you know you can’t make it to. Make sure you get someone you trust, who can go to the conference, to co-host the session. Then lead the organisation of the session (so your name is on all the emails inviting speakers etc…,), to maxmise exposure.\nThen of course, you can’t make it, but your trusted colleague can go and introduce speakers. They should apologise to speakers and attendees that unfortunately you couldn’t make it, which ensures every one remembers you organised this session. Have your colleague hand out your business card too and suggest people follow up with you.\nThen after the session, write a blog about it. My blog about our session on Ridge to Reef management at a conferencea few years ago still gets plenty of hits.\nA blog will also pay it forwards - other people that couldn’t make your session can also learn about the frontier of research your session was addressing."
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#take-longer-trips-to-a-single-location",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#take-longer-trips-to-a-single-location",
    "title": "Growing your international scientific career as a parent",
    "section": "Take longer trips to a single location",
    "text": "Take longer trips to a single location\nIf you’ve travelled with young kids you will know it is easier to go one place and set up there, rather than dash around to a new city every other day.\nSo plan an overseas sabbatical with your family of say 3-6 months. This way you have time to build some meaningful collaborations. Also, put the trip on your CV. Work overseas is considered an important CV ‘trait’, but can be hard if you are tied to a single city/university for family reasons.\nI have seen a few people list sabbaticals on their CV and it works well to communicate their international reputation I think."
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#visit-every-scientist-that-visits-your-city",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#visit-every-scientist-that-visits-your-city",
    "title": "Growing your international scientific career as a parent",
    "section": "Visit every scientist that visits your city",
    "text": "Visit every scientist that visits your city\nI’m lucky to live in a city with three (or so) universities. This means a lot of visiting academics from overseas, with whom I can connect.\nIt surprises me how often folk say they are too busy to attend talks from visiting scientists, even those visiting their own institution. Seriously, make this a priority.\nToday I spent 2 hours commuting on my bike, after staying up most of the night with my newborn, just so I could make it from my uni to another one and meet a leading international scientist for a 1 hour lunch meeting. Still, easier than flying 30 hours to meet her.\nSo go to the talks of visiting scientists and then ask to meet them and talk science.\nThese kind of chats can turn out to be very valuable. I just had accepted a paper that was written with a scientist on sabbatical. The paper started as the direct result of a few chats over coffee."
  },
  {
    "objectID": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#count-the-science-you-didnt-do-when-you-were-on-leave",
    "href": "posts/2018-04-05-having-an-international-career-as-a-parent/index.html#count-the-science-you-didnt-do-when-you-were-on-leave",
    "title": "Growing your international scientific career as a parent",
    "section": "Count the science you didn’t do when you were on leave",
    "text": "Count the science you didn’t do when you were on leave\nIf you take a fair chunk of leave for parental care, like months or years, then you should account for missed opportunities when you when apply for grants or jobs.\nOne way to do this is: add up your total productivity (e.g. number of papers over 2 years) then multiply by 1 over you real FTE (full time equivalent). So if you published 3 papers in 2 years part-time (0.5 FTE), then your adjusted productivity is 6 papers.\nEmily Nicholson explains this great idea (her idea!) in more detail.\nIn fact, if you are taking parental leave, you should also read this post.\nThat’s all I have now, but message me on Twitter if you have more ideas (or other useful links)."
  },
  {
    "objectID": "posts/2017-08-24-equity-fishery-recovery/index.html",
    "href": "posts/2017-08-24-equity-fishery-recovery/index.html",
    "title": "Socially equitable recovery of fisheries",
    "section": "",
    "text": "Efforts to recover fisheries are often contentious. Recovering fisheries may cause economic hardship to fishers who have to reduce their catches in order to allow fish stocks to replenish.\n\nIn a new paper we explore how equitable distribution of catch restrictions across fishers may trade-off with fast and economically efficient recovery (open access pre-print here).\nIn 2004 Australia’s coral trout fishery, which operates in the Great Barrier Reef World Heritage Area, was reformed. These reforms were intended to increase the abundance of fish in the water for ecological reasons and also to increase the profitability of the fishery by making it easier to catch fish.\nReforms included reducing the number of licenses and increasing the area of the Great Barrier Reef marine park. The reforms were estimated to have cost government $250 million in assistance pay-outs to fishers and associated industries.\nIn part the cost blow-out has been attributed to perceptions of inequitable redistribution of catches.\nIn our new work, we asked if compromises must be made between the three goals of: (1) sharing catch restrictions ‘equitably’; (2) the abundance that fish stocks recover to; and (3) the fishery’s overall economic value.\nIt turned out that equitable distribution of catches led to fisheries that had either a lower overall value or lower abundance of fish than an approach that ignored equity and optimised overall profit. In other terms, equitable distributions are economically inefficient.\nHowever, the result hinged on two key assumptions.\nFirst, equitable distribution of catches was only inefficient if there were some individual fishers that made much more profits than others. For instance, some fishers may be more effective at finding the right times and places to go fishing, so can catch the same amount of fish at a lower cost.\nWhen this was the case, the most profitable fishery could be achieved if we shut the fishery to those fishers who spend more to catch the same amount of fish.\nClearly, shutting down some individuals in not very equitable.\nDetermining how much individuals vary in their fishing efficient is therefore important. When reforming fisheries with greater variability there will be stronger compromises between total profits and equity.\nA second key assumption was how we defined ‘equity’. You could define equitable restrictions in terms of everyone reducing their catches by an equal amount, or reducing them in proportion to their historical catches.\nGreater restrictions were less equitable if we measured equity as a proportion of an individual’s historic catches, whereas greater restrictions were more equitable if we measured equity as the total amount of catch.\nFor any given fishery it is therefore important to determine what type of equity matters most to the fishers that participate it. Finding a consensus on equity will be important to determine if the redistribution of catches is equitable.\nIn most fisheries reforms managers do not decide directly who gets to catch what post-reform. Instead, they may often offer voluntary buy-backs of fishing effort on a market to encourage fishers to reduce their effort. Our results could ultimately inform how such markets could be designed to be more equitable.\nOur results also suggest expectations for marked improvements in profits when recovering fisheries should be tempered, because often such solutions rely on the inequitable redistribution of catches. Ultimately, reforms that are percieved as being more just to fishers may be more likely to obtain political support and be successful in the long-term."
  },
  {
    "objectID": "posts/2017-12-12-new-postdoc-phd-positions/index.html",
    "href": "posts/2017-12-12-new-postdoc-phd-positions/index.html",
    "title": "New postdoc and PhD positions on multiple stressors of ecosystems",
    "section": "",
    "text": "Applications for the postdoc job and PhD position have now closed, but I have kept the post here for reference\nProfessor Rod Connolly (Griffith University), Professor Isabelle Côté (Simon Fraser University) and I will be recruiting a postdoc and a PhD student in 2018 to work on our project “Predicting interactive effects of multiple stressors on marine ecosystems” funded by the Australian Research Council.\nWe will advertise officially next year, but feel free to get in touch with me ahead of time (chris.brown@griffith.edu.au) if you are interested in discussing either of these positions.\n\nThe project aims to create an innovative model to predict how multiple stressors interact to affect marine ecosystems across large spatial scales. The interactive effects of multiple stressors are poorly quantified in the marine environment, and natural resource managers are not able to confidently incorporate these complex interactions into prioritizing management efforts in space and time. The project intends to develop models, validated in the field that will predict, a-priori, the type of interactions that may occur. This should enable ecosystem managers to better prioritise actions to more effectively conserve valuable and threatened marine ecosystems.\n\nFor the postdoc position we are seeking a PhD ecologist who is competent in quantitative methods. The project may include a mix of statistical analysis, numerical modelling and field experiments. We are interested to hear from statistical modellers, numerical modellers or field ecologists who are competent with quantitative analysis (e.g. in R). The project’s emphasis is on modelling, but local field work is possible for interested candidates if we need to fill in specific knowledge gaps to complete the models. Candidates must have excellent communication skills and a strong track record of publishing in peer-reviewed journals.\nThe postdoc position is nominally for 2.5 years full-time at academic level A (PhD graduate level); however, these terms are currently flexible. Please get in touch for more information.\nFor the PhD position we are seeking someone interested in the study of threatened marine ecosystems, particularly seagrass meadows, and ecosystem management. Candidates may undertake field work or purely modelling/analytical based work. Candidates must be enthusiastic about the analysis of data and should be keen to learn the R program language if they do not already have skills in this area.\nAustralian Resident applicants to Griffith’s PhD program should have received Honours 1 or equivalent. A lead authored publication in a peer-reviewed journal will be helpful.\nInternational applicants to Griffith’s PhD program must have led at least one peer-reviewed paper in an internationally recognised journal.\nDetails on the application process to a PhD at Griffith, including deadlines are here."
  },
  {
    "objectID": "posts/2016-02-10-SoTM_day1_synopsis/index.html",
    "href": "posts/2016-02-10-SoTM_day1_synopsis/index.html",
    "title": "Species on the Move - Day 1 synopsis",
    "section": "",
    "text": "Species on the Move - Day 1 synopsis\n10 February 2016, Hobart Tasmania\nThis is my take on some of the highlights and main themes from day one of the Species on the Move conference, in Hobart. See also the day 2 synopsis and day 3 synopsis and some lessons learned.\n\n\n\nThis is my take that is obviously biased towards the talks I saw. I feel your pain Gretta!\n\n\nIt is striking that the ecological effects of climate change are now a personal experience for many people. The ecological impacts of climate change are no longer an abstract concept scientists talk about, or something that happens in models and experiments. I think this speaks to how pervasive the impacts of climate change have become.\nWe should not underestimate the psychological impact that witnessing climate change has personally\nEmma Lee, a Tasmanian Aborigine, gave a passionate talk about how her cultural history in the form of ‘living sites’ or middens was literally being washed away by sea level rise. For a culture that has already lost so much, the further loss of some of their last remaining cultural sites, dating back thousands of years, is devastating. She spoke about the power of science as an outlet to help Aboriginal people tell their story about change over 10 000 years of life in Tasmania. Science can also help uncover some of the lost history of Aboriginals, before it is washed away.\nTero Mustonen spoke about the contribution of indigenous people in Finland to science, through their monitoring of species range shifts and permaforst melting. Locally, in Tasmania Ingrid Van Putten showed that fishers are already attributing the changes they see in fisheries to climate change, well some of them. Which ones they choose to attribute to climate change can be predicted by cognitive biases.\nFrom these talks I learnt that science has an important role to play in helping people communicate their stories and comprehend change.\nClimate change is now so rapid that individual scientists have seen their predictions borne our during the course of their careers.\n\n\n\nThe conference was succesful at attracting broad interest in the media, including from ABC news.\n\n\nWell, sort of. There are always unexpected ecological outcomes. Camille Parmesan spoke of how she had predicted climate-driven extinction of a butterfly species and it almost did go extinct while policy makers were arguing over what to do about it. The butterfly ended up saving itself by evolving to utilise a new host plant so saved itself.\nThe conference organisers should be be commended for a series of plenary talks that is representative of both career-stages and gender in our field\nI was pleased to see a balanced list of both men and women plenary speakers. This is representative of the relatively young field of climate change ecology where many of the most influential researchers are women (think Parmesan, Elith, Poloczanska, Sorte, Sunday to name a few). Other, more established, fields should aspire to have such a diverse population. Just speculating, but I think other fields suffer from an established, ‘old guard’ who prevent the career progression of younger women (e.g. through sexist reviews). Climate change ecology doesn’t seem to have this problem to the same extent, which makes it a more enjoyable and stimulating field to participate in. The science has benefitted greatly from this diversity too.\nIt was also great to see some exceptional early career researchers like Jenn Sunday from UBC, included on the plenary list. In her short career, Sunday has become the leading name in the theory of range edges and range shifts. It was great to hear where the research frontiers are from the upcoming generation of climate change ecologists.\nOn the topic of Sunday’s talk we learnt that Species traits can help predict variation in range shifts.\nClimate change ecology has made a natural progression from documenting range shifts to asking why different species vary in the rate of their range shift. Sunday spoke about large range size and omnivory as determinants of more rapid range shifts. She then backed up these findings with solid ecological theories that provide new hypotheses for our field to test.\n\n\n\nWe should be careful to repeat resurveys, to make sure we don’t miss rare species, like this 40-spotted pardalote (striated pardalote also pictured).\n\n\nOther talks also pointed to the role of species traits in determing rates of range shift. For instance, Jonathan Lenoir spoke about how herbaceous plants were moving faster up elevational gradients than woody species.\nOther talks suggested caution in interpreting the data underlying range shifts, myself included.\nMorgan Tingley gave us five issues we should consider when comparing historical and resurvey data. He advised caution in interpreting historical data and pointed to the need to thoroughly know your data and check its source. His suggestions included ‘repeat repeat repeat’ - that you should repeat occupancy surveys so you can calculate detection rates of your species and in doing so separate false absences of a species from true extirpations.\nWe also heard about the mechanisms of leading edge shifts. Mike Kearney in his plenary spoke about a new R package NicheMapR which promises to change the way we do species distribution models from statistical to mechanistic. Dave Schoeman spoke about temperature controls on the leading edge of ghost crabs the ‘ideal’ study species for climate range shifts. Finally, Cascade Sorte talked resolving the effect of ocean currents on range shifts.\nThat is all I have time to write. I wish I could say more. Before you finish, don’t forget to vote on the video competition!"
  },
  {
    "objectID": "posts/2016-06-08-reef-connectivity/index.html",
    "href": "posts/2016-06-08-reef-connectivity/index.html",
    "title": "Wind predicts connectivity for coral reef fish",
    "section": "",
    "text": "Wind predicts connectivity for coral reef fish\nConnectivity of organisms among habitats is fundamental to population dynamics, the evolution of species and their conservation. Two paradigms of connectivity dominate marine science. The first is the movement of larvae from their natal sites to their settlement sites. The second is the movement of fish from juvenile habitats, like mangroves, to adult habitats, like coral reefs.\nIn a new paper published in the journal Ecology, we show that sheltered conditions have both high larval and habitat connectivity and we subsequently predict a synergistic increase in fish biomass in sheltered environments.\n\n\n\nWe hypothesized that larval retention would be lower on more exposed coasts, because of wind-driven advection. Exposed coasts also will have lower reef to mangrove connectivity, because mangroves prefer sheltered conditions. Under sheltered conditions, connectivity to mangroves and of larvae was hypothesized to be greater, leading to a synergistic increase in adult fish biomass on reefs. Image from our paper\n\n\nRarely have these two paradigms been linked. However, linkages among them could be important. For instance, we should see higher abundances of adult fish in locations where ocean currents facilitate arrival of larvae to their favourite juvenile habitats. Such places may offer double the benefits to larvae and may be especially productive locations for fisheries, or key spawning sites to reserve in protected areas.\nWe asked if the high larval connectivity in terms of larvae returning to their natal site, was correlated with high connectivity among juvenile and adult habitats. To study larval and habitat linkages, we used data and comprehensive larval modelling from The Bahamas.\nWe found for three different fish species that the reef sites with higher retention of larvae were also more likely to have connected coral reefs and mangroves. The fish species larvae spend different amounts of time in the open ocean, suggesting the relationship is general across many species.\nIt turned out that exposure to wind and waves explained both high retention and connectivity among coral reefs and mangroves. The explanation is simple - mangroves like sheltered conditions so coral reefs are typically only close to mangroves in sheltered locations. Fish larvae are carried by ocean currents, so if there are high wind and waves they are more likely to be carried away. In sheltered conditions, the larvae hang around.\nThe effect of exposure on both larval and habitat connectivity is a real boon for fish populations. Our models predicted a synergistic increase in fish biomass in sheltered locations that resulted from a positive feedback loop: More larvae retained near their preferred mangroves meant more juveniles migrating to reefs where they spawned more larvae that settled in mangroves… and so on.\nOf course there is much work left to do. The predicted synergistic increase in fish biomass is yet to be tested in the field. However, our approach generates quantitative relationships between exposure and fish biomass that could be tested in against field surveys.\nAnother caveat to consider is that connectivity may also benefit a fish’s predators.\nValidating our predictions would be of great benefit to the conservation of reef fish. For instance, using one variable along - exposure - we can predict the reef sites most suited to fish and identify productivity hotspots that are likely important for natural ecosystems and fisheries.\nRead more: Brown, C. J., Harborne, A. R., Paris, C. B. and Mumby, P. J. (2016), Uniting paradigms of connectivity in marine ecology. Ecology. Accepted Author Manuscript. doi:10.1002/ecy.1463"
  },
  {
    "objectID": "posts/2016-09-26-learning-from-failures/index.html",
    "href": "posts/2016-09-26-learning-from-failures/index.html",
    "title": "Learning from failures",
    "section": "",
    "text": "Learning from failures\n\n“There is no winning and losing, only winning and learning”\n“Own your mistakes”\nSiri Lindley two times Triathlon World Cup winner1\n\nAn old adage goes ‘learn from your mistakes’. In general, this is good advice. Mistakes are an opportunity to reflect on what went wrong and what you could improve upon next time you are faced with a similar situation. The problem is, it is not always that easy to see failures as mistakes.\nBy failures I mean ‘lack of success’, which could be due to your own fault, other people, or random change. By mistakes I specifically mean ‘lack of success attributed to your own actions’. If you do not view failures as being a result of your own actions, you cannot ask how you can improve upon similar situations in the future.\nBut it can hard to accept that a failure is our own mistake. Most of us are socially preconditioned to avoid blame. It can also be hard emotionally to blame yourself. Accepting a failure as your own entails accepting greater responsibility for your actions.\nOf course, you need to be honest with yourself. Most failures are a mix of your own decisions, other people’s and random chance.\nAs an example, consider those particularly nasty reviews you have likely got (or will get eventually if you are just starting out in academia) on a paper you submitted.\n\n\n\nA quote from a review I got as a young researcher on a now published paper\n\n\nWhen confronted with highly negative criticism, it is easy to lay the blame on the reviewer. “They have a narrow view of the world”, “they are old and grumpy”, “they were annoyed because we didn’t cite more of their own work”, “they missed the point” are all phrases I have heard (or spoken!) in response to difficult reviews.\nWhile there may be elements of truth in those comments, your objective is obviously to get the paper published. The reviewer may be grumpy, because they were tired and overworked. The solution - write for tired and overworked people! Just blaming others doesn’t help you improve it for the next round of review. It may be better to take some of the blame on yourself and ask how you could avoid such comments in future. For instance, try turning\n\n“They have a narrow view of the world” into “we need to do a better job of providing context for this study”;\n“they are old and grumpy” into “we need to work to establish our work in light of older research paradigms”;\n“they were annoyed because we didn’t cite more of their own work” into “we should be careful to acknowledge a broader range of literature”; and\n“they missed the point” into “we need to state the problem we are addressing more clearly”.\n\n\nOwning your mistakes\nSo how can you own your mistakes so they have maximal (positive) psychological impact so you learn the lesson? Here are a few ways I have thought about.\n\nTell other people about your mistakes. For instance, acknowledge to your coauthors that a particular review criticism related to part of the paper you had written. The flip side is you should acknowledge your own and other people’s successes too.\nWrite about mistakes in your annual review. At my uni we have annual reviews of our performance and PhD students have regular reviews on the progress of their thesis. It is tempting to just put all your successes in these reports. I think it is important to acknowledge mistakes in these forums. It will help you consolidate the lesson and perhaps provide your supervisors an opportunity to give you helpful advice. As an example, I received criticism on a grant application that I had too many co-authored (not first authored) publications. While such comments can hurt, because of the time we have invested in our careers, looking at this criticism I realised I had been investing too much time on helping with other people’s projects. Acknowledging this bias helped me set goals in my annual report for focussing on leading publications, and rapidly turn the situation around.\nSee avoided losses as success. For instance, say you start a project that you expect will require 2 months of lab work. Two months in you have made little progress, and you realise the project is much harder than you initially thought. If you decide to terminate the project now you could save a lot more wasted time that would be better spent on other things. While you could view this as 2 months wasted, you could also view it in light of the future time you save by stopping the project early. Thus, I think it is helpful to report on such mistakes as successes, because you avoided more wasted time.\nTry to think objectively when other people offer you excuses. For instance, your supervisor might write-off a bad review as coming from their grumpy academic nemesis. That might be the case, but doesn’t mean you can’t still learn how to improve your paper from the feedback you received (as above).\nDo a pre-mortem on new projects. I learned this one from Daniel Kahneman’s book ’Thinking, Fast and Slow. You do a pre-mortem once you have written the initial conception and plan for a project. Get yourself and other team members to sit down and imagine all the ways the project could fail. Have them write down a list of ways the project could fail independently before you discuss your lists. Then see if there are ways to design the project to avoid failures or mitigate losses. Pre-mortems help us learn from mistakes before they even happen and are particularly important when we can’t afford to make mistakes.\n\nSo be hard on yourself if you want to learn from your mistakes, but not too hard.\n1: Once again I have drawn inspiration from endurance athletics. I got the idea for this blog post from listening to a podcast where Tony Robbins interviews Siri Lindley about what made her successful as a Triathlete. I highly recommend you check it out."
  },
  {
    "objectID": "posts/2016-05-19-vectorize/index.html",
    "href": "posts/2016-05-19-vectorize/index.html",
    "title": "Sensitivity analysis in R the easy way",
    "section": "",
    "text": "Is the tedious process of running sensitivity analyses on your numerical model getting you down? Have you got loops inside of loops inside of loops in an attempt to iterate your model across all parameter combinations?\nI recently discovered there is a much easier way to do sensitivity analyses, using the functions expand.grid() and Vectorize() you can run your sensitivity analyses in just two lines of code. Here’s how it works.\n\n\nnsds &lt;- 30 nmeans &lt;- 50 means &lt;- seq(-1, 1, length.out = nmeans) sds &lt;- seq(0.01, 3, length.out = nsds) probs &lt;- c(0.025, 0.5, 0.975)\ndat &lt;- expand.grid(means = means, sds = sds)\nvpnorm &lt;- Vectorize(pnorm, vectorize.args = c(‘mean’, ‘sd’), SIMPLIFY = TRUE)\nquants &lt;- vpnorm(probs, mean = dat\\(means, sd = dat\\)sds)\ndat2 &lt;- data.frame(cbind(dat, t(quants)))\npopmod &lt;- function(Bt, r, K, Ht){ Bt + ((rBt)  (1 - ((Bt/K)))) - Ht }\nsimmod &lt;- function(tmax, B1, K, r,fmort){ B &lt;- rep(NA, tmax) B[1] &lt;- B1 H &lt;- rep(NA, tmax) for (t in 2:tmax){ H[t] &lt;- B[t-1]*fmort B[t] &lt;- popmod(B[t-1], r, K, H[t]) } list(B = B, H = H) }\nvsimmod &lt;- Vectorize(simmod, vectorize.args = c(‘fmort’, ‘r’), SIMPLIFY = F)\ntmax &lt;- 100 B1 &lt;- 1 K &lt;- 20\nnr &lt;- 100 nF &lt;- 50 rvec &lt;- seq(0.01, 0.5, length.out = nr) Fvec &lt;- seq(0, 0.1, length.out = nF)\ndat &lt;- expand.grid(r = rvec, Fmort = Fvec)\nresult &lt;- vsimmod(tmax = tmax, B1 = B1,K=K, r=dat\\(r, fmort=dat\\)Fmort)\ndat\\(Bend &lt;- unlist(lapply(result, function(x) x[[1]][tmax]))\ndat\\)Hend &lt;- unlist(lapply(result, function(x) x[[2]][tmax]))\nFirst up we need to create some data, for testing here we go:\nn &lt;- 100\nx1 &lt;- rnorm(n, mean = 5, sd = 1)\ny1 &lt;- 2*x1 + rnorm(n, mean = 0, sd = 2)\ny2 &lt;- -5*x1 + rnorm(n, mean = 0, sd = 2)\ny3 &lt;- rnorm(n, mean = -1, sd = 2)\ny4 &lt;- 9*rnorm(n, mean = 0, sd = 2) + 5\nWe just created an x-variable and four y-variables that depend on that x-variable, with normal errors.\nLet’s start a plot, with 2*2 window:\npar(mfrow = c(2,2))\nplot(x1, y1)\nNow we need to identify the boundaries of the axis on each sub-plot as it is created. We can do that using the par() command:\naxbounds &lt;- par('usr')\nWhich gives us the bounds on the x and y axes for whatever plot is open right now.\nWe can then get the range of each axis like this:\naxrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\nNext, define our labels. We have four plots and we can use letters or LETTERS to obtain letters of the alphabet. ie:\nlabels &lt;- LETTERS[1:4]\nGives us the labels a, b, c, d. Or we could also do:\nlabels &lt;- paste0('(',letters[1:4],')')\nWhich gives us (a), (b), (c), (d)\nNow we need to position the labels, relative to each axes range. We can do this by taking the x-axis minimum value and y-axis maximum value and then adding an offset to the axis range. Adding the offset in this way means the labels will always be placed in the same place, even if the axis scale varies. We can place the label using the text command.\ntext(axbounds[1] -\n    (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n    label[1], xpd=NA,...)\nThe argument xpd=NA is neccesary, because without it new additions to the plot outside of the plot window are invisible.\nSo those are the elements we need. To get it all finished, we can stitch these all together in a new function:\n\n\n\nfiglabel &lt;- function(label ='A', xoffset = 0.1, yoffset = 0.08,...){\n    axbounds &lt;- par('usr')\n    axrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\n    text(axbounds[1] -\n        (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n        label, xpd=NA,...)\n    }\nWe use it like this:\n    par(mfrow = c(2,2))\n    plot(x1, y1)\n    figlabel(labels[1], font = 2)\n    plot(x1, y2)\n    figlabel(labels[2], font = 2)\n    plot(x1, y3)\n    figlabel(labels[3], font = 2)\n    plot(x1, y4)\n    figlabel(labels[4], font = 2)\nThe result is this:"
  },
  {
    "objectID": "posts/2016-05-19-vectorize/index.html#sensitivity-analysis-in-r-the-easy-way",
    "href": "posts/2016-05-19-vectorize/index.html#sensitivity-analysis-in-r-the-easy-way",
    "title": "Sensitivity analysis in R the easy way",
    "section": "",
    "text": "Is the tedious process of running sensitivity analyses on your numerical model getting you down? Have you got loops inside of loops inside of loops in an attempt to iterate your model across all parameter combinations?\nI recently discovered there is a much easier way to do sensitivity analyses, using the functions expand.grid() and Vectorize() you can run your sensitivity analyses in just two lines of code. Here’s how it works.\n\n\nnsds &lt;- 30 nmeans &lt;- 50 means &lt;- seq(-1, 1, length.out = nmeans) sds &lt;- seq(0.01, 3, length.out = nsds) probs &lt;- c(0.025, 0.5, 0.975)\ndat &lt;- expand.grid(means = means, sds = sds)\nvpnorm &lt;- Vectorize(pnorm, vectorize.args = c(‘mean’, ‘sd’), SIMPLIFY = TRUE)\nquants &lt;- vpnorm(probs, mean = dat\\(means, sd = dat\\)sds)\ndat2 &lt;- data.frame(cbind(dat, t(quants)))\npopmod &lt;- function(Bt, r, K, Ht){ Bt + ((rBt)  (1 - ((Bt/K)))) - Ht }\nsimmod &lt;- function(tmax, B1, K, r,fmort){ B &lt;- rep(NA, tmax) B[1] &lt;- B1 H &lt;- rep(NA, tmax) for (t in 2:tmax){ H[t] &lt;- B[t-1]*fmort B[t] &lt;- popmod(B[t-1], r, K, H[t]) } list(B = B, H = H) }\nvsimmod &lt;- Vectorize(simmod, vectorize.args = c(‘fmort’, ‘r’), SIMPLIFY = F)\ntmax &lt;- 100 B1 &lt;- 1 K &lt;- 20\nnr &lt;- 100 nF &lt;- 50 rvec &lt;- seq(0.01, 0.5, length.out = nr) Fvec &lt;- seq(0, 0.1, length.out = nF)\ndat &lt;- expand.grid(r = rvec, Fmort = Fvec)\nresult &lt;- vsimmod(tmax = tmax, B1 = B1,K=K, r=dat\\(r, fmort=dat\\)Fmort)\ndat\\(Bend &lt;- unlist(lapply(result, function(x) x[[1]][tmax]))\ndat\\)Hend &lt;- unlist(lapply(result, function(x) x[[2]][tmax]))\nFirst up we need to create some data, for testing here we go:\nn &lt;- 100\nx1 &lt;- rnorm(n, mean = 5, sd = 1)\ny1 &lt;- 2*x1 + rnorm(n, mean = 0, sd = 2)\ny2 &lt;- -5*x1 + rnorm(n, mean = 0, sd = 2)\ny3 &lt;- rnorm(n, mean = -1, sd = 2)\ny4 &lt;- 9*rnorm(n, mean = 0, sd = 2) + 5\nWe just created an x-variable and four y-variables that depend on that x-variable, with normal errors.\nLet’s start a plot, with 2*2 window:\npar(mfrow = c(2,2))\nplot(x1, y1)\nNow we need to identify the boundaries of the axis on each sub-plot as it is created. We can do that using the par() command:\naxbounds &lt;- par('usr')\nWhich gives us the bounds on the x and y axes for whatever plot is open right now.\nWe can then get the range of each axis like this:\naxrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\nNext, define our labels. We have four plots and we can use letters or LETTERS to obtain letters of the alphabet. ie:\nlabels &lt;- LETTERS[1:4]\nGives us the labels a, b, c, d. Or we could also do:\nlabels &lt;- paste0('(',letters[1:4],')')\nWhich gives us (a), (b), (c), (d)\nNow we need to position the labels, relative to each axes range. We can do this by taking the x-axis minimum value and y-axis maximum value and then adding an offset to the axis range. Adding the offset in this way means the labels will always be placed in the same place, even if the axis scale varies. We can place the label using the text command.\ntext(axbounds[1] -\n    (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n    label[1], xpd=NA,...)\nThe argument xpd=NA is neccesary, because without it new additions to the plot outside of the plot window are invisible.\nSo those are the elements we need. To get it all finished, we can stitch these all together in a new function:\n\n\n\nfiglabel &lt;- function(label ='A', xoffset = 0.1, yoffset = 0.08,...){\n    axbounds &lt;- par('usr')\n    axrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\n    text(axbounds[1] -\n        (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n        label, xpd=NA,...)\n    }\nWe use it like this:\n    par(mfrow = c(2,2))\n    plot(x1, y1)\n    figlabel(labels[1], font = 2)\n    plot(x1, y2)\n    figlabel(labels[2], font = 2)\n    plot(x1, y3)\n    figlabel(labels[3], font = 2)\n    plot(x1, y4)\n    figlabel(labels[4], font = 2)\nThe result is this:"
  },
  {
    "objectID": "posts/2019-09-21-scicomm-myths/index.html",
    "href": "posts/2019-09-21-scicomm-myths/index.html",
    "title": "Three myths youve probably heard about scicomm",
    "section": "",
    "text": "The idea of an ivory tower is far from my experience. I trained in a lab of conservation biologists who were working with NGOs and government regularly to inform on environmental management.\nWhen I moved to Griffith, applied funding was the dominant source of funding for my research institute. My colleagues are very successful at working with industry and government to do novel science, but also to deliver sound scientific advice to these partners.\nLooking further, I see so many peers doing great scicomm, everything from education in schools, to writing policy briefs, to appearing frequently in the news media.\nI have heard there ivory towers still out there. Like a university that won’t recognize a Prof’s leadership on the IPCC assessment reports (global syntheses) because the work is too ‘applied’.\nAll of us can of course do more and learn more to be better at scicomm. And the work it involves does need more recognition from universities. But, In my experience, the ivory tower is an outdated concept. We’ve worked hard to come down from the ivory tower."
  },
  {
    "objectID": "posts/2019-09-21-scicomm-myths/index.html#myth-1-academics-scientists-are-stuck-in-an-ivory-tower",
    "href": "posts/2019-09-21-scicomm-myths/index.html#myth-1-academics-scientists-are-stuck-in-an-ivory-tower",
    "title": "Three myths youve probably heard about scicomm",
    "section": "",
    "text": "The idea of an ivory tower is far from my experience. I trained in a lab of conservation biologists who were working with NGOs and government regularly to inform on environmental management.\nWhen I moved to Griffith, applied funding was the dominant source of funding for my research institute. My colleagues are very successful at working with industry and government to do novel science, but also to deliver sound scientific advice to these partners.\nLooking further, I see so many peers doing great scicomm, everything from education in schools, to writing policy briefs, to appearing frequently in the news media.\nI have heard there ivory towers still out there. Like a university that won’t recognize a Prof’s leadership on the IPCC assessment reports (global syntheses) because the work is too ‘applied’.\nAll of us can of course do more and learn more to be better at scicomm. And the work it involves does need more recognition from universities. But, In my experience, the ivory tower is an outdated concept. We’ve worked hard to come down from the ivory tower."
  },
  {
    "objectID": "posts/2019-09-21-scicomm-myths/index.html#myth-2-only-academics-will-read-your-papers",
    "href": "posts/2019-09-21-scicomm-myths/index.html#myth-2-only-academics-will-read-your-papers",
    "title": "Three myths youve probably heard about scicomm",
    "section": "Myth 2: Only academics will read your papers",
    "text": "Myth 2: Only academics will read your papers\nI pretty much believed this one until recently. But then, a pre-print of our study on tiger shark declines was discovered by lawyers (without any help from us) and used in an environmental court case ((open-access version)[https://www.biorxiv.org/content/biorxiv/early/2019/01/13/395509.full.pdf])).\nIt does help to do engagement and promotion of the work. I’m sure the news interviews and tweeting we did about the paper helped build awareness of its existence.\nAnd, I would never trust that people will just find and read my papers. If I think a study has important policy implications, then I will do scicomm on it. This could include writing blogs, writing for newsletters or doing a press release.\nThese actions require a bit of luck, for instance, a press release needs to be topical and happen during a quiet news week to get good uptake in the news media.\nYou could also do more active engagement, like sending the paper, a blog, or an executive summary directly to the people who might want to know about it. Or you could just ring them up and ask to meet."
  },
  {
    "objectID": "posts/2019-09-21-scicomm-myths/index.html#myth-3-if-you-are-tweeting-you-are-doing-scicomm",
    "href": "posts/2019-09-21-scicomm-myths/index.html#myth-3-if-you-are-tweeting-you-are-doing-scicomm",
    "title": "Three myths youve probably heard about scicomm",
    "section": "Myth 3: If you are Tweeting you are doing scicomm",
    "text": "Myth 3: If you are Tweeting you are doing scicomm\nLet’s define scicomm using a common definition: a scientist talking about science to a non-scientist.1\nSo talking to other scientists (‘inreach’) is not scicomm, talking to people outside of academia is scicomm (‘outreach’).\nA clever analysis looked at the followers of prominent ecologists on Twitter. It found that you had to have about 1000 followers to be doing outreach.\nSo if you have &lt;1000 followers then, on average, you not doing much scicomm.\nThe study encourages us to build a following on twitter, so that we do eventually reach a broader audience."
  },
  {
    "objectID": "posts/2019-09-21-scicomm-myths/index.html#why-tackle-the-myths",
    "href": "posts/2019-09-21-scicomm-myths/index.html#why-tackle-the-myths",
    "title": "Three myths youve probably heard about scicomm",
    "section": "Why tackle the myths?",
    "text": "Why tackle the myths?\nI wanted to write about these three myths, because I still hear them pretty commonly. I hope that by recognizing them as myths, we can give ourselves and each other better recognition for the scicomm we do. And I hope recognizing these myths will help us keep scicomm training up to speed with scientists needs.\nThat last topic is something science communicator Tom Rayner and I will be writing about soon.\n1: Science communicator Tom Rayner gave me this definition. Neither of us are sure it’s quite right, but that’s a topic for another blog."
  },
  {
    "objectID": "posts/2024-01-05-credible-intervals-with-GAMs/index.html",
    "href": "posts/2024-01-05-credible-intervals-with-GAMs/index.html",
    "title": "Probability intervals with GAMs",
    "section": "",
    "text": "A neat trick with GAMs (generalized additive models) fit using the mgcv package is that you can use simulation to get Bayesian credible intervals. This is handy because it means you can easily calculate probabilities for any arbitrary transformation of your parameters.\nFor instance, we used this trick to calculate the probability that animal abundance was greater in an experimental treatment vs the control. Using a GAM meant we could also account for non-linear temporal effects.\nThe example I give below is for probability of exceeding a threshold.\nSimon Wood explains in his book that there is a theoretical equivalence between GAMs from his package and an empirical Bayesian analysis. In short, the GAM penalty parameters (the ones that control the wiggliness of the non-linear fits) are analogous to Bayesian priors.\nThe reason we say ‘empirical’ Bayesian analysis is that the smoothing parameters are estimated from data, rather than having a prior.\n\nGetting credible intervals from a GAM\n\nSet-up data\nThis is well documented in Wood’s book as well as in the mgcv help files, see examples in ?predict.gam.\nI made myself a handy little function to implement the CIs, which we will use in this post.\nHere’s the set-up, including sourcing in my function\nlibrary(mgcv)\n\n## Loading required package: nlme\n\n## This is mgcv 1.8-42. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(ggplot2)\nsource(\"gam-CIs.R\")\nNow let’s make up some data we can fit too. Let’s assume we are measuring biological responses (`y’) to a toxin:\nset.seed(2024)\nn &lt;- 50\ntoxin &lt;- sort(runif(n, 0, 1))\ndat &lt;- data.frame(toxin = toxin)\ndat &lt;- within(dat, {\n  ytrue = 6*toxin  - 7*toxin^2\n  yobs = ytrue + rnorm(n, sd = 0.2)\n})\n\nggplot(dat) + \n  aes(x = toxin, y= yobs) + \n  geom_point() + \n  geom_line(aes(y = ytrue))\n\nytrue is the true values of the function, and yobs are our data on this variable.\n\n\nFit a GAM\nNow fit a GAM and add its predictions to our dataframe:\nm1 &lt;- gam(yobs ~ s(toxin), \n          data = dat)\ndat$ypred &lt;- predict(m1)\n\nggplot(dat) + \n  aes(x = toxin, y= yobs) + \n  geom_point() + \n  geom_line(aes(y = ytrue)) + \n  geom_line(aes(y = ypred), col = 'red')\n\nThe red line just shows the predictions from the fitted GAM, relative to the true curve (black line).\n\n\nPlotting predictions with CIs\nThe first application of the empirical Bayes trick is to create a plot of predictions with credible intervals.\nI won’t go into the detail of the implementation, please see Wood’s book and ?predict.gam for more info. But in short, you can sample predictions from the joint distribution of parameters and their covariances. We are assuming that parameters and their uncertainties are described by a multivariate normal distribution (methods like HMC and MCMC allow you to relax this assumption, such as when parameter values are skewed or multi-modal).\nWe need to account for parameter covariances because a higher estimate of one parameter might imply a lower or higher estimate for another parameter. e.g. slope and intercept values for a linear fit are negatively correlated (a low slope high intercept fit will have similar sums of squares to a high slope low intercept fit).\nMy function (simulate_gam_CIs) just implements the instructions given in ?predict.gam. If you are using more complex models I recommend diving into the code yourself, as I can’t guarantee it works in all cases (e.g. doesn’t work for models with coefficients on the variance parameter like the gaulss family).\ngamCIs &lt;- simulate_gam_CIs(m1,\n                 newdata = dat,\n                 forms = list(~x), \n                 random_var = NULL,\n                 offset = 0,\n                 probs = c(0.025, 0.5, 0.975),\n                 nsims = 1000)\nThe function is drawing 1000 values from the posterior of the model. It then transforms the values according to the function given at forms. In this case we are not doing any transformation.\nFinally, it calculates the 2.5, 50 and 97.5% quantiles across those 1000 values, at each x value in the dataframe. It outputs a list of transformations, where each item corresponds to one of the formulas.\nSo basically here we now have 95% credible intervals, and the median.\nLet’s plot that:\nggplot(dat) + \n  aes(x = toxin, y= yobs) + \n  geom_point() + \n  geom_line(aes(y = ytrue)) + \n  geom_line(data = gamCIs[[1]], aes(y = X50.), col = 'purple') + \n  geom_ribbon(data = gamCIs[[1]], aes(ymin = X2.5., ymax = X97.5.),\n              color = NA, fill = \"purple\", alpha = 0.5)\n\n\n\nProbability of exceeding a threshold\nA non-linear transform we might like to do is to find the probability of the mean exceeding a threshold, any arbitrary value of x. Let’s find that probability for exceeding a value of 1 all the observed values of x.\nIn this case we don’t want quantiles over the posterior samples, we want to know what’s the probability of x&gt;1. Since the function x&gt;1 returns a logical, we can simply sum() over the posterior samples and divide by the number of samples to get the probability. So below we specify func_to_apply = \"sum\".\ngam_prob_exceed1 &lt;- simulate_gam_CIs(m1,\n                 newdata = dat,\n                 forms = list(~x&gt;1), \n                 random_var = NULL,\n                 offset = 0,\n                 probs = c(0.025, 0.5, 0.975),\n                 nsims = 1000, \n                 func_to_apply = \"sum\")\n\nggplot(gam_prob_exceed1[[1]]) + \n  aes(x = toxin, y =prob) + \n  geom_line() +\n  ylab(\"Probability of the mean of y exceeding a value of 1\")\n\nCouple of important points. When providing a formula x is the name I used in the function to refer to the linear predictor (ie predictions for mean of our response), its not the ‘x-variable’.\nWe are predicting the probability the mean exceeds a threshold above, not the probability of data observations exceeding a threshold. That would be called a predictive interval and we would need to additionally sample from the observation distribution (in this case a normal).\n\n\n\nApplications\nI find this empirical Bayesian interpretation of GAMs very handy. I use it when:\n\nIts impractical to fit a full Bayesian model, such as for large datasets or simulation applications where I need to repeat fitting 1000s of times. GAMs are generally much faster than numerical Bayesian methods like HMC or MCMC.\nI’m working with a collaborator that knows GAMs but not Bayesian models\nI want to get predictions on the natural scale for a GAM that is fitted with a link function (e.g. poisson with log-link)."
  },
  {
    "objectID": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html",
    "href": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html",
    "title": "Should I learn sf or sp for spatial R programming",
    "section": "",
    "text": "I recently received an email about some of the short-courses on my webpage. The correspondant was asked whether my course on Maps and GIS in R was still relevant, it being written in 2014.\nThe R environment is changing so fast, it is worth asking whether course material that is a few years old is still relevant.\nMuch of my 2014 course still remains relevant, like the aspects about spatial projections, raster package. The material about points and polygons is still relevant, but may not be soon. Finally, there are some things the course misses that I would include now, like integration with leaflet maps (which is in some of my later courses).\nHere are a few major changes in the R environment for spatial analysis as I see it, and some important trends."
  },
  {
    "objectID": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html#spatial-data-structures-and-the-simple-features-package",
    "href": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html#spatial-data-structures-and-the-simple-features-package",
    "title": "Should I learn sf or sp for spatial R programming",
    "section": "Spatial data-structures and the simple features package",
    "text": "Spatial data-structures and the simple features package\nThe sp package for R provides spatial data-structures (termed ‘classes’) and a few utility functions (like spplot for plotting). In the past having this package has been essential for just about any spatial analysis in R, whether it was GIS or spatial modelling.\nA newish package on the block sf has much the same functionality as sp, but promises to be much more convenient and flexible. It has been around a few years (I think), but there was little information about it. The release of the book Geocomputation with R and an increasing number of solutions on stackoverlow will probably see an many more people using sf.\nsf works with a language independent standard for structuring spatial data termed ‘simple features’. Just a few of the many nice aspects of sf are:\nIntuitive data structures sp data, like the SpatialPolygonsDataframe were composed of lists inside lists and were quite hard to decompose (there was a good reason why it was coded like this, but I forget why). sf objects are just data-frames that are collections of spatial objects. Each row is a spatial object (e.g. a polgyon), that may have data associated with it (e.g. its area) and a special geo variable that contains the coordinates.\nIntuitive operations One thing that frustrated me about spatial operations (like intersection) with sp and rgeos (the package for geographic operations), was that a dataframe sp object changed to a different, data-less, class when the operation was performed. sf objects don’t change class when you apply spatial operations to them (and they keep their associated data).\nSpatial indexing which can massively speed up spatial queries, like intersecting polygons, especially on large datasets."
  },
  {
    "objectID": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html#so-should-i-learn-sf-or-sp-now",
    "href": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html#so-should-i-learn-sf-or-sp-now",
    "title": "Should I learn sf or sp for spatial R programming",
    "section": "So should I learn sf or sp now?",
    "text": "So should I learn sf or sp now?\nThat’s a tough question. If you have time, I would say, learn to use both. sf is pretty new, so a lot of packages that depend on spatial classes still rely on sp. So you will need to know sp if you want to do any integration with many other packages, including raster (as of March 2018).\nHowever, in the future we should see an increasing shift toward the sf package and greater use of sf classes in other packages. I also think that sf is easier to learn to use than sp.\nCurrently I use a hybrid of sf and sp, which has not been too hard to get my head around. I do most spatial operations in sf, and convert to sp objects only when I have to, such as if I wanted to rasterize a polygon object."
  },
  {
    "objectID": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html#what-was-missing-in-the-2014-course",
    "href": "posts/2018-03-23-should-I-learn-sp-or-sf/index.html#what-was-missing-in-the-2014-course",
    "title": "Should I learn sf or sp for spatial R programming",
    "section": "What was missing in the 2014 course?",
    "text": "What was missing in the 2014 course?\nA major trend in R packages now is integration with other platforms.\nThis is exemplified by the leaflet package, which allows one to create interactive web maps via R. I think teaching some of this stuff is essential now, even in begginer courses (it is pretty easy to make these maps). In fact, I do teach it now, even in general R courses, like this one for conservation biologists.\nCreating interactive maps is a lot of fun and potentially very useful. For instance see Ross Dwyer’s maps that let you explore the distributions of sharks and rays around the world (this one may take a moment to load, as the data-sets are large).\nSo in the future GIS and spatial analysis in R should become more straightfoward and you should be able to smoothly integrate R’s outputs with many other programs."
  },
  {
    "objectID": "posts/2020-02-19-spatial-gam-predictions/index.html",
    "href": "posts/2020-02-19-spatial-gam-predictions/index.html",
    "title": "Spatial predictions with GAMs and rasters",
    "section": "",
    "text": "One powerful use of GAMs is for interpolating to unsampled locations. We can combine GAMs with raster package to conveniently predict a GAM model to places we have not got data.\n\n\nWe’ll simulate some spatial data based on rasters. There are two spatial covariates, x1 and x2. We use these to simulate ‘true’ values of the response, based on linear and polynomial relationships with x2 and x1 respectively.\nlibrary(raster)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nrbase &lt;- raster(extent(c(0,1,0,1)), nrow = 50, ncol = 50)\nrx1 &lt;- rx2 &lt;- rbase\nrx1[] &lt;- xFromCell(rbase, 1:ncell(rbase))\nrx2[] &lt;- yFromCell(rbase, 1:ncell(rbase))\nrtrue &lt;- 6*rx1  - 7*rx1^2- 4*rx2\npar(mfrow = c(2,2))\nplot(rx1, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"x1\")\nplot(rx2, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"x2\")\nplot(rtrue, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"True values\")\n\n\n\n\nHere we create some random study site locations, then extract the values of the covariates from the raster at the study site locations.\nset.seed(42)\nsite_means &lt;- data.frame(x = runif(50), y = runif(50)) %&gt;%\n  mutate(site = 1:50,\n         x1 = extract(rx1, cbind(x,y)),\n         x2 = extract(rx2, cbind(x,y)),\n         eta = extract(rtrue, cbind(x,y)),\n         z = rnorm(50, sd = 1),\n         yhat = eta + z)\nNow we will assume 3 transects at each site with a a group random effect (so transects at each site share a common deviation from the ‘true effect’). There is also individual transect level variation.\ndat &lt;- inner_join(site_means,\n                  expand.grid(site = 1:50, transect = 1:3)) %&gt;%\n  mutate(b = rnorm(150, mean = yhat, sd = 0.5))\n\n## Joining, by = \"site\"\n\ng1 &lt;- ggplot(dat) +\naes(x = eta, y = b) +\ngeom_point()\n\ng2 &lt;- ggplot(dat) +\n  aes(x = yhat, y = b) +\n  geom_point()\n\ng3 &lt;- ggplot(site_means) +\naes(x = x, y = y, color = yhat) +\ngeom_point()\n\ng3 | g1 /g2\n The plot shows site means (colours) for location, then the relationship between the ‘true’ values (eta) and the response (‘b’) and the site means (yhat) and the response.\n\n\n\nNow we fit a GAM, with smoothers on covariates x1 and x2 and a random effect (bs = \"re\") for the sites.\nlibrary(visreg)\nlibrary(mgcv)\n\ndat$sitef &lt;- factor(dat$site)\nm1 &lt;- gam(b ~ s(x1) + s(x2) + s(sitef, bs = 're'),\n          data = dat)\nvisreg(m1)\n\nThe GAM detects some non-linearity with x1 and a linear relationship with x2.\nCheck the summary:\nsummary(m1)\n\n##\n## Family: gaussian\n## Link function: identity\n##\n## Formula:\n## b ~ s(x1) + s(x2) + s(sitef, bs = \"re\")\n##\n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -1.2716     0.1322  -9.619 4.83e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Approximate significance of smooth terms:\n##             edf Ref.df      F  p-value    \n## s(x1)     3.424  3.494  7.982 5.76e-05 ***\n## s(x2)     1.000  1.000 49.915 1.01e-10 ***\n## s(sitef) 40.718 47.000  9.500  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## R-sq.(adj) =  0.901   Deviance explained = 93.1%\n## GCV = 0.32768  Scale est. = 0.22688   n = 150\nThe edf on s(x2) is exactly 1, confirming the GAM has just fit a linear relationship (correctly) for that covariate. The edf on s(x1) is 3.4, indicating it has probably overfit that covariate a little (for a 2nd order polynomial, we’d expect an edf of 2).\nDon’t get too excited about the high deviance explained, because that calculation includes the deviance explained by the random effect smoother.\nWe can see how much variation ended up in the random effect with gam.vcomp (which counter to its name gives variance components as standard deviations):\ngam.vcomp(m1)\n\n##        s(x1)        s(x2)     s(sitef)\n## 1.335173e+01 1.685585e-04 8.934146e-01\nSo the SD for sites was about 0.89, which is close to the true value of 1.\n\n\n\nWe can use our GAM to predict across the entire raster. We need to stack the rasters together and name each layer of the stack so that it corresponds to the covariate names in the call to gam().\nOne trick here is that we want to predict to the ‘average’ site, not to individual sites, so we need to take out the random site effect. We can do this by creating a raster of zeros and calling it ‘sitef’. Then the predictions will return values for the average site (given of course x1 and x2 values).\nrzeros &lt;- rbase\nrzeros[] &lt;- 0\nrstack &lt;- stack(rx1, rx2, rzeros)\nnames(rstack) &lt;- c(\"x1\", \"x2\", \"sitef\")\nrpred &lt;- predict(rstack, m1)\n\n## Warning in predict.gam(model, blockvals, ...): factor levels 0 not in original\n## fit\nThis returns a warning, but don’t worry, it was designed to work this way!\nWe used a raster stack to form predictions, so the predictions will come out as a raster, so we can just plot them as rasters:\npar(mfrow = c(2,2))\nplot(rpred, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"Predictions\")\nplot(rtrue, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"True values\")\nplot(rpred - rtrue, col = RColorBrewer::brewer.pal(11, \"RdBu\"),\n     main = \"Prediction - true\")\n\nOverall our model has captured the main spatial gradients, but is tending to under-predict the region in the middle (in red) and southern portion of the space and slightly over-predict in the top corners.\n\n\n\nThis is just bonus content, because I find it interesting that you can get Bayesian posterior distributions from a fitted GAM.\nThis example is straight out of `?predict.gam. These will be conditional (not marginalized) on the site random effect.\nnewdat &lt;- with(dat,\n               data.frame(x1 = seq(min(x1), max(x1),\n                                   by = 0.01),\n               x2 = mean(x2),\n               sitef = 0)) #again set sitef to zero\nm1p &lt;- predict(m1, newdata = newdat, type = \"lpmatrix\")\n\n## Warning in predict.gam(m1, newdata = newdat, type = \"lpmatrix\"): factor levels 0\n## not in original fit\nNow we have a predictions matrix, sample from it:\nrmvn &lt;- function(n,mu,sig) { ## MVN random deviates\n  L &lt;- mroot(sig);m &lt;- ncol(L);\n  t(mu + L%*%matrix(rnorm(m*n),m,n))\n}\n\nbr &lt;- rmvn(1000,coef(m1),m1$Vp)\nm1post &lt;- matrix(NA, nrow = nrow(newdat),\n                 ncol = 1000)\nfor (i in 1:1000){\n  m1post[,i] &lt;- m1p %*% br[i,]\n}\nCIdat &lt;- t(apply(m1post, 1, quantile, probs = c(0.025, 0.5, 0.975))) %&gt;%\n  data.frame(., newdat)\n\ng1 &lt;- ggplot(CIdat) +\n  aes(x = x1, y = X50.) +\n  geom_point() +\n  geom_ribbon(aes(ymin = X2.5., ymax = X97.5.),\n              color = NA, alpha = 0.5) +\n  ylab(\"Predictions\") +\n  ggtitle(\"Predcitions with 95% C.I.\")\n\ng2 &lt;- ggplot(data.frame(b = m1post[1,])) +\n  aes(x = b) +\n  geom_density(fill = \"Purple\") +\n  ggtitle(\"Posterior density at x = 0.01\")\n\ng1 / g2"
  },
  {
    "objectID": "posts/2020-02-19-spatial-gam-predictions/index.html#simulate-some-spatial-data",
    "href": "posts/2020-02-19-spatial-gam-predictions/index.html#simulate-some-spatial-data",
    "title": "Spatial predictions with GAMs and rasters",
    "section": "",
    "text": "We’ll simulate some spatial data based on rasters. There are two spatial covariates, x1 and x2. We use these to simulate ‘true’ values of the response, based on linear and polynomial relationships with x2 and x1 respectively.\nlibrary(raster)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nrbase &lt;- raster(extent(c(0,1,0,1)), nrow = 50, ncol = 50)\nrx1 &lt;- rx2 &lt;- rbase\nrx1[] &lt;- xFromCell(rbase, 1:ncell(rbase))\nrx2[] &lt;- yFromCell(rbase, 1:ncell(rbase))\nrtrue &lt;- 6*rx1  - 7*rx1^2- 4*rx2\npar(mfrow = c(2,2))\nplot(rx1, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"x1\")\nplot(rx2, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"x2\")\nplot(rtrue, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"True values\")"
  },
  {
    "objectID": "posts/2020-02-19-spatial-gam-predictions/index.html#extract-data-at-study-sites",
    "href": "posts/2020-02-19-spatial-gam-predictions/index.html#extract-data-at-study-sites",
    "title": "Spatial predictions with GAMs and rasters",
    "section": "",
    "text": "Here we create some random study site locations, then extract the values of the covariates from the raster at the study site locations.\nset.seed(42)\nsite_means &lt;- data.frame(x = runif(50), y = runif(50)) %&gt;%\n  mutate(site = 1:50,\n         x1 = extract(rx1, cbind(x,y)),\n         x2 = extract(rx2, cbind(x,y)),\n         eta = extract(rtrue, cbind(x,y)),\n         z = rnorm(50, sd = 1),\n         yhat = eta + z)\nNow we will assume 3 transects at each site with a a group random effect (so transects at each site share a common deviation from the ‘true effect’). There is also individual transect level variation.\ndat &lt;- inner_join(site_means,\n                  expand.grid(site = 1:50, transect = 1:3)) %&gt;%\n  mutate(b = rnorm(150, mean = yhat, sd = 0.5))\n\n## Joining, by = \"site\"\n\ng1 &lt;- ggplot(dat) +\naes(x = eta, y = b) +\ngeom_point()\n\ng2 &lt;- ggplot(dat) +\n  aes(x = yhat, y = b) +\n  geom_point()\n\ng3 &lt;- ggplot(site_means) +\naes(x = x, y = y, color = yhat) +\ngeom_point()\n\ng3 | g1 /g2\n The plot shows site means (colours) for location, then the relationship between the ‘true’ values (eta) and the response (‘b’) and the site means (yhat) and the response."
  },
  {
    "objectID": "posts/2020-02-19-spatial-gam-predictions/index.html#fit-a-gam",
    "href": "posts/2020-02-19-spatial-gam-predictions/index.html#fit-a-gam",
    "title": "Spatial predictions with GAMs and rasters",
    "section": "",
    "text": "Now we fit a GAM, with smoothers on covariates x1 and x2 and a random effect (bs = \"re\") for the sites.\nlibrary(visreg)\nlibrary(mgcv)\n\ndat$sitef &lt;- factor(dat$site)\nm1 &lt;- gam(b ~ s(x1) + s(x2) + s(sitef, bs = 're'),\n          data = dat)\nvisreg(m1)\n\nThe GAM detects some non-linearity with x1 and a linear relationship with x2.\nCheck the summary:\nsummary(m1)\n\n##\n## Family: gaussian\n## Link function: identity\n##\n## Formula:\n## b ~ s(x1) + s(x2) + s(sitef, bs = \"re\")\n##\n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -1.2716     0.1322  -9.619 4.83e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Approximate significance of smooth terms:\n##             edf Ref.df      F  p-value    \n## s(x1)     3.424  3.494  7.982 5.76e-05 ***\n## s(x2)     1.000  1.000 49.915 1.01e-10 ***\n## s(sitef) 40.718 47.000  9.500  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## R-sq.(adj) =  0.901   Deviance explained = 93.1%\n## GCV = 0.32768  Scale est. = 0.22688   n = 150\nThe edf on s(x2) is exactly 1, confirming the GAM has just fit a linear relationship (correctly) for that covariate. The edf on s(x1) is 3.4, indicating it has probably overfit that covariate a little (for a 2nd order polynomial, we’d expect an edf of 2).\nDon’t get too excited about the high deviance explained, because that calculation includes the deviance explained by the random effect smoother.\nWe can see how much variation ended up in the random effect with gam.vcomp (which counter to its name gives variance components as standard deviations):\ngam.vcomp(m1)\n\n##        s(x1)        s(x2)     s(sitef)\n## 1.335173e+01 1.685585e-04 8.934146e-01\nSo the SD for sites was about 0.89, which is close to the true value of 1."
  },
  {
    "objectID": "posts/2020-02-19-spatial-gam-predictions/index.html#predict-everywhere",
    "href": "posts/2020-02-19-spatial-gam-predictions/index.html#predict-everywhere",
    "title": "Spatial predictions with GAMs and rasters",
    "section": "",
    "text": "We can use our GAM to predict across the entire raster. We need to stack the rasters together and name each layer of the stack so that it corresponds to the covariate names in the call to gam().\nOne trick here is that we want to predict to the ‘average’ site, not to individual sites, so we need to take out the random site effect. We can do this by creating a raster of zeros and calling it ‘sitef’. Then the predictions will return values for the average site (given of course x1 and x2 values).\nrzeros &lt;- rbase\nrzeros[] &lt;- 0\nrstack &lt;- stack(rx1, rx2, rzeros)\nnames(rstack) &lt;- c(\"x1\", \"x2\", \"sitef\")\nrpred &lt;- predict(rstack, m1)\n\n## Warning in predict.gam(model, blockvals, ...): factor levels 0 not in original\n## fit\nThis returns a warning, but don’t worry, it was designed to work this way!\nWe used a raster stack to form predictions, so the predictions will come out as a raster, so we can just plot them as rasters:\npar(mfrow = c(2,2))\nplot(rpred, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"Predictions\")\nplot(rtrue, col = RColorBrewer::brewer.pal(11, \"RdBu\"), main = \"True values\")\nplot(rpred - rtrue, col = RColorBrewer::brewer.pal(11, \"RdBu\"),\n     main = \"Prediction - true\")\n\nOverall our model has captured the main spatial gradients, but is tending to under-predict the region in the middle (in red) and southern portion of the space and slightly over-predict in the top corners."
  },
  {
    "objectID": "posts/2020-02-19-spatial-gam-predictions/index.html#posterior-predictions",
    "href": "posts/2020-02-19-spatial-gam-predictions/index.html#posterior-predictions",
    "title": "Spatial predictions with GAMs and rasters",
    "section": "",
    "text": "This is just bonus content, because I find it interesting that you can get Bayesian posterior distributions from a fitted GAM.\nThis example is straight out of `?predict.gam. These will be conditional (not marginalized) on the site random effect.\nnewdat &lt;- with(dat,\n               data.frame(x1 = seq(min(x1), max(x1),\n                                   by = 0.01),\n               x2 = mean(x2),\n               sitef = 0)) #again set sitef to zero\nm1p &lt;- predict(m1, newdata = newdat, type = \"lpmatrix\")\n\n## Warning in predict.gam(m1, newdata = newdat, type = \"lpmatrix\"): factor levels 0\n## not in original fit\nNow we have a predictions matrix, sample from it:\nrmvn &lt;- function(n,mu,sig) { ## MVN random deviates\n  L &lt;- mroot(sig);m &lt;- ncol(L);\n  t(mu + L%*%matrix(rnorm(m*n),m,n))\n}\n\nbr &lt;- rmvn(1000,coef(m1),m1$Vp)\nm1post &lt;- matrix(NA, nrow = nrow(newdat),\n                 ncol = 1000)\nfor (i in 1:1000){\n  m1post[,i] &lt;- m1p %*% br[i,]\n}\nCIdat &lt;- t(apply(m1post, 1, quantile, probs = c(0.025, 0.5, 0.975))) %&gt;%\n  data.frame(., newdat)\n\ng1 &lt;- ggplot(CIdat) +\n  aes(x = x1, y = X50.) +\n  geom_point() +\n  geom_ribbon(aes(ymin = X2.5., ymax = X97.5.),\n              color = NA, alpha = 0.5) +\n  ylab(\"Predictions\") +\n  ggtitle(\"Predcitions with 95% C.I.\")\n\ng2 &lt;- ggplot(data.frame(b = m1post[1,])) +\n  aes(x = b) +\n  geom_density(fill = \"Purple\") +\n  ggtitle(\"Posterior density at x = 0.01\")\n\ng1 / g2"
  },
  {
    "objectID": "posts/2024-10-09-recovery-times-coastal-habitats/index.html",
    "href": "posts/2024-10-09-recovery-times-coastal-habitats/index.html",
    "title": "Speeding up the recovery and restoration of coastal habitats",
    "section": "",
    "text": "In our new study we predict the timescales of recovery for seagrass meadows after a disturbance. We found that recovery times are often longer and more variable than might be expected. However, appropriate management interventions can significantly accelerate the recovery process.\nOur modelling approach could be applied to any coastal habitat, including coral reefs, kelp forests, salt marsh and mangrove forests. In can inform on expected timescales of recovery and optimal restoration actions. We designed the model because we noticed how marine protected area science had benefited from models that predicted the recovery of fish populations. We wanted to develop a similar modelling basis for coastal habitat restoration.\n\nHere’s an FAQ for the study, generated from the study by Google Notebook and edited and verified for accuracy by me.\n\nWhat factors influence the recovery time of coastal habitats?\n\nThe recovery time of coastal habitats, such as seagrass meadows, coral reefs, and mangrove forests, is influenced by several factors:\nConnectivity: The presence of nearby healthy habitat patches that can act as propagule (seeds, larvae, dislodged shoots) sources is crucial. Higher connectivity generally leads to faster recovery.\nDispersal: The ability of propagules to successfully travel from source habitats to the disturbed area. Dispersal is affected by factors like currents, winds, and the dispersal mechanisms of the species.\nRecruitment: The successful establishment of dispersed propagules in the disturbed area. Recruitment depends on suitable environmental conditions, such as sediment stability, light availability, and water quality.\nGrowth rate: The inherent growth rate of the species influences how quickly it can repopulate the area.\nChronic stressors: Ongoing pressures like pollution or low light conditions can significantly hinder recovery.\n\nHow does the model in the study predict recovery times?\n\nThe study developed a mathematical model that simulates the recovery of coastal habitats. It incorporates:\nHabitat patches: Represents the disturbed area and nearby remnant habitats as interconnected patches.\nPropagule production: Estimates the amount of propagules produced by each remnant patch based on its size and condition.\nDispersal and recruitment: Models the probability of propagules successfully dispersing and recruiting to the disturbed patch, considering connectivity, dispersal limitations, and environmental conditions.\nGrowth: Uses a growth function (e.g., logistic growth for seagrass) to simulate habitat regeneration in the disturbed patch based on factors like light availability and species-specific growth rates.\nThe model runs simulations with different parameter combinations to predict recovery times and evaluate the effectiveness of various management interventions.\n\nWhat management interventions can accelerate the recovery of coastal habitats?\n\nThe study highlights three key management interventions:\nSpreading propagules or planting seedlings: Directly increases propagule supply to overcome dispersal limitations, particularly in isolated areas.\nStabilizing recruitment conditions: Addresses recruitment limitations by creating more favorable conditions for propagule establishment, such as using biodegradable structures to stabilize sediments or improve substrate.\nMitigating chronic stressors: Improves overall ecosystem health and enhances both dispersal and recruitment by addressing factors like pollution, nutrient loading, or low light conditions.\nThe most effective intervention depends on the specific context and the dominant limitations to recovery.\n\nWhy is it important to understand the difference between dispersal and recruitment limitations?\n\nDistinguishing between dispersal and recruitment limitations is crucial for effective management:\nDispersal limitation: Occurs when propagules don’t reach the disturbed area due to limited connectivity or inefficient dispersal mechanisms. Solution: Focus on interventions that enhance propagule supply, like seeding or planting.\nRecruitment limitation: Arises when propagules reach the disturbed area but fail to establish due to unfavorable environmental conditions. Solution: Prioritize interventions that improve local conditions, such as stabilizing sediments or improving water quality.\nIdentifying the dominant limitation helps target restoration efforts for the fastest recovery times and greatest rates of success.\n\nHow does low light stress affect the recovery of coastal habitats?\n\nLow light stress, often caused by pollution or turbidity, significantly hinders recovery by:\nReducing growth: Limits the photosynthetic ability of habitat-forming species, slowing down their growth and expansion.\nIncreasing recovery time: Extends the time it takes for the disturbed area to regain its functionality and ecological value.\nReduces propagule supply: Low light conditions can limit the production of propagules in remnant habitats, further impeding recovery.\nAddressing low light conditions, often through catchment management to improve water quality, is crucial for successful restoration, especially in light-limited environments.\n\nHow can this research inform the future of coastal habitat restoration?\n\nThis study provides a valuable framework for:\nIdentifying recovery bottlenecks: By understanding the factors limiting recovery (dispersal, recruitment, growth), managers can prioritize appropriate interventions.\nPredicting recovery timelines: The model offers estimates of recovery times, helping set realistic expectations and inform adaptive management strategies.\nOptimizing restoration efforts: Simulating different management scenarios can identify the most effective approach for specific contexts and maximize restoration success.\nBy combining ecological understanding with modeling tools, this research paves the way for more efficient and successful coastal habitat restoration efforts globally.\nBrown CJ, Campbell MD, Collier CJ, Turschwell MP, Saunders MI, Connolly RM. 2024 Speeding up the recovery of coastal habitats through management interventions that address constraints on dispersal and recruitment. Proc. R. Soc. B 291: 20241065."
  },
  {
    "objectID": "posts/2022-05-19-science-meets-parliament/index.html",
    "href": "posts/2022-05-19-science-meets-parliament/index.html",
    "title": "Science Meets Parliament",
    "section": "",
    "text": "By Dr Mischa Turschwell, Research Fellow\nThanks to the Australian Society for Fish Biology, I had the privilege of attending Science Meets Parliament earlier this year. Unfortunately, due to COVID-19 my hopes of a chance in-person encounter with a famous politician were dashed, but nonetheless Science Meets Parliament 2022 still managed to deliver an action packed week of virtual presentations, workshops and masterclasses. As per the name of the event, the majority of activities focussed on how we—as scientists—can better engage with decision makers. I got to hear from physics whizzes Professor Brian Cox and Kirsten Banks, as well as Nobel prize winning immunologist Professor Peter Doherty.\nThe real kicker was having the opportunity to have an hour-long meeting with a Minister. I was paired with Nationals senate whip, Senator Perin Davey who was genuinely interested in my science. We discussed the challenges of environmental watering in the Murray-Darling Basin, and spoke about some of the cool science and technology emerging in Australian waters through the Blue Economy CRC. Senator Davey gave me some great advice on how to engage with MPs and senior decision makers. Here are a few of the top tips:\nKeep it simple. Remember, a lot of parliamentarians don’t have a scientific background so keep the terminology simple and only discuss 1-3 points. Keep any written pitch to 1 page and always make sure you have the evidence to back up your claims.\nFocus on the solution. A large majority of the time when scientists get the chance meet decision makers or politicians, they focus on asking for/complaining about a lack of funding. Turn the tables and instead focus on pitching your expertise as solution to a problem (then hopefully the funding will flow!).\nBe respectfully persistent. Pollies are a busy bunch. If they haven’t responded to you, it’s probably slipped their mind. They will not mind if you follow up and persevere with your enquiry, just make sure you engage in a respectful manner. The old saying goes, ‘persistence pays off’. Localise the issue. Can you translate what your science means in a local context? What does it mean for their electorate? Make your pitch personally relatable as having a vote attached can be hugely influential.\nDifferentiate key players (decision makers) from your target audience. We tend to focus on target audience (which of course is key for wider science communication), but often decision makers will need a different form of engagement (see points above about politicians being busy).\nAt the time of writing, I haven’t yet attended the Gala dinner but lucky for me that’s an in-person event so maybe I’ll get that chance encounter after all!"
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "",
    "text": "Here is my recommended reading list for students just starting a PhD in some aspect of quantitative ecology. I’ve written this reading list to keep in mind that many new PhDs may not have a lot of math or stats from their earlier degrees."
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#the-role-of-statistics-in-science",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#the-role-of-statistics-in-science",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "The role of statistics in science",
    "text": "The role of statistics in science\nShipley, Cause and Correlation in Biology\nChapters 1 and 2 give a general introduction to the modern philosophy of science, with a focus on statistics, cause and correlation. This is a great introduction to how we use correlation to infer causation in modern science, and how science often progresses by building evidence for or against multiple competing hypotheses.\nChapters 3 onwards are more focused on the methods of structural equation modelling and will be of most interest to students who will be using statistical models such as GLMs.\nHilborn and Mangel, The Ecological Detective\nThe first few chapters are worth reading for an excellent introduction to the philosophy of science. Later chapters focus on theory and application of Bayesian statistical methods."
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#bayesian-statistics",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#bayesian-statistics",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\nMcElreath, Statistical Rethinking\nIf you are going to be doing anything with Bayesian methods, then this book is gold. McElreath uses easily understood analogies to break down the jargon and complexity of Bayesian models and make them accessible to a general science audience.\nYou can even watch this whole book in his lecture series on YouTube."
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#ecological-modelling",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#ecological-modelling",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "Ecological modelling",
    "text": "Ecological modelling\nOtto and Day, A biologists guide to mathematical modelling in Ecology and Evolution.\nI’d recommend the first few chapters for everyone again, because they will give you a good overview of what modelling is about. Later chapters will be good for students who will be specifically using process models (e.g. with differential equations) in their PhD."
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#data-visualisation",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#data-visualisation",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "Data visualisation",
    "text": "Data visualisation\nThere are lots of great books on this topic, but one of my favourites is Cairo, The Truthful Art.\nCairo uses really compelling examples to show how we can use data visualisation and simple statistics to accurately communicate scientific knowledge."
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#skills-for-a-successful-phd",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#skills-for-a-successful-phd",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "Skills for a successful PhD",
    "text": "Skills for a successful PhD\nYou can’t go past Gardiner and Kearn’s books. There’s a whole bunch to recommend here, but the Seven Secrets is probably a good place to start."
  },
  {
    "objectID": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#your-field-of-study",
    "href": "posts/2020-02-18-new-phd-reading-quant-ecol/index.html#your-field-of-study",
    "title": "Reading for new quantitative ecology PhD students",
    "section": "Your field of study",
    "text": "Your field of study\nYou should of course be also reading papers in your specific field of study. A few tips for finding these are:\n\nLook in the top review journals, like Trends in Ecology and Evolution and Biological Reviews, for recent articles in your general area of interest. These journals in particular have high communication standards and articles are pitched at a more general audience than in specialist journals.\nSearch (e.g. with web of science) for review articles on your topic in discipline specific journals. For instance, if its conservation planning, then look in Conservation Letters, Conservation Biology and Biological Conservation.\nMake sure you read the new and old literature. One way to identify very influential older papers is to sort results of a lit search by citation count. When you start your PhD you have your chance to read some of the classics. You probably won’t feel you have time to read this sort of thing later on (though don’t let me hold you back if you want to!).\nAsk your peers for their favourite papers to read.\nFollow up on citations. For instance, if you see an article that is repeatedly cited"
  },
  {
    "objectID": "posts/2018-04-18-isotope-mixing-models/index.html",
    "href": "posts/2018-04-18-isotope-mixing-models/index.html",
    "title": "Ecologists are misusing isotope mixing models",
    "section": "",
    "text": "The study of animal movement, animal diets and more generally nutrient and carbon flows in ecosystems has been transformed by stable isotope ecology. However, in a new paper we present evidence that misuse of isotope mixing models, the statistical tools that underlie the analysis of stable isotopes, may be leading many studies to false conclusions.\nThe problem arises because Bayesian methods are used in the analysis of stable isotope ratios, without thought to how the priors affects the results."
  },
  {
    "objectID": "posts/2018-04-18-isotope-mixing-models/index.html#the-problem-with-the-priors",
    "href": "posts/2018-04-18-isotope-mixing-models/index.html#the-problem-with-the-priors",
    "title": "Ecologists are misusing isotope mixing models",
    "section": "The problem with the priors",
    "text": "The problem with the priors\nIn our analysis, we focus on the use of stable isotopes to estimate the diet compositions of animals. Though the analysis of stable isotope ratios can equally well be applied to understanding nutrient and carbon flows in ecosystems, like in the pictured mangrove forest.\nOne of the most popular approaches for analysing isotope data are Bayesian mixing models. Bayesian mixing models account for variation in the isotopic signature of different potential food sources. This means we get estimates of proportional contribution of food items to a diet with associated uncertainty intervals.\nBayesian methods require specification of priors, which reflect our prior knowledge about what the animal consumes. The model then integrates the prior with the data to obtain the final estimates, termed posteriors.\nThe problem arises, because most ecologists use the ‘default’ prior, apparently without considering its effect on their posterior estimates of diet composition.\nIn many other settings Bayesian priors can be designed so they have only a weak influence on the posterior, this is not the case in mixing models. Take the image below. The blue probability distribution shows the ‘default’ priors for the contribution of each of three food items to a gull species’ diet.\n\nClearly the prior distribution is not totally uninformative. The prior has a mode near 10% and a mean of 1/3 (because their are three potential food sources).\nThe priors for a diet cannot in fact be totally flat across all proportions, simply because the sum across all proportions must equal 1. The plotted priors are in fact the ‘marginal’ priors, so they reflect the fact that there are more ways to add three numbers and get one when those numbers are near 1/3, than near 0 or 1.\nSo if you run an analysis with a small amount of data, or very noisy data, the result you get out will basically be the prior.\nThis means that any potential diet items thrown into the analysis, in case they matter, can come out with a mean contribution near 1/3 (or 1/ however many sources you had), leading researchers to the unexpected conclusion that unlikely diet items are suddenly very important."
  },
  {
    "objectID": "posts/2018-04-18-isotope-mixing-models/index.html#quantifying-learning",
    "href": "posts/2018-04-18-isotope-mixing-models/index.html#quantifying-learning",
    "title": "Ecologists are misusing isotope mixing models",
    "section": "Quantifying learning",
    "text": "Quantifying learning\nWe propose a way to get around this issue is to quantify how much is learned over and above the priors.\nThe idea is simple, we can use the maths of information theory to measure how much the posterior estimates diverge from the priors.\nIn the above picture you see the Hellinger distance for each posterior. This measures the divergence between prior and posterior, with values near zero indicating identical distributions and values near one indicating the posterior has positive probability everywhere the prior had zero probability (ie maximal divergence).\nAnother statistic is the Kullback-Leibler divergence, which we also discuss in the paper.\nSo, if we have not learned much (ie the data are weak), then the Hellinger distance and Kullback-Leibler divergence will be near zero.\nThere are several ways we can get high information divergence, and a high value warrants further investigation.\nFor instance, the data may support the prior, in which case we have similar mean estimates for diet, but uncertainty about diets has decreased.\nOr the data may contradict the priors, in which case we have different mean estimates, or even the same mean with much greater uncertainty."
  },
  {
    "objectID": "posts/2018-04-18-isotope-mixing-models/index.html#power-analysis",
    "href": "posts/2018-04-18-isotope-mixing-models/index.html#power-analysis",
    "title": "Ecologists are misusing isotope mixing models",
    "section": "Power analysis",
    "text": "Power analysis\nA handy side-effect of our application of information theory is that it provides a means for conducting power analysis with Bayesian mixing models. If we simulate data-sets with different numbers of samples we can estimate the sample size required for the data to overcome the prior.\nThis type of power analysis could be useful a-prior analysis to use to help optimise sampling design. For instance, we might want to optimise the number of samples taken from animals for expense or ethical reasons."
  },
  {
    "objectID": "posts/2018-04-18-isotope-mixing-models/index.html#summary",
    "href": "posts/2018-04-18-isotope-mixing-models/index.html#summary",
    "title": "Ecologists are misusing isotope mixing models",
    "section": "Summary",
    "text": "Summary\nMany of the problems associated with interpreting Bayesian mixing models could be avoided if people followed the best practice guidelines. We propose some handy evaluation methods to avoid some of the pitfalls of Bayesian mixing models.\nOur analyses can be conducted with an R package, ‘remixsiar’, that works with simmr and mixsiar, two common packages for Bayesian mixing models.\nWe hope use of these methods will improve the study of nutrient and carbon flows in ecosystems.\nFeel free to message me on Twitter if you have any thoughts or comments."
  },
  {
    "objectID": "posts/2018-01-19-intro-to-glms/index.html",
    "href": "posts/2018-01-19-intro-to-glms/index.html",
    "title": "General Linear Models The Basics",
    "section": "",
    "text": "General linear models are one of the most widely used statistical tool in the biological sciences. This may be because they are so flexible and they can address many different problems, that they provide useful outputs about statistical significance AND effect sizes, or just that they are easy to run in many common statistical packages.\nThe maths underlying General Linear Models (and Generalized linear models, which are a related but different class of model) may seem mysterious to many, but are actually pretty accessible. You would have learned the basics in high school maths.\nWe will cover some of those basics here. You might like to read my blog on Genealized linear model link functions after this one.\n\n\nAs the name suggests General Linear Models rely on a linear equation, which in its basic form is simply:\nyi = α + βxi + ϵ*i\nThe equation for a straight line, with some error added on.\nIf you aren’t that familiar with mathematical notation, notice a few things about this equation (I have followed standard conventions here). I used normal characters for variables (i.e. things you measure) and Greek letters for parameters, which are estimated when you fit the model to the data.\nyi are your response data, I indexed the y with i to indicate that there are multiple observations. xi is variously known as a covariate, predictor variable or explanatory variable. α is an intercept that will be estimated. α has the same units as y. (e.g. if y is number of animals, then α is expected the number of animals when x = 0).\nβ is a slope parameter that will also be estimated. β is also termed the effect size because it measures the effect of x on y. β has units of ‘y per x’. For instance, if x is temperature, then β has units of number of animals per degree C. β thus measures how much we expect y to change if x were to increase by 1.\nFinally, don’t forget ϵi, which is the error. ϵi will measure the distance between each prediction of yi made by the model and the observed value of yi.\nThese predictions will simply be calculated as:\nyi = α + βxi*\n(notice I just removed the ϵi from the end). You can think of the linear predictions as: the mean or ‘expected’ value a new observation yi would take if we only knew xi and also as the ‘line of best fit’.\n\n\n\nNow we know the model, we can generate some idealized data. Hopefully this will then give you a feel for how we can fit a model to data. Open up R and we will create these parameters:\nn &lt;- 100\nbeta &lt;- 2.2\nalpha &lt;- 30\nWhere n is the sample size and alpha and beta are as above.\nWe also need some covariate data, we will just generate a sequence of n numbers from 0 to 1:\nx &lt;- seq(0, 1, length.out = n)\nThe model’s expectation is thus this straight line:\ny_true &lt;- beta * x + alpha\nplot(x, y_true)\n\nBecause we made the model up, we can say this is the true underlying relationship. Now we will add error to it and see if we can recover that relationship with a general linear model.\nLet’s generate some error:\nsigma &lt;- 2.4\nset.seed(42)\nerror &lt;- rnorm(n, sd = sigma)\ny_obs &lt;- y_true + error\nplot(x, y_obs)\nlines(x, y_true)\n\nHere sigma is our standard deviation, which measures how much the observations y vary around the true relationship. We then used rnorm to generate n random normal numbers, that we just add to our predicted line y_true to simulate observing this relationship.\nCongratulations, you just created a (modelled) reality a simulated an ecologist going out and measuring that reality.\nNote the set.seed() command. This just ensures the random number generator produces the same set of numbers every time it is run in R and it is good practice to use it (so your code is repeatable). Here is a great explanation of seed setting and why 42 is so popular.\nAlso, check out the errors:\nhist(error)\n\nLooks like a normal distribution hey? That’s because we generated them from a normal distribution. That was a handy trick, because the basic linear model assumes the errors are normally distributed (but not necessarily the raw data).\nAlso note that sigma is constant (e.g. it doesn’t get larger as x gets larger). That is another assumption of basic linear models called ‘homogeneity of variance’.\n\n\n\nTo fit a basic linear model in R we can use the lm() function:\nm1 &lt;- lm(y_obs ~ x)\nIt takes a formula argument, which simply says here that y_obs depends on (the tilde ~) x. R will do all the number crunching to estimate the parameters now.\nTo see what it came up with try:\ncoef(m1)\n\n## (Intercept)           x\n##   30.163713    2.028646\nThis command tells us the estimate of the intercept ((Intercept)) and the slope on x under x. Notice they are close to, but not exactly the same as alpha and beta. So the model has done a pretty decent job of recovering our original process. The reason the values are not identical is that we simulated someone going and measuring the real process with error (that was when we added the normal random numbers).\nWe can get slightly more details about the model fit like this:\nsummary(m1)\n\n##\n## Call:\n## lm(formula = y_obs ~ x)\n##\n## Residuals:\n##     Min      1Q  Median      3Q     Max\n## -7.2467 -1.5884  0.1942  1.5665  5.3433\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)  30.1637     0.4985  60.503   &lt;2e-16 ***\n## x             2.0286     0.8613   2.355   0.0205 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 2.511 on 98 degrees of freedom\n## Multiple R-squared:  0.05357,    Adjusted R-squared:  0.04391\n## F-statistic: 5.547 on 1 and 98 DF,  p-value: 0.0205\nI’m not going to go overboard with explaining this output now, but notice a few key things. With the summary, we get standard errors for the parameter estimates (which is a measure of how much they might vary). Also notice the R-squared, which can be handy. Finally, notice that the Residual standard error is close to the value we used for sigma, which is because it is an estimate of sigma from our simulated data.\nYour homework is play around with the model and sampling process. Try change alpha, beta, n and sigma, then refit the model and see what happens.\n\n\n\nSo did you do the homework? If you did, well done, you just performed a simple power analysis (in the broad sense).\nIn a more formal power analysis (which is what you might have come across previously) could systematically vary n or beta and for 1000 randomised data sets and then calculate the proportion out of 1000 data-sets that your p-value was ‘significant’ (e.g. less than a critical threshold like the ever-popular 0.05). This number tells you how good you are at detecting ‘real’ effects.\nHere’s a great intro to power analysis in the broad sense: Bolker, Ecological Models and Data in R\nOne more point. Remember we said above about some ‘assumptions’. Well we can check those in R quite easily:\nplot(m1, 1)\n\nThis shows a plot of the residuals (A.K.A. errors) versus the predicted values. We are looking for ‘heteroskedasticity’ which is a fancy way of saying the errors aren’t equal across the range of predictions (remember I said sigma is a constant?).\nAnother good plot:\nplot(m1, 2)\n\nHere we are looking for deviations of the points from the line. Points on the line mean the errors are approximately normally distributed, which was a key assumption. Points far from the line could indicate the errors are skewed left or right, too fat in the middle, or too in the middle skinny. More on that issue here\n\n\n\nSo the basics might belie the true complexity of situations we can address with General Linear Models and their relatives Generalized Linear Models. But, just to get you excited, here are a few things you can do by adding on more terms to the right hand side of the linear equation:\n\nModel multiple, interacting covariates.\nInclude factors as covariates (instead of continuous variables). Got a factor and a continuous variable? Don’t bother with the old-school ANCOVA method, just use a linear model.\nInclude a spline to model non-linear effects (that’s a GAM).\nAccount for hierarchies in your sampling, like transects sampled within sites (that’s a mixed effects model)\nAccount for spatial or temporal dependencies.\nModel varying error variance (e.g. when the variance increases with the mean).\n\nYou can also change the left-hand side, so that it no longer assumes normality (then that’s a Generalized Linear Model). Or even add chains of models together to model pathways of cause and effect (that’s a ‘path analysis’ or ‘structural equation model’)\nIf this taster has left you keen to learn more, then check out any one of the zillion online courses or books on GLMs with R, or if you can get to Brisbane, come to our next course (which as of writing was in Feb 2018, but we do them regularly).\nNow you know the basics, practice, practice, practice and pretty soon you will be running General Linear Models behind your back while you watch your 2 year old, which is what I do for kicks."
  },
  {
    "objectID": "posts/2018-01-19-intro-to-glms/index.html#linear-equations",
    "href": "posts/2018-01-19-intro-to-glms/index.html#linear-equations",
    "title": "General Linear Models The Basics",
    "section": "",
    "text": "As the name suggests General Linear Models rely on a linear equation, which in its basic form is simply:\nyi = α + βxi + ϵ*i\nThe equation for a straight line, with some error added on.\nIf you aren’t that familiar with mathematical notation, notice a few things about this equation (I have followed standard conventions here). I used normal characters for variables (i.e. things you measure) and Greek letters for parameters, which are estimated when you fit the model to the data.\nyi are your response data, I indexed the y with i to indicate that there are multiple observations. xi is variously known as a covariate, predictor variable or explanatory variable. α is an intercept that will be estimated. α has the same units as y. (e.g. if y is number of animals, then α is expected the number of animals when x = 0).\nβ is a slope parameter that will also be estimated. β is also termed the effect size because it measures the effect of x on y. β has units of ‘y per x’. For instance, if x is temperature, then β has units of number of animals per degree C. β thus measures how much we expect y to change if x were to increase by 1.\nFinally, don’t forget ϵi, which is the error. ϵi will measure the distance between each prediction of yi made by the model and the observed value of yi.\nThese predictions will simply be calculated as:\nyi = α + βxi*\n(notice I just removed the ϵi from the end). You can think of the linear predictions as: the mean or ‘expected’ value a new observation yi would take if we only knew xi and also as the ‘line of best fit’."
  },
  {
    "objectID": "posts/2018-01-19-intro-to-glms/index.html#simulating-ideal-data-for-a-general-linear-model",
    "href": "posts/2018-01-19-intro-to-glms/index.html#simulating-ideal-data-for-a-general-linear-model",
    "title": "General Linear Models The Basics",
    "section": "",
    "text": "Now we know the model, we can generate some idealized data. Hopefully this will then give you a feel for how we can fit a model to data. Open up R and we will create these parameters:\nn &lt;- 100\nbeta &lt;- 2.2\nalpha &lt;- 30\nWhere n is the sample size and alpha and beta are as above.\nWe also need some covariate data, we will just generate a sequence of n numbers from 0 to 1:\nx &lt;- seq(0, 1, length.out = n)\nThe model’s expectation is thus this straight line:\ny_true &lt;- beta * x + alpha\nplot(x, y_true)\n\nBecause we made the model up, we can say this is the true underlying relationship. Now we will add error to it and see if we can recover that relationship with a general linear model.\nLet’s generate some error:\nsigma &lt;- 2.4\nset.seed(42)\nerror &lt;- rnorm(n, sd = sigma)\ny_obs &lt;- y_true + error\nplot(x, y_obs)\nlines(x, y_true)\n\nHere sigma is our standard deviation, which measures how much the observations y vary around the true relationship. We then used rnorm to generate n random normal numbers, that we just add to our predicted line y_true to simulate observing this relationship.\nCongratulations, you just created a (modelled) reality a simulated an ecologist going out and measuring that reality.\nNote the set.seed() command. This just ensures the random number generator produces the same set of numbers every time it is run in R and it is good practice to use it (so your code is repeatable). Here is a great explanation of seed setting and why 42 is so popular.\nAlso, check out the errors:\nhist(error)\n\nLooks like a normal distribution hey? That’s because we generated them from a normal distribution. That was a handy trick, because the basic linear model assumes the errors are normally distributed (but not necessarily the raw data).\nAlso note that sigma is constant (e.g. it doesn’t get larger as x gets larger). That is another assumption of basic linear models called ‘homogeneity of variance’."
  },
  {
    "objectID": "posts/2018-01-19-intro-to-glms/index.html#fitting-a-model",
    "href": "posts/2018-01-19-intro-to-glms/index.html#fitting-a-model",
    "title": "General Linear Models The Basics",
    "section": "",
    "text": "To fit a basic linear model in R we can use the lm() function:\nm1 &lt;- lm(y_obs ~ x)\nIt takes a formula argument, which simply says here that y_obs depends on (the tilde ~) x. R will do all the number crunching to estimate the parameters now.\nTo see what it came up with try:\ncoef(m1)\n\n## (Intercept)           x\n##   30.163713    2.028646\nThis command tells us the estimate of the intercept ((Intercept)) and the slope on x under x. Notice they are close to, but not exactly the same as alpha and beta. So the model has done a pretty decent job of recovering our original process. The reason the values are not identical is that we simulated someone going and measuring the real process with error (that was when we added the normal random numbers).\nWe can get slightly more details about the model fit like this:\nsummary(m1)\n\n##\n## Call:\n## lm(formula = y_obs ~ x)\n##\n## Residuals:\n##     Min      1Q  Median      3Q     Max\n## -7.2467 -1.5884  0.1942  1.5665  5.3433\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)  30.1637     0.4985  60.503   &lt;2e-16 ***\n## x             2.0286     0.8613   2.355   0.0205 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 2.511 on 98 degrees of freedom\n## Multiple R-squared:  0.05357,    Adjusted R-squared:  0.04391\n## F-statistic: 5.547 on 1 and 98 DF,  p-value: 0.0205\nI’m not going to go overboard with explaining this output now, but notice a few key things. With the summary, we get standard errors for the parameter estimates (which is a measure of how much they might vary). Also notice the R-squared, which can be handy. Finally, notice that the Residual standard error is close to the value we used for sigma, which is because it is an estimate of sigma from our simulated data.\nYour homework is play around with the model and sampling process. Try change alpha, beta, n and sigma, then refit the model and see what happens."
  },
  {
    "objectID": "posts/2018-01-19-intro-to-glms/index.html#final-few-points",
    "href": "posts/2018-01-19-intro-to-glms/index.html#final-few-points",
    "title": "General Linear Models The Basics",
    "section": "",
    "text": "So did you do the homework? If you did, well done, you just performed a simple power analysis (in the broad sense).\nIn a more formal power analysis (which is what you might have come across previously) could systematically vary n or beta and for 1000 randomised data sets and then calculate the proportion out of 1000 data-sets that your p-value was ‘significant’ (e.g. less than a critical threshold like the ever-popular 0.05). This number tells you how good you are at detecting ‘real’ effects.\nHere’s a great intro to power analysis in the broad sense: Bolker, Ecological Models and Data in R\nOne more point. Remember we said above about some ‘assumptions’. Well we can check those in R quite easily:\nplot(m1, 1)\n\nThis shows a plot of the residuals (A.K.A. errors) versus the predicted values. We are looking for ‘heteroskedasticity’ which is a fancy way of saying the errors aren’t equal across the range of predictions (remember I said sigma is a constant?).\nAnother good plot:\nplot(m1, 2)\n\nHere we are looking for deviations of the points from the line. Points on the line mean the errors are approximately normally distributed, which was a key assumption. Points far from the line could indicate the errors are skewed left or right, too fat in the middle, or too in the middle skinny. More on that issue here"
  },
  {
    "objectID": "posts/2018-01-19-intro-to-glms/index.html#the-end",
    "href": "posts/2018-01-19-intro-to-glms/index.html#the-end",
    "title": "General Linear Models The Basics",
    "section": "",
    "text": "So the basics might belie the true complexity of situations we can address with General Linear Models and their relatives Generalized Linear Models. But, just to get you excited, here are a few things you can do by adding on more terms to the right hand side of the linear equation:\n\nModel multiple, interacting covariates.\nInclude factors as covariates (instead of continuous variables). Got a factor and a continuous variable? Don’t bother with the old-school ANCOVA method, just use a linear model.\nInclude a spline to model non-linear effects (that’s a GAM).\nAccount for hierarchies in your sampling, like transects sampled within sites (that’s a mixed effects model)\nAccount for spatial or temporal dependencies.\nModel varying error variance (e.g. when the variance increases with the mean).\n\nYou can also change the left-hand side, so that it no longer assumes normality (then that’s a Generalized Linear Model). Or even add chains of models together to model pathways of cause and effect (that’s a ‘path analysis’ or ‘structural equation model’)\nIf this taster has left you keen to learn more, then check out any one of the zillion online courses or books on GLMs with R, or if you can get to Brisbane, come to our next course (which as of writing was in Feb 2018, but we do them regularly).\nNow you know the basics, practice, practice, practice and pretty soon you will be running General Linear Models behind your back while you watch your 2 year old, which is what I do for kicks."
  },
  {
    "objectID": "posts/2016-03-28-recreational-fishing-draft/index.html",
    "href": "posts/2016-03-28-recreational-fishing-draft/index.html",
    "title": "Impact of recreational fishing areas",
    "section": "",
    "text": "Will commercial fishing closures enhance recreational fisheries?\nRecently the Queensland Government has declared several net fishing free zones in bays and estuaries of central and North Queensland. The aim of these zones is to enhance recreational fisheries. However, these zones have been contentious as they will cost commercial fishers, with some likely to lose their jobs.\nAlso of concern is the lack of scientific assessment to support the closures. I am preparing a paper that attempts to fill this gap where possible and suggests where further work is needed for a proper assessment. The paper is now submitted, but get in touch if you are interested to know more.\nSee the Tangled net report from ABC news for more info."
  },
  {
    "objectID": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html",
    "href": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html",
    "title": "Points to consider when framing a regional study for the international literature",
    "section": "",
    "text": "At lab meeting today we discussed how to take your regional study and frame it for the global literature. This framing is essential for engaging international audiences and navigating editors and reviewers when publishing in international journals.\nThanks to the Seascapemodels and Conservation Action Team labs at Utas and Griffith Uni for their contributions."
  },
  {
    "objectID": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html#foundational-principles",
    "href": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html#foundational-principles",
    "title": "Points to consider when framing a regional study for the international literature",
    "section": "Foundational Principles",
    "text": "Foundational Principles\n1. Consider your audience\nIf you’re targeting an international journal, your readers will be international scientists. They will be less interested in hearing about the specific organisms in your part of the ocean and more interested in understanding what your results mean for their own research contexts. Frame your findings in terms of broader ecological patterns, processes, or principles that transcend geographic boundaries.\n2. Establish global context through strategic citations\nCite global studies and literature from multiple regions, not just your local area. Global studies are often well-cited because they provide the comparative framework that regional studies need. When citing studies from other regions, be strategic about which ones you include:\n\nChoose contrasting systems: Select studies from regions with different environmental conditions (temperate vs. tropical, shallow vs. deep, pristine vs. degraded) to highlight what makes your system unique or generalizable.\nInclude similar systems for comparison: Find studies from ecologically similar regions to show whether your findings represent broader patterns or regional anomalies.\nReference methodological precedents: Cite studies that used similar approaches in other regions, even if the ecological focus differs, to establish the credibility of your methods.\nDraw from different disciplines: Include studies from related fields (e.g., terrestrial ecology, freshwater systems) when they provide relevant theoretical context or methodological insights.\n\nPosition your region within this broader context by explicitly comparing and contrasting your findings with patterns observed elsewhere. For example: “While coral bleaching in the Caribbean typically occurs at 29°C, our Pacific sites showed bleaching at 31°C, suggesting regional adaptation to higher baseline temperatures.” This demonstrates that you understand the global state of knowledge and can articulate where your work fits."
  },
  {
    "objectID": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html#specific-framing-strategies",
    "href": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html#specific-framing-strategies",
    "title": "Points to consider when framing a regional study for the international literature",
    "section": "Specific Framing Strategies",
    "text": "Specific Framing Strategies\n3. Connect to established theories and frameworks\nRelate your case-study to general ecological theories or established analytical frameworks. For example, your work could be an application of the DPSIR (Drivers-Pressures-State-Impact-Response) framework to your region, or a test of the recruitment limitation hypothesis in a new system. This approach transforms your regional study from a descriptive account into a contribution to broader scientific understanding.\n4. Demonstrate novelty without claiming “firsts”\nState your contributions by explaining why your approach or findings matter, rather than simply claiming precedence. Avoid phrases like “this is the first time.” Instead, draw parallels to successful applications in other fields or regions. For example, rather than “This is the first time we’ve used machine learning for fisheries,” try “Machine learning has proven highly effective for predictions in agriculture and could provide similar benefits for fisheries management.”\n5. Highlight unique perspectives and unexpected findings\nShow how your case-study reveals new patterns or challenges existing assumptions compared to other regional studies. Focus on findings that offer fresh insights into well-studied processes. For example: “While marine heatwaves are known to cause widespread mortality, our study reveals that toxic plankton blooms can flourish during these events, creating compound ecological impacts.”\n6. Present methodological innovations\nFrame your work as developing or refining methodological approaches, with your regional study serving as a proof-of-concept. This strategy is particularly powerful when you’ve adapted techniques from other disciplines or combined existing methods in novel ways. Emphasize how the method could be applied in other regions or systems, making your local study a stepping stone to broader applications."
  },
  {
    "objectID": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html#practical-applications-and-impact",
    "href": "posts/2025-08-21-points-framing-regional-study-international-literature/index.html#practical-applications-and-impact",
    "title": "Points to consider when framing a regional study for the international literature",
    "section": "Practical Applications and Impact",
    "text": "Practical Applications and Impact\n7. Demonstrate real-world applications\nWhen conducting applied research, explicitly show how your science has been used for conservation or management decisions. One excellent example is Rick Hamilton’s turtle tracking paper. While countless studies track turtles, Hamilton got his work published in an international conservation journal by demonstrating how the tracking data directly informed conservation actions. He also capitalized on a review highlighting that most tracking studies fail to influence conservation practice, establishing his work’s unique value.\n8. Connect to global goals and initiatives\nLink your research to internationally recognized objectives and frameworks. This connection is increasingly common and effective. For example: “SDG 14.4 aims to make fisheries sustainable by 2020. Achieving this goal requires better models of climate impacts on fish populations—precisely what our study provides.” Other relevant frameworks include the Convention on Biological Diversity targets, national biodiversity strategies, and organizational goals like those of the Global Mangrove Alliance. These connections help editors and reviewers understand your work’s broader significance."
  },
  {
    "objectID": "posts/2019-08-04-study-or-phd/index.html",
    "href": "posts/2019-08-04-study-or-phd/index.html",
    "title": "Should I do more study or start a PhD?",
    "section": "",
    "text": "Finishing your undergraduate studies is a time to make big decisions about your career. Several people have asked me recently if they should pursue more study in graduate coursework or apply to a PhD program. People also ask if I think they should start a PhD or work go out into the job market.\nI can relate my own experiences making these decisions (and I will in a bit). But these career decisions are very individual, so my experiences may not be relevant to every case. The better advice I can give is to ask yourself these questions:\nWhat, where, how, who, when"
  },
  {
    "objectID": "posts/2019-08-04-study-or-phd/index.html#what-are-my-options",
    "href": "posts/2019-08-04-study-or-phd/index.html#what-are-my-options",
    "title": "Should I do more study or start a PhD?",
    "section": "What are my options?",
    "text": "What are my options?\nIf you haven’t already done so, it’s a good idea to list your career/training options systematically. Once you’ve put down the obvious ones, think a bit more creatively and put some ‘outside the box’ options down too.\nFor instance, you may be considering taking further courses in statistics versus starting a PhD. But maybe a third option is to seek work as technical support on a research team, and learn the technical skills on the job.\nOr maybe the option is between pursuing a PhD locally, or going overseas.\nYou should then check that the options are viable. For instance, contact the graduate office of possible universities and see what qualifications are needed to get into a PhD program. In North America you generally need masters, whereas in Australia you need a qualification equivalent to First Class Honours.\nThere are other logistical things you should consider too, like financial viability of more study for your personally.\nFor me at the end of my Honours degree the options where: apply to an Australian PhD program, apply to an international PhD program, find a job as a government scientist. After some email and research I very quickly figured out an international PhD was very unlikely, because I didn’t have a masters. So I focused my efforts on a local PhD or a job."
  },
  {
    "objectID": "posts/2019-08-04-study-or-phd/index.html#where-do-i-want-to-be-in-3-5-years",
    "href": "posts/2019-08-04-study-or-phd/index.html#where-do-i-want-to-be-in-3-5-years",
    "title": "Should I do more study or start a PhD?",
    "section": "Where do I want to be in 3-5 years?",
    "text": "Where do I want to be in 3-5 years?\nAsk yourself where you want to get to in the mid-term. You could make this in 10 years, but I find most people find it hard to think that far ahead.\nIf this question seems difficult, you can try and ask it in different ways. Like, if I was successful in my career, where would I be and what would I be doing?\nFor me I wanted to be having an impact on society and helping the environment through science (or so I thought, see the next step). So working in government seemed like a pretty good option, but that didn’t rule out a PhD."
  },
  {
    "objectID": "posts/2019-08-04-study-or-phd/index.html#how-will-this-decision-be-good-for-me",
    "href": "posts/2019-08-04-study-or-phd/index.html#how-will-this-decision-be-good-for-me",
    "title": "Should I do more study or start a PhD?",
    "section": "How will this decision be good for me?",
    "text": "How will this decision be good for me?\nNow we’ve got some of the practical things on paper, we need to go a bit deeper into your core values. So pick one of the options under the What question and ask yourself:\nHow will this choice be good for me?\nOnce you’ve found the answer ask the question again, and again, but more specific. We want to get to the core reason you are trying to make this decision. The first few answers are usually quite superficial, so we need to keep asking. As an example, I’ll relate my own answers from some years ago:\nQuestion: How will starting this PhD be good for me?\nAnswer: I’ll get to study amazing topics that have an impact on society in a lab full of young and inspiring people.\nQuestion: How will studying these topics be good for me?\nAnswer: I’ve always wanted to be a marine biologist. Also, I’ll have the independence to pursue my own research interests.\nQuestion: How will pursing my own research interests be good for me?\nAnswer: I enjoy being creative and having the independence to be creative in my own way.\nSo basically, a decision that for me on the face of it was just about doing interesting things and pursuing the career I’d always wanted, turned out to be about the freedom to be creative (among other things of course, I’m sure your goals will be more complex than this).\nYou might like to repeat the above questions for the other options you listed under ‘What’."
  },
  {
    "objectID": "posts/2019-08-04-study-or-phd/index.html#who-has-influenced-this-decision",
    "href": "posts/2019-08-04-study-or-phd/index.html#who-has-influenced-this-decision",
    "title": "Should I do more study or start a PhD?",
    "section": "Who has influenced this decision?",
    "text": "Who has influenced this decision?\nNow you have a better idea of what you want to achieve and why, think about the people and experiences that have influenced these values.\nNow I’m mid-career, its surprising how many people I meet who are dissatisfied in their career as a scientist, accountant, lawyer, or whatever career. They are in the career because at an early stage, someone advised them to follow that career (like their parents or an important early mentor), and they invested so much in it that they never changed.\nSo think about who is influencing you.\nIt might also be experiences that influence you. For me, seeing the environment change (for the worse) was a big influence, but so too were a lot of people and mentors.\nWhat you are trying to do is find your own goals and not follow a career for other people’s goals.\nAnother important part of the ‘who’ question is to seek advise and multiple different perspectives on your decision. So if you can talk to people from a range of career stages and ask them about their career.\nTalk to current PhD students, to get a better idea of the joys and challenges of doing a PhD (it’s not easy, you need to be motivated!). Talk to successful career scientists. Talk to people who have longer careers, in any career, and find out how they made their career choices. You want to get lots of different perspectives on this decision.\nIf you are happy you truly have your own goals then proceed below. If not, then perhaps you want to revisit the what, where, how steps above."
  },
  {
    "objectID": "posts/2019-08-04-study-or-phd/index.html#when-do-i-need-to-make-these-decisions",
    "href": "posts/2019-08-04-study-or-phd/index.html#when-do-i-need-to-make-these-decisions",
    "title": "Should I do more study or start a PhD?",
    "section": "When do I need to make these decisions?",
    "text": "When do I need to make these decisions?\nThe what, where how, who may take you a few hours, or it may take months. For me it took months, including applying to numerous PhD programs locally and overseas. I applied for jobs too and even got offered one in a government research organisation. In the end however, I realised I wanted creative freedom, so I took a PhD scholarship offer.\nOnce you feel like you have a grip on your values, then its time to figure out how to get there. So look at your 3-5 year goal. Now work backwards from that to now, to figure out what steps you need to take to get you there. Maybe you need to do more study, so you can get into a PhD program, or maybe you don’t.\nFor me it was pretty simple:\n5 year goal was to have a research job (government or uni) where I could have an impact on environmental issues, but also be senior enough that I’d have the freedom to pursue my own ideas. So to get that job I needed a PhD. I had the qualifications already to apply for a PhD scholarship in Australia, and options for PhD scholarships in my specific field, so that was the decision I ended up making.\nA note of thanks to Mark Douglas and Ethos Australia for inspiring this post at recent workshop I took on research leadership."
  },
  {
    "objectID": "posts/2018-07-05-survey-your-audience/index.html",
    "href": "posts/2018-07-05-survey-your-audience/index.html",
    "title": "Survey your audience and visualise the results with R and Google forms",
    "section": "",
    "text": "I wanted to make my presentation on dataviz at the UQ School of Bioinformatics more interactive.\nA quiz is a good way to engage your audience. Given I was giving a talk about R datavisuals I thought it would be fun to visualise the quiz results using R live with the audience. To top it off, we posted the results to Twitter.\nThis blog describes is how I did that.\n{% include r-courses-ad.html %}\nYou could also use this system to survey our audience and share the results live. Just prepare you R code and set it to run at a certain time during your talk with a task scheduling algorithm.\n\n\nI used Google Forms to do my quiz. You can take it here. I posed a few questions that challenged the audience to think about the best way to visualise data.\nIt is pretty easy to set up a survey if you have a gmail account. A few tips:\n\nYou can add images, which is great posing questions about results.\nI used the ‘short answer’ input for numeric answers. If you click the validation tab at the bottom of each ‘short answer’ question you can require users enter certain types of numbers (e.g. within a range).\nThink carefully about limiting required inputs if you want to avoid bugs that might arise from unexpected answers.\nThere is a green button at the top of the form that let’s you link it to a google sheet. Do this.\nYou can make the sheet public, so other people can use it, but changing the sharing settings.\n\n\n\n\nI used the googlesheets package to read my survey answers from the (public) spreadsheet. You will need to authenticate yourself first:\nlibrary(googlesheets)\ngs_ls()\nThis will prompt you to login to your google account and authenticate an app that allows the connection to happen.\nNow we can load our data:\nsheet_url &lt;- \"https://docs.google.com/spreadsheets/d/10i3v3NIVpgmURyLVzsiadPAMGeqa7dLFcDb9sqFe8KA/edit#gid=1513779153\"\ndataviz &lt;- gs_url(sheet_url) #creates connection\nIf you want to keep your sheet private you can use gs_ls() to list all your sheets, and then pick a name to read it in. e.g. like this:\ndataviz &lt;- gs_title(\"Dataviz quiz 2018 v2 (Responses)\")\ndat &lt;- gs_read(dataviz)\n\n\n\nThe file dat we just read in is a dataframe like object (actually a tibble) where each column is a question and each row is a response. The first column is a time stamp.\nAll other columns are titled with your questions.\nIt will make life easier if we rename the columns to shorter (but still descriptive) names.\nnewnames &lt;- c(\"timestamp\", \"shopping\",\n              \"bar_percent\",\n              \"pie_percent\",\n              \"room\",\n              \"cb_age\")\nnames(dat) &lt;- newnames\nNow let’s create some dataviz\nlibrary(ggplot2)\ndatplot &lt;- na.omit(dat)\nggplot(datplot, aes(x = room, y = cb_age)) +\n  geom_boxplot() +\n  xlab(\"Position in room\") +\n  ylab(\"Guess at CB's age\") +\n  ylim(0, 75) +\n  theme_bw()\n\nA boxplot of the audience’s guesses at my age by their position in the room. I limited the y-axis because there were some outrageously large numbers!\n\n\n\nWe could show the audience the results on our screen. But why not let Twitter know too!\nFor this, I used the rtweet package. rtweet is pretty simple to use once you’ve set up an app on Twitter’s API and authorised R to access it. So get rtweet then look at the vignette vignette(\"auth\"). Follow the instructions to the letter and you shouldn’t have any problems.\nOnce authorisation is done, its a simple matter to save our plot as a png to use in a tweet:\nmyplot &lt;- ggplot(datplot, aes(x = room, y = cb_age)) +\n  geom_boxplot()\nggsave(filename = \"myplot.png\", myplot)\nNow just write your tweet and send it off to twitter.\nlibrary(rtweet)\nnewstatus = \"Chris age as surveyed at  #UQwinterSchool\n@DoktrNick @UQwinterSchool\"\n\npost_tweet(status = newstatus,\n           media = \"myplot.png\")\n\n\n\nSo I tried this as a way of doing a live R tutorial. Next step would be to try and integrate it into a talk without showing the R coding. For that you would either need to get a friend to run the code or use a scheduler (like the taskscheduleR R package).\nBe careful though! You never know what answers people may give if allowed. So design you code to be robust to strange answers (like that I am 100 years old)."
  },
  {
    "objectID": "posts/2018-07-05-survey-your-audience/index.html#setting-up-the-survey",
    "href": "posts/2018-07-05-survey-your-audience/index.html#setting-up-the-survey",
    "title": "Survey your audience and visualise the results with R and Google forms",
    "section": "",
    "text": "I used Google Forms to do my quiz. You can take it here. I posed a few questions that challenged the audience to think about the best way to visualise data.\nIt is pretty easy to set up a survey if you have a gmail account. A few tips:\n\nYou can add images, which is great posing questions about results.\nI used the ‘short answer’ input for numeric answers. If you click the validation tab at the bottom of each ‘short answer’ question you can require users enter certain types of numbers (e.g. within a range).\nThink carefully about limiting required inputs if you want to avoid bugs that might arise from unexpected answers.\nThere is a green button at the top of the form that let’s you link it to a google sheet. Do this.\nYou can make the sheet public, so other people can use it, but changing the sharing settings."
  },
  {
    "objectID": "posts/2018-07-05-survey-your-audience/index.html#connecting-to-your-survey-answers-in-r",
    "href": "posts/2018-07-05-survey-your-audience/index.html#connecting-to-your-survey-answers-in-r",
    "title": "Survey your audience and visualise the results with R and Google forms",
    "section": "",
    "text": "I used the googlesheets package to read my survey answers from the (public) spreadsheet. You will need to authenticate yourself first:\nlibrary(googlesheets)\ngs_ls()\nThis will prompt you to login to your google account and authenticate an app that allows the connection to happen.\nNow we can load our data:\nsheet_url &lt;- \"https://docs.google.com/spreadsheets/d/10i3v3NIVpgmURyLVzsiadPAMGeqa7dLFcDb9sqFe8KA/edit#gid=1513779153\"\ndataviz &lt;- gs_url(sheet_url) #creates connection\nIf you want to keep your sheet private you can use gs_ls() to list all your sheets, and then pick a name to read it in. e.g. like this:\ndataviz &lt;- gs_title(\"Dataviz quiz 2018 v2 (Responses)\")\ndat &lt;- gs_read(dataviz)"
  },
  {
    "objectID": "posts/2018-07-05-survey-your-audience/index.html#analysing-your-data",
    "href": "posts/2018-07-05-survey-your-audience/index.html#analysing-your-data",
    "title": "Survey your audience and visualise the results with R and Google forms",
    "section": "",
    "text": "The file dat we just read in is a dataframe like object (actually a tibble) where each column is a question and each row is a response. The first column is a time stamp.\nAll other columns are titled with your questions.\nIt will make life easier if we rename the columns to shorter (but still descriptive) names.\nnewnames &lt;- c(\"timestamp\", \"shopping\",\n              \"bar_percent\",\n              \"pie_percent\",\n              \"room\",\n              \"cb_age\")\nnames(dat) &lt;- newnames\nNow let’s create some dataviz\nlibrary(ggplot2)\ndatplot &lt;- na.omit(dat)\nggplot(datplot, aes(x = room, y = cb_age)) +\n  geom_boxplot() +\n  xlab(\"Position in room\") +\n  ylab(\"Guess at CB's age\") +\n  ylim(0, 75) +\n  theme_bw()\n\nA boxplot of the audience’s guesses at my age by their position in the room. I limited the y-axis because there were some outrageously large numbers!"
  },
  {
    "objectID": "posts/2018-07-05-survey-your-audience/index.html#share-the-results",
    "href": "posts/2018-07-05-survey-your-audience/index.html#share-the-results",
    "title": "Survey your audience and visualise the results with R and Google forms",
    "section": "",
    "text": "We could show the audience the results on our screen. But why not let Twitter know too!\nFor this, I used the rtweet package. rtweet is pretty simple to use once you’ve set up an app on Twitter’s API and authorised R to access it. So get rtweet then look at the vignette vignette(\"auth\"). Follow the instructions to the letter and you shouldn’t have any problems.\nOnce authorisation is done, its a simple matter to save our plot as a png to use in a tweet:\nmyplot &lt;- ggplot(datplot, aes(x = room, y = cb_age)) +\n  geom_boxplot()\nggsave(filename = \"myplot.png\", myplot)\nNow just write your tweet and send it off to twitter.\nlibrary(rtweet)\nnewstatus = \"Chris age as surveyed at  #UQwinterSchool\n@DoktrNick @UQwinterSchool\"\n\npost_tweet(status = newstatus,\n           media = \"myplot.png\")"
  },
  {
    "objectID": "posts/2018-07-05-survey-your-audience/index.html#next-steps",
    "href": "posts/2018-07-05-survey-your-audience/index.html#next-steps",
    "title": "Survey your audience and visualise the results with R and Google forms",
    "section": "",
    "text": "So I tried this as a way of doing a live R tutorial. Next step would be to try and integrate it into a talk without showing the R coding. For that you would either need to get a friend to run the code or use a scheduler (like the taskscheduleR R package).\nBe careful though! You never know what answers people may give if allowed. So design you code to be robust to strange answers (like that I am 100 years old)."
  },
  {
    "objectID": "posts/2017-05-14-timeseries-uncertainty-gif/index.html",
    "href": "posts/2017-05-14-timeseries-uncertainty-gif/index.html",
    "title": "Visualising uncertainty in time-series using animations",
    "section": "",
    "text": "I was reading an example of visualising election outcome uncertainty using a wobbly needle and wondered, could animating uncertainty help communicate for the kind of work I do on population modelling?\n\nOne challenge when plotting uncertainty intervals for population trends is that confidence intervals don’t capture the wandering nature of population processes. The figure here shows the median of a poisson random walk, with 90% quantiles.\nIn fact, each trend line will wander around, like the next image. So, the median and intervals give a false impression that an individual trend will experience relative stability.\n\n \n\nA graph of the median and quantiles doesn’t represent the uncertainty in any single trend line very well.\n\n\nI decided to try and animate the uncertainty, using R.\nThe first step was to try and install the R package animate. Unfortunately I was in a world of Unix coding pain trying to get animation to work. In particular to get R to talk to the ImageMagick program. After cutting and pasting numerous commands from StackOverflow to my terminal I gave up. It seems I need to update my OS before I can get any further (from Mavericks).\nIn the end I found I could use ImageMagick directly from the command line. So now what we need is a folder full of .png files that we wish to turn into the animation. That Unix code to do that is at the end of this blog.\nFirst we should set up a random walk process. I decided to go for a Poisson random walk, because it will have integer numbers, like a real population. I wrote a little function to achieve this:\nwalkmod &lt;- function(t, yinit, thetasd, rho){\n  y &lt;- numeric(t)\n  y[1] &lt;- yinit\n  theta &lt;- rnorm(t, sd = thetasd)\n  for(i in 2:t){\n    lambda &lt;- (y[i-1]*rho + yinit*(1-rho))* exp(theta[i])\n    y[i] &lt;- rpois(1, lambda)\n  }\n  return(y)\n}\nThe core equation is in the for loop. It just says the mean abundance at a given time is dependent on abundance at the last time-step with correlation rho plus some extra variation theta. We then sample the observed abundance from a poisson distribution.\nNow let’s use our function. We will load in the purrr package to help plotting replicate runs of the random walk and the leaflet package because it has some nice functions for making colour scales.\nlibrary(purrr)\nlibrary(leaflet)\nt &lt;- 50\nyinit &lt;- 10\nthetasd &lt;- 0.01\nrho &lt;- 0.9\nn &lt;- 30\ny &lt;- matrix(NA, nrow = t, ncol = n)\ntmat &lt;- matrix(rep(1:t), nrow = t, ncol = n)\nfor (i in 1:n){\n    set.seed(i)\n    y[,i] &lt;- walkmod(t, yinit, thetasd, rho)\n}\nymax &lt;- max(y)\nmedy &lt;- apply(y, 1 , median)\nWe just looped over the walkmod function n times so we could generate n random walks.\nNow let’s build a colour scale. I would like to colour the lines by the abundance quantile they end on. So we apply colorQuantile to all abundances in the last time-step to get a red to blue (RdBu) colour palette for the lines:\nquants &lt;- c(0, 0.25, 0.5, 0.75, 1)\nyquant &lt;- quantile(y[t,], probs = quants)\nmycols &lt;- colorQuantile(\"RdBu\", domain = y[t,], probs = quants)\nThe trick to making a .gif animation is just to make a bunch of figures and save them as .png files in the order you want them to animate (or if you can get animate package to work within R you can export the .gif directly without having to save the .pngs).\nSo we just loop over walkmod plotting our lines each time. Note I use walk from the purrr package to redraw the background lines in grey each time.\nthispath &lt;- \"/insert/your/path/here/\"\nfor (i in 1:n){\n    fname &lt;- paste(thispath, i, \".png\", sep = \"\")\n    png(filename =fname, width = 480, height = 480)\n    plot(0, 0, xlim = c(0, t), ylim = c(0, ymax),\n         bty = 'n', type = \"n\", las = 1,\n         ylab = \"Abundance\", xlab = \"Time\")\n    lines(1:t, medy, col = \"grey90\", lwd = 2)\n    walk(1:i, ~lines(tmat[,.x], y[,.x], lwd = 0.7, col = grey(0.5, 0.2)))\n  lines(1:t, y[,i], lwd = 2, col = mycols(y[t, i]))\n    legend(\"topleft\", legend = quants,\n        lwd = 2, col = mycols(yquant),\n    title = \"Quantile t=100\", cex = 0.8)\n  dev.off()\n}\nIf you want to check this from R, without saving the files, then turn of the png() function that saves the files and insert Sys.sleep(0.1) at the end of the loop, so it will print each figure in turn but pause for 1/10 th of second each time.\nWe have one final figure to plot, that is the median line. We can do this outside the for loop:\nfname &lt;- paste(thispath, i+1, \".png\",  sep = \"\")\npng(filename =fname, width = 480, height = 480)\nplot(0, 0, xlim = c(0, t), ylim = c(0, ymax),\n     bty = 'n', type = \"n\", las = 1,\n     ylab = \"Abundance\", xlab = \"Time\", main = \"Median\")\nwalk(1:i, ~lines(tmat[,.x], y[,.x], lwd = 0.5, col = grey(0.5, 0.2)))\nlines(1:t, medy, lwd = 3, col = \"purple\")\ndev.off()\nFinally, here is the unix code to run imageMagick from your terminal to turn your saved .png files into a gif.\ncd \"your/path/here\"\nmagick *.png images.gif\nconvert images.gif -set delay 30 \\( +clone -set delay 100 \\) +swap +delete images_pause.gif\nThe first line creates the gif and the second line adds a pause of 30 1/100ths of a second after each image and a pause of 1 second at the end of the loop."
  },
  {
    "objectID": "posts/2017-05-14-timeseries-uncertainty-gif/index.html#visualising-uncertainty-in-time-series-using-animations",
    "href": "posts/2017-05-14-timeseries-uncertainty-gif/index.html#visualising-uncertainty-in-time-series-using-animations",
    "title": "Visualising uncertainty in time-series using animations",
    "section": "",
    "text": "I was reading an example of visualising election outcome uncertainty using a wobbly needle and wondered, could animating uncertainty help communicate for the kind of work I do on population modelling?\n\nOne challenge when plotting uncertainty intervals for population trends is that confidence intervals don’t capture the wandering nature of population processes. The figure here shows the median of a poisson random walk, with 90% quantiles.\nIn fact, each trend line will wander around, like the next image. So, the median and intervals give a false impression that an individual trend will experience relative stability.\n\n \n\nA graph of the median and quantiles doesn’t represent the uncertainty in any single trend line very well.\n\n\nI decided to try and animate the uncertainty, using R.\nThe first step was to try and install the R package animate. Unfortunately I was in a world of Unix coding pain trying to get animation to work. In particular to get R to talk to the ImageMagick program. After cutting and pasting numerous commands from StackOverflow to my terminal I gave up. It seems I need to update my OS before I can get any further (from Mavericks).\nIn the end I found I could use ImageMagick directly from the command line. So now what we need is a folder full of .png files that we wish to turn into the animation. That Unix code to do that is at the end of this blog.\nFirst we should set up a random walk process. I decided to go for a Poisson random walk, because it will have integer numbers, like a real population. I wrote a little function to achieve this:\nwalkmod &lt;- function(t, yinit, thetasd, rho){\n  y &lt;- numeric(t)\n  y[1] &lt;- yinit\n  theta &lt;- rnorm(t, sd = thetasd)\n  for(i in 2:t){\n    lambda &lt;- (y[i-1]*rho + yinit*(1-rho))* exp(theta[i])\n    y[i] &lt;- rpois(1, lambda)\n  }\n  return(y)\n}\nThe core equation is in the for loop. It just says the mean abundance at a given time is dependent on abundance at the last time-step with correlation rho plus some extra variation theta. We then sample the observed abundance from a poisson distribution.\nNow let’s use our function. We will load in the purrr package to help plotting replicate runs of the random walk and the leaflet package because it has some nice functions for making colour scales.\nlibrary(purrr)\nlibrary(leaflet)\nt &lt;- 50\nyinit &lt;- 10\nthetasd &lt;- 0.01\nrho &lt;- 0.9\nn &lt;- 30\ny &lt;- matrix(NA, nrow = t, ncol = n)\ntmat &lt;- matrix(rep(1:t), nrow = t, ncol = n)\nfor (i in 1:n){\n    set.seed(i)\n    y[,i] &lt;- walkmod(t, yinit, thetasd, rho)\n}\nymax &lt;- max(y)\nmedy &lt;- apply(y, 1 , median)\nWe just looped over the walkmod function n times so we could generate n random walks.\nNow let’s build a colour scale. I would like to colour the lines by the abundance quantile they end on. So we apply colorQuantile to all abundances in the last time-step to get a red to blue (RdBu) colour palette for the lines:\nquants &lt;- c(0, 0.25, 0.5, 0.75, 1)\nyquant &lt;- quantile(y[t,], probs = quants)\nmycols &lt;- colorQuantile(\"RdBu\", domain = y[t,], probs = quants)\nThe trick to making a .gif animation is just to make a bunch of figures and save them as .png files in the order you want them to animate (or if you can get animate package to work within R you can export the .gif directly without having to save the .pngs).\nSo we just loop over walkmod plotting our lines each time. Note I use walk from the purrr package to redraw the background lines in grey each time.\nthispath &lt;- \"/insert/your/path/here/\"\nfor (i in 1:n){\n    fname &lt;- paste(thispath, i, \".png\", sep = \"\")\n    png(filename =fname, width = 480, height = 480)\n    plot(0, 0, xlim = c(0, t), ylim = c(0, ymax),\n         bty = 'n', type = \"n\", las = 1,\n         ylab = \"Abundance\", xlab = \"Time\")\n    lines(1:t, medy, col = \"grey90\", lwd = 2)\n    walk(1:i, ~lines(tmat[,.x], y[,.x], lwd = 0.7, col = grey(0.5, 0.2)))\n  lines(1:t, y[,i], lwd = 2, col = mycols(y[t, i]))\n    legend(\"topleft\", legend = quants,\n        lwd = 2, col = mycols(yquant),\n    title = \"Quantile t=100\", cex = 0.8)\n  dev.off()\n}\nIf you want to check this from R, without saving the files, then turn of the png() function that saves the files and insert Sys.sleep(0.1) at the end of the loop, so it will print each figure in turn but pause for 1/10 th of second each time.\nWe have one final figure to plot, that is the median line. We can do this outside the for loop:\nfname &lt;- paste(thispath, i+1, \".png\",  sep = \"\")\npng(filename =fname, width = 480, height = 480)\nplot(0, 0, xlim = c(0, t), ylim = c(0, ymax),\n     bty = 'n', type = \"n\", las = 1,\n     ylab = \"Abundance\", xlab = \"Time\", main = \"Median\")\nwalk(1:i, ~lines(tmat[,.x], y[,.x], lwd = 0.5, col = grey(0.5, 0.2)))\nlines(1:t, medy, lwd = 3, col = \"purple\")\ndev.off()\nFinally, here is the unix code to run imageMagick from your terminal to turn your saved .png files into a gif.\ncd \"your/path/here\"\nmagick *.png images.gif\nconvert images.gif -set delay 30 \\( +clone -set delay 100 \\) +swap +delete images_pause.gif\nThe first line creates the gif and the second line adds a pause of 30 1/100ths of a second after each image and a pause of 1 second at the end of the loop."
  },
  {
    "objectID": "posts/2020-09-29-R-course-for-data-wrangling/index.html",
    "href": "posts/2020-09-29-R-course-for-data-wrangling/index.html",
    "title": "New course on preparation and graphing of biological data in R",
    "section": "",
    "text": "We’re running a short online introduction to data preparation and graphing. The course will cover the basics of pre-processing data, including common data wrangling routines. We will use real world data from the Reef Life Survey to demonstrate these tools and calculate some common summary statistics.\nREGISTER HERE\nThe course is suitable for people who have used R before, but who want to learn more about how to use R to make data pre-processing and graphing more efficient, repeatable and error free.\nThe course is taught by Dr Chris Brown and Dr Christina Buelow. Chris is Senior Lecturer at Griffith University and has a decade of experience in teaching research students and professionals how to use R for data analysis and graphs. He has taught hundreds of students across both in person and online courses.\nChristina is a quantitative ecologist at Griffith University and is experienced in using R for data cleaning, GIS, statistical modelling and running applications on the cloud.\nThe first course is 22nd October from 9-12 pm AEST.\nCost is $75.00 general/$50.00 student (GST inclusive).\nYou will learn the basics of how to organize projects in R, how to use popular data wrangling tools like dplyr and tidyr to correct errors in data and join different dataframes. You will also learn about how to calculate basic statistical summaries and how the ggplot package can be used to make publication quality plots efficiently.\nThis is a course for students and professionals with some R experience. At a minimum attendees should have R and R studio installed on their computers and know their way around R’s console (e.g. how to install packages, read in data and make simple plots). Attendees should have installed R and the necessary packages before the course.\nThe course is based on Chris and Christina’s online course notes that are available free to download. Join us online if you want to get the benefits of person to person tuition.\nThis event will be held on Microsoft Teams, an online platform that allows video livestream and interactive chat functions/screen sharing.\nA link to Microsoft Teams event and the course notes will be emailed to participants prior to the course date.\nAny questions, please contact: conservationhackers@gmail.com"
  },
  {
    "objectID": "posts/2025-10-22-environmental-impacts-LLMs/index.html",
    "href": "posts/2025-10-22-environmental-impacts-LLMs/index.html",
    "title": "Environmental impacts of large language models",
    "section": "",
    "text": "Just want to quickly highlight a few investigative blogs on the environmental impacts of LLMs.\nThere are two main issues: water use (for cooling servers) and carbon emissions generated by creating the hardware, training the models and then using the models to get responses (‘inference’).\nThis blog does a deep dive on the water issues. The key finding is that the water issues seem to be overstated (he says ‘fake’ but I don’t agree with that word choice, water issues are real, just overstated).\nThen academic Hannah Ritchies and author of ‘Sustainability by Numbers’ page sums up the carbon cost of chatGPT. She draws heavily on Andy Masley detailed work.\nIn particular, blog author (Andy Masley) argues that the public has limited attention span for environmental issues and AI isn’t the biggest priority. He sums it up nicely:\n“AI water use is mostly fake, personal prompt costs are mostly fake, but AI as a whole is going to put a huge strain on our electrical grid going forward that’s going to matter a lot for the green energy transition, local electricity prices, and air pollution”\nBy fake, he means your personal usage is trivially small. He highlights golf courses and streaming videos as much larger uses of water and emissions respectively.\nWhat also matters is the counterfactual - ie what would you be doing if you weren’t using chatGPT? If you would be online shopping, using a microwave or streaming videos instead, then your personal emissions are probably slightly lower if you switch your time to chatGPT.\nIf you are concerned about the impacts of your AI use you can easily do a personal offset by commiting to some simple behaviour changes. e.g. turn you computer off at night and turn it off at the power plug."
  },
  {
    "objectID": "posts/2025-10-22-environmental-impacts-LLMs/index.html#what-about-coders",
    "href": "posts/2025-10-22-environmental-impacts-LLMs/index.html#what-about-coders",
    "title": "Environmental impacts of large language models",
    "section": "What about coders?",
    "text": "What about coders?\nOne unaddressed question in these posts is the personal impacts of genAI power users (like me). By that I mean people using it for coding. The blogs above deal with people who are doing the occaisonal short chatGPT prompt. Coders might be leaning on AI hundreds of times a day for prompts that range from short to very long (and more energy intensive).\nThat’s something I’d like to do the numbers for later on."
  },
  {
    "objectID": "posts/2021-08-01-PhD-projects-available/index.html",
    "href": "posts/2021-08-01-PhD-projects-available/index.html",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "",
    "text": "Current until March 2023\nOur lab uses quantitative tools, including statistics, AI and numerical models, to help conservation managers address a range of issue facing marine ecosystems. Below are two current projects where we are recruiting students.\nWe excel in training students to be outstanding science communicators, who’s work has real-world impact. Chris’ supervision style thus emphasizes quantitative training and networking with a diverse range of scientists and stakeholders. For instance, our students frequently partner with conservation groups such that their research is directly addressing a pressing environmental management issue."
  },
  {
    "objectID": "posts/2021-08-01-PhD-projects-available/index.html#conservation-and-restoration-of-coastal-habitats",
    "href": "posts/2021-08-01-PhD-projects-available/index.html#conservation-and-restoration-of-coastal-habitats",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Conservation and restoration of coastal habitats",
    "text": "Conservation and restoration of coastal habitats\nWork with the team on Global Wetlands Project to help solve pressing challenges for conservation of coastal wetland habitats. PhD and Honours projects available to work on predicting the impact of climate change on coastal restoration and ecosystem services. Email Chris (chris.brown@griffith.edu.au) for more information. PhD and honours projects available."
  },
  {
    "objectID": "posts/2021-08-01-PhD-projects-available/index.html#predicting-unprecedented-ecological-futures",
    "href": "posts/2021-08-01-PhD-projects-available/index.html#predicting-unprecedented-ecological-futures",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Predicting unprecedented ecological futures",
    "text": "Predicting unprecedented ecological futures\nWe are entering an era of science where ecosystem change is uncharted by historical observations. Observations are the foundation of predictive science, so how can we predict a future that is outside the envelope of the environments?\nWork with our team to address the question of how we predict ecological futures, to inform management for marine and coastal ecosystems. Humanities footprint on the Earth’s ecosystem has never been greater. The oceans are increasingly being industrialized, and climate change means the footprint of human economic activity now touches even the most remote ecosystems.\nCoastal ecosystems exemplify the challenges we face in managing our growing impacts. Coasts are the thin line on which much of the human population and industrial activity rests. This human activity places great pressures on coastal ecosystems, and in many places we are losing species and the ecological functions that support human livelihoods, like fisheries.\nPredicting the future for coastal ecosystems is critical for creating a sustainable relationship between humanity and nature. Ecological predictions allow us to identify options for better managing our relationship with ecosystems."
  },
  {
    "objectID": "posts/2021-08-01-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "href": "posts/2021-08-01-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Criteria for potential PhD applicants",
    "text": "Criteria for potential PhD applicants\nPlease consider the below criteria before contacting us. We will not be considering students for applications that do not meet these criteria, because they will not be competitive for funded positions. Further details on Griffith University’s application process can be found here.\nOur primary research methods are quantitative tools. Potential applicants should have either experience in quantiative methods like R, or a very strong desire to learn more quantitative skills. Chris does not supervise projects that are primarily field or lab based, except as cosupervisor with other group leaders.\nOur students come with training in a diverse range of fields, including field ecology, maths and IT. Our lab group is outstanding at supporting quantitative training and current PhD students have ample opportunities to learn quantitative skills by working with their peers, postdoctoral researchers, Chris and through specialized training courses.\n\nAustralian students\nAustralian citizens and permanent residents should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters by research) in a relevant field.\n\n\nInternational students\nDue to international travel restrictions international student scholarships are currently extremely limited. To be competitive for a scholarship international students should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters with a 40 credit point research thesis) in a relevant field. They must be lead author on at least one publication in an internationally recognized peer-reviewed journal. To be considered, journals should be internationally recognized, for instance they should have an Impact Factor rating from ISI or be listed by Scimago.\nPreference will be given to international students who are already located in Australia."
  },
  {
    "objectID": "posts/2021-08-01-multiple-stressors-ecosystems/index.html",
    "href": "posts/2021-08-01-multiple-stressors-ecosystems/index.html",
    "title": "Predicting interactive effects of multiple stressors on marine ecosystems",
    "section": "",
    "text": "We are working towards models to predict the impacts of multiple stressors on coastal habitats, like seagrass and mangroves, funded by an Australian Research Council Discovery Project.\nThe project team is lead by Dr Chris Brown and includes Professor’s Rod Connolly (Griffith) and Isabelle Cote (Simon Fraser University), postdoc Dr Mischa Turschwell, and PhD students Olivia King and Laura Griffiths.\nKey results to date are:\n\nAn up-to-date global scale compilation of seagrass trends. We show that almost 20% of monitored seagrass has been lost globally, but that there is considerable regional variation in rates of loss. In forthcoming work we have generated a globally comprehensive map of risk of seagrass loss, based on cumulative impacts from multiple stressors.\n\n\n\nA review looking at how cumulative impacts to seagrass ecosystems are managed across 20 global case-studies. We found that in most places policy and management actions were mismatched to the impacts seagrass faced. Notable were management gaps in addressing extreme climate events, which are key global drivers of seagrass loss.\n\n\n\nA global scale analysis of drivers of mangrove loss, showing variable effects of cumulative impacts to mangroves across nations, but also that rates of loss are lower in protected areas.\nA review of the interactive effects of pesticides and low light on aquatic primary producers. PhD student Olivia King led this review of the physiology of these stressors, which she drew on to hypothesize how these stressors will interact. She is now testing those hypotheses in a series of lab experiments."
  },
  {
    "objectID": "posts/2023-04-07-using-chatgpt-to-learn-R/index.html",
    "href": "posts/2023-04-07-using-chatgpt-to-learn-R/index.html",
    "title": "Tips for using chatGPT to learn R",
    "section": "",
    "text": "ChatGPT can help you learn R code. Here are some tips my team and I have worked out for ways to use the model to help with learning R.\nThere is also an askgpt package now that is meant to integrate ChatGPT directly into RStudio. However, I’ve been unable to get this package to work on my laptop.\n\nAsk GPT to explain R code\nIf you can find code on the web, from collaborators or a supervisor you don’t understand, then trying asking GPT to explain it to you. Here’s an example:\nPrompt: explain this R code: m1 &lt;- gam(y ~ s(year, by = sites), data = my_data, family = “poisson”)\nGPT: The code m1 &lt;- gam(y ~ s(year, by = sites), data = my_data) specifies a generalized additive model (GAM) in R.\nHere’s what each part of the code means:\nm1 &lt;- : assigns the output of the model to the object named m1. gam() : specifies that we want to fit a generalized additive model. y ~ s(year, by = sites) : specifies the model formula, where y is the response variable and year is the predictor variable of interest. The function s() specifies that year is a smooth function, which allows for a flexible fit to the data. The by = sites argument specifies that we want to fit separate smooth functions for each sites level, which is a way to model site-specific effects.*\nOverall, this code fits a GAM to the data in my_data, with year as a smooth predictor variable and site-specific effects modeled with by = sites.\nIt gets everything right.\nThis tool is helping me keep meetings with my research students at a high level. We talk about what are appropriate methods and statistical designs, and don’t need to spend as much time discussing how to code those.\n\n\nAsk GPT advice on appropriate statistical models\nYou use GPT to get started on a new problem. Say you had count data but weren’t sure how to analyse it or even the name of the correct method. You could ask something this:\nPrompt: How would I do a regression model with count data in R\nGPT: There are several ways to model count data in R, but one popular method is to use Poisson regression or Negative Binomial regression. Here’s a step-by-step guide on how to fit a Poisson regression model in R:…\nAnd GPT continues to explain how to write a poisson GLM in R (one appropriate way to do regression with count data).\nYou can often improve GPT’s answers by breaking your questions into smaller chunks. So first ask it for advice on regression of count data. Then ask for an example in R.\n\n\nGenerate simulated datasets\nTo gain a more general understanding of a method it might help you to work with some simulated (made-up data). GPT can help you write code to do this as well. Following on from the example above I asked it to generate simulated data.\nPrompt: How would I simulate some data to test the poisson glm?\nGPT: To simulate count data for testing a Poisson GLM, you can use the rpois() function in R, which generates random numbers from a Poisson distribution with a given mean. Here is an example of how to simulate count data with two predictor variables:\nset.seed(123) # for reproducibility\n\nn &lt;- 100 # number of observations\nx1 &lt;- rnorm(n) # predictor variable 1\nx2 &lt;- rnorm(n) # predictor variable 2\nlambda &lt;- exp(0.5 + 0.8*x1 - 0.2*x2) # log-linear model with intercept 0.5 and coefficients 0.8 and -0.2\n\ny &lt;- rpois(n, lambda) \n… And then GPT continues to explain what the above code does.\n\n\nCreate customised examples\nIt helps to learn R and statistics if the examples you are learning from are for a familar context. This one one reason why there are so many R books and tutorials that overlap in the methods they explain, just in different contexts, like “R for Ecologists”, “R for Medical Research”, ’R for Microbiology” etc…\nYou can get GPT to write you customised tutorial. That way you get the explanation in a context you know. It can be easier to learn a tangible application in a context you know, before you try to learn how to use a method more generally.\nFollowing the poisson regression example above, we could ask GPT to modify its explanation for a specific context:\nPrompt: Rewrite the R code for simulating data using an example of what predicts the number of easter eggs the easter bunny brings\nGPT: Sure, here’s an example of how to simulate count data based on a linear model that predicts the number of Easter eggs brought by the Easter bunny:…\nThen GPT continued with basically the same code as above, where it gave x1 and x2 as the predictors. It also provided a warning at the end:\nNote that this is just an example, and in reality, the number of Easter eggs brought by the Easter bunny is likely to depend on more complex factors that may not be accurately captured by a simple linear model. Additionally, the true distribution of the count data may be more complex than a Poisson distribution, and may require a more advanced simulation approach to accurately capture its properties\nI further prompted it again to explain what x1 and x2 could be\nPrompt: What would be predictors of the number of easter eggs?\nThere are many potential predictors that could influence the number of Easter eggs brought by the Easter bunny. Here are a few possibilities: Number of children in a household: It stands to reason that the number of Easter eggs brought by the Easter bunny would be higher in households with more children, since the bunny would have more individuals to provide for. Household income: Depending on the cultural context, the number of Easter eggs brought by the Easter bunny could be influenced by the socioeconomic status of the household. For example, in some cultures, more affluent households may be more likely to provide larger quantities of Easter eggs….*\nAnd it continued with other plausible suggestions.\n\n\nMaking stats more fun\nYou can go further with the bunnies above, using any example you like to keep yourself entertained (topics that are well known on the web will work best). For instance, I prompted GPT to write an explanation of generalized additive models in the style of an 18th century author. It used the classic ‘cars’ dataset. So I additionally prompted it to use horses as an example instead..\nAny style that is well known on the internet should work. You can ask for a crime novel with a gripping plot twist, in the style of a particular author like J.K. Rowling or as a script from Netflix series Bridgerton. If you are really passionate about R you could even try asking GPT for an explanation in the style of 50 Shades of Grey…\n\n\nPitfalls and words of warning\nChatGPT is just another tool we can use for learning. It can be part of your toolbox, but should not be the sole tool you use, it has pitfalls. It can give wrong or biased answers. Generally its correct, but some responses I’ve prompted have errors in the R code or in statistical logic. So always check your code independently. This should include running tests or working with someone who knows the correct way.\nChatGPT can also generate code with bugs, which could be hard for beginners to debug (especially if they are the types of bugs that don’t throw errors).\nMake sure you ask it clearly framed questions. Additionally, it has a ‘token’ limit (tokens are parts of words), so give it lots of smaller requests rather than one huge one.\nGPT may learn from text you enter, so don’t put in personal information or your best research ideas.\nFinally, I’m now seeing R blog posts that I’m sure are written by ChatGPT or similar models (you can tell by the style). I don’t have a problem with this, so long as the code is checked and corrected. So when searching for tutorials online make sure they are from reputable authors who you can trust to have verified their code and corrected any mistakes in explanation."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html",
    "href": "posts/2021-02-02-basic-math-biology/index.html",
    "title": "Basic math biologist should know by heart",
    "section": "",
    "text": "Here’s my list of basic math biologists that will help a biologist understand fundamental biological principles and be able to broadly read the literature. If you are working in statistics or modelling or a specialist area that is math heavy, you will obviously need to know much more.\nThanks to the contributions from others made via this Twitter thread.\nThis is a work in progress. I will keep updating this list, so message me if you have more to add (or remove!)."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#proficiency-at-counting",
    "href": "posts/2021-02-02-basic-math-biology/index.html#proficiency-at-counting",
    "title": "Basic math biologist should know by heart",
    "section": "Proficiency at counting",
    "text": "Proficiency at counting\nCounting might seem so basic a skill it is not worth mentioning, but counting accurately in the field and lab takes practice. Try counting penguins in a colony of thousands, cells under a microscope, or a moving school of fish and you’ll know what I mean. If you are going to work on any abundant organism, you will need to count well.\nYou get bonus credentials if you can count and keep track of multiple categories in your head at the same time (if you want to practice this, just get the ebird app go to a local park and try counting as many bird species as you can without looking at your phone)"
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#adept-at-handing-different-measurement-scales",
    "href": "posts/2021-02-02-basic-math-biology/index.html#adept-at-handing-different-measurement-scales",
    "title": "Basic math biologist should know by heart",
    "section": "Adept at handing different measurement scales",
    "text": "Adept at handing different measurement scales\nWe often converting among different measurement scales, whether it be length scales, concentrations, areas or volumes. Obviously this requires ability at adding, subtracting, multiplication and division. I find areas and volumes often trip people up, so I don’t take this knowledge for granted."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#y-ax-b",
    "href": "posts/2021-02-02-basic-math-biology/index.html#y-ax-b",
    "title": "Basic math biologist should know by heart",
    "section": "y = ax + b",
    "text": "y = ax + b\nThe linear equation is ubiquitous in biology and especially statistics. Bonus credentials for understanding y = ax + b + error\nIt is helpful to understand this equation well, such as how to calculate the slope of a line from two coordinates and how to find the x or y intercept."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#calculate-distances-and-areas",
    "href": "posts/2021-02-02-basic-math-biology/index.html#calculate-distances-and-areas",
    "title": "Basic math biologist should know by heart",
    "section": "Calculate distances and areas",
    "text": "Calculate distances and areas\nPrimarily in 2 dimensions, but bonus points for three dimensions."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#logs-and-powers",
    "href": "posts/2021-02-02-basic-math-biology/index.html#logs-and-powers",
    "title": "Basic math biologist should know by heart",
    "section": "Logs and powers",
    "text": "Logs and powers\nBecause why multiply when you can add?\nIt’s useful to know what a base is, that log(0) is undefined, how log10(x) relates to 10^x, that log(1) = 0, log of a number &lt;1 is negative, log of a number more than 1 is positive, that log(a*b) = log(a) + log(b) and that log(a+b) doesn’t equal log(a) + log(b)."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#exponential-growth-and-the-exponential-equation",
    "href": "posts/2021-02-02-basic-math-biology/index.html#exponential-growth-and-the-exponential-equation",
    "title": "Basic math biologist should know by heart",
    "section": "Exponential growth and the exponential equation",
    "text": "Exponential growth and the exponential equation\nExponential growth and decline are particularly common in biology (and physics, and finance, and economics…), from temperate dependent rates to population growth."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#intuitive-understanding-of-probability-and-uncertainty",
    "href": "posts/2021-02-02-basic-math-biology/index.html#intuitive-understanding-of-probability-and-uncertainty",
    "title": "Basic math biologist should know by heart",
    "section": "Intuitive understanding of probability and uncertainty",
    "text": "Intuitive understanding of probability and uncertainty\nThis means being able to think about the future (or the outcome of an event or experiment) in probabilistic terms, rather than definitive terms. Biology is ruled by lady luck, not by fate."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#conditional-probability",
    "href": "posts/2021-02-02-basic-math-biology/index.html#conditional-probability",
    "title": "Basic math biologist should know by heart",
    "section": "Conditional probability",
    "text": "Conditional probability\nThis is fundamental to statistics, but also to thinking about the outcomes of experiments."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#rules-of-probability",
    "href": "posts/2021-02-02-basic-math-biology/index.html#rules-of-probability",
    "title": "Basic math biologist should know by heart",
    "section": "Rules of probability",
    "text": "Rules of probability\nIncluding additive rule, multiplicative rule and the meaning of independence and conditional independence."
  },
  {
    "objectID": "posts/2021-02-02-basic-math-biology/index.html#derivates-and-integrals",
    "href": "posts/2021-02-02-basic-math-biology/index.html#derivates-and-integrals",
    "title": "Basic math biologist should know by heart",
    "section": "Derivates and integrals",
    "text": "Derivates and integrals\nIt is helpful to know that a derivate defines a rate and that an integral relates to a sum. This will help you read and understand modelling papers better. I don’t think non-modelling biologists need to know the rules of differentiation or integration.\n\nTentative list\nI’m still thinking about whether I also include: normal distribution (bell curve), central limit theorem, additive property of variances. I haven’t included specialist statistical concepts (e.g. p-values, or bayes theorem), but I think all of the above sets one up to understand those applications of probability. I haven’t included matrix algebra, I don’t think you need to know that unless you are working in modelling or statistics, but message me if you have examples that disagree."
  },
  {
    "objectID": "posts/2025-10-23-deer-harvest-reduction/index.html",
    "href": "posts/2025-10-23-deer-harvest-reduction/index.html",
    "title": "Does data from protected areas help to improve ecolgoical forecasts? New pre-print",
    "section": "",
    "text": "Sharing the research work of my Master’s student, Leigha Aitken, that is now available as a pre-print. She sought to answer the question:\nDoes data from protected areas help to improve ecological forecasts?\nAs a pre-print it is not peer-reviewed, though it has been examined and reviewed by UTas staff in thesis form and we did address those concerns. Abstract below, full pdf on SSRN.\n\nTitle: Transferability of models to novel site conditions: comparing abundance forecasts between different exploitation regimes\n\n\nAbstract\nModels are increasingly used to make forecasts to support fisheries management by translating ecological theory into quantitative predictions. However, forecasts for fisheries species are often constrained by limited data from unfished reference conditions. No-take marine reserves (NTMRs) can provide insight into unfished population dynamics, potentially improving models’ predictive performance. This study asked whether incorporating NTMR data improves the accuracy of annual abundance forecasts over eight years. We compared the predictive performance of first-order autoregressive models that were trained on abundance data from either NTMR sites, fished sites, or a mix of both site types. Each model was then tested by predicting to sites it was not trained on (‘transferred’). We tested the forecasts for both fished and unfished species with 23 years of time-series abundance data from the Maria Island (Tasmania) NTMR and nearby control sites. We found a general decrease in the performance of fished and NTMR models when forecasts were transferred to the opposite site type. Model transferability was species-specific, with the NTMR model reducing forecasting errors by approximately 10% at fished sites for one sustainably fished species. In contrast, for other species, the fished model reduced forecast errors by approximately 4% and 56% at NTMR sites. Additionally, we found that models trained on a mix of site types consistently underperformed. These findings highlight the importance of accounting for site- and species-specific dynamics. While NTMR data offers valuable insights into unfished conditions, further work is required to create models that transfer well to any exploitation regime."
  },
  {
    "objectID": "posts/2018-10-16-understanding-the-glm-link/index.html",
    "href": "posts/2018-10-16-understanding-the-glm-link/index.html",
    "title": "Generalized Linear Models understanding the link function",
    "section": "",
    "text": "Generalized Linear Models (‘GLMs’) are one of the most useful modern statistical tools, because they can be applied to many different types of data. Count, binary ‘yes/no’, and waiting time data are just some of the types of data that can be handled with GLMs.\nWe often call such data ‘non-normal’ because its distribution doesn’t follow a normal distribution (also known as the bell curve or Gaussian Distribution).\nIn this post I will look at how GLMs use a ‘link function’ to model non-normal data. I think there is a sort of beautiful elegance in the maths of how the link function works. Understanding this theory will also help you build better models for your data and interpret them in more nuanced ways.\nWe will step through the math behind the GLM and build it up from scratch.\nGLMs extend ‘General Linear Models’ (confusing names I know), read this post first if you are not yet familiar with General Linear Models.\nThanks is due to Brad Biggerstaff for emailing some errors in terminology in my original post (which I’ve updated and hopefully fixed 19/10/2018).\n\n\nI learned about the Normal distribution in primary school. The normal distribution is central to much of statistics (no pun intended), but there are many types of data that don’t meet the basic assumptions of the normal.\nThe normal distribution has ‘infinite support’, which means values modelled by the normal can take any negative or positive number. Often the normal is a pretty reasonable approximation of data that doesn’t meet these assumptions, but there are many cases when using a normal for data that isn’t will lead us to make errors in our inferences.\nStatisticians have invented whole families of distributions to describe any type of data you can imagine, from the morbid (the Exponential distribution for deaths or decay), to wealth inequality (the Pareto distribution) and even days of the year (the Von Mises distribution).\nMore specifically, we should think of the distribution as a description of the process that generates the data.\nLet’s start with something simple. Let’s assume you have been counting fish on coral reefs. So your data are count data.\nIf the counts are large they may well look pretty normal. But there are some important differences.\nCounts are integers, whereas the normal distribution is for continuous data that can include any fraction.\nCounts also can’t be less than zero, but the Normal distribution model’s stochastic processes that draw zeros and negative numbers.\nStatisticians have invented many distributions for counts, one of the simplest is the Poisson distribution. It is a model of positive integers. It has one parameter λ, which is both its mean and variance.\nLet’s see what that looks like with some simple R code to draw random numbers from two Poisson distributions:\nn &lt;- 1000\nset.seed(42)\nx1 &lt;- rpois(n, lambda = 1)\nx10 &lt;- rpois(n, lambda = 10)\nmean(x1)\n\n## [1] 0.976\n\nvar(x1)\n\n## [1] 0.9663904\n\nmean(x10)\n\n## [1] 10.083\n\nvar(x10)\n\n## [1] 10.75086\nWe just sampled random numbers from two Poisson distributions with means of 1 and 10. Notice that the means and variances of each are approximately equal (not exactly equal because of we drew a small random sample).\nYou can think of this sampling from the Poisson as a model of count data. Let’s see what that looks like:\npar(mfrow=c(1,2))\nhist(x1, xlim = c(0, 25), seq(0, 25, by = 1))\nhist(x10, xlim = c(0, 25), seq(0, 25, by = 1))\n\nSo the data drawn from the poisson with lambda = 1 are concentrated near zero and strongly skewed (not very Normal). The data with lambda = 10 are approximately normally distribution and have a much larger variance than the former data.\n\n\n\nSo far our Poisson model only has one parameter, a mean (and variance). But what if we wanted the mean to change?\nFor instance, we might have counted fish on different types of coral reefs and we want to test whether there are difference abundances on each type of reef.\nOr we might have counted fish across a gradient of pollution and we want to know how their numbers change from low to high pollution.\nI will call these hypothesized causes of changes in fish counts ‘covariates’. Others might call them explanatory variables, treatments (if experiments) or predictor variables.\nWe are using Generalized Linear Models, so we could include the covariates as variables in a simple linear equation, after all that is what we do with linear regression (and general linear models):\n\n\n\nimg\n\n\nLet’s generate some such data ourselves. We will assume pollution is measured on a zero to one (low to high) scale, that the mean number of fish with no pollution =4 and that on average there are no fish anymore when pollution levels = 0.5 (half their maximum).\nThe plot below illustrates our model for the mean expected number of fish across the pollution gradient. In this case we are building a model to simulate some data to practice with:\nn &lt;- 50\nbeta &lt;- -8 #effect of polluted reefs\nalpha &lt;- 4 #intercept = mean at 'zero' pollution\nx &lt;-  seq(0, 1, length.out = n) #pollution levels\nymean &lt;- alpha + beta*x\nplot(x, ymean, type = 'l', xlab = \"Pollution level\", ylab = \"Number of fish counted\")\nabline(h = 0, lty = 2, lwd = 2)  \n\nThere is something odd about this model: we are predicting negative fish (on average) for pollution levels over 0.5.\nIt gets even worse if we model sampling with a normal distribution:\nset.seed(55)\nyobs_normal &lt;- ymean + rnorm(n)\nplot(x, ymean, type = 'l', xlab = \"Pollution level\", ylab = \"Number of fish counted\")\npoints(x, yobs_normal)\nabline(h = 0, lty = 2, lwd = 2)\n\nClearly if we went in the real world to sample data, we wouldn’t ever get any negative counts.\nSo we are going to have to redesign our model, which is the basis of our statistical tests, so that negative values don’t happen.\nBut we still want to use a linear model, because they are convenient to work with mathematically and convenient when it comes to estimating the unknown effects.\n\n\n\nSo now we come to link functions. Link functions elegantly solve the problem of using linear models with with non-normal data. There are many types of link functions, but we will look at one that is popular for use with count data.\nIf you wanted to stop a linear function from taking negative values what is one simple transformation you could make?\nWell, you could take the function’s exponent. We will redo the above linear equation as log-linear. I will change the parameter names to reflect that they are now logs:\ngamma &lt;- -3.2 #effect of polluted reefs\nalpha &lt;- 4 #intercept = mean at 'zero' pollution\nyexp &lt;- alpha*exp(gamma*x)\nplot(x, yexp, type = 'l', xlab = \"Pollution level\", ylab = \"Number of fish counted\")\nabline(h = 0, lty = 2, lwd = 2)\n\nHere we have the equation y = alpha *exp(gamma*x) which is the same as the linear equation for log(y): log(y) = log(alpha) +gamma*x. Note I have retained alpha=4 in both, because for both equations alpha is the expected value at pollution of zero.\nI changed the slope parameter in the log-linear equation to gamma because it is not a direct analogue of our slope parameter beta above.\nOne of the nice things about the log-linear equation is that the slope parameter now represents multiples of change. For instance, gamma = -3.2 means the abundance declines about 25 times decline (=1/exp(-3.2)) when going from a pollution level of 0 to 1.\nAbundance declines about a five times decline if we go from a pollution of 0 to 0.5 (= 1/exp(-3.2*0.5)). Noting this will be important when we come to interpreting fitted models below.\nWe could now use this exponential curve as the mean (and variance!) of a Poisson:\nyobs_pois &lt;- rpois(n, yexp)\nplot(x, yexp, type = 'l', xlab = \"Pollution level\",\n     ylab = \"Number of fish counted\",\n     ylim = c(0, 8))\npoints(x, yobs_pois)\n\nNotice that no data fall below zero now. Also, notice how the variance of the samples gets smaller as the mean gets smaller.\n\n\n\nIn the real world, you will have the sample points, but not the ‘true’ mean. In the example above we just made up the true mean ourselves. In the real world Nature provides the ‘truth’ about how pollution impacts fish abundance and the best we can do is take as many measurements as we can and hope to get near the truth.\nTo estimate the effect of the pollution covariate you can use R’s glm() function:\nm1 &lt;- glm(yobs_pois ~ x, family = poisson(link = \"log\"))\ncoef(m1)\n\n## (Intercept)           x\n##    1.409704   -3.345646\nThe values we printed give the estimates for the intercept and slope coeffcients (alpha and gamma). You can check that these are similar to the true estimates we provided by comparing them to log(alpha) and gamma.\nI have specified above the type of distribution to use (family = poission()) and which link to use. \"log\" is in fact the default choice, but I put it there so you know you can change it.\nFor instance, you can use \"identity\" link for data that is far from zero. If you use the identity link, which is basically no link function, your model will be linear, not log-linear, so your slope estimate will once again be additive.\nTechnically we would say we fitted a Poisson Generalized Linear Model with a log link function. So the model’s predictions for the data will be Poisson distributed about its estimate for the mean. Note that the data can contain zeros, but the mean of the Poisson is always &gt;0.\nSo what do the coefficients mean? Remember the coefficients are on the log scale. So the mean abundance at a pollution level of zero = {r} exp(coef(m1)[1]) and a change in pollution from 0 to 1 causes an estimated {r} 1/exp(coef(m1)[2]) times decline in fish abundance.\nLet’s also plot the fitted model with standard errors.\nypredict &lt;- predict(m1, type = \"response\", se = TRUE)\nplot(x, yexp, type = 'l', xlab = \"Pollution level\",\n     ylab = \"Number of fish counted\",\n     ylim = c(0, 8))\nlines(x, ypredict$fit, lwd = 2, col = \"red\", lty = 2)\n#Add lines for standard errors\nlines(x, ypredict$fit + ypredict$se.fit, lty = 3, col = \"red\")\nlines(x, ypredict$fit - ypredict$se.fit, lty = 3, col = \"red\")\n#plot observations\npoints(x, yobs_pois)\nlegend('topright', legend = c(\"True mean\", \"Estimated mean\"),\n       lwd = 2, lty = c(1,2), col = c(\"black\", \"red\"))\n\nYou can see the fitted line falls close to the ‘true’ line, and the standard errors are pretty tight around our best estimate.\nThe fitting algorithm itself is attempting the maximise the log-likelihood of the observations given the mean (in technical speak). You can read more about likelihoods here.\nIt is also worth noting that we still need to do assumption checks, like we would for a regression with normal errors. For instance, we can check the model’s residuals (the difference between the data and the model’s predicted values) to see that the residual variance approximately equals the mean. A cool way to check assumptions of the Poisson model is to use ‘rootograms’, look it up.\n\n\n\nSo in my introduction I claimed that maths of GLMs is beautiful. I think that because the maths is nicely coherent with the way nature often works.\nWe wanted to fit a linear function to data that can’t be less than zero, because linear functions are convenient to work with. So we used a log link function to describe the mean and to ensure that the mean is always greater than zero.\nWe ended up with a model where the slope describes multiples of change in fish abundance over the pollution gradient. So the model itself is actually multiplicative, not additive.\nIf you think about it, natural processes that generate counts often are multiplicative, not additive. For instance, we may talk about ‘fish multiplying’ when they breed, because population growth can be exponential.\nSo our mathematically convenient link function actually ended up being a better description of the natural process.\nThe effort to use a non-negative model also forced us to think about using a more appropriate distribution for the data: the Poisson rather than the Normal. The Poisson has the variance increasing with the mean.\nOnce again, natural processes that generate counts often lead to increases in the variance in situations where we count more. Counts near zero will naturally have low variance, because they are constrained by zero, whereas higher counts will naturally have a greater variabilty.\nYou can also relax the assumption of mean = variance with other GLM distributions like the negative binomial.\nIt turns out that proper models of variance are crucial for getting the standard-errors right, and so crucial for detecting real effects over spurious ones.\nImagine if you used a Normal distribution and assumed equal variances. You might spuriously attribute differences between groups from high counts to some covariate, but the difference is actually just natural variation. Conversely, you might miss differences between groups with low counts, because a smaller difference at low counts should actually be statistically significant.\nThe increased power we get to detect differences at low counts with a GLM over a regression happens because it is the multiple of the difference that matters with the log link, not the absolute (additive) difference.\nMy final point is to remember that coefficients from a model with a log link (and some other links too, like the logit) are multiplicative. This can be very useful when it comes to making sense of your results and may change the way you present your findings.\nFor instance, we used this key insight from a GLM to make a case that pollution from logging causes a 24 times decline in the abundance of a threatened fish species.\nBefore we considered using the GLM, we had actually presented the results in terms of a % change in fish abundance. But % are not as easy to generalize, because they depend on your baseline. Multiples do not.\nHope you found this post helpful, and as always you can get me on Twitter if you have comments or questions.\n\n\nI wanted to add a brief appendix to address this question, because the answer is insightful as to how link functions work.\nTry take the data we generated above and fit two GLMs (you will have to add a small number so you can log the zeros, not ideal but a common practice)\nyobsplus &lt;- yobs_pois+0.1\nmodel1 &lt;- glm(yobsplus ~ x, family = gaussian(link = \"log\"))\nmodel2 &lt;- glm(log(yobsplus) ~ x, family = gaussian(link = \"identity\"))\nIn the first model we fitted a Gaussian (=Normal distributed errors) with a log link. In the second we fitted a Gaussian to log(y) with the identity link (which is no link).\nNow compare the results. Notice that the estimate of the slope is quite different. Why is this?\nThe model with the log link is fitting the mean on the log scale, the Gaussian errors will be on the natural scale. So the residual (or error) variance will be constant for all mean values of y.\nThe model with the log of the data and identity link is fitting the mean and variance on the log scale. So if we retransform log(y) back to y, the variance will change with the mean.\nSo a log link isn’t the same as a log transformation. The transformation changes the raw data. The link function doesn’t touch the raw data, instead you can think of it as a transformation of the model for the mean of the raw data."
  },
  {
    "objectID": "posts/2018-10-16-understanding-the-glm-link/index.html#choosing-the-right-distribution-for-your-data",
    "href": "posts/2018-10-16-understanding-the-glm-link/index.html#choosing-the-right-distribution-for-your-data",
    "title": "Generalized Linear Models understanding the link function",
    "section": "",
    "text": "I learned about the Normal distribution in primary school. The normal distribution is central to much of statistics (no pun intended), but there are many types of data that don’t meet the basic assumptions of the normal.\nThe normal distribution has ‘infinite support’, which means values modelled by the normal can take any negative or positive number. Often the normal is a pretty reasonable approximation of data that doesn’t meet these assumptions, but there are many cases when using a normal for data that isn’t will lead us to make errors in our inferences.\nStatisticians have invented whole families of distributions to describe any type of data you can imagine, from the morbid (the Exponential distribution for deaths or decay), to wealth inequality (the Pareto distribution) and even days of the year (the Von Mises distribution).\nMore specifically, we should think of the distribution as a description of the process that generates the data.\nLet’s start with something simple. Let’s assume you have been counting fish on coral reefs. So your data are count data.\nIf the counts are large they may well look pretty normal. But there are some important differences.\nCounts are integers, whereas the normal distribution is for continuous data that can include any fraction.\nCounts also can’t be less than zero, but the Normal distribution model’s stochastic processes that draw zeros and negative numbers.\nStatisticians have invented many distributions for counts, one of the simplest is the Poisson distribution. It is a model of positive integers. It has one parameter λ, which is both its mean and variance.\nLet’s see what that looks like with some simple R code to draw random numbers from two Poisson distributions:\nn &lt;- 1000\nset.seed(42)\nx1 &lt;- rpois(n, lambda = 1)\nx10 &lt;- rpois(n, lambda = 10)\nmean(x1)\n\n## [1] 0.976\n\nvar(x1)\n\n## [1] 0.9663904\n\nmean(x10)\n\n## [1] 10.083\n\nvar(x10)\n\n## [1] 10.75086\nWe just sampled random numbers from two Poisson distributions with means of 1 and 10. Notice that the means and variances of each are approximately equal (not exactly equal because of we drew a small random sample).\nYou can think of this sampling from the Poisson as a model of count data. Let’s see what that looks like:\npar(mfrow=c(1,2))\nhist(x1, xlim = c(0, 25), seq(0, 25, by = 1))\nhist(x10, xlim = c(0, 25), seq(0, 25, by = 1))\n\nSo the data drawn from the poisson with lambda = 1 are concentrated near zero and strongly skewed (not very Normal). The data with lambda = 10 are approximately normally distribution and have a much larger variance than the former data."
  },
  {
    "objectID": "posts/2018-10-16-understanding-the-glm-link/index.html#linear-models",
    "href": "posts/2018-10-16-understanding-the-glm-link/index.html#linear-models",
    "title": "Generalized Linear Models understanding the link function",
    "section": "",
    "text": "So far our Poisson model only has one parameter, a mean (and variance). But what if we wanted the mean to change?\nFor instance, we might have counted fish on different types of coral reefs and we want to test whether there are difference abundances on each type of reef.\nOr we might have counted fish across a gradient of pollution and we want to know how their numbers change from low to high pollution.\nI will call these hypothesized causes of changes in fish counts ‘covariates’. Others might call them explanatory variables, treatments (if experiments) or predictor variables.\nWe are using Generalized Linear Models, so we could include the covariates as variables in a simple linear equation, after all that is what we do with linear regression (and general linear models):\n\n\n\nimg\n\n\nLet’s generate some such data ourselves. We will assume pollution is measured on a zero to one (low to high) scale, that the mean number of fish with no pollution =4 and that on average there are no fish anymore when pollution levels = 0.5 (half their maximum).\nThe plot below illustrates our model for the mean expected number of fish across the pollution gradient. In this case we are building a model to simulate some data to practice with:\nn &lt;- 50\nbeta &lt;- -8 #effect of polluted reefs\nalpha &lt;- 4 #intercept = mean at 'zero' pollution\nx &lt;-  seq(0, 1, length.out = n) #pollution levels\nymean &lt;- alpha + beta*x\nplot(x, ymean, type = 'l', xlab = \"Pollution level\", ylab = \"Number of fish counted\")\nabline(h = 0, lty = 2, lwd = 2)  \n\nThere is something odd about this model: we are predicting negative fish (on average) for pollution levels over 0.5.\nIt gets even worse if we model sampling with a normal distribution:\nset.seed(55)\nyobs_normal &lt;- ymean + rnorm(n)\nplot(x, ymean, type = 'l', xlab = \"Pollution level\", ylab = \"Number of fish counted\")\npoints(x, yobs_normal)\nabline(h = 0, lty = 2, lwd = 2)\n\nClearly if we went in the real world to sample data, we wouldn’t ever get any negative counts.\nSo we are going to have to redesign our model, which is the basis of our statistical tests, so that negative values don’t happen.\nBut we still want to use a linear model, because they are convenient to work with mathematically and convenient when it comes to estimating the unknown effects."
  },
  {
    "objectID": "posts/2018-10-16-understanding-the-glm-link/index.html#simulating-poisson-data-from-a-linear-model",
    "href": "posts/2018-10-16-understanding-the-glm-link/index.html#simulating-poisson-data-from-a-linear-model",
    "title": "Generalized Linear Models understanding the link function",
    "section": "",
    "text": "So now we come to link functions. Link functions elegantly solve the problem of using linear models with with non-normal data. There are many types of link functions, but we will look at one that is popular for use with count data.\nIf you wanted to stop a linear function from taking negative values what is one simple transformation you could make?\nWell, you could take the function’s exponent. We will redo the above linear equation as log-linear. I will change the parameter names to reflect that they are now logs:\ngamma &lt;- -3.2 #effect of polluted reefs\nalpha &lt;- 4 #intercept = mean at 'zero' pollution\nyexp &lt;- alpha*exp(gamma*x)\nplot(x, yexp, type = 'l', xlab = \"Pollution level\", ylab = \"Number of fish counted\")\nabline(h = 0, lty = 2, lwd = 2)\n\nHere we have the equation y = alpha *exp(gamma*x) which is the same as the linear equation for log(y): log(y) = log(alpha) +gamma*x. Note I have retained alpha=4 in both, because for both equations alpha is the expected value at pollution of zero.\nI changed the slope parameter in the log-linear equation to gamma because it is not a direct analogue of our slope parameter beta above.\nOne of the nice things about the log-linear equation is that the slope parameter now represents multiples of change. For instance, gamma = -3.2 means the abundance declines about 25 times decline (=1/exp(-3.2)) when going from a pollution level of 0 to 1.\nAbundance declines about a five times decline if we go from a pollution of 0 to 0.5 (= 1/exp(-3.2*0.5)). Noting this will be important when we come to interpreting fitted models below.\nWe could now use this exponential curve as the mean (and variance!) of a Poisson:\nyobs_pois &lt;- rpois(n, yexp)\nplot(x, yexp, type = 'l', xlab = \"Pollution level\",\n     ylab = \"Number of fish counted\",\n     ylim = c(0, 8))\npoints(x, yobs_pois)\n\nNotice that no data fall below zero now. Also, notice how the variance of the samples gets smaller as the mean gets smaller."
  },
  {
    "objectID": "posts/2018-10-16-understanding-the-glm-link/index.html#the-beauty-of-the-link-combining-linear-models-with-bespoke-distributions",
    "href": "posts/2018-10-16-understanding-the-glm-link/index.html#the-beauty-of-the-link-combining-linear-models-with-bespoke-distributions",
    "title": "Generalized Linear Models understanding the link function",
    "section": "",
    "text": "In the real world, you will have the sample points, but not the ‘true’ mean. In the example above we just made up the true mean ourselves. In the real world Nature provides the ‘truth’ about how pollution impacts fish abundance and the best we can do is take as many measurements as we can and hope to get near the truth.\nTo estimate the effect of the pollution covariate you can use R’s glm() function:\nm1 &lt;- glm(yobs_pois ~ x, family = poisson(link = \"log\"))\ncoef(m1)\n\n## (Intercept)           x\n##    1.409704   -3.345646\nThe values we printed give the estimates for the intercept and slope coeffcients (alpha and gamma). You can check that these are similar to the true estimates we provided by comparing them to log(alpha) and gamma.\nI have specified above the type of distribution to use (family = poission()) and which link to use. \"log\" is in fact the default choice, but I put it there so you know you can change it.\nFor instance, you can use \"identity\" link for data that is far from zero. If you use the identity link, which is basically no link function, your model will be linear, not log-linear, so your slope estimate will once again be additive.\nTechnically we would say we fitted a Poisson Generalized Linear Model with a log link function. So the model’s predictions for the data will be Poisson distributed about its estimate for the mean. Note that the data can contain zeros, but the mean of the Poisson is always &gt;0.\nSo what do the coefficients mean? Remember the coefficients are on the log scale. So the mean abundance at a pollution level of zero = {r} exp(coef(m1)[1]) and a change in pollution from 0 to 1 causes an estimated {r} 1/exp(coef(m1)[2]) times decline in fish abundance.\nLet’s also plot the fitted model with standard errors.\nypredict &lt;- predict(m1, type = \"response\", se = TRUE)\nplot(x, yexp, type = 'l', xlab = \"Pollution level\",\n     ylab = \"Number of fish counted\",\n     ylim = c(0, 8))\nlines(x, ypredict$fit, lwd = 2, col = \"red\", lty = 2)\n#Add lines for standard errors\nlines(x, ypredict$fit + ypredict$se.fit, lty = 3, col = \"red\")\nlines(x, ypredict$fit - ypredict$se.fit, lty = 3, col = \"red\")\n#plot observations\npoints(x, yobs_pois)\nlegend('topright', legend = c(\"True mean\", \"Estimated mean\"),\n       lwd = 2, lty = c(1,2), col = c(\"black\", \"red\"))\n\nYou can see the fitted line falls close to the ‘true’ line, and the standard errors are pretty tight around our best estimate.\nThe fitting algorithm itself is attempting the maximise the log-likelihood of the observations given the mean (in technical speak). You can read more about likelihoods here.\nIt is also worth noting that we still need to do assumption checks, like we would for a regression with normal errors. For instance, we can check the model’s residuals (the difference between the data and the model’s predicted values) to see that the residual variance approximately equals the mean. A cool way to check assumptions of the Poisson model is to use ‘rootograms’, look it up."
  },
  {
    "objectID": "posts/2018-10-16-understanding-the-glm-link/index.html#the-beauty-of-the-link-combining-linear-models-with-bespoke-distributions-to-describe-natural-processes",
    "href": "posts/2018-10-16-understanding-the-glm-link/index.html#the-beauty-of-the-link-combining-linear-models-with-bespoke-distributions-to-describe-natural-processes",
    "title": "Generalized Linear Models understanding the link function",
    "section": "",
    "text": "So in my introduction I claimed that maths of GLMs is beautiful. I think that because the maths is nicely coherent with the way nature often works.\nWe wanted to fit a linear function to data that can’t be less than zero, because linear functions are convenient to work with. So we used a log link function to describe the mean and to ensure that the mean is always greater than zero.\nWe ended up with a model where the slope describes multiples of change in fish abundance over the pollution gradient. So the model itself is actually multiplicative, not additive.\nIf you think about it, natural processes that generate counts often are multiplicative, not additive. For instance, we may talk about ‘fish multiplying’ when they breed, because population growth can be exponential.\nSo our mathematically convenient link function actually ended up being a better description of the natural process.\nThe effort to use a non-negative model also forced us to think about using a more appropriate distribution for the data: the Poisson rather than the Normal. The Poisson has the variance increasing with the mean.\nOnce again, natural processes that generate counts often lead to increases in the variance in situations where we count more. Counts near zero will naturally have low variance, because they are constrained by zero, whereas higher counts will naturally have a greater variabilty.\nYou can also relax the assumption of mean = variance with other GLM distributions like the negative binomial.\nIt turns out that proper models of variance are crucial for getting the standard-errors right, and so crucial for detecting real effects over spurious ones.\nImagine if you used a Normal distribution and assumed equal variances. You might spuriously attribute differences between groups from high counts to some covariate, but the difference is actually just natural variation. Conversely, you might miss differences between groups with low counts, because a smaller difference at low counts should actually be statistically significant.\nThe increased power we get to detect differences at low counts with a GLM over a regression happens because it is the multiple of the difference that matters with the log link, not the absolute (additive) difference.\nMy final point is to remember that coefficients from a model with a log link (and some other links too, like the logit) are multiplicative. This can be very useful when it comes to making sense of your results and may change the way you present your findings.\nFor instance, we used this key insight from a GLM to make a case that pollution from logging causes a 24 times decline in the abundance of a threatened fish species.\nBefore we considered using the GLM, we had actually presented the results in terms of a % change in fish abundance. But % are not as easy to generalize, because they depend on your baseline. Multiples do not.\nHope you found this post helpful, and as always you can get me on Twitter if you have comments or questions.\n\n\nI wanted to add a brief appendix to address this question, because the answer is insightful as to how link functions work.\nTry take the data we generated above and fit two GLMs (you will have to add a small number so you can log the zeros, not ideal but a common practice)\nyobsplus &lt;- yobs_pois+0.1\nmodel1 &lt;- glm(yobsplus ~ x, family = gaussian(link = \"log\"))\nmodel2 &lt;- glm(log(yobsplus) ~ x, family = gaussian(link = \"identity\"))\nIn the first model we fitted a Gaussian (=Normal distributed errors) with a log link. In the second we fitted a Gaussian to log(y) with the identity link (which is no link).\nNow compare the results. Notice that the estimate of the slope is quite different. Why is this?\nThe model with the log link is fitting the mean on the log scale, the Gaussian errors will be on the natural scale. So the residual (or error) variance will be constant for all mean values of y.\nThe model with the log of the data and identity link is fitting the mean and variance on the log scale. So if we retransform log(y) back to y, the variance will change with the mean.\nSo a log link isn’t the same as a log transformation. The transformation changes the raw data. The link function doesn’t touch the raw data, instead you can think of it as a transformation of the model for the mean of the raw data."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "",
    "text": "This blog supplements my talk for Marine Alliance for Science and Technology Scotland workshop on Cumulative Stressors. Some of it is published by myself and others and some of it I intend to publish. Get in touch if you are interested in knowing more."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#summary",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#summary",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "Summary",
    "text": "Summary\nThe biodiversity of most marine regions is affected my multiple human-caused stressors. These stressors can interact in unpredictable ways that can expedite the rate of taxonomic and functional extinction, or reduce the effectiveness of conservation actions. I will provide an overview for how can stressors interact to affect marine populations, and what decision makers can do to respond to multiple stressors that interact. Science needs to progress beyond simple ‘additive’ models and consider the processes that underlie interactions among stressors, so we can make more accurate predictions about how stressors interact. Decisions makers are faced with considerable uncertainty about the outcomes of conservation actions when there are multiple stressors. Decision science can help identify priority actions for conservation when faced with uncertainty."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#introduction",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#introduction",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "Introduction",
    "text": "Introduction\nConservation management aims to avoid species extinction, whether that is taxonomic extinction or functional extinction. But, we never really know why species go extinct. There are typically too many circumstantial stressors of an organism to be able to partition out which one caused the extinction. Here I define stressors as human caused changes to the environment that have a detrimental effect on a marine population or ecosystem. Examples include mortality from fisheries by-catch and sub-lethal stressors like toxins that affect reproductive health. For instance, one of the most famous examples of extinction was the extinction of the Caribbean Monk Seal. The monk seal’s extinction was ostensibly caused by hunting, but overfishing of their food resources may also have contributed (McClenachan and Cooper 2008). Dugong, an important herbivore of seagrass meadows may also be functionally extinct along many areas of Queensland’s coast, having decline 97% since the 1970s (Hughes et al. 2005). Their decline has been caused by a combination of habitat loss from poor water quality, hunting, boat strikes and entanglement in fishing gear. Determining the cause of these declines is important for identifying what actions managers can take to avert those declines.\nMost marine ecosystems are affected by multiple stressors (Halpern et al. 2015), making it hard to attribute a population’s decline to any single stressor. Stressors can also interact in unexpected ways, meaning that population declines can occur faster or slower than we might have expected based on their effects in isolation (Côté et al. 2016). Thus, extinction is often driven by the inextricable combination of multiple stressors. The inseparable effects of multiple stressors poses several challenges to the science of extinction and to the management of nature, both of which tend to be reductionist.\nFirst, there is the estimation challenge: in the wild systems that we study it is arguably more common to observe multiple confounding stressors acting at once, rather than their effects in isolation. Therefore, it can be difficult to measure the impact of individual stressors and determine what benefits (or not) acting on one or the other might bring for averting extinction.\nSecond, there is the uncertainty challenge: stressor interactions mean that actions on single stressors may have unexpected outcomes, and actions to address one stressor may sometimes even worsen the population’s decline. How stressors interact to affect a population will often remain unknown. In fact, stressors are now so globally extensive that the number of potential interactions we could study will in most region far exceed the capacity of science to measure those interactions. Thus, when determining the benefits of conservation actions, managers need to act in the face of considerable uncertainty about those benefits.\nFinally, there is the prioritization challenge: management agencies have constraints on what they can do, often for economic (e.g. budget) or social (e.g. conflicting human uses of the marine environment) reasons. When choosing what actions to take to lessen stressors, management must decide how to prioritise to find those actions that give the best outcomes for the population given the constraints they face.\nHere I will ask what managers and science can do to avert extinctions in the face of multiple stressors that have uncertain interactions. The focus will be on managing multiple different types stressors, not cumulative impacts. Cumulative impacts science is an all encompassing term that includes the impacts of repeat events of the same stressor (e.g. multiple marinas) and also the impacts of multiple different stressors on populations. When assessing the impact of multiple stressors we often need to consider how they will interact.\nFirst, I will provide a conceptual overview for how multiple stressors can interact to affect marine populations (the field is in disarray with only broad agreement on how to even classify different types of interactions). Second, I will examine the prevalence of different types of interactions and ask whether we can ever make educated guesses about how stressors interact (perhaps soon, but the science is not established yet). Third, in situations when interactions are unknown I will ask what is the safest for managers to assume and I will describe the paradox of managing interacting stressors. Finally, I draw on the conservation planning literature to provide a revised road map for managing multiple (and potentially interacting) impacts."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#conceptual-overview-of-interactions-among-multiple-stressors",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#conceptual-overview-of-interactions-among-multiple-stressors",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "Conceptual overview of interactions among multiple stressors",
    "text": "Conceptual overview of interactions among multiple stressors\nAt a conceptual level, interactions among multiple stressors occur when stressors interact via some physical or biological process. For instance, interactions can be purely driven by physical process, such as warming polar oceans also causing freshening due to sea ice melt (Boyd and Brown 2015). Examples of biological processes underlying interactions include the increased susceptibility of corals to bleaching in clearer water (due to light stress). Ecological interactions can also cause stressor interactions, due to feed-backs within food-webs. For instance, ocean acidification and fishing may interact to affect some benthic molluscs by slowing their growth, but fishing will reduce predation pressure.\nWhen we quantify interactions we need a null model with which to compare our observations (from the field or experiments) with our expectation. In the past these null models have been based on the ANOVA methodology. Therefore, if we want to know how two stressors interact, we measure the response variable (e.g. growth of seagrass) in a crossed trial. The interaction is then measured as the difference between the sum of effects from the single stressor trials and the outcome of the crossed trial. If the outcome of the crossed trial is greater than the sum then the interaction is said to by synergistic, if it is less than the sum then the interaction is said to be antagonistic.\nInteractions matter if you are managing just a single stressor. From a management perspective a synergy will mean declines happen faster than you expected in the combination of two stressors, but that acting on one stressor brings a serendipitous benefit in terms of reducing that stressor and its interaction with another stressor (Brown et al. 2013).\nFor instance, we could speculate that poor water quality and boat traffic would have a synergistic impact on dugong population trends. Poor water quality reduces the growth of seagrass the food of dugongs. Boats can collide with dugongs causing injury and death. Dugongs will travel to find high quality food patches (e.g. Sheppard et al. 2010). This may increase the risk of boat strikes. So we could infer that by improving water quality so that seagrass grows more densely we may also serendipitously reduce the risk of boat strikes. Therefore, the benefit of actions on water quality has an additional benefit in terms of reducing boat mortalities.\nAn antagonism means that acting on one stressor may be ineffective, because the other stressor still dominates. In some cases you may also have mitigative interactions, whereby acting on one stressor actually makes the impacts of the other stressor worse.\nAntagonisms can occur when you have dominance effects. For instance, say that toxins and disease are most likely to kill old or weak seals. Then if we have a disease outbreak in an area that has a high toxin load, we might see little additional seal deaths – the toxins already killed all the susceptible seals!\nAntagonisms can also be mitigative, meaning that one stressor weakens or even reverses the effect of another (such effects have also termed mitigating synergies). Such mitigative effects may often occur in food-webs. e.g. if two stressors have a synergistic effect on a species predator, then their combination may have an mitigative effect on the prey species.\nSo determining and classifying the type of interactions occurring between multiple stressors is important for management.\nCan we predict how two stressors will interact?\nIt will often be important to predict how stressors interact without having to collect new data to determine those interactions (Côté et al. 2016). For instance, we might be limited in our funding or time we have to gather new information that allows us to make the most effective decision. Therefore, it would be helpful if we could predict a-priori how strongly two stressors may interact.\nIn the past the scientific literature has been most concerned with identifying synergies. It is easy to see why, they potentially mean biodiversity will under go much greater declines than we expect based on reductionist science that studies a single stressor in isolation. So, is there any reason to expect that synergies will be particularly prevalent?\nNumerous meta-analyses of experimental studies have shown that synergies are no more prevalent than antagonisms or just additive interactions. In fact, you tend to just get a 1/3 each way (Côté et al. 2016). That interactions are apparently random means it will be hard to predict a-priori what the outcome will be.\nHowever, if you look across multiple meta-analyses (a meta-analysis of the meta-analyses!) and ask when and where interactions occur you do observe some generalisations (Côté et al. 2016). Antagonisms are most likely to occur at the community/ecosystem level, for instance in the response of diversity of community biomass to multiple stressors. This may be because ecosystems have many self regulating feed-back loops that mitigate the effects of stressors.\nAntagonisms are rare at the physiological level (e.g. for responses like respiration and photosynthesis). At this level additive and synergistic interactions are more likely. At the individual organism or population levels (e.g. growth, abundance, behaviour) you tend to see a mix of interaction types.\nFurther work is needed to explore why these generalities occur. One reason may be that different disciplines actually apply different definitions to determine what is synergistic vs. antagonistic. See Piggott et al. 2015 for a detailed discussion of different approaches to classifying interactions.\nThis confusion in the scientific literature about what constitutes a synergistic or antagonistic interaction is a challenge for making management decisions. What we are really interested in for managers is: what is the benefit of acting on one stressor for the population?\nI think we need to move beyond the simple definition of interactions based on comparison to an additive model and think more about how process models can inform on types of interactions. While we often know very little about how two stressors interact, we may in fact know a lot about the processes by which a population responds to a stressor.\nWe can build experiments in process models of multiple stressors to predict what the outcome of managing a single stressor will be.\nFor instance, Viv Tulloch, my PhD student modelled Baleen whale recovery from harvesting in the Southern Ocean (Tulloch et al. 2017). Through her co-supervisor Eva Plaganyi, she was able to obtain data on historic catches on whales from the International Whaling Commission. She also obtained corrections for the ‘missing Soviet catches’ between 1947 and 1973.\nShe developed a whale population model that accurately reflected past trends in whale population size. The model was also coupled to climate and low-trophic level models that predict trends in Antarctic Krill biomass, a key prey source for Baleen whales.\nRecovery for Blue and Fin whales are considerably slower post-harvesting when compared the coupled climate model to a model with constant climate, indicating an important interaction between the cessation of harvesting pressure and climate.\nIn future work she intends to explore how a fishery for krill may affect these recovery trajectories, and thus can provide advice on how krill catches should be regulated to enable whale recovery.\nProcess models of interactions also end up making a better ‘null’ model than the old ANOVA model. If our observations of decline end up being greater (synergistic) or less than (antagonistic) what our process model predicted, then we know we are missing something that requires further study. Ultimately this is what a good null model should do, test our predictions and help us identify areas where the theory needs more work.\nSee the report by the Committee on the Assessment of the Cumulative Effects of Anthropogenic Stressors on Marine Mammals (2017) for an excellent example of how we can predict the interaction of two stressors that are of a similar type (e.g. both toxins).\nThe idea of using process models to predict interactions is a subject of ongoing work for me.\nFor instance, see the below figure, where I predict the impacts of trawling and turbidity on seagrass area using a simple process model (left) and the standard additive impacts model used in many maps (right, e.g. Halpern et al. 2013).\n\nThe cumulative impacts model suggests that reducing trawling will have the same benefit for seagrass, regardless of the turbidity. Whereas, the process model ways reducing trawling will have the greatest benefit when turbidity is moderate. This occurs because trawling removes seagrass, whereas turbidity slows seagrass growth. Therefore, seagrass area will be less able to regrow into the empty patches created by trawling when turbidity is higher."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#applying-science-to-the-management-of-multiple-stressors",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#applying-science-to-the-management-of-multiple-stressors",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "Applying science to the management of multiple stressors",
    "text": "Applying science to the management of multiple stressors\nGiven our limited ability to predict how multiple stressors will interact, what actions should managers take in the face of uncertainty? One thing a manager could do is make a pragmatic assumption of a certain type of interaction and use that to inform safe, or risk-averse, decisions.\nParadoxically, a manager who is concerned with conserving a species should assume stressor interactions are synergistic if there are estimating the magnitude of population decline, but antagonistic if they are deciding what to do about the decline. I call this the “paradox of multiple stressor interactions”.\nWhy does this paradox occur?\nWell if you are trying to predict the magnitude of a decline from information about the impacts of single stressors the most risk-averse assumption is that in combination those stressors will have a greater than additive effect.\nHowever, if have observed a population decline in the presence of multiple confounded stressors, assuming their interaction is antagonistic compels you to act on both at the same time. If you were to assume their interaction was synergistic, acting on one stressor may have little benefit for the population if the interaction was in fact antagonistic (Brown et al. 2013).\nWhen one stressor is ‘unstoppable’, such as a stressor related to climate change that a local manager cannot act on, then you need to try act on the other stressor in refuges from the unstoppable one. For instance, restoring water quality on coral reefs in areas that are natural refuges from bleaching and cyclones (Brown et al. 2014).\nSo in practice, what do managers tend to assume currently? My PhD student Laura Griffiths, is currently looking for an answer for this question, and she is reviewing scores of management plans for coastal ecosystems to see how management of multiple stressors is approached in practice. We would be glad to take feed-back and suggestions on this lit review if you have comments.\nMy suspicion is that management plans may often assume ‘additive’ (ie no) interaction among stressors. I base this assumption on the fact that ‘threat mapping’ has now become quite prevalent as tool to inform management. Threat maps typically assume stressors don’t interact, a design decision that was intended to be ‘conservative’. In fact we now know that additive interactions are very optimistic if you are seeking the quantify the benefits of acting on a stressor."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#road-map-for-prioritizing-conservation-actions-when-managing-multiple-stressors",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#road-map-for-prioritizing-conservation-actions-when-managing-multiple-stressors",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "Road map for prioritizing conservation actions when managing multiple stressors",
    "text": "Road map for prioritizing conservation actions when managing multiple stressors\nTo conclude, I wanted to provide a brief road map for prioritizing actions to avert population decline in the face of multiple stressors. This road map is largely based on conservation decision science and particularly the application of cost-effectiveness to prioritizing decisions (see (Game et al. 2013, Brown et al. 2015, Giakoumi et al. 2015)), but with a few twists to account for interactions.\nWe will step through the road-map using a simplified example of Moreton Bay’s dugongs. I have skipped some of the details of conservation decision making, but you can read more in the papers I have mentioned. For simplicity we will consider just two stressors, though the same concepts apply if we have more stresors.\n\nStep 1: Determine the objective and constraints on conservation actions\nPrioritization needs to begin with the objective you are seeking to maximise or minimise. At this point you should also identify constraints. In a conservation context the objective will often be to avoid or reduce the rate of population decline and common constraints are budgetary or social (e.g. closing fishing grounds) constraints. For the dugongs, our objective is to avert population decline in Moreton Bay. An important constraint is the budget we have to spend on management measures.\nIt is important that the objective is precise, because the spatial and temporal extent of our objective will affect which actions can most effectively achieve that objective (Game et al. 2013, see also steps below). For instance, we may take a very different set of actions if our objective is to conserve some dugongs anywhere in Moreton Bay, versus having a high density of dugongs in a particular place.\n\n\nStep 2: Determine actions that will be prioritized\nScientists often talk about prioritizing places, threats or species. But places, species and threats do not consume the resources conservation is constrained by, the actions to protect them do. This has resulted in many misleading conservation plans that prioritize places without saying what you will do there. For the dugongs, the actions are closing areas to boat traffic to protect them from collisions or catchment management measures that will improve water quality in the bay and enhance growth of their key food – seagrass. If there are important unknowns we could also specify actions that are to learn about a threat. For instance, shark fishing may have caused population declines in sharks an important predator of dugongs (e.g. Wirsing et al. 2007)\n\n\nStep 3: Determine the cost of available actions\nFor the actions to protect dugongs, the associated costs are the costs of planting trees (to reduce erosion and improve water quality) and the cost of enforcing areas closed to boating. A more complex and thorough consideration of costs might also consider social and political costs of these actions.\n\n\nStep 4: Determine the benefits of competing actions\nWe want to know what benefit each action brings to our objective. For the dugongs this is simply the improvement in the population growth rate if we implement either or both actions. At this point it is important to consider the potential for interactions among the stressors that may influence the effectiveness of the actions. Ideally we will have a long time-series of dugong population trends in areas with crossed combinations of the different stressors. However, such an abundance of data is unlikely. A more pragmatic approach is therefore to model, either conceptually or quantitatively, the processes by which the stressors affect the population.\nFor instance, earlier I speculated that boat strikes and poor water quality will have synergistic impact on dugong population trends (because hungry dugongs are more likely to encounter boats). We could use a simple GIS model to estimate the likely magnitude of this interaction. For instance, we could quantify whether we thought dugong travel distances under low food conditions, as estimated by animal tracking studies would likely bring dugongs into contact with a greater number of boats. We could then use a demographic model to estimate the impacts of boat strikes on overall population growth.\n\n\nStep 5: Prioritize actions based on their cost effectiveness\nNow we understand the costs, benefits and interactions between different actions we should use a quantitative prioritize to choose the set of actions that maximises dugong population growth. One simple method is to calculate the cost-benefit ratio and use this to choose actions until we have expended all of the budget (e.g. Giakoumi et al. 2015).\nIf we have modelled the interactions correctly, then their importance will be apparent at this stage, without us having to explicitly consider it. For instance, it may be that the serendipitous benefit of conserving seagrass for reducing boat strikes is so great that this is all we need to do to save the dugongs. On the other hand, it may be that the number of boat strikes each year is so great that it outweighs any seagrass food effects, and thus it is essential that we act on boat strikes to achieve any benefits for dugongs.\n\n\nStep 6: Sensitivity of the action plan to interactions\nA final step is to consider the sensitivity of the action plan to uncertainties in this process. Uncertainty could creep in anywhere, but in the context of multiple stressors, this means repeating the prioritization assuming different types of interactions. The optimal set of actions will then be chosen based on the decision makers preference for risk.\nA risk-averse manager will use the sensitivity analysis to identify actions that are always effective regardless of how the stressors interact. For instance, we may want to focus actions that halt boat traffic in regions that have the best existing water quality (this is in fact the management arrangement we have in Moreton Bay), a strategy that will successfully save some dugongs regardless of whether boating and seagrass interact synergistically or antagonistically.\nA risk-seeking manager may want to do the converse – identify which set of actions has the greatest maximal benefit. “Go hard or go home” would be the motto for such a manager. Their objective is to have a pristine dugong population or none at all. Such a manager will throw all their resources at the set of actions that have the greatest potential benefit, but which may also have the greatest risk of failure."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#concluding-statements",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#concluding-statements",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "Concluding statements",
    "text": "Concluding statements\nI have argued that contemporary conservation management must consider the potential for interactions among multiple stressors. Interactions that cause more rapid declines than we might expect paradoxically offer the greatest potential for managers to avert extinctions. We need more science to try to identify generalities in how stressors interact, because we can never hope to study all potential interactions. An important way forward is to incorporate our understanding of stress processes into models that can predict the impact of multiple stressors on populations. Doing so will provide more accurate predictions to managers, and also more useful null models for testing scientific theories. Decision makers faced with managing multiple stressors should consider all potential interactions and make decision that take account of the potential risk of misestimating the benefits of conservation actions."
  },
  {
    "objectID": "posts/2017-10-06-managing-multiple-stressors/index.html#references",
    "href": "posts/2017-10-06-managing-multiple-stressors/index.html#references",
    "title": "Managing for uncertain interactions among multiple stressors of marine populations",
    "section": "References",
    "text": "References\nIf you can’t access one of the pubs cited here, please email me and I will try to get one to you.\nBoyd, P. W. and C. J. Brown. 2015. Modes of interactions between environmental drivers and marine biota. Frontiers in Marine Science 2:9.\nBrown, C. J., M. Bode, O. Venter, M. D. Barnes, J. McGowan, C. A. Runge, J. E. Watson, and H. P. Possingham. 2015. Effective conservation requires clear objectives and prioritizing actions, not places or species. Proceedings of the National Academy of Sciences 112:E4342-E4342.\nBrown, C. J., M. I. Saunders, H. P. Possingham, and A. J. Richardson. 2013. Managing for interactions between local and global stressors of ecosystems. Plos One 8:e65765.\nBrown, C. J., M. I. Saunders, H. P. Possingham, and A. J. Richardson. 2014. Interactions between global and local stressors of ecosystems determine management effectiveness in cumulative impact mapping. Diversity and Distributions 20:538-546.\nCôté, I. M., E. S. Darling, and C. J. Brown. 2016. Interactions among ecosystem stressors and their importance in conservation. Proceedings of the Royal Society Series B, Biological Scinces 283:20152592.\nApproaches to Understanding the Cumulative Effects of Stressors on Marine Mammals. 2017. Committee on the Assessment of the Cumulative Effects of Anthropogenic Stressors on Marine Mammals. National Academies Press, DC\nGame, E. T., P. Kareiva, and H. P. Possingham. 2013. Six common mistakes in conservation priority setting. Conservation Biology 27:480-485.\nGiakoumi, S., C. J. Brown, S. Katsanevakis, M. I. Saunders, and H. P. Possingham. 2015. Using threat maps for cost-effective prioritization of actions to conserve coastal habitats. Marine Policy 61:95-102.\nHalpern, B. S., M. Frazier, J. Potapenko, K. S. Casey, K. Koenig, C. Longo, J. S. Lowndes, R. C. Rockwood, E. R. Selig, and K. A. Selkoe. 2015. Spatial and temporal changes in cumulative human impacts on the world’s ocean. Nature Communications 6.\nHughes, T. P., D. R. Bellwood, C. Folke, R. S. Steneck, and J. Wilson. 2005. New paradigms for supporting the resilience of marine ecosystems. Trends in ecology & evolution 20:380-386.\nMcClenachan, L. and A. B. Cooper. 2008. Extinction rate, historical population structure and ecological role of the Caribbean monk seal. Proceedings of the Royal Society of London B: Biological Sciences 275:1351-1358.\nPiggott, Townsend, Matthaei. 2015. Reconceptualizing synergism and antagonism among multiple stressors. Ecology and Evolution. 5: 1538-1547.\nSheppard JK, Marsh H, Jones RE, Lawler IR. Dugong habitat use in relation to seagrass nutrients, tides, and diel cycles. Marine mammal science. 2010 Oct 1;26(4):855-79.\nTulloch, V. J., É. E. Plagányi, R. Matear, C. J. Brown, and A. J. Richardson. 2017. Ecosystem modelling to quantify the impact of historical whaling on Southern Hemisphere baleen whales. Fish and Fisheries.\nWirsing AJ, Heithaus MR, Dill LM. Fear factor: do dugongs (Dugong dugon) trade food for safety from tiger sharks (Galeocerdo cuvier)?. Oecologia. 2007 Oct 1;153(4):1031-40."
  },
  {
    "objectID": "posts/2016-07-08-rstats-for-conservation/index.html",
    "href": "posts/2016-07-08-rstats-for-conservation/index.html",
    "title": "R programming tools for conservation scientists",
    "section": "",
    "text": "8 July 2016\nThis week I’m at the Society for Conservation Biology, Oceanea Conference in Brisbane. I taught a workshop on Conservation Programming for the workshop, and that has got me thinking about the quantitative tools are most commonly used by conservation scientists. So here is my guide to the packages in R that will let you access the most common tools.\nA good starting place is my post from the International Coral Reef Symposium a few weeks ago. The tools described therein are really useful for most types of data-analysis.\nIt is a shame, but I have seen relatively few new R tools presented at this conference. Jeff Hanson’s package for identifying protected areas is one exception. Given the reliance of conservation science on often sophisticated tools, and our limited budgets for science, it would be nice to see more open-access tools and sharing of tools.\nMark Burgmann emphasised the importance of open science in his plenary. If you share your code, others can build on that and improve it.\nHere I will focus on the more specialised tools that may be used by conservation scientists.\n\n\nNetwork analysis is increasingly popular in conservation science and the main package for network analysis in R is igraph. igraph is a massive package that has numerous algorithms, but to give a few examples, you could use it to:\n\nIdentify the shortest path between two habitat patches - this may represent connectivity corridor.\nCalculate network ‘degree’. This may be useful for instance in social network analysis to find out how many degrees of separation there are between different people.\nIdentify habitat patches that are key stepping stones for species migrations. Here is a great example of using this approach for identifying reserve sites for fish based on larval connectivity.\n\nI haven’t worked much with network data myself, but I image once you collect the data there is a lot of data processing required to ‘make the connections’. I recommend dplyr and tidyr for this processing, though I would be keen to hear what other tools people have used for processing network data.\n\n\n\nMuch of conservation science uses spatial data - whether it is about selecting sites for protection, mapping threats to ecosystems, or remote sensing of ecosystem extent. R has a huge number of spatial packages but probably the easiest place to start is raster package, which is used for woking with gridded data.\nCheck out the introductory course on my webpage to get started.\nOne nice thing about raster package is that the help files are really well written and organised. That is why I recommend it as the best starting point for spatial analysis in R.\n\n\n\nOnce you have mastered gridded spatial data you may want to work with points, lines and polygons. The difference with raster data is that points, lines and polygons are recorded on a continuous spatial surface, rather than a grid. There is a huge range of applications for this kind of data, too many to list here. But a few examples I have seen in talks are mapping animal tracks (lines), mapping species occurrences (points), mapping protected areas (polygons) and looking at overlap between species ranges (points or polygons) and protected areas.\nThere are three packages you need to know to get started with spatially continuous data. They are:\n\nsp which is primarily used to describe spatial data frames. E.g. it provides data structures for associating individual points, lines and polygons with data, like animal IDs, areas or status.\nrgdal which is a portal to the GDAL framework for transforming projections. rgdal also lets you import ARC GIS datafiles into R with the function readOGR().\nrgeos is used for geographic operations like intersecting polygons, calculating areas and so on. These packages take a fair bit of practice to learn, and unfortunately there is not that much help available at the introductory level. I recommend getting a good working knowledge of R programming before you start with them. Also try my introductory course on spatial analysis once you are familiar with R.\n\n\n\n\nThe most fun you can have in R, I think, is with leaflet package. You can make your interactive web maps of your own data, like these in my Rstats course for conservation science. Check out the leaflet page for more info.\n\n\n\nMany sub-fields of conservation science use optimisation to identify conservation priorities, such as what actions can be best spend our limited budget on to protect marine ecosystems.\nI actually don’t think R is the best programming language for optimisation (the last time I had to optimise actions for conservation I used the Julia language), however there are a huge range of tools available. The best tools will be written in another language (like C) and have an R portal to operate them. The reason is that R is relatively slow to process calculations, and optimisation often relies on numerical solvers.\nR is, however, a good place to start learning about optimisation if you are already familiar with the language. Check out the task view on optimisation to see what tools are available.\nA good place to start is Iadine Chades and colleague’s Markov decision toolbox. The toolbox let’s you optimise what actions you would take over time to acheive some goal, say maximise the abundance of a threatened species.\n\n\n\nOddly enough, I haven’t seen that much population modelling at this conference. Maybe I was in the wrong sessions, or maybe there just isn’t that much anymore. Spatial analysis seems to dominate these days. Population modelling is useful for projecting extinction risk and quantifying the effectiveness of alternate management strategies, among many other applications.\nThere are many R packages for population modelling, but check out IPMpack to get started with demographic modelling.\n\n\n\nThe R program is an incredibly useful tool for conservation science. It has a huge range of applications and importantly, most R packages are open source - meaning anyone can download and us them or build on them to create new packages.\nThey were my top 6, but if you have seen other interesting tools that are of use for conservation science, let me know."
  },
  {
    "objectID": "posts/2016-07-08-rstats-for-conservation/index.html#r-programming-tools-for-conservation-scientists",
    "href": "posts/2016-07-08-rstats-for-conservation/index.html#r-programming-tools-for-conservation-scientists",
    "title": "R programming tools for conservation scientists",
    "section": "",
    "text": "8 July 2016\nThis week I’m at the Society for Conservation Biology, Oceanea Conference in Brisbane. I taught a workshop on Conservation Programming for the workshop, and that has got me thinking about the quantitative tools are most commonly used by conservation scientists. So here is my guide to the packages in R that will let you access the most common tools.\nA good starting place is my post from the International Coral Reef Symposium a few weeks ago. The tools described therein are really useful for most types of data-analysis.\nIt is a shame, but I have seen relatively few new R tools presented at this conference. Jeff Hanson’s package for identifying protected areas is one exception. Given the reliance of conservation science on often sophisticated tools, and our limited budgets for science, it would be nice to see more open-access tools and sharing of tools.\nMark Burgmann emphasised the importance of open science in his plenary. If you share your code, others can build on that and improve it.\nHere I will focus on the more specialised tools that may be used by conservation scientists.\n\n\nNetwork analysis is increasingly popular in conservation science and the main package for network analysis in R is igraph. igraph is a massive package that has numerous algorithms, but to give a few examples, you could use it to:\n\nIdentify the shortest path between two habitat patches - this may represent connectivity corridor.\nCalculate network ‘degree’. This may be useful for instance in social network analysis to find out how many degrees of separation there are between different people.\nIdentify habitat patches that are key stepping stones for species migrations. Here is a great example of using this approach for identifying reserve sites for fish based on larval connectivity.\n\nI haven’t worked much with network data myself, but I image once you collect the data there is a lot of data processing required to ‘make the connections’. I recommend dplyr and tidyr for this processing, though I would be keen to hear what other tools people have used for processing network data.\n\n\n\nMuch of conservation science uses spatial data - whether it is about selecting sites for protection, mapping threats to ecosystems, or remote sensing of ecosystem extent. R has a huge number of spatial packages but probably the easiest place to start is raster package, which is used for woking with gridded data.\nCheck out the introductory course on my webpage to get started.\nOne nice thing about raster package is that the help files are really well written and organised. That is why I recommend it as the best starting point for spatial analysis in R.\n\n\n\nOnce you have mastered gridded spatial data you may want to work with points, lines and polygons. The difference with raster data is that points, lines and polygons are recorded on a continuous spatial surface, rather than a grid. There is a huge range of applications for this kind of data, too many to list here. But a few examples I have seen in talks are mapping animal tracks (lines), mapping species occurrences (points), mapping protected areas (polygons) and looking at overlap between species ranges (points or polygons) and protected areas.\nThere are three packages you need to know to get started with spatially continuous data. They are:\n\nsp which is primarily used to describe spatial data frames. E.g. it provides data structures for associating individual points, lines and polygons with data, like animal IDs, areas or status.\nrgdal which is a portal to the GDAL framework for transforming projections. rgdal also lets you import ARC GIS datafiles into R with the function readOGR().\nrgeos is used for geographic operations like intersecting polygons, calculating areas and so on. These packages take a fair bit of practice to learn, and unfortunately there is not that much help available at the introductory level. I recommend getting a good working knowledge of R programming before you start with them. Also try my introductory course on spatial analysis once you are familiar with R.\n\n\n\n\nThe most fun you can have in R, I think, is with leaflet package. You can make your interactive web maps of your own data, like these in my Rstats course for conservation science. Check out the leaflet page for more info.\n\n\n\nMany sub-fields of conservation science use optimisation to identify conservation priorities, such as what actions can be best spend our limited budget on to protect marine ecosystems.\nI actually don’t think R is the best programming language for optimisation (the last time I had to optimise actions for conservation I used the Julia language), however there are a huge range of tools available. The best tools will be written in another language (like C) and have an R portal to operate them. The reason is that R is relatively slow to process calculations, and optimisation often relies on numerical solvers.\nR is, however, a good place to start learning about optimisation if you are already familiar with the language. Check out the task view on optimisation to see what tools are available.\nA good place to start is Iadine Chades and colleague’s Markov decision toolbox. The toolbox let’s you optimise what actions you would take over time to acheive some goal, say maximise the abundance of a threatened species.\n\n\n\nOddly enough, I haven’t seen that much population modelling at this conference. Maybe I was in the wrong sessions, or maybe there just isn’t that much anymore. Spatial analysis seems to dominate these days. Population modelling is useful for projecting extinction risk and quantifying the effectiveness of alternate management strategies, among many other applications.\nThere are many R packages for population modelling, but check out IPMpack to get started with demographic modelling.\n\n\n\nThe R program is an incredibly useful tool for conservation science. It has a huge range of applications and importantly, most R packages are open source - meaning anyone can download and us them or build on them to create new packages.\nThey were my top 6, but if you have seen other interesting tools that are of use for conservation science, let me know."
  },
  {
    "objectID": "posts/2025-09-18-ai-r-workshops/index.html",
    "href": "posts/2025-09-18-ai-r-workshops/index.html",
    "title": "Generative AI and R workshops in Hobart Australia",
    "section": "",
    "text": "In person Hobart Australia 11th and 12th November 2025\nCourse page and further details"
  },
  {
    "objectID": "posts/2025-09-18-ai-r-workshops/index.html#registration",
    "href": "posts/2025-09-18-ai-r-workshops/index.html#registration",
    "title": "Generative AI and R workshops in Hobart Australia",
    "section": "Registration",
    "text": "Registration\nThe R program is a powerful tool for data analysis, but has a steep learning curve. Join us for a 2-day workshop that will accelerate your R skills. Our modern approach to teaching leverages AI to help you create analyses with natural language, and code with fewer errors, but higher accuracy.\nPresenter: A/Prof Chris J Brown c.j.brown@utas.edu.au Guest presenter: Professor Anthony Richardson, The University of Queensland and CSIRO Environment\nCost per day: $200 students, $300 professionals. Waiver available for indigenous scholars. Register for 1 or 2 days."
  },
  {
    "objectID": "posts/2025-09-18-ai-r-workshops/index.html#day-1-11th-nov-quality-data-analysis-with-ai-coding-assistants",
    "href": "posts/2025-09-18-ai-r-workshops/index.html#day-1-11th-nov-quality-data-analysis-with-ai-coding-assistants",
    "title": "Generative AI and R workshops in Hobart Australia",
    "section": "Day 1 (11th Nov): Quality data analysis with AI coding assistants",
    "text": "Day 1 (11th Nov): Quality data analysis with AI coding assistants\nSuitable for R or Python users**. We’ll show you how to use large language models (like chatGPT) directly in your R or Python coding environment. You can accelerate your coding and get personalised help on your research. But there are risks, AI makes mistakes. We’ve developed guidelines to help you get the most out of AI for coding. We’ll also discuss the ethical implications of using AI to assist with research and whether ‘vibe coding’ is appropriate for scientists. This workshop is suitable for people with limited coding experience (e.g. can read data into Rstudio and make plots) as well as experts who want to know more about AI assistants."
  },
  {
    "objectID": "posts/2025-09-18-ai-r-workshops/index.html#day-2-12th-nov-generalized-linear-models-in-r",
    "href": "posts/2025-09-18-ai-r-workshops/index.html#day-2-12th-nov-generalized-linear-models-in-r",
    "title": "Generative AI and R workshops in Hobart Australia",
    "section": "Day 2 (12th Nov): Generalized linear models (in R**)",
    "text": "Day 2 (12th Nov): Generalized linear models (in R**)\nGLMs are a powerful tool for statistical analysis and prediction. We’ll teach the basics of the linear model and how it can be extended to all sorts of data in the generalized linear model. We’ll cover model verification, estimating effect size and significance and making predictions. The beauty of GLMs is that you only need some basic principles and you are ready to tackle many types of data, not like the old days when you had to know a different test for each new type of data.\n\nThere may be additional subscription costs for AI software of up to USD20. Students can get free licenses. I will email instructions for getting software licenses prior to the workshops.\n\n**Note these workshops are not suitable for coding beginners. If you’ve never used R or Python before we suggest you take an introductory workshop. IMAS runs one, feel free to contact us for more info on that. As a rule of thumb, if you have Rstudio, know how to read data into R and make some plots you are ready for these workshops."
  },
  {
    "objectID": "posts/2017-08-08-seascapemodelsv2/index.html",
    "href": "posts/2017-08-08-seascapemodelsv2/index.html",
    "title": "seascapemodels.org version 2",
    "section": "",
    "text": "I just (8-8-2017) updated my webpage to version 2 styling. If something is missing that you use, please let me know.\nHope you enjoy the new page."
  },
  {
    "objectID": "posts/2015-07-20-Atom_R_Snippets/2015-07-20-Atom_R_Snippets.html",
    "href": "posts/2015-07-20-Atom_R_Snippets/2015-07-20-Atom_R_Snippets.html",
    "title": "A fast start to R scripts",
    "section": "",
    "text": "A fast start to R scripts\n\n\nI recently heard about the Atom text editor. It has been designed for coders by coders. The idea is to have a text editor that works on any OS and is hackable. That means you can customise any of its features, or write your own features. It acheives OS independence by using web-technology, i.e. Javascript and CSS. If you know these it is easy start hacking Atom, if not, they are not that hard to learn (certainly easier than R!). Regardless, you don’t need to know any programming language to use Atom. I am now trying it out for R programming.\n\n  &lt;p&gt;\n     In the past I have primarily used the R inbuilt scripter for writing\n     code and RStudio for teaching. My hope is that Atom will make life\n     easier and will also be flexibile, but easy enough to use for teaching.\n  &lt;/p&gt;\n\nTo help convince you about why switching to Atom is a good idea, here is an example of one of its cool features:  Snippets\n\n\n\n\n\nAtom Snippets allow you to type a shortcut and hit tab, and it will fill in the rest for you. One way to use Snippets would be to set up Snippets for common program structures, to save you time in re-writing them every time you start a new project\n\n\nOne of the most common things to do in R is to import data and manipulate it into a usable form. So here is a step by step guide for how to set up such a snippet in Atom\n\n\nStep 1: Install Atom\n\n\nAtom is available here\n\n\nStep 2: Open Your Snippets\n\n\nGot to the ‘Atom’ menu and select ‘Open your snippets’\n\n\nStep 3: Write your snippet\n\n\nSnippet code uses the .cson (CoffeeScript Object Notation) structure to delineate commands and code. Here is an example to create a common R script:\n\n &lt;pre&gt;&lt;code&gt;'.source.r':\n  'R data import':\n    'prefix': 'R_data_import'\n    'body': \"\"\"\n    rm(list = ls())\n    setwd('')\n    library(dplyr)\n    library(tidyr)\n    dat &lt;- read.csv('.csv', header = TRUE)\n    head(dat)\n    \"\"\"&lt;/code&gt; &lt;/pre&gt;\n\nThe first line is the file type, the second line is the name, the third line is the shortcut name and the fourth line ‘body’ is the snippet. You can enclose multiline snippets in ‘“““’ as I have done here.\n\n\nNow, create a new file and save it with the ‘.r’ (lower case is important) extension. When you start writing your file type ‘R_data_’ and a drop down box should appear. Scroll down and select ‘R_data_import’ and hit tab. It should auto-fill your script and you are ready to begin with some data wrangling.\n\n\nIf you want to evalute code you write in Atom, check out Atoms ‘eval-r’ package (OSX only)."
  },
  {
    "objectID": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html",
    "href": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html",
    "title": "How to get leadership experience in your PhD",
    "section": "",
    "text": "Many modern PhD students expect to learn leadership skills in their PhD studies. In a previous post I discussed why leadership skills are important for successful careers. Here I look at what students can do to further their leadership experience and how supervisors and institutions can help.\nIf you missed the past post I defined leadership generally as:\nLeaders create opportunities that let them reach their goals.\nSo this might include leading other people, but could also include being a scientific leader in your discipline."
  },
  {
    "objectID": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#creating-your-own-opportunities-as-a-student",
    "href": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#creating-your-own-opportunities-as-a-student",
    "title": "How to get leadership experience in your PhD",
    "section": "Creating your own opportunities as a student",
    "text": "Creating your own opportunities as a student\nA key action you can take in your PhD is to create your own opportunities to practice leadership.\nA sense of purpose (goal setting) and the ability to understand diverse perspectives are important aspects of leadership. Many students start with only a vague idea of where they want to go in their career and limited understanding of how other people can help (or hinder) that goal.\nSo developing leadership means exposing yourself to situations where you must:\n\nHave a sense of purpose\nUnderstand other people’s motives and find ways to work with them, or change them, to help you reach your goals\n\nMost PhDs cover these aspects. For instance, researchers are usually motivated by a sense of purpose, such as helping the environment. Most research also requires working with and around other people.\nBut I think supervisors and universities can do more to meet the demand for developing leadership skills in PhD students."
  },
  {
    "objectID": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#where-to-start-developing-as-a-leader",
    "href": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#where-to-start-developing-as-a-leader",
    "title": "How to get leadership experience in your PhD",
    "section": "Where to start developing as a leader",
    "text": "Where to start developing as a leader\nIt can be hard to get formal leadership experience early on in your career (for instance, you typically won’t have the finding to hire someone and supervise them yourself). But, examples of demonstrating leadership can make your CV stand out from the crowd.\nA key role for me in my postdoc days was volunteering to teach courses and run workshops in the R programming language, which is my own research tool of choice.\nMy purpose was to further other people’s education and improve the quality of statistical analysis.\nObviously working with other people is a key part of teaching. But to see people successfully change their habitats to use R consistently, I had to learn to step out of my own perspective of ‘R is great, why wouldn’t you use it?’ and start to try and understand why other people might struggle with R e.g. as in this post.\nTaking on other people’s perspectives is a key step in developing as a leader."
  },
  {
    "objectID": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#ideas-for-getting-leadership-experience",
    "href": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#ideas-for-getting-leadership-experience",
    "title": "How to get leadership experience in your PhD",
    "section": "Ideas for getting leadership experience",
    "text": "Ideas for getting leadership experience\nBe creative about exposing yourself to leadership opportunities. Some ideas from the people around me:\n\nRecruit and supervise undergrad students looking for volunteer research experience.\nVolunteer to run organise group activities, like student social clubs or journal club.\nBecome a representative for your student or postdoc society.\nVolunteer for professional societies, such as helping to organise conferences or workshops.\n\nTry to find roles that are close to your own career goals.\nPicking an activity that fits with your career goals will help further develop your sense of purpose, and creates a more coherent story for you to sell yourself with on job and grant applications.\nThe R program is my research tool of choice, so it just makes sense that my leadership was around training others to use R."
  },
  {
    "objectID": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#how-can-universities-and-supervisors-help-develop-leaders",
    "href": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#how-can-universities-and-supervisors-help-develop-leaders",
    "title": "How to get leadership experience in your PhD",
    "section": "How can universities and supervisors help develop leaders?",
    "text": "How can universities and supervisors help develop leaders?\nIf you are looking to start a PhD, then do your background research on the supervisors and the university’s culture. If leadership is important to you, ask potential supervisors how they help leaders grow. Ask them if they support students to take on other roles that help develop leadership, like volunteering for societies. And look at what their grads have gone on to do.\nMany well established and renown labs will offer more traditional training, which will emphasise training in a scientific field over developing more diverse skills in leadership.\nSome highly productive labs create a lot of research that fits nicely with the supervisor’s own research goals, but are not necessarily training students who will be leaders in a topic of their own choosing.\nI think that an important part of developing leaders is helping students develop their own research questions, which should stem from what intrigues them. So ask prospective supervisors some questions to see how far they would be willing to support you in developing your own research interests."
  },
  {
    "objectID": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#borrowing-from-the-business-world",
    "href": "posts/2019-01-05-how-to-learn-leadership-in-phd/index.html#borrowing-from-the-business-world",
    "title": "How to get leadership experience in your PhD",
    "section": "Borrowing from the business world",
    "text": "Borrowing from the business world\nFor me, I like to find new ways I can expose my students and postdocs to leadership experiences. This can often fit quiet naturally with the PhD research. For instance, organising, running and leading workshops with other scientists or stakeholders.\nI also think that academics can borrowing techniques from the business coaching world to help students grow as leaders. This can be as simple as broadening the scope of supervisor-student discussions.\nOne technique is to discuss with a student where their goals ultimately come from: from within themselves or were they handed to them by other people? (a technique from growth edge coaching).\nBringing more diverse ideas about training into the PhD experience will enhance the value of a PhD in the modern job market, no matter whether the student goes onto an academic career or otherwise."
  },
  {
    "objectID": "posts/2017-10-06-making-a-difference/index.html",
    "href": "posts/2017-10-06-making-a-difference/index.html",
    "title": "Frustration and success when trying to make a difference",
    "section": "",
    "text": "Perhaps a stereotype is the young scientist who has graduated from university and is setting out in the world to make a difference. In our field they will be typically concerned with preventing some type of ecological degradation.\nThe stereotype continues that the enthusiastic young person soon becomes old and jaded as they learn to realise that many of the efforts of scientists to communicate impending disaster go unnoticed.\nFinally the once enthusiastic environmental scientist quits and goes off the find ‘a real job’ in banking or perhaps as an accountant.\nMany of us are studying environmental science because we have noticed the many injustices humanity has wrought on nature and we want to do something about it. But ‘doing something about it’ can be much harder than we often think.\nI just attended the MASTS conference in Scotland and there was a lot of talk about the science policy interaction. Some people I met were enthusiastic and hopeful, others were pessimistic about the future of ocean ecosystems. Other expressed dismay at the apparent lack of consideration of evidence by policy makers.\nI think that when I started my career I naively thought making a difference will follow quite a linear path:\n\nDo some science\nTell policy makers about said science\nPolicy makers update policy and laws based on your evidence\nJob done\n\nOn reflection, the road to making a difference is often longer and more indirect than I might have initially hoped. However, having walked that road and watched others do it for about 10 years now, I have seen many successes and many frustrations too.\nProf Peter Tyack, a marine mammal biologist related a story to me about naval sonar excercises causing strandings of beaked whales. The issue was first recognised in about 2000, and it took about 10 years before they were able to get a change in how naval excercises were conducted. But that 10 years of research and engagement with the navy did result in some very positive changes to their operations, which now avoid most the activities that risk causing strandings.\nSo the road to impact often takes longer than you might hope. But there may be unexpected benefits along the way. For instance, the whale researchers have a learned alot about whale echo-location and human acoustic impacts during the research they used to help modify navy operations. This is valuable information as they now confront issues caused by offshore energy development.\nSo if you are on this path to making a difference, or about to start on it, treat it as a learning experience. You might not have an impact in the way you initially hoped, but in trying to do so you will learn better ways for next time.\nI think there are some things we can do in our careers that will help us in your quest to make a difference.\nExperiment with different courses of influence. See what works for you. Here are some ways you can use a career in science make a difference, there are many more:\n\nWork with government staff to inform their policies/legislation\nEngage with the public media on issues and comment as an expert\nWork with environmental NGOs to inform how they spend their funds\nWrite a letter to your local politician about an issue\nReview environmental policies\nContribute to calls for public comment\nWrite a critique of government policy\nDonate what you can to environmental causes and maybe even try to get a better paid job so you can donate more (seriously, some people see this as the best way to make a difference)\n\nNone of these courses is direct, but all can have impact. The more often and more immediately you can start experimenting with many different paths for making a difference, the sooner you will get good at it.\nBuild trust and integrity with policy/decision makers. A good point that one of the presenters at the conference made. They also noted we often focus now on learning good communication skills - which are also important. However, working with policy makers also requires them to trust you. Building trust and integrity takes time and long-term relationships. So the sooner you start working with decision makers the better.\nSome ways to build trust and integrity are:\n\nGain the esteem of other scientists. Having other people say you are good helps build you integrity. Yes, in the science world the best assured way to get the esteem of your colleagues is to publish, publish, publish.\nGet a relevant qualification (like a PhD). Also, make sure you let people know you have it. You can do this subtly, like putting it in your email signature.\nWrite a book about an issue. Papers are good, but books establish that you have a broader or greater knowledge of a topic are often more accessible to non-scientists.\nBe in the public eye and provide incisive and reliable commentry. Like getting engaged with the media, as above\nJoin a professional society, like the Society for Conservation Biology\n\nFinally, one of the most helpful things you can do is make your science policy relevant. A good example came up today of experimental studies that look at the impacts of multiple toxins on organisms.\nSuch work often uses unrealistically extreme levels of toxins, to make sure they find some effect if it is really there (and have something to publish). But such extreme experiments are not that helpful for setting water quality guidelines. If you could also test toxicity at environmentally relevant levels, say at existing legislated thresholds for what constitutes pollution, then you might have some results that can more directly inform policy (maybe they should reduce the threshold!).\nThis is in fact what Tyack did with this work on how sound impacts whales. They determined the db above which whales responded. Thus, the navy could set limits for how loud their equipment could be when there were whales around.\nSo if I am ever lucky enough to see a beaked whale, I will remember I may well have have marine mammal scientists to thank that they are still around. Knowing the work you do has helped the environment is the fuel you need for persisting in a career in science."
  },
  {
    "objectID": "posts/2016-07-25-write-like-a-runner/index.html",
    "href": "posts/2016-07-25-write-like-a-runner/index.html",
    "title": "Write like a runner trains",
    "section": "",
    "text": "Improve your writing by practising like a runner trains\nWhenever I complain about about having to edit some writing, my wife likes to remind me that my skill at writing has improved a lot in the past few years. Not that I am great at writing now, what she is saying (in a nice way) is that my writing really sucked a few years ago. My PhD advisors were largely unavailable in the months leading up to submission of my thesis and I was lucky to have her help with a lot of the editing.\n\n\n\nVariety is key to improving your running, and writing\n\n\nWhy has my writing improved from terrible to passable? Well you could put it down to the practice I have had smacking the keyboard since graduating, having written more papers and grants than during my PhD. I also write more regularly now (every day). You need to write regularly to get better, just like you need to run regularly to get fit. Many professionals recommend writing every day, usually in your best working hours, to maximise efficiency.\nWhile regular writing has certainly improved my productivity, I don’t think it’s the whole reason I have improved my writing skill. In fact, if all I did was write papers, my writing would probably be much the same as it was a few years ago.\nI think the reason I have improved at writing is that I have been writing like a runner trains. Writing like a runner wasn’t a deliberate choice, I stumbled on the approach by accident. But now I know it works I advise students to do the same. So here’s the logic.\nIf you are training to run a mid-distance race, like 10kms, you shouldn’t just go out and run 10ks every second day. If you run long distances slowly as training then you will run the race slowly too.\nLooking back at my times in parkrun, a weekly running event I attend semi-periodically, I don’t see any improvement. Probably because I usually just go out for long runs at a comfortable pace and never practice sprinting.\nA better training schedule is to break up some longer distance runs with interval sprint training. Say every second training session you do sprints in intervals. Then you develop both the endurance required for longer events and a faster pace.\nSo how does running training apply to improving your writing?\nWell I think writing papers is analogous to long distance endurance running. It usually takes a long time, you slog over accurately putting methods and findings into words, carefully explaining the study’s implications and ensuring you have used all the appropriate references.\nMost of the writing practice we get in our PhDs is doing the long slow writing of papers and our thesis. That is like only running long-distances. You need to do some sprint writing too.\nSprint writing is one reason I blog. Blogging is fast paced writing. You have an idea, you write it down, edit it once and post it. That’s it. No slogging over finding references, or carefully rewording sentences to get perfect flow (maybe it shows here?).\nAnother way to think about diversity in training your writing is to think about what ‘better writing means’. You can think about better writing in broadly two ways: First, better could mean writing of a higher quality, such as more accurately describing your findings, or creating a more cohesive story. Or you could also be better at writing by writing at the same quality, but doing so faster. To learn to write faster, you need to practice writing faster! Now, I wouldn’t generally advise trying to write papers faster, at least initially. If you make mistakes in papers, that could hurt your reputation. So you need to find another outlet. Blogging is one way to have an outlet for your writing.\nThe second thing I did to improve my writing since graduating was seek professional help. Good runners employ trainers, so why shouldn’t we too. After all, as Joshua Schimel has said, scientists are professional writers. Well I didn’t employ anyone exactly, but I did read Schimel’s book and practice his exercises. I would say there was almost an overnight improvement in my writing (I think my wife agrees).\nAnother problem with trying to improve your writing by writing papers alone is that feedback is typically slow and far between. Psychology tells us that the easiest skills to learn are those were we get instant feedback. Like running or dancing with a trainer who videos you and then critiques your style as you watch the playback.\nIn our group we have recently initiated weekly get togethers to critique each others writing. The idea is to get more frequent feedback on new writing without having to wait to make it through co-author’s inboxes. The sessions have been very popular and I have found, very helpful.\nIncidentally, runners who have a diverse training schedule are also less prone to injury. I speculate that there is an analogue in academic writing. If you do more varied styles of writing you are less likely to burn-out on writing and give up altogether.\nI actually thought of this blog while I was out on a long slow run this afternoon. The idea inspired and I threw a few sprints into my run. Hopefully I can bring this idea full circle and improve my running times too."
  },
  {
    "objectID": "posts/2017-08-04-conservationoptimism/index.html",
    "href": "posts/2017-08-04-conservationoptimism/index.html",
    "title": "Impact of the conservation optimism hashtag",
    "section": "",
    "text": "The hashtag #conservationoptimism became popular during the recent International Congress for Conservation Biology symposium. Michael Burgass asked me what its twitter impact was, so here is a quick analysis.\nNote that I edited this post (24/08/2017) after Michael Burgass asked me why so few tweets. It turns out (and I hadn’t realised) that the twitter API (Application programming interface) will only return tweets up to 2 weeks old. You have to pay to get older ones. So the data in this post is only relevant to the period over the ICCB conference.\nMichael also asked about #iamaconservationist, but that has too few tweets so far to make for a meaningful analysis (about 100).\nThere were not a huge amount of of tweets over ICCB conference, so it is hard to say too much from a broad brush quantitative analysis like this, but here are a few stats.\nThe first tweet I got was only on 2017-07-26 and until now there have been 813 tweets from 390 users of which 133 are original tweets and the rest RTs.\n\nI think that spike on Wednesday corresponds to EJ Milner-Gulland’s plenary at ICCB.\nThe distribution of tweets per user is not quite as skewed as other hashtags I have analysed. I guess becaues there are fewer users and it is the sort of hashtag that people only post once to (thought they may RT multiple others).\n\n\n\nHere is a table of the most popular tweets.\n\n\n\n\n\n\n\n\nHandle\nTweet\n# retweets\n\n\n\n\nMorenaMills\n.@EJMilnerGulland inspiring action and optimism at #ICCB2017 #conservationoptimism #IAmAConservationist. More info: https://t.co/FrNtVl8U1q https://t.co/jjE9fBeMFw\n49\n\n\nsuewalker2008\nPost Doc opportunity on zebras in Africa! Please share widely 😊 #ICCB2018 #conservationoptimism https://t.co/gyXziTWinq\n34\n\n\nICCB2017\nKeep the #conservationoptimism going! Thank you @EJMilnerGulland for a fantastic message of hope! #ICCB2017 #iamaconservationist https://t.co/i4g6Q5S4TB\n20\n\n\nabigailrdan\nExciting PhD opportunity in Australia for promising marine scientists who want to channel #conservationoptimism @LAFFellows @sfgucsb https://t.co/K9astyMqH4\n17\n\n\n\n\n\n\nThe most popular place is Africa (probably because of that job tweet above), the most popular user EJ Milner-Gulland. Other popular words include inspire, great, share, future.\n\n\n\nTerm\n# of tweets\n\n\n\n\niccb\n398\n\n\nejmilnergulland\n256\n\n\niamaconservationist\n176\n\n\ninspir\n98\n\n\ngreat\n96\n\n\nconserv\n89\n\n\nshare\n65\n\n\nconservoptimism\n62\n\n\noptim\n60\n\n\ncheck\n59\n\n\naction\n54\n\n\nopportun\n53\n\n\ninfo\n51\n\n\ntime\n51\n\n\ngive\n50\n\n\nmorenamills\n50\n\n\nplenari\n47\n\n\nwant\n47\n\n\nwatch\n43\n\n\nstori\n41\n\n\nwork\n40\n\n\npost\n39\n\n\nlearn\n38\n\n\nafrica\n37\n\n\nthank\n37\n\n\npleas\n36\n\n\ndoc\n35\n\n\nfutur\n35\n\n\ntcogyxzitwinq\n35\n\n\nwide\n35\n\n\n\nThat’s all. It would be good to do a full historical analysis, but as I note above you probably have to pay for that."
  },
  {
    "objectID": "posts/2017-08-04-conservationoptimism/index.html#impact-of-the-conservation-optimism-hashtag",
    "href": "posts/2017-08-04-conservationoptimism/index.html#impact-of-the-conservation-optimism-hashtag",
    "title": "Impact of the conservation optimism hashtag",
    "section": "",
    "text": "The hashtag #conservationoptimism became popular during the recent International Congress for Conservation Biology symposium. Michael Burgass asked me what its twitter impact was, so here is a quick analysis.\nNote that I edited this post (24/08/2017) after Michael Burgass asked me why so few tweets. It turns out (and I hadn’t realised) that the twitter API (Application programming interface) will only return tweets up to 2 weeks old. You have to pay to get older ones. So the data in this post is only relevant to the period over the ICCB conference.\nMichael also asked about #iamaconservationist, but that has too few tweets so far to make for a meaningful analysis (about 100).\nThere were not a huge amount of of tweets over ICCB conference, so it is hard to say too much from a broad brush quantitative analysis like this, but here are a few stats.\nThe first tweet I got was only on 2017-07-26 and until now there have been 813 tweets from 390 users of which 133 are original tweets and the rest RTs.\n\nI think that spike on Wednesday corresponds to EJ Milner-Gulland’s plenary at ICCB.\nThe distribution of tweets per user is not quite as skewed as other hashtags I have analysed. I guess becaues there are fewer users and it is the sort of hashtag that people only post once to (thought they may RT multiple others).\n\n\n\nHere is a table of the most popular tweets.\n\n\n\n\n\n\n\n\nHandle\nTweet\n# retweets\n\n\n\n\nMorenaMills\n.@EJMilnerGulland inspiring action and optimism at #ICCB2017 #conservationoptimism #IAmAConservationist. More info: https://t.co/FrNtVl8U1q https://t.co/jjE9fBeMFw\n49\n\n\nsuewalker2008\nPost Doc opportunity on zebras in Africa! Please share widely 😊 #ICCB2018 #conservationoptimism https://t.co/gyXziTWinq\n34\n\n\nICCB2017\nKeep the #conservationoptimism going! Thank you @EJMilnerGulland for a fantastic message of hope! #ICCB2017 #iamaconservationist https://t.co/i4g6Q5S4TB\n20\n\n\nabigailrdan\nExciting PhD opportunity in Australia for promising marine scientists who want to channel #conservationoptimism @LAFFellows @sfgucsb https://t.co/K9astyMqH4\n17\n\n\n\n\n\n\nThe most popular place is Africa (probably because of that job tweet above), the most popular user EJ Milner-Gulland. Other popular words include inspire, great, share, future.\n\n\n\nTerm\n# of tweets\n\n\n\n\niccb\n398\n\n\nejmilnergulland\n256\n\n\niamaconservationist\n176\n\n\ninspir\n98\n\n\ngreat\n96\n\n\nconserv\n89\n\n\nshare\n65\n\n\nconservoptimism\n62\n\n\noptim\n60\n\n\ncheck\n59\n\n\naction\n54\n\n\nopportun\n53\n\n\ninfo\n51\n\n\ntime\n51\n\n\ngive\n50\n\n\nmorenamills\n50\n\n\nplenari\n47\n\n\nwant\n47\n\n\nwatch\n43\n\n\nstori\n41\n\n\nwork\n40\n\n\npost\n39\n\n\nlearn\n38\n\n\nafrica\n37\n\n\nthank\n37\n\n\npleas\n36\n\n\ndoc\n35\n\n\nfutur\n35\n\n\ntcogyxzitwinq\n35\n\n\nwide\n35\n\n\n\nThat’s all. It would be good to do a full historical analysis, but as I note above you probably have to pay for that."
  },
  {
    "objectID": "posts/2017-10-04-the-coal-state/index.html",
    "href": "posts/2017-10-04-the-coal-state/index.html",
    "title": "Queenland The Coal State",
    "section": "",
    "text": "I’m over here in Scotland at their National marine science and technology conference (‘MASTS’). There are a lot of talks about the ecological issues associated with green energy derived from wind, waves and tides.\n\nSeeing these environmental issues crop with green energy made me think how lucky the Australian environment is that we are focussing our efforts on deriving and exporting energy created with good old carbon.\nQueensland is leading the way in keeping the coal tradition going, with new coal mines that have recently been criticised by the less enlightened.\nQueenslanders should call our state “The Coal State” not “The Sunshine State” like we currently do. What’s sunshine ever done for us? You can’t run much more than a few web servers with it, and its proven to give you cancer.\nCoal rarely makes people ill, a single power plant causing only about 18 deaths a year in Victoria. Coal also provides stacks of cheap energy.\nA rogue group of scientists believe that climate change is caused by greenhouse gas emissions, such as all the CO2 that comes from burning coal. This extremist group (something like 97% of scientists)) will probably also tell you that climate change has dire consequences for humanity and life on earth.\nWell, even if that is true, the global emissions of carbon are now so huge that Aussie coal mining doesn’t really matter. One mine in Queensland will only contribute about half a percent of the global emissions we have left before the Earth global warming will exceed 2 degrees. 0.5% from just one mine is pretty minor when you consider Australia’s human population is only about 0.33% of the global population.\nWe are also planting heaps of trees in Australia. If you didn’t know it, trees actually suck carbon out of the atmosphere. So the trees we plant are helping to offsett emissions we create from doing things like clearing native forests for farming and burning coal.\nAnyway, the heroin we produce gets sold overseas to countries like India who use it, so it is really their problem. Sorry did I say heroin? I meant coal.\nThose impoverished people in India really need our coal. Having power will raise their standards of living and maybe even their life expectancies. The dirty coal we sell is so cheap everyone can afford it, and the health affects of living near power plants that burn the stuff are pretty unsubstantiated anyway.\nWhat we are doing is putting national interests aside to help humanity at a global scale. We are trading in the land rights of indigenous Australian’s so we can dig up more coal and to give people overseas the chance of cheap power. If that doesn’t balance out historical inequities created by colonialism I don’t know what will.\nConservationists and some scientists will tell you that mining and transporting coal has impacts on local ecosystems too.\nYou can be rest assured that our governments in Australia are very concerned about the environmental impacts of coal mining. Queensland is a state known for its excessive levels of ‘green tape’, which make it difficult for everyday Aussie overseas companys to make an honest profit out of mining.\nThe government’s level of concern is evidenced by the hefty fines they hand out for breaches of environmental protocols. The coal company Adani recently got wacked with a $12,000 fine for letting toxic stormwater spill out into the waters adjacent to the Great Barrier Reef.\nAnyway, people matter too. What you can’t dispute is that coal mining provide significant numbers of jobs to honest hard working Australian’s.\nThe creation of a new coal mine may create up to 1500 new jobs over the life of the project.\nEveryone knows the 10s of thousands of ongoing jobs in tourism are not really related to the Great Barrier Reef. All those people are coming for another great Aussie industry - Casinos. But that’s another story.\nSo get behind Australia’s coal mines. Our environment needs coal power as much as our economy does."
  },
  {
    "objectID": "posts/2016-11-14-photos-to-spatialstat/index.html",
    "href": "posts/2016-11-14-photos-to-spatialstat/index.html",
    "title": "From geotagged photos to species distribution models",
    "section": "",
    "text": "My camera comes with an inbuilt GPS allowing you to geotag photos. I was on a family holiday on the beautiful Hornby Island (near Vancouver) and started wondering why the oysters growing on the rocks were so patchy in their distribution. This observation motivated me to get out and count oysters. The camera let me geotag photos of the quadrats I used to standardise my counts.\nThe oyster data-set has inspired a series of blog posts taking you from geotagged photos, making an interactive web map and finally, using the data in a spatial model. Here is the contents:\n\n\n\n\n\n\n\n\nI hope these posts using his simple example inspire some bigger ideas for your own spatial analysis."
  },
  {
    "objectID": "posts/2016-11-14-photos-to-spatialstat/index.html#from-geotagged-photos-to-species-distribution-models",
    "href": "posts/2016-11-14-photos-to-spatialstat/index.html#from-geotagged-photos-to-species-distribution-models",
    "title": "From geotagged photos to species distribution models",
    "section": "",
    "text": "My camera comes with an inbuilt GPS allowing you to geotag photos. I was on a family holiday on the beautiful Hornby Island (near Vancouver) and started wondering why the oysters growing on the rocks were so patchy in their distribution. This observation motivated me to get out and count oysters. The camera let me geotag photos of the quadrats I used to standardise my counts.\nThe oyster data-set has inspired a series of blog posts taking you from geotagged photos, making an interactive web map and finally, using the data in a spatial model. Here is the contents:\n\n\n\n\n\n\n\n\nI hope these posts using his simple example inspire some bigger ideas for your own spatial analysis."
  },
  {
    "objectID": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html",
    "href": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html",
    "title": "Keeping up your publication track record when you change jobs",
    "section": "",
    "text": "It is exciting to start a new job, but getting those old publications finished can be difficult to manage. You might have a lot of new work to do, so finding time can be hard. Your new supervisor may also not be supportive of you spending time to finish publications from past projects.\nWe discussed this issue with the lab group today. It is topical for some, with Mischa Turschwell starting a new postdoc with us last week.\nAn important point raised was whether your new job was academic (where you are expected to write pubs) or in another area (where papers are not part of the job). But first we will deal general strategies."
  },
  {
    "objectID": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#general-strategies-for-keeping-the-pubs-going",
    "href": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#general-strategies-for-keeping-the-pubs-going",
    "title": "Keeping up your publication track record when you change jobs",
    "section": "General strategies for keeping the pubs going",
    "text": "General strategies for keeping the pubs going\nThe first theme that emerged in discussions was general strategies for keeping up your publication rate, which apply wherever you are in your career. Key amongst these is setting aside high-quality time each week (ideally every morning) for writing. This should be high quality writing time where you just write, no email, looking up references or chatting. You can’t finish papers without writing, but writing is hard so it is easy to put off.\nDespite all the admin activities I have these days, I still set aside several hours 3 days a week for high quality writing time.\nPrioritising projects can also help. If you have multiple old projects to finish, then try to just work on one at a time. Doing too many at once (trust me I make this mistake all the time) splits your attention and you end up making less progress.\nTry these spreadsheet tools from iThinkwell to help you prioritise and stay focused. I used the team writing spreadsheet with papers I lead and those of my students.\nI recently had to work through some tricky reviews that had me very unmotivated about finishing the paper. Seeing it at the top of my priority list helped me focus on getting it done and not being distracted by other more interesting projects.\nIf you have lots of old projects you might want to cull some altogether. Reflect on how complete those old projects are and how much time they need until completion. If it is a lot, it might be worth writing to the co-authors and telling them that you will no longer be leading that paper. Another way to tell if a project should be removed is check in with it 3 months after starting a new academic job. If your new papers are more progressed than this old one, it probably isn’t complete enough to be bothered with anymore.\nRemove them from your priority list (you can always keep a backup list somewhere else). It is demoralising if you list is too long.\nIf you are aiming for academic success, finishing up those old pubs will be key to your success, despite what your new supervisor wants. As a rule of thumb, you should aim to have at least one paper in review at all times by the end of your PhD. I currently have 1-4 papers I’m leading in review at all times. Aim to increase your productivity into your postdoc years by getting more efficient at finishing up past projects (some tips for how are here)."
  },
  {
    "objectID": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#non-academic-jobs",
    "href": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#non-academic-jobs",
    "title": "Keeping up your publication track record when you change jobs",
    "section": "Non-academic jobs",
    "text": "Non-academic jobs\nIf your new job doesn’t support publishing academic journals then you are really going to have to finish those past pubs in your spare time. Laura related a story of getting a job before she even finished her masters. Many weekends and late nights later she finished her masters, all while working in a full-time position.\nAs above you might have to let some things go if it is all too much to fit in. Or you might want to downgrade expectations for your paper.\nFor instance, you PhD supervisor might have been pushing for your to write up a data chapter for a high impact journal, but you would need to include a synthesis of other data-sets from across the globe. Perhaps you could instead focus on just publishing results from your own data and aim for a different more regional journal. Getting something finished is better than nothing."
  },
  {
    "objectID": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#academic-jobs-on-specific-projects",
    "href": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#academic-jobs-on-specific-projects",
    "title": "Keeping up your publication track record when you change jobs",
    "section": "Academic jobs on specific projects",
    "text": "Academic jobs on specific projects\nYour situation here really depends on your supervisor. You are employed on a specific project so it is totally reasonable of them to expect you to spend all working hours working on your new project. Not all supervisors will have this expectation however, so it is worth discussing expectations up front.\nMy own opinion is that a new postdoc can spend some time finishing old projects, so long as they are progressing their new work. For Mischa we discussed he spend his writing time a few days a week getting a paper from his last postdoc finished. The rest of his time can be spent doing the easier tasks of reading and reviewing the datasets we need for his new projects.\nHe is sharing some writing time to work on outlines and concepts for this new paper. Writing is an important part of developing ideas, even if you don’t use those exact words in the final paper.\nHopefully by the time it comes to writing up the new project, the old one will be submitted.\nIt is broadly in a supervisor’s best interests for postdocs to publish their old papers - because it helps the postdoc be more successful in the long-run. Successful postdocs reflect well on their supervisors.\nIf you supervisor requires 100% commitment to the new project, or the new project just has you too busy during the days, then you will have to try the tactics above for jobs that don’t allow you to publish. Just remember, writing is hard and you only get so many good hours of it per day, so you will need to save some brain power for that in mornings (ideal), evenings or weekends."
  },
  {
    "objectID": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#fellowships",
    "href": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#fellowships",
    "title": "Keeping up your publication track record when you change jobs",
    "section": "Fellowships",
    "text": "Fellowships\nThis is the ideal situation because you have near total autonomy over your time. You can finish up your old papers while you build new collaborations and get to know your colleagues.\nI’ve been lucky enough to jump from one fellowship to another. But saying that I certainly could have benefitted from some of the prioritisation tools described above. One meta-analysis I stared in my PhD was so out of date by the time I got around to submitting it that the reviews requested we update our literature review."
  },
  {
    "objectID": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#planning-for-success",
    "href": "posts/2018-06-12-changing-jobs-keeping-up-pubs/index.html#planning-for-success",
    "title": "Keeping up your publication track record when you change jobs",
    "section": "Planning for success",
    "text": "Planning for success\nSo I hope this helps get those old papers finished. The keys really are to plan your approach to finishing papers and to make sure you understand your new supervisor’s expectations."
  },
  {
    "objectID": "posts/2018-09-19-protecting-habitats-for-sustaining-fisheries/index.html",
    "href": "posts/2018-09-19-protecting-habitats-for-sustaining-fisheries/index.html",
    "title": "Improving fisheries means saving their habitats",
    "section": "",
    "text": "Our new research published in Fish and Fisheries argues that protecting fish habitats is critical to recovering the world’s fisheries (here’s an open access version).\nThe study reviewed the habitat requirements for fish stocks across the globe. Nearly half of those reviewed were found to use habitats that are known to be in decline.\n\nOur guide to assessing the risk of misclassification of stock status when there are unrecorded changes in fish habitat\nOverfishing is often pinned as the only cause of declines in the productivity of fisheries, which implies stricter fisheries regulations are the sole solution to overfishing. We found that nearly half of the world’s best researched fish stocks are using habitats that are in decline, like seagrass and mangroves.\nMany fish habitats are under-threat from human activities like coastal development, pollution and destructive fishing that not only catches fish, but also damages habitats on the ocean floor. Queensland for instance, has lost large areas of its seagrass meadows and mangrove forests to development ¬of coastal infrastructure and pollution.\nThe study looks at how protecting or restoring lost habitats can improve the status of fisheries, so that the fisheries can sustainably support higher catches. If habitats are declining and we don’t know about it, we can mistakenly believe that fish stocks are better or worse off than they actually are (see figure).\nManaging fish catch is an important part of ensuring we have sustainable fisheries that support food production and jobs. But we can’t just count on good fisheries management. Protecting the fish habitats we have left and restoring lost habitats like mangroves is crucial.\nIt has previously been estimated that 31% of fish stocks globally are over-exploited, meaning that catches could be higher if fishing pressure was reduced. But these previous estimates ignored the effects of habitat loss on the productivity of fisheries.\nWe analysed habitat use for 418 of the best researched fish stocks (those in the RAM Legacy stock assessment database). These are stocks for which detailed monitoring data are available. They are commonly used to estimate how sustainable fish are at the global scale.\nThis was a work of several years, first conceived as part of my ARC DECRA project and now has finally reached print. Study co-author Andrew Broadley and I reviewed hundreds of papers to identify what types of habitats were used by the 418 stocks.\nWe also published the results of that review, the data for which is available for free in the Fishscape database.\nThe results suggest we need to look beyond fisheries to other activities that are degrading habitats, like pollution that comes from on land, and coastal development.\n“Restoration of critical fish habitats such as mangroves, seagrass, and floodplains will increase the resilience of fish populations against overfishing. For example, in the Wet Tropics, northern Australia, reconnecting coastal lagoons with mangroves and the Great Barrier Reef has been the primary objective of many restoration projects for fish species such as barramundi to have a better chance of growing and reproducing” said study co-author Dr Fernanda Adame.\nManaging fish habitats can be hard – we need to coordinate our management across the land and sea and across different industries, like infrastructure development and fisheries. But doing so is crucial to supporting the jobs created by fisheries and the opportunity to fish recreationally that is enjoyed by millions of Australians.\nWe hope to continue to use this work to inform our new project on mapping the health of coastal wetlands globally."
  },
  {
    "objectID": "posts/2016-06-20-ICRS-day1/index.html",
    "href": "posts/2016-06-20-ICRS-day1/index.html",
    "title": "International Coral Reef Symposium Day 1",
    "section": "",
    "text": "International Coral Reef Symposium Day 1 - My highlights\n20 June 2016, Honolulu Hawaii\nSee also my blog on ICRS: Key insights that will be updated daily.\nPlenary\nThe theme of this conference is science to policy and the plenary speakers wasted no time in emphasising the importance of communicating science to broader audiences to the 2500 people in attendance.\nAs evidence of the importance of reef science to policy, there are three presidents from sovereign nations in attendance at the conference.\nRuth Gates, president of the International Society for Reef Studies, wants us to tell the society how to best achieve impact to conserve coral reefs. To this ends, there will be special sessions on Friday to synthesise the conference’s science for policy and public communication.\nBob Richmond, the conference convener, emphasised the interdisciplinary nature of the conference, with scientists, educators and policy-makers in attendance. He said we need to move from academic outputs, like papers, towards policy impact.\nA few stats &gt;1500 oral presentations, &gt;700 posters and 45 sessions. Ruth Gates acknowledged the ‘mammoth’ effort Bob Richmond and the conference organizing team had made to put this together. It’s great to be here and I am certainly grateful for their efforts to bring this event together.\n\n\n\nScience to policy is the central theme of this conference. Source: http://twitter.com\n\n\nDarwin Award Medal - Jack Randall\nJack Randall received the Darwin award medal, the International Society for Reef Studies highest honour. Dr Randall gracefully accepted the award and told a series of amusing and touching stories from his career.\nDr Randall described purchasing his first aqua-lung. He didn’t know where to get an air tank from, so he went to the welding shop and got a bottle of oxygen (audience gasps). Someone told him later that if he had dived much deeper than he did, he wouldn’t be hear today (oxygen is toxic deeper than about 6 metres).\nDr Randall is an eminent coral reef fish taxonomist whose career has spanned &gt;70 years. During his career he described &gt;700 new marine species and pioneered scientific diving. You can read more about his productive and influential career on Christie Wilcox’s artile for the Hakai Magazine.\n\n\n\nSeveral people noted on Twitter that there have been 7 Darwin awards, but none yet to women. Let’s hope we see more nominations for women for the next award. Source: http://twitter.com\n\n\nKeynote from President Remengesau of Palau\nPresident Remengesau said that in Palau the ocean is life and it brings us together. He wants us to realize that Palau is not a ‘small island state’ but a ‘large ocean state’. Like many Palauans the ocean is integral to his cultural identity and livelihood. He considers himself a fisherman who has taken time off for public service. Palau has a &gt;1000 year long history of ocean stewardship.\nPalau has established shark sanctuaries and has led the effort to establish protected areas across the Pacific. Economic incentives have been integral to ocean protection. For instance, they have implemented a green tax as part of the depature tax at airports. This helps support ocean conservation efforts.\nAnother example comes from reef sharks, whicha are estimated to be worth $1.9 Million over the course of their lifespan - 100s times more than their value for shark-fin soup.\nTraditional ecological knowledge is the backbone of Palau’s management and governance. Palau’s local scientists (sorry missed the name) emphasised the need to return to traditional governance systems, like species and temporal closures. Subsistence fishing is therefore a key part of their protected area efforts, as it ensures food-security of local people into the future.\nPresident Remengesau believes large marine protected areas are essential for both marine biodiversity conservation and supporting local livelihoods. Palau has declared one of the world’s largest marine protected area systems. This decision is science-based.\nSustainable development of tourism and fisheries while protecting reef biodiversity that is threatened by climate change is a key dilemma faced by many island nations. Palau is pushing back by reducing local stresses to reefs. They are also strongly engaged with global policy push for greenhouse sustainability.\nHe concluded by saying we are at a cross-road for making decisions about the future health of reefs. It is essential to build trusting and long-term relationships to bridging the science to policy gap. We need 2016 to be the year of implementation for projects that can conserve reefs and dependent livelihoods.\n\n\n\nMany people were touched and inspired by a memorial session on reef fish ecology held in honour of Dr Glenn Almany. I only met Glenn a few times, but the science he did has influenced my work and he has left a lasting legacy for coral reef science and conservation. Source: http://twitter.com\n\n\nCorals of the world\nDr Charlie Veron announced the Corals of the World, an online platform for coral geography and identification. It’s exciting to see coral reef ecology going online and reaching a greater audience.\nEcosystem-based management\nI was in the EBM session this morning. The session was opened by Fogarty, who spoke about the importance of considering trade-offs to achieve ecosystem based management.\nNext up was Brainard, speaking on behalf of Pomeroy about the Coral Triangle Initiative’s Ecosystem Approach to Fishery Management. Brainard defined the Ecological approach to fishery management as trying to achieve a balance between ecological and societal wellbeing through effective governance.\nAgain Brainard emphasised the importance of balancing multiple objectives. Many countries, e.g. PNG, Indonesia, have a history of ecosystem approaches going back to the 90s. We have learned much from these early examples, such as the work on marine protected areas in the Philippines.\nA challenge is that while the principles of ecosystem approach are in national policies, data shortages and barriers to implementation are major challenges.\nLopez-Rivera from NOAA fisheries talked about the challenges of implementing science-based management. An attempt to reform management in Puerto-Rico failed because stakeholders were not willing to compromise on important trade-offs between fishing access and management and they also felt there was not enough evidence to support management changes.\nNash @NasherK reviewed indicators of fishing pressure and found most didn’t quantify actual gradients in fishing pressure!\nNash worked in the Seychelles to develop indicators of fishing pressure. Importantly, high size diversity of herbivorous fish was key to support coral reef resilience. This suggests size diversity is a useful indicator of both fishing pressure and ecological responses to disturbances.\nAltman, from Boston University, talked about how conventional approaches to management focus on protecting the state of ecosystems, e.g. number of species, biomass hotspots and so on. Whereas ecosystem based management often focusses on functions of biodiversity, like services to humans and resilience of ecosystems to disturbance.\nTropicalisation of temperate reefs\nIt has been an interesting year for temperature reefs. Extremely warm ocean temperatures this year, which are driving a global bleaching event on coral reefs, has also seen large migrations of tropical species to temperature waters.\n\n\n\nThis years global bleaching event was on everyone’s mind. Source: http://twitter.com\n\n\nAustralia’s south-east reefs are undergoing Tropicalisation. Booth has been doing surveys of tropical fish for many years now to find out which species are moving and to determine the mechanisms driving their movement and establishment success.\nThis summer has been particularly warm and has seen a large increase in the number of tropical fish arriving. Arrivals include surgeon fish - important herbivores on reefs that can change ecosystems through their herbivory. However, most don’t survive the winter, because it is too cold.\nBooth joked that by the next conference, Sydney may house coral reefs. It has happened at a similar latitude in the northern hemisphere!\nSommer is testing the hypothesis that environmental filtering is stronger is areas of high abiotic stress. By looking at the traits of high latitude coral species, they found that species with more similar traits were more likely to survive at higher latitudes. So corals will probably only expand to areas that are similar to their core ranges.\nSommer also found that critical temperatures and temperature variability were more important for controlling coral species range edges than mean temperature - this is an important lesson for predictive modelling.\nOn Australia’s west coast, heat wave events are also seeing unprecedented recruitment of tropical fish in temperate waters. Following these changes will be important for future management of species like this grouper, which are important to fisheries.\nMarine protected areas\nBob Pressey spoke about ineffective targets in conservation planning. Often reserves are ‘residual’ in that they are placed in areas that avoid pressures, like fishing, so don’t actually achieve anything for conservation.\nWe also need to be careful of coverage targets. Conservation objectives for area alone have become institutionalized, but the don’t reflect the quality of protection very well.\nFinally, Pressey talked about the importance of assessing the counterfactual - what would have happened if we didn’t take a management action. For residual reserves what happened will be very similar to the counterfactual, so we discover that our action didn’t really have any impact for improving conservation.\nGabby Ahmadia provided a concrete example of designing a reserve monitoring program to quantify the counterfactual. She used impact evaluation methodology to identify paired ‘control-impact’ sites. They allow comparison of how biodiversity is faring inside protected areas to outside, while controlling for other differences, such as level of exposure to wind and access to markets.\nAhmadia showed that using this thorough evaluation methodology has a large effect on how you classify the success of MPA indicators like fish biomass.\nSemmens presented analysis of citizen science data from the REEF surveys for Bonaire’s marine park. Bonaire’s park has a few small no-take protected areas, but largely allows subsistence fishing. There are many issues with citizen science data, however, Semmens points out that data collected by recreational divers on how many fish they see is ideal to evaluate the tourism experience.\nCeccarelli analysed effects of protection in the Great Barrier Reef marine park. The park mostly protects coral trout - the main fishery species. There is much functional redundancy across multiple species in the system, so overall fish community composition is similar inside and outside reserves.\nThat’s all from me for day 1. If you have anything to add, feel free to email me."
  },
  {
    "objectID": "posts/2021-09-22-R-course-Species-distribution-models/index.html",
    "href": "posts/2021-09-22-R-course-Species-distribution-models/index.html",
    "title": "Current approaches to Species Distribution Modelling in R",
    "section": "",
    "text": "My course notes for “Current approaches to Species Distribution Modelling in R” for World Fisheries Congress 2021 are now free online.\nThe course covers tidyverse and sf workflows for models, a beginners intro to modelling options and how to make spatial predictions.\nThere’s also bonus tutorial on the course site on how to simulate data in R to test model inferences."
  },
  {
    "objectID": "posts/2020-03-21-facing-uncertainty-with-growth-mindset/index.html",
    "href": "posts/2020-03-21-facing-uncertainty-with-growth-mindset/index.html",
    "title": "Facing uncertainty with a growth mindset",
    "section": "",
    "text": "Dear team,\nEven as we face major upheaval in our lives, social interactions and health, I am noticing some bright spots.\nIt’s reminded me what really matters among all the things we have and often take for granted. We are now homeschooling our kids and they are loving it (well only two days in, but so far so good!). The homeschooling is a real break from the rush of the school-commute-work-repeat routine. I’m getting to spend stacks of quality time with my children, and they seem very happy because of that.\nI’m really learning things about them that I hadn’t noticed before, like how interested my eldest child is in moths.\nMore generally, its broadened my perspective on my career. Over the past year or so I’ve been making the transition from individual researcher to team leader. Overall that has been a joy. Watching early career researchers learn and develop their own scientific questions is one of the most fulfilling experiences I’ve had in my career.\nBut it also hasn’t always been easy for me. I’ve often felt pushed for time. I’ve tried to maintain my own scientific identity and productivity, as well as put time into supporting this team. I don’t get to do as much actual research myself as I’d like to, because there are so many meetings, emails and admin to do.\nSometimes our research interests align. Sometimes they diverge and I need to put aside my own interests in order to best support you achieving your own goals.\nI’ve faced this transition to team leader with a growth mindset. This has been inspired by outstanding mentors and also the excellent books of Jennifer Garvey Berger.\nCurrent events have really challenged my ‘growth mindset’. But, in truth, there is plenty of growing we can do.\nFor me, if anything, this has really pushed me to focus even more on being a leader of scientists, rather than just a scientist. I feel less concerned about my own research goals now and more focused on supporting careers, PhD completions and healthy lives of my team.\nI’ve never felt more grateful to be part of this amazing research team. We are lucky to have a close network of support in our team and I think our connections will endure, even as we are forced into online life. For me too, thinking of ways to strengthen our group further has been a very positive distraction from all the bad news out there.\nIt’s hard to be optimistic right now and I’m not sure what other opportunities for growth this global situation will present. But there are sure to be many. So let’s keep our eyes and minds open to them.\nYours truly, Chris"
  },
  {
    "objectID": "posts/2023-06-06-why-teach-GLMs/index.html",
    "href": "posts/2023-06-06-why-teach-GLMs/index.html",
    "title": "Why GLMs should be a priority when teaching statistics",
    "section": "",
    "text": "There is often debate over the best place to start when teaching students inferential statistics. The teaching of statistics should start with basic understanding of what data are, probability and data visualisation. Beyond that students usually start to learn about inferential methods.\nThe debate then becomes should we start with classic methods like ANOVA, general (and generalized) linear models, or machine learning methods.\nThis debate applies to latter year undergraduate courses (2nd or third year) and also the day to week long workshops that are routinely run using R and or other software.\nI argue that the teaching of inferential statistics should start with general (and generalized) linear models (GLMs).\nGLMs are the foundational framework for model-based thinking, which dominates modern statistical analysis. The general linear model generalizes many of the historical methods for testing and encompasses what were traditionally called ’t-tests, anova and linear regression and ancova.\nThen, extensions of ‘general’ into ‘generalized’ linear models supersede tests for non-normally distributed data such as contingency table tests, logistic regression and even ordination and manova (multivariate stats). Teaching GLMs gives students the overarching principles they need to go away on their own and learn any modern statistical test.\nIn the past we gave students a long list of tools to deal with exceptions, with few common linkages between the different tools. This makes it harder for students to draw a common thread between tools. I think this lack of coherency is a key reason students finish a course but then can’t remember what test they need for a particular situation. We remember things better if they fit within a coherent framework.\nMuch better if students finish the course knowing what is possible, and that they can find the specific tweaks they need for a particular dataset (e.g. modelling a poisson vs gaussian distribution). Students will be much more likely then to be able to find a good model for any data analysis problem they are presented with.\nOne of the arguments for teaching ANOVA first was that it gives students an understanding of how variance is decomposed into difference components of the model (e.g. treatment vs residual error). A fair point, but variance decomposition can also be done with GLM, since ANOVA is just a special case of GLM.\nAn advantage of exploring variance components in a GLM framework is that it also emphasises effect size and facilitates graphical exploration of the analysis. Classic ANOVA methods promote interpretation via tables of numbers (e.g. p-values). The overinterpretatoin of p-values has resulted in many spurious or meaningless findings.\nGLMs encompass both statistical testing (p-values) and estimates of effect size (important to interpret relevance to real world). Effect sizes are then straightforward to visualize through making plots of parameter estimates with confidence intervals, or direclty by making predictions of response for different treatment or covariate values.\nThis leads me to the point that GLMs can be used as a predictive tool. In contrast, a classic ANOVA does not estimate differences in the mean (only variance explained by differences in means), so you cannot predict values of a response variable from an ANOVA. Predictive tools have increasing relevance in modern workforces.\nIn my own area of marine science for instance, fishery scientist may use a GLM to quantify how fish catch varies with different mesh sizes of nets (smaller mesh size will generally will catch smaller fish). The fitted GLM can then be used to make predictions that standardise comparison of catches from nets with different mesh sizes. (Read about another example that is a GLM version of ANCOVA).\n\n\n\nGraph showing GLM predictions of abundance for different habitat areas\n\n\nFigure: Example of using a GLM to correct for differences in habitat use across two regions. In the left panel we model fish abundance just on habitat. However, region 2 has more habitat but no fish, so the fit is negative. On the right we additionally control for region, and see that abundance increases with habitat in region 1.\nThe predictive ability of GLMs is a step towards more sophisticated methods for prediction. Basic GLMs are the first step towards methods that include hierarchical models (random effects), and non-normal distributions (e.g. poisson for count data). Distribution types and hierarchical models are themselves a step towards maching learning and AI methods. Random effects for instance are a type of shrinkage technique. The principle of shrinkage is used across many different model classes (e.g. GAMs, ridge regression, regression trees, neural networks) to improve predictive power.\nThe discussion about what to teach can sometimes be side-tracked about what software to use, often the argument is between R and Python. Now I’m a huge fan of R and don’t know much python, but I think the debate over software is a distraction from the main game.\nMore important is that students learn about quantitative analysis and principles such as the estimation of effect sizes. Some programming is helpful, to give them flexibility of options. Whether they learn that in R or Python is secondary. Obviously the software that is used in their (intended) workplace will be most helpful. However, often it is unknown what software they will encounter most. So just give them the fundamental theory and skills to learn and they will be enabled to adapt and learn on the job."
  },
  {
    "objectID": "posts/2016-03-06-how-long-to-write-grant/index.html",
    "href": "posts/2016-03-06-how-long-to-write-grant/index.html",
    "title": "How long does it take to write major national grant?",
    "section": "",
    "text": "How long does it take to write for major national grants?\nDuring “grant season” (~Dec - March in Aus) you might find your colleagues bemoan all the work they have to do writing grants, or turning down meetings because they are too busy. I certainly felt that way writing for a major Australian Research Council grant this year.\nBut, I wondered how much time does it take to write for a national grant scheme. Since October I have been logging the hours I spend on different projects, like a consultant, in an effort to gain data on project time and improve my time-management. I added up the time I spent writing for an ARC grant, where I was lead author, and the answer surprised me:\n\n69 hours and 30 minutes or nearly 12 full working days (at least1)\n\n\n\nTime spent by days writing for a grant \n\n\nA few caveats on my estimate, which is certainly an under-estimate. It counts writing, formal (organised) meetings with co-authors, budgeting, some reading and all the administrative stuff. The time doesn’t count emails, reading papers, or time thinking while bike-riding, walking the pram, trying to go to sleep etc…\nThe time also doesn’t count time from my colleagues. The grant includes significant input from 3 collaborators and numerous reviews from non-collaborators and research office staff. Also, I started records in November, so prep work before hand (including an internal pre-review) isn’t counted here.\nSome further context for this grant. There is likely to be considerable variability for different people and projects. I am an early career researcher, so maybe my time is slower than more experience researchers, or better writers. However, I did put in for an ARC grant last year so I had a head start on some of the sections about your CV and research contributions. I am also familiar with the ARC format, something I spent a considerable amount of time figuring out the first time I put in for a grant.\nMy grant is also modelling focussed, so that probably saved me some time in the budgeting step, as that could get very tricky if there was lots of field work to budget for.\n\n\n\nThe screen you see on completion of 12 days solid work \n\n\n\n\n\nMaybe 12 days underestimates the mental effort?\n12 days doesn’t sound like as much as I was expecting. However, maybe the raw numbers under-estimate the mental effort that goes into grant writing. I probably have at most 3-4 hours maximal mental effort per day, which makes the grant look more like a month’s solid work, using the best hours of the day.\nI will be interested to hear how much time other people (tweet me) spend on grants and whether my numbers sound reasonable.\nAs a footnote, the message you get when you finally hit submit is rather disappointing (see image). Consider that it takes ~33 hours of climbing to ascend Everest from base camp and you will be rewarded with a view from the top of the world. Or for a computing analogy, if you complete a computer game you are usually rewarded with a video and the solution to the game’s story. It takes only about 13 hours to complete some of the best selling computer games. Obviously the final rewarded is receiving the grant and successfully investigating your ideas, but that is a highly uncertain outcome (~15% success rate for ARCs) that is years down the track. It would be nice if online grant submissions unlocked some kind congratulations page.\n\nNote added 8/03/2016. I have discussed this post with several colleagues all of whom suggests it takes much longer than 70 hours to write a major grant. As I have not included time for reading and thinking in my estimate, it is likely that time would add considerably to the time taken to complete the grant."
  },
  {
    "objectID": "posts/2021-08-04-global-mangrove-carbon-emission/index.html",
    "href": "posts/2021-08-04-global-mangrove-carbon-emission/index.html",
    "title": "Combining local knowledge and scientific surveys to estimate declines in threatened species",
    "section": "",
    "text": "In the study “Community-based management fails to halt declines of bumphead parrotfish and humphead wrasse in Roviana Lagoon, Solomon Islands” we report on declines in two threatened fish species bumphead parrotfish (Bolbometopon muricatum) and humphead wrasse (Cheilinus undulatus).\nAnnouncing here I’ve made the R code for this paper freely available. A unique feature of this analysis is that we integrated local knowledge into the analysis of the scientific survey data.\nTo do this, we used a Bayesian GLM, where the rate of decline was informed by both change in fish density in the scientific survey data (2000-2018) and change in the maximum catch rate that local fishers could recall for the 1980s, 2000s and 2018.\nThe integration of local knowledge meant we could verify the declines in catch rate with scientific survey data, and adjust for bias between fisher recall and the surveys. We could then infer the population density in the 1980s, when there were no scientific surveys.\nUnfortunately, the significant magnitudes of the declines over multiple generations suggest these species meet the criteria for being ‘Critically Endangered’ on the IUCN Red List (at least at this local site, if not regionally).\nRead more about the study in the paper:\nHamilton RJ, Hughes A, Brown CJ, Leve T, Kama W. Community-based management fails to halt declines of bumphead parrotfish and humphead wrasse in Roviana Lagoon, Solomon Islands. Coral Reefs. 2019 Jun;38(3):455-65."
  },
  {
    "objectID": "posts/2024-09-04-phd-opportunity-integrated-assessment-marine-ecosystems/index.html",
    "href": "posts/2024-09-04-phd-opportunity-integrated-assessment-marine-ecosystems/index.html",
    "title": "PhD opportunity Integrated assessment of marine ecosystems",
    "section": "",
    "text": "I’m looking to recruit a new PhD student (international or domestic) to work on integrating ecological and social dynamics for the assessment of marine ecosystems. The project includes a CSIRO top-up so total annual living allowance scholarship of ~$42,000.\nMore details on the UTas page.\nThe project will be based at the University of Tasmania, in Hobart, Australia, and will be supervised by myself and other members of the Centre for Marine Socioecology and CSIRO.\nThe integrated assessment of development and use impacts on Australia’s marine estate examines the comprehensive effects of human activities on the marine environment (direct and indirect). By evaluating the consequences of development and resource use, it aids in informed decision-making, sustainable management, and the use and protection of Australia’s marine ecosystems and biodiversity. Integrated Ecosystem Assessment has been called for in Australia by the National Marine Science Committee as part of planning Australia ocean use and maximising benefit. In Australia, IEA could support managers and policy makers to find solutions that support triple-bottom line outcomes across multiple use including fisheries, cultural, tourism, aquaculture and renewable energy.\nAims/Objectives Develop modelling approaches for integrated ecosystem assessment that represent the human dimensions and support social-ecological system stewardship. Assess triple-bottom-line outcomes (economic, ecological and social) from changes to management in the marine environment. The focal area will be south-east marine ecosystems, including Bass Strait soft sediment ecosystems and the Great Southern Reef. The work will aim to inform on policies for ecosystem assessment in government and national assessments such as the State of Environment Reporting.\nMethods This project is supported by a CSIRO R+ top-up and project funds. Ecosystem models that include qualitative network models and dynamic end-to-end models will be further developed to include social dynamics. Emphasis will be placed on representing cross-sectorial interactions, such as the influence of offshore infrastructure on changes in fishing distribution, and corresponding changes in ecosystems.\nThe case-study application to Bass Strait will assess dynamic interactions among fisheries, aquaculture, renewables and protected areas. The case-study for the Great Southern Reef will assess interactions among restoration, recreational fisheries and cultural values."
  },
  {
    "objectID": "posts/2017-06-18-estimating-popn-decline/index.html",
    "href": "posts/2017-06-18-estimating-popn-decline/index.html",
    "title": "Quantifying the magnitude of a population decline with Bayesian time-series modelling",
    "section": "",
    "text": "Population abundances tend to vary year to year. This variation can make it make it hard detect a change and hard to quantify exactly what that change is.\nBayesian time-series analysis can help us quantify a decline and put uncertainty bounds on it too. Here I will use the R-INLA package to fit a time-series model to a population decline.\n\nFor instance, take the pictured time-series. Quantifying change as the difference between the first and last time-points is obviously misleading. Doing so would imply that abundance has declined by 77% from the historical value.\nAnother approach would be to compare the average of the first and last decades. Doing so would yield a 72% decline.\nA better way might be to model the population trend over time and then estimate our change from the model. An advantage of a model is we can be more specific (and transparent) about assumptions. Using Bayesian time-series analysis we can also pick a model that is appropriate for the distribution of the data. e.g. here I will use negative binomial errors, because the abundances are counts.\n\nSimulating a time-series\nLet’s simulate a simple time-series of counts that declines at a constant (exponential) rate:\nset.seed(1969)\nn  &lt;-  50\nrho  &lt;-  0.8\nprec  &lt;-  10\ninit &lt;- 7\nacoef &lt;- c(rep(-0.01, n/2), rep(-0.03, n/2))\neta &lt;- init + (acoef*(1:n))\ny &lt;- rnbinom(n, mu  =  exp(eta), size  =  5)\ndata  &lt;-  list(y =y, z = 1:n)\nThe results are pictured above. In short we calculate eta which is the linear predictor. Its exponent determines the mean of the negative binomial process, hence the exponential (not linear) decline. The size = 5 means that the count data will be moderately over-dispersed - that is quite noisy (the variance will be &gt; than the mean).\nNotice that I put a bit of a ‘kink’ in eta by having the rate of decline (acoef), increase halfway through the time-series like this:\n\nWe can calculate the magnitude of change like this: 1 - (y[n]/y[1]) = 0.77 which can be interpreted as what fraction of the intial years biomass has been lost?\nWhat we want to do is put a smoother over time, so our estimate accounts for short-term variation. We also want to get credibility intervals (95% probability intervals) on the estimate of decline.\n\n\nFitting a time-series model with INLA\nR-INLA is a pretty handy package. It is a very fast way to fit generalized linear models and can also handle a huge range of different types of random effects. Here we will use an auto-regressive lag one process. That is saying that abundance at time t depends on abundance at time t-1 with some constant correlation coefficient.\nWe could also include other covariates in the model, for instance, abundance of a predator that might eat our organism of interest. However, we won’t go into that here.\nFirst up, we specify the formula in INLA’s notation:\nlibrary(INLA)\nf1 &lt;- y ~ 1 + f(z, model = \"ar1\")\nWhich just says model y as a function of an intercept term (constant mean) + an autoregressive process that depends on z (just an index for the year). The f() is INLA’s standard notation for modelling its different random effects models (AKA latent models), here we chose the ar1 option.\nNow the trick is to get INLA to compute the % difference between the expectation for the first and last time points as it goes. That way we not only get an estimate of the change but also we will get the full posterior distribution, so we can get our CIs. Note I put emphasis on expectation, because we won’t simply be calculating the difference between the first and last points (we did that above), but will in fact be calculating the difference between the model’s estimated mean values for the first and last points.\nTo do that we use INLA’s linear combination feature.\nThis feature let’s us compute a linear combination of the estimated covariates. Here we will ask for the difference between the first and last values of z (the time-series). inla will multiply the z values by the numbers we give below (here -1 and 1), then sum them together.\nlc1 &lt;- inla.make.lincomb(z = c(-1, rep(NA, n-2), 1))\nnames(lc1) = \"lc1\"\nThe row of NA in the middle just says to ignore the middle points. Note that we have also ignored the intercept. More on this in a moment.\nNow we can fit the model, using negative binomial errors and specifying our linear combination:\nm1 &lt;- inla(f1,family = \"nbinomial\", data  =  data,\n    lincomb = lc1,\n    control.inla = list(lincomb.derived.only = TRUE),\n    control.predictor = list(compute = TRUE, link = 1)\n)\nAnd the results summary can be obtained (not shown here):\nsummary(m1)\nNote the large estimate for rho, which is the auto-correlation parameter. That happens because we have such a strong decline.\nWe could have also fit just a linear trend to our data, however, because the rate varies over time (and we wouldn’t know that beforehand if we had real data, not data we made up), it is nice to use ar1 which has a kind of smoothing effect.\nAlso worth noting that modelling an trending (non-stationary) time-series with an ar1 is not technically correct if we want to estimate rho or forecast. However, here we use ar1 because it acts like a smoother in a GAM [e.g. see here.\n\n\nExamining the model’s predictions\nIt is pretty straightforward to plot INLA’s predictions for the time-series. They are all stored in the m1 object:\nplot(data$z, data$y, col = 'grey', type = 'l', lwd = 2, xlab = \"years\", ylab = \"Abundance\")\nlines(data$z, m1$summary.fitted.values$mean, col = \"tomato\", lwd = 2)\nlines(data$z, m1$summary.fitted.values[,3], col = \"tomato\", lwd = 1, lty = 2)\nlines(data$z, m1$summary.fitted.values[,5], col = \"tomato\", lwd = 1, lty = 2)\nlegend('topright', legend = c(\"observed\", \"expected\", \"95% CIs\"), lty = c(1,1,2),\n       col = c(\"grey\", \"tomato\", \"tomato\"))\n\nNow we can extract the change in population size from our linear combination:\nm1$summary.lincomb.derived$mean\n\n## [1] -1.081196\nHowever, this is not quite the number we want. It is the change in eta or the linear predictor. Negative binomial models have a log-link, so that is the change in log-space. We want the real change. If we calculate:\n100 * (1 - exp(m1$summary.lincomb.derived$mean))\n\n## [1] 66.08103\nwe get the change as a % from the historical mean The reason being is that the linear combination is log(eta[n]) - log(eta[1]). Taking the exponent gets the change and one minus that is the % loss.\nIt is easy now to get the CIs on the linear combination:\nm1$summary.lincomb.derived\n\n##     ID      mean       sd 0.025quant 0.5quant 0.975quant      mode kld\n## lc1  1 -1.081196 0.309715  -1.695217 -1.08053 -0.4716486 -1.079157   0\nWe could also look at the marginal and plot the posterior distribution of the % loss:\nlosses &lt;- 100 * (1 - exp(m1$marginals.lincomb.derived[[1]][,1]))\ndens &lt;- m1$marginals.lincomb.derived[[1]][,2]\nplot(losses, dens, type = 'l', xlim = c(0, 100),\n     xlab = \"% loss\", ylab = \"density\", las = 1,\n      main = \"Posterior density for % loss\")\npolygon(x = c(losses, rev(losses)),\n        y = c(dens, rep(0, length(dens))), col = \"turquoise3\")"
  },
  {
    "objectID": "posts/2025-10-21-modelbased/index.html",
    "href": "posts/2025-10-21-modelbased/index.html",
    "title": "modelbased package for plotting interactions",
    "section": "",
    "text": "I’ve been a big fan of the visreg package for a while, it makes it convenient to plot predictions from models. But recently I was struggling with a model that had many two-way interactions terms.\nI checked out the key alternative to visreg, ggeffects, which is very popular (the paper has over 2000 citations). But the ggeffects page states it will be superseded by the modelbased package. So here’s a quick demo of how to use modelbased to plot interaction effects from a GLM.\nmodelbased is part of an ecosystem of packages for handling statistical models: easystats.\nFirst I’m going to load data on coral cover and fish abundance from my Example ecological data repo:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.5 (red = needs update)\n✔ bayestestR  0.17.0   ✔ correlation 0.8.8 \n✖ datawizard  1.2.0    ✔ effectsize  1.0.1 \n✔ insight     1.4.2    ✔ modelbased  0.13.0\n✖ performance 0.15.1   ✔ parameters  0.28.2\n✔ report      0.6.1    ✔ see         0.12.0\n\nRestart the R-Session and update packages with `easystats::easystats_update()`.\n\nlibrary(readr)\n# Load data from https://github.com/cbrown5/example-ecological-data\n\ndat_url &lt;- \"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/fish-coral-cover-sites.csv\"\n\n#Convert counts of coral to proportions\ndat &lt;- read_csv(dat_url) %&gt;%\n    mutate(\n        cb_cover = cb_cover / n_pts, \n        soft_cover = soft_cover / n_pts\n    ) \n\nRows: 49 Columns: 13\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): flow, logged\ndbl (11): site, reef.id, pres.topa, pres.habili, secchi, coordx, coordy, cb_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we’ll fit a simple poisson GLM. pres.topa is count of topa fish, a fish that likes to live in branching corals (cb_cover), and logged is a binary variable indicating whether the site was near to logging or not (which causes pollution that harms fish and corals).\n\nm1 &lt;- glm(pres.topa ~ cb_cover *  logged,\n          data = dat,\n          family = poisson)\nsummary(m1)\n\n\nCall:\nglm(formula = pres.topa ~ cb_cover * logged, family = poisson, \n    data = dat)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -3.839      1.124  -3.415 0.000639 ***\ncb_cover                     8.691      1.979   4.392 1.12e-05 ***\nloggedNot logged             5.313      1.136   4.675 2.93e-06 ***\ncb_cover:loggedNot logged   -7.830      2.017  -3.882 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 426.61  on 48  degrees of freedom\nResidual deviance: 265.33  on 45  degrees of freedom\nAIC: 353.22\n\nNumber of Fisher Scoring iterations: 6\n\n\nIt simple to then plot the interaction effects. I’m going to do so on link scale (log scale for poisson models):\n\npr &lt;- estimate_means(m1, by = c(\"cb_cover\", \"logged\"),\n        estimate = \"typical\",\n        type = \"link\")\n\nplot(pr) +\n    labs(\n        x = \"Hard coral cover\",\n        y = \"Predicted Topa abundance (log scale)\",\n        color = \"Soft coral cover\", \n        fill = \"Soft coral cover\"\n    ) +  \n    theme_classic()\n\n\n\n\n\n\n\n\nOne neat thing that modelbased does is give you different options for how to estimate marginal means. The default is “typical”, which estimates marginal means at the average value of numerical predictors and averages over frequencies of categorical predicotrs. But you can also do “average” which averages over sample distributions and “population” which attemps to create a counterfactual for all possible combinations of variables.\nThese answer different questions, see the documentation for more details.\nI’m going to make a dataframe of all three methods so we can compare them:\n\npr_typical &lt;- estimate_means(m1, by = \"cb_cover\",\n        estimate = \"typical\",\n        type = \"link\")\n\npr_average &lt;- estimate_means(m1, \n    by = list(cb_cover = unique(dat$cb_cover)),\n        estimate = \"average\",\n        type = \"link\")\n\npr_population &lt;- estimate_means(m1, by = c(\"cb_cover\"),\n        estimate = \"population\",\n        type = \"link\")\n\npr_all &lt;- bind_rows(\n    pr %&gt;% mutate(method = \"typical by logging\"),\n    pr_typical %&gt;% mutate(method = \"typical\"),\n    pr_average %&gt;% mutate(method = \"average\"),\n    pr_population %&gt;% mutate(method = \"population\")\n)\n\nNow we plot them:\n\nggplot(pr_all, aes(x = cb_cover, y = Mean, color = logged)) +\n    geom_point() +\n    geom_linerange(aes(ymin = CI_low, ymax = CI_high)) +\n    facet_wrap(~method) +\n    labs(\n        x = \"Hard coral cover\",\n        y = \"Predicted Topa abundance (log scale)\",\n        color = \"Logged\"\n    ) +  \n    theme_classic()\n\n\n\n\n\n\n\n\nAnd one more plot just of population and typical to more easily see the diffence:\n\npd &lt;- position_dodge(width = 0.05)\npr_all %&gt;%\n    filter(method %in% c(\"population\", \"typical\")) %&gt;%\nggplot(., aes(x = cb_cover, y = Mean, color = method)) +\n    geom_point(position = pd) +\n    geom_linerange(aes(ymin = CI_low, ymax = CI_high), \n    position = pd) +\n    labs(\n        x = \"Hard coral cover\",\n        y = \"Predicted Topa abundance (log scale)\",\n        color = \"Logged\"\n    ) +  \n    theme_classic()\n\n\n\n\n\n\n\n\nYou can see see some key differences. The typical method shows effects of hard coral cover for a typical observation. There are slightly more unlogged than logged sites in the data:\n\ntable(dat$logged)\n\n\n    Logged Not logged \n        21         28 \n\n\nSo the typical sample is different from a balanced population level average.\nThe population and typical predictions fall in between the predictions that are conditional on logging status, because both are kinds of averages over the two logging categories.\nPredictions from the ‘average’ method show an inconsistent pattern, because they are averaging over samples. You can roughly see the trends for logged and unlogged sites.\nFinally, let’s convince ourselves that the unbalanced data is causing these differences. I’m going to randomly remove some unlogged sites to create a balanced dataset, and refit the model:\n\n#randomly remove 7 'Not logged' sites to create balance\nset.seed(707)\nirm &lt;- sample(which(dat$logged == \"Not logged\"), 7)\ndat2 &lt;- dat[-irm, ]\n\nm2 &lt;- glm(pres.topa ~ cb_cover *  logged,\n          data = dat2,\n          family = poisson)\n\npr_typical2 &lt;- estimate_means(m2, by = \"cb_cover\",\n        estimate = \"typical\",\n        type = \"link\")\n\npr_population2 &lt;- estimate_means(m2, by = c(\"cb_cover\"),\n        estimate = \"population\",\n        type = \"link\")\n\npr_all2 &lt;- bind_rows(\n    pr_typical2 %&gt;% mutate(method = \"typical\"),\n    pr_population2 %&gt;% mutate(method = \"population\")\n)\n\npd &lt;- position_dodge(width = 0.02)\nggplot(pr_all2, aes(x = cb_cover, y = Mean, \n        color = method)) +\n    geom_point(position = pd) +\n    geom_linerange(aes(ymin = CI_low, ymax = CI_high),\n        position = pd) +\n    labs(\n        x = \"Hard coral cover\",\n        y = \"Predicted Topa abundance (log scale)\",\n        color = \"Logged\"\n    ) +  \n    theme_classic()\n\n\n\n\n\n\n\n\nYou can see the two types of predictions are identical with balanced data."
  },
  {
    "objectID": "posts/2016-06-22-rstats-for-coral-reefs/index.html",
    "href": "posts/2016-06-22-rstats-for-coral-reefs/index.html",
    "title": "R programming tools for coral reef scientists",
    "section": "",
    "text": "22 Jun 2016\nI’m sitting here at the International Coral Reef Symposium in Hawaii. Looking at all the exciting graphs, data and analysis going up got me thinking: what are the key Rstats tools coral reef scientists should learn?\nStarting out with Rstats can be overwhelming because there are so many tools and packages available (but check out some of the free courses on my page). So here is a quick list of the key tools (packages and R functions) you should learn.\n\n\nA data visualisation tool tops the list. Data visualisation is the first step of analysis - you can quickly look for interesting patterns and check for errors. The ggplot2 (‘grammar of graphics 2’) package makes dataviz quick and easy. You can rapidly slice and dice your data in different ways for viz and also overplot simple analyses, like regressions rapidly. Check out the guide here.\n\n\n\nLinear models are becoming the dominant statistical tool for univariate data analysis in ecology and have largely replaced ANOVA methods. For instance, if you want to know how coral growth relates to temperature, how fish biomass relates to fishing pressure and so on you would probably use a linear model. Linear models are implemented in R with the function lm().\nA handy guide for analysis and interpretation of linear models has just been published and check out the books by the same author for guidance on how to code them up.\nA few quick examples on why lm() is so great. Just using a single continuous predictor - that is linear regression. Multiple continous predictor variables: multiple regression. If you combine categorical and continuous predictor variables you have your classic ANCOVA analysis. Use the predict() function to see what your trend lines look like. You can get confidence interval using the confint() functions. Do away with p-values they are dated 20th century statistics. Well ok p-values have their uses, but confidence intervals let you check how strong your effect is, as well as it’s significance.\n\n\n\nOnce you mastered lm(), glm() (generalized linear model) is a must for ecologists. We often deal with non-normal data (ie doesn’t fit a normal distribution), like presence/absence data and counts. lm() is inappropriate for this type of data. Way back in the 70s some clever people invented GLMs as a generalization of logistic regression. GLMs can handle many types of data. Check out Zuur et al.s book for a handy guide.\n\n\n\nEcological data is never simple and our data are often grouped. For instance, we often have blocks in experiments or transects within sites from field surveys. The package lme4 lets you implement random effects in linear models. Random effects are a flexible way to account for nuisance sources of variation, which may be important but we don’t have a specific hypothesis for which way the effect will go. Two key functions in lme4 are lmer() which is the random effects version of lm() and glmer(), the random effects version of glm(). Again, check out Zuur et al.s book for a handy guide.\n\n\n\nGot a large data-set? Need to merge data-sets by a common ID (e.g. site names)? Need to generate new variables from exisiting variables? Need to create data summaries? The package dplyr (‘data pliers’) is a must for all these tasks. Check out the vignette.\n\n\n\nOk, so you have entered your data (or got it from someone else), but wait a minute, the way you entered it doesn’t let you use ggplot2, dplyr, lm().. (if you followed my guide you shouldn’t have this problem). Do you need to reenter it? Nope, tidyr (for tidy data) to the rescue. The package tidyr lets you reformat data in ways (e.g. from wide format to long format) in the way that most stats packages prefer it. Check out this blog.\n\n\n\nThe package vegan provides good old multi-variate analysis, without the cost of Primer. Admittedly this package is slightly harder to use than Primer, but hey if you are already using R, the jump to primer is easy. Check out the vignettes to get started.\nThat’s my top 7. Good luck getting started."
  },
  {
    "objectID": "posts/2016-06-22-rstats-for-coral-reefs/index.html#r-programming-tools-for-coral-reef-scientists",
    "href": "posts/2016-06-22-rstats-for-coral-reefs/index.html#r-programming-tools-for-coral-reef-scientists",
    "title": "R programming tools for coral reef scientists",
    "section": "",
    "text": "22 Jun 2016\nI’m sitting here at the International Coral Reef Symposium in Hawaii. Looking at all the exciting graphs, data and analysis going up got me thinking: what are the key Rstats tools coral reef scientists should learn?\nStarting out with Rstats can be overwhelming because there are so many tools and packages available (but check out some of the free courses on my page). So here is a quick list of the key tools (packages and R functions) you should learn.\n\n\nA data visualisation tool tops the list. Data visualisation is the first step of analysis - you can quickly look for interesting patterns and check for errors. The ggplot2 (‘grammar of graphics 2’) package makes dataviz quick and easy. You can rapidly slice and dice your data in different ways for viz and also overplot simple analyses, like regressions rapidly. Check out the guide here.\n\n\n\nLinear models are becoming the dominant statistical tool for univariate data analysis in ecology and have largely replaced ANOVA methods. For instance, if you want to know how coral growth relates to temperature, how fish biomass relates to fishing pressure and so on you would probably use a linear model. Linear models are implemented in R with the function lm().\nA handy guide for analysis and interpretation of linear models has just been published and check out the books by the same author for guidance on how to code them up.\nA few quick examples on why lm() is so great. Just using a single continuous predictor - that is linear regression. Multiple continous predictor variables: multiple regression. If you combine categorical and continuous predictor variables you have your classic ANCOVA analysis. Use the predict() function to see what your trend lines look like. You can get confidence interval using the confint() functions. Do away with p-values they are dated 20th century statistics. Well ok p-values have their uses, but confidence intervals let you check how strong your effect is, as well as it’s significance.\n\n\n\nOnce you mastered lm(), glm() (generalized linear model) is a must for ecologists. We often deal with non-normal data (ie doesn’t fit a normal distribution), like presence/absence data and counts. lm() is inappropriate for this type of data. Way back in the 70s some clever people invented GLMs as a generalization of logistic regression. GLMs can handle many types of data. Check out Zuur et al.s book for a handy guide.\n\n\n\nEcological data is never simple and our data are often grouped. For instance, we often have blocks in experiments or transects within sites from field surveys. The package lme4 lets you implement random effects in linear models. Random effects are a flexible way to account for nuisance sources of variation, which may be important but we don’t have a specific hypothesis for which way the effect will go. Two key functions in lme4 are lmer() which is the random effects version of lm() and glmer(), the random effects version of glm(). Again, check out Zuur et al.s book for a handy guide.\n\n\n\nGot a large data-set? Need to merge data-sets by a common ID (e.g. site names)? Need to generate new variables from exisiting variables? Need to create data summaries? The package dplyr (‘data pliers’) is a must for all these tasks. Check out the vignette.\n\n\n\nOk, so you have entered your data (or got it from someone else), but wait a minute, the way you entered it doesn’t let you use ggplot2, dplyr, lm().. (if you followed my guide you shouldn’t have this problem). Do you need to reenter it? Nope, tidyr (for tidy data) to the rescue. The package tidyr lets you reformat data in ways (e.g. from wide format to long format) in the way that most stats packages prefer it. Check out this blog.\n\n\n\nThe package vegan provides good old multi-variate analysis, without the cost of Primer. Admittedly this package is slightly harder to use than Primer, but hey if you are already using R, the jump to primer is easy. Check out the vignettes to get started.\nThat’s my top 7. Good luck getting started."
  },
  {
    "objectID": "posts/2020-03-27-adding-desktop-based-work-to-your-phd/index.html",
    "href": "posts/2020-03-27-adding-desktop-based-work-to-your-phd/index.html",
    "title": "Adding desktop based work to your PhD? Here’s some things to think about",
    "section": "",
    "text": "The current pandemic is causing widespread disruptions to research, as travel, universities, labs and field stations shut-down. As a PhD student your timeline for submission of your thesis probably won’t be moved back because of the disruptions. Funding for scholarships still has the same limits as always. From what I’ve seen, universities are expecting students to adapt their work to the disruption and will not be putting forward blanket extensions to all PhDs.\nUnderstandably this situation creates a lot of anxiety.\nOne way to adapt your research is to replace field or lab work with desktop based research work. Whether this is appropriate for your PhD is a case-by-case decision and something you should discuss with your supervisor and mentors.\nA desktop based project might involve analysis of existing datasets, modelling or a literature review.\nIt seems like pursuing desktop based work is the advice university graduate schools and many supervisors will be giving their students. So thousands of researchers worldwide are about to embark on desktop based work, whether they have experience in this style of research or not.\nAs an ecosystem modelling, desktop based work is my bread and butter. So here’s a few things to think about if you are considering switching some of your field or lab research to modelling or data analysis.\n\n\nMany people underestimate how long it takes to develop models or data-analysis, just as I tend to under-estimate how complex field work will be. A story. Researchers in Australia wanted to watch fruit bats with a $7000 thermal camera. Sounds easy to me, just stick the camera up a tree. Problem is, coconut crabs might find the camera and rip it apart.\nThere are set backs in modelling too, that can be just as time consuming (though often less exciting). Like, the spatial data you want to process fills up your RAM, sending you on a 3 week journey looking for a suitable cloud-based processing service. Or the R package you’ve been using turns out to have a major bug.\nI commonly get asked to review or edit modelling papers. There are lots of common mistakes I see from groups not used to modelling. I could fill a whole blog with tips on that, but here’s a few major ones\n\nApplying an inappropriate modelling to the research question you want to answer. There are lots of models out there you can just download and start using. But models are usually developed for quite specific research questions, it may not be appropriate to use that same model for addressing different questions.\nBasic statistical mistakes, like treating psuedo-replicates as true replicates or mis-interpreting p-values.\nAbsence of sensitivity analysis, in the general sense (if you don’t know what that means, time to do some reading).\n\nThere’s a couple of things you can do to avoid these mistakes.\nFirst, get quantitative collaborators on board early and communicate with the regularly. Maybe your supervisor or lab group already has expertise in modelling, that is great, make use of it. Otherwise, you’ll have to put yourself out there and go looking (or ask your supervisor for help).\nCollaborators with expertise in the modelling you want to do will help you save time by avoiding common mistakes and help you find the right kind of methods for your research question.\nSecond, do your reading. Ecological modelling is a very large field with established norms for analysis. So read the literature in your area and find our what flies, what methods people commonly use.\nHere’s a few books I recommend my new ecology students read before starting modelling.\n\n\n\nGive yourself time to upskill. Take training appropriate to the type of modelling you want to do. And give yourself time in your PhD timeplan to do the training. It takes time to do modelling, and even more time if you are learning by doing.\nIn fact, if you are used to working on a well set-up study system, a modelling study may well take longer than doing the field or lab study. There is a lot of back and forwards between developing the model, finding suitable parameter settings, and updating your analysis.\nModelling studies are never really done, because you can always go back and tweak something and create different results. It takes time to get it right, longer than many people may think. (By right I mean results you can explain and make sense in terms of the ecology).\nSome good basic skills to have are of course in the R program, especially data wrangling skills and plotting skills (e.g. see my online notes for these things).\nThen, beyond that, the skills you will need depend a lot on the type of modelling you will do.\n(I’ll look to run some online training in the R program for marine scientists in the next few months. I was signed up for doing this anyway at 2 conferences, but now they are off! Stay tuned here for updates)\n\n\n\nYou may well be looking for data to analyse too. Good news is there is a lot of freely available data out there these days. But there are a few traps to avoid.\nThe biggest and best known datasets (e.g. like Landsat or the world database of protected areas) are very well analysed already. So it might be hard to find a novel question just using this data. This is also why it’s important to do your background reading. As they used to say, 30 minutes in the library can save weeks of fieldwork (or in this case modelling).\nCombining existing data together in new ways can be a good way to make something novel out of old data. So if you bring together landsat with some local data you have access too, you may well have a novel study.\nRemember that if you are using data, even if you’ve downloaded it ‘for free’ it is generally courteous to contact the data providers and let them know what you are doing. Obviously, you don’t need to ask to use landsat data (but do read the data license). But, say you want to use data published with a study, you should write the authors and tell them what you are doing. They may already be working on that analysis, so this simple step is both polite and will save you time.\nOtlet, a data sharing service, are offering a ‘match-making’ service to help students and postdocs find data. Thanks Gretta Pecl for sharing this tip.\n\n\n\nIf you are a field or lab ecologists about to embark on a modelling study, you have some unique strengths to bring to a modelling study.\nYou probably have a different perspective than most full-time modellers. You may have a better understanding of how the data is generated, and the issues that poses to making ecological interpretations. You may also see ecology operating at a different ‘scale’. For instance, I find as a modeller I tend to see more uniformity and generalities across different places, whereas field scientists see more of the details and interesting differences. Neither of us is right or wrong, we just bring different perspectives.\nBringing your perspective to your modelling will help make it novel.\nYou might also have knowledge about datasets that other people can’t access. Make the most of these opportunities, because they will add more ‘novelty’ to your analysis. For instance, maybe your supervisor is sitting on some ‘old’ data they never published. Or maybe there is a unique dataset sitting with a local government monitoring authority that you can get with the right paperwork.\nThe pandemic is a major disruption to all research. But there are some opportunities to pursue a new research direction and upskill. Talk to your supervisor about how to adapt your PhD to keep it on track. I hope you can make something positive out of this situation that is forced upon us, and be successful in your PhD."
  },
  {
    "objectID": "posts/2020-03-27-adding-desktop-based-work-to-your-phd/index.html#adding-desktop-based-work-to-your-phd-heres-some-things-to-think-about",
    "href": "posts/2020-03-27-adding-desktop-based-work-to-your-phd/index.html#adding-desktop-based-work-to-your-phd-heres-some-things-to-think-about",
    "title": "Adding desktop based work to your PhD? Here’s some things to think about",
    "section": "",
    "text": "The current pandemic is causing widespread disruptions to research, as travel, universities, labs and field stations shut-down. As a PhD student your timeline for submission of your thesis probably won’t be moved back because of the disruptions. Funding for scholarships still has the same limits as always. From what I’ve seen, universities are expecting students to adapt their work to the disruption and will not be putting forward blanket extensions to all PhDs.\nUnderstandably this situation creates a lot of anxiety.\nOne way to adapt your research is to replace field or lab work with desktop based research work. Whether this is appropriate for your PhD is a case-by-case decision and something you should discuss with your supervisor and mentors.\nA desktop based project might involve analysis of existing datasets, modelling or a literature review.\nIt seems like pursuing desktop based work is the advice university graduate schools and many supervisors will be giving their students. So thousands of researchers worldwide are about to embark on desktop based work, whether they have experience in this style of research or not.\nAs an ecosystem modelling, desktop based work is my bread and butter. So here’s a few things to think about if you are considering switching some of your field or lab research to modelling or data analysis.\n\n\nMany people underestimate how long it takes to develop models or data-analysis, just as I tend to under-estimate how complex field work will be. A story. Researchers in Australia wanted to watch fruit bats with a $7000 thermal camera. Sounds easy to me, just stick the camera up a tree. Problem is, coconut crabs might find the camera and rip it apart.\nThere are set backs in modelling too, that can be just as time consuming (though often less exciting). Like, the spatial data you want to process fills up your RAM, sending you on a 3 week journey looking for a suitable cloud-based processing service. Or the R package you’ve been using turns out to have a major bug.\nI commonly get asked to review or edit modelling papers. There are lots of common mistakes I see from groups not used to modelling. I could fill a whole blog with tips on that, but here’s a few major ones\n\nApplying an inappropriate modelling to the research question you want to answer. There are lots of models out there you can just download and start using. But models are usually developed for quite specific research questions, it may not be appropriate to use that same model for addressing different questions.\nBasic statistical mistakes, like treating psuedo-replicates as true replicates or mis-interpreting p-values.\nAbsence of sensitivity analysis, in the general sense (if you don’t know what that means, time to do some reading).\n\nThere’s a couple of things you can do to avoid these mistakes.\nFirst, get quantitative collaborators on board early and communicate with the regularly. Maybe your supervisor or lab group already has expertise in modelling, that is great, make use of it. Otherwise, you’ll have to put yourself out there and go looking (or ask your supervisor for help).\nCollaborators with expertise in the modelling you want to do will help you save time by avoiding common mistakes and help you find the right kind of methods for your research question.\nSecond, do your reading. Ecological modelling is a very large field with established norms for analysis. So read the literature in your area and find our what flies, what methods people commonly use.\nHere’s a few books I recommend my new ecology students read before starting modelling.\n\n\n\nGive yourself time to upskill. Take training appropriate to the type of modelling you want to do. And give yourself time in your PhD timeplan to do the training. It takes time to do modelling, and even more time if you are learning by doing.\nIn fact, if you are used to working on a well set-up study system, a modelling study may well take longer than doing the field or lab study. There is a lot of back and forwards between developing the model, finding suitable parameter settings, and updating your analysis.\nModelling studies are never really done, because you can always go back and tweak something and create different results. It takes time to get it right, longer than many people may think. (By right I mean results you can explain and make sense in terms of the ecology).\nSome good basic skills to have are of course in the R program, especially data wrangling skills and plotting skills (e.g. see my online notes for these things).\nThen, beyond that, the skills you will need depend a lot on the type of modelling you will do.\n(I’ll look to run some online training in the R program for marine scientists in the next few months. I was signed up for doing this anyway at 2 conferences, but now they are off! Stay tuned here for updates)\n\n\n\nYou may well be looking for data to analyse too. Good news is there is a lot of freely available data out there these days. But there are a few traps to avoid.\nThe biggest and best known datasets (e.g. like Landsat or the world database of protected areas) are very well analysed already. So it might be hard to find a novel question just using this data. This is also why it’s important to do your background reading. As they used to say, 30 minutes in the library can save weeks of fieldwork (or in this case modelling).\nCombining existing data together in new ways can be a good way to make something novel out of old data. So if you bring together landsat with some local data you have access too, you may well have a novel study.\nRemember that if you are using data, even if you’ve downloaded it ‘for free’ it is generally courteous to contact the data providers and let them know what you are doing. Obviously, you don’t need to ask to use landsat data (but do read the data license). But, say you want to use data published with a study, you should write the authors and tell them what you are doing. They may already be working on that analysis, so this simple step is both polite and will save you time.\nOtlet, a data sharing service, are offering a ‘match-making’ service to help students and postdocs find data. Thanks Gretta Pecl for sharing this tip.\n\n\n\nIf you are a field or lab ecologists about to embark on a modelling study, you have some unique strengths to bring to a modelling study.\nYou probably have a different perspective than most full-time modellers. You may have a better understanding of how the data is generated, and the issues that poses to making ecological interpretations. You may also see ecology operating at a different ‘scale’. For instance, I find as a modeller I tend to see more uniformity and generalities across different places, whereas field scientists see more of the details and interesting differences. Neither of us is right or wrong, we just bring different perspectives.\nBringing your perspective to your modelling will help make it novel.\nYou might also have knowledge about datasets that other people can’t access. Make the most of these opportunities, because they will add more ‘novelty’ to your analysis. For instance, maybe your supervisor is sitting on some ‘old’ data they never published. Or maybe there is a unique dataset sitting with a local government monitoring authority that you can get with the right paperwork.\nThe pandemic is a major disruption to all research. But there are some opportunities to pursue a new research direction and upskill. Talk to your supervisor about how to adapt your PhD to keep it on track. I hope you can make something positive out of this situation that is forced upon us, and be successful in your PhD."
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html",
    "href": "posts/2019-11-03-power-analysis-intro/index.html",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "A collaborator once asked me: “can you do that thing where you take a little bit of data and model it up to make lots more data?”\nDoes it sound suspicious to fake a lot of data from a little bit of data? It depends on the context. Making up data is totally appropriate if you want to do a power analysis.\nAnd ‘making-up’ data (let’s call it simulation from now on) for power analysis is a handy tactic for designing effective surveys and experiments. This post will look at why and how.\n\n\nIn the narrow-sense, power analysis it about type II error, or the chance your data and analysis won’t be able to detect an effect that is really there.\nType II errors are the quiet sibling to type I errors, which tend to get more focus. The whole “&lt;0.05 is significant”” mentality is about type I errors.\nHaving a low type I is important if you want to say a new drug, which may have side-effects, works.\nBut in many fields, type II errors are more consequential. In environmental science in particular, we don’t want significant (in the non-statistical sense of the word) environmental change to go unnoticed. Poor power would mean we are unlikely to detect environmental change in our acidification experiments, or from say an oil spill decimating a seabird colony.\nPragmatically, if you are doing a PhD you have a limited amount of time to get publishable results. So you’d want enough power to detect an effect, should it be there.\nPower analysis is also much more useful type II errors, when used in the broader sense of the term.\nBroad-sense power analysis is about how well data and a statistical model work together to measure an effect. So it could be about whether we measure the right effect (not just whether we measure it at all).\nIn the broad and narrow sense, power analysis is a really helpful tool when you are designing experiments, or a field survey.\nFor more on traditional power analysis in environmental stats I recommend the textbook Quinn and Keough (or google it to find a pdf) or in the broad-sense Bolker’s excellent book\n\n\n\nLet’s say you want to know whether there are more fish inside a marine reserve (that have no fishing) than outside the reserve. You are going to do a number of standardized transects inside and outside the reserve and count the numbers of fish.\nYour fish species of interest are a very abundant sweetlips and a rather rare humphead wrasse. What’s the chance that you would be able to detect a two times difference in abundance for each fish species between reserves and fished areas?\nWe can address this question with power analysis by simulating ‘fake’ data for the surveys where there is a doubling of abundance, then fitting a statistical model to the fake data, then deciding whether or not the difference is ‘significant’ (e.g. p&lt;0.05). Then we repeat that a 1000 times and count up the % of times we said there was a difference. That % is the power.\nSo we need to decide on a few things ahead of time, the sample size of surveys, the expected (mean) abundance values and the variance in abundance. This is where you could draw on earlier literature to make estimated guesses. The sample size is up for grabs and trying different sample sizes could be part of your power analysis.\nLet’s assume there are normally 10 sweetlips per transect and 1 humphead wrasse per transect.\nAs the data are counts we’ll assume they are Poisson distributed. This amounts to assuming mean = variance, so the variance of sweetlips across transects is 10 and wrasse is 1.\n\n\n\nTo answer this question with R we are going to use quite a few handy packages:\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(tidyr)\npurrr is handing for creating 1000s of randomised datasets, ggplot2 is for plots, broom is for cleaning the 1000s of models we’ll fit, dplyr and tidyr are for data wrangling.\nNow let’s create a function that simulates data and fits a model. This may look overwhelming, but don’t worry about the R details if you’re not that into R. All we are doing is creating a function that simultions some data from two groups (reserve or not) for n transects, and then fits a GLM and finally it spits out a p-value for whether there was a significant difference in the simulated data.\nsim &lt;- function(n, x1, x2){\n  x &lt;- rep(c(x1, x2), each = n/2)\n  y &lt;- rpois(n, lambda = x)\n  m1 &lt;- glm(y ~ x, family = \"poisson\") %&gt;% tidy()\n  m1\n}\nNow we can use our simulation function to simulate counting wrasse on 20 transects (10 inside and 10 outside the reserve), and then fitting the GLM to that data:\nset.seed(2001) #just do this to get the same result as me\nsim(100, 1, 2)\n\n## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   -0.522     0.287     -1.82 0.0690  \n## 2 x              0.618     0.167      3.69 0.000222\nSo we get a table with mean estimated difference (on log scale), standard errors and p-values.\n\n\n\nNow we use purrr to do this 1000 times:\nmout &lt;- map(1:1000, ~sim(20, 1, 2))\nWhich results in 1000 lists, yuck. Let’s do some data wrangling on the output:\n mout2 &lt;- mout %&gt;%\n  bind_rows(.id = \"rep\") %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(Signif = p.value &lt; 0.05,\n         rep = as.numeric(rep))\nhead(data.frame(mout2))\n\n##   rep term      estimate std.error     statistic     p.value Signif\n## 1   1    x  6.931472e-01 0.4082471  1.697862e+00 0.089533876  FALSE\n## 2   2    x  8.266786e-01 0.4531632  1.824240e+00 0.068115744  FALSE\n## 3   3    x  5.877867e-01 0.3220304  1.825252e+00 0.067963001  FALSE\n## 4   4    x  1.145132e+00 0.4339488  2.638865e+00 0.008318414   TRUE\n## 5   5    x  1.823216e-01 0.3496022  5.215114e-01 0.602010565  FALSE\n## 6   6    x -2.823275e-13 0.3429971 -8.231194e-13 1.000000000  FALSE\nNow we get a dataframe of the 1000 simulations, indicating whether p for the difference between reserve vs unreserved was &lt;0.05 (column ‘Signif’).\nTo get the power, we just sum Signif and divide by the 1000 trials:\nsum(mout2$Signif)/1000\n\n## [1] 0.408\nSo an ~40% chance we’d detect a 2x difference in wrasse abundance with 20 transects. This is the 2-sided probability, arguably for this question we could also use a one-sided test.\nTry it again for the sweetlips (expected abundance doubling from 10 to 20). You’ll see you get much more power with this more abundance species (almost 100%).\nYou could try this with different sample sizes to get an idea of how much effort you need to invest in doing transects in order to see a difference (if the difference is really there of course).\n\n\n\nHow close does our approach get us to the 2x difference? We can also answer that by looking at the estimates from the GLM:\nggplot(mout2, aes(x = exp(estimate))) +\n  geom_density(fill = \"tomato\") +\n  theme_bw() +\n  geom_vline(xintercept = 2) +\n  xlab(\"Estimated times difference\")\n\nThis distribution shows the expected outcomes we’d estimate over 1000 repeats of the surveys. So the solid vertical line is the ‘real’ difference. Note the long tail to the left of drastic overestimates. It is common with small sample sizes that we might overestimate the true effect size. More on this later.\nI took the exponent of the estimate (estimated mean difference), because the Poisson GLM has a log link, so the estimate is on the log scale. Taking its exponent means it is now interpreted as a times difference (as per the x-axis label).\n\n\n\nIt is reasonably well known that over-use of p-values can contribute to publication bias, where scientists tend to publish papers about significant and possibly overestimated effect sizes, but never publish the non-significant results. This bias can be particularly bad with small sample sizes, because there’s a reasonable chance we’ll see a big difference and therefore, make a big deal about it.\nWe can look at this phenomena in our simulations. First, let’s take the mean of our estimated effect sizes for those trials that were significant and those that were not:\nsignif_mean &lt;- mean(exp(filter(mout2, Signif)$estimate))\nnonsignif_mean &lt;- mean(exp(filter(mout2, !Signif)$estimate))\nall_mean &lt;- mean(exp(mout2$estimate))\nc(all_mean, signif_mean, nonsignif_mean)\n\n## [1] 2.210280 3.062810 1.622725\nSo average effect size for the significant trials is &gt;3x (remember the real difference is 2x). If we take the average across all trials it is closer to the truth (2.3x).\nClearly if we only publish the significant results, over many studies this will add up to a much bigger difference than is really there. This can be a problem in some fields. I don’t think publication bias particularly affects studies of marine reserves, because typically there are multiple research questions, so the researchers will publish anyway.\nLet’s look at this as a plot. We’ll do the same distribution as above, but with different colours for significant versus non-significant.\nggplot(mout2, aes(x = exp(estimate), fill = Signif)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  geom_vline(xintercept = 2) +\n  xlab(\"Estimated times difference\") +\n  xlim(0,5)\n\nYou can clearly see the significant trials almost always overestimate the true difference (vertical line).\nWhat’s the solution? Make sure you report on non-significant results. And try to aim for larger sample sizes."
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html#a-brief-introduction-to-power-analysis",
    "href": "posts/2019-11-03-power-analysis-intro/index.html#a-brief-introduction-to-power-analysis",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "In the narrow-sense, power analysis it about type II error, or the chance your data and analysis won’t be able to detect an effect that is really there.\nType II errors are the quiet sibling to type I errors, which tend to get more focus. The whole “&lt;0.05 is significant”” mentality is about type I errors.\nHaving a low type I is important if you want to say a new drug, which may have side-effects, works.\nBut in many fields, type II errors are more consequential. In environmental science in particular, we don’t want significant (in the non-statistical sense of the word) environmental change to go unnoticed. Poor power would mean we are unlikely to detect environmental change in our acidification experiments, or from say an oil spill decimating a seabird colony.\nPragmatically, if you are doing a PhD you have a limited amount of time to get publishable results. So you’d want enough power to detect an effect, should it be there.\nPower analysis is also much more useful type II errors, when used in the broader sense of the term.\nBroad-sense power analysis is about how well data and a statistical model work together to measure an effect. So it could be about whether we measure the right effect (not just whether we measure it at all).\nIn the broad and narrow sense, power analysis is a really helpful tool when you are designing experiments, or a field survey.\nFor more on traditional power analysis in environmental stats I recommend the textbook Quinn and Keough (or google it to find a pdf) or in the broad-sense Bolker’s excellent book"
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html#power-analysis-for-experimental-and-survey-design",
    "href": "posts/2019-11-03-power-analysis-intro/index.html#power-analysis-for-experimental-and-survey-design",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "Let’s say you want to know whether there are more fish inside a marine reserve (that have no fishing) than outside the reserve. You are going to do a number of standardized transects inside and outside the reserve and count the numbers of fish.\nYour fish species of interest are a very abundant sweetlips and a rather rare humphead wrasse. What’s the chance that you would be able to detect a two times difference in abundance for each fish species between reserves and fished areas?\nWe can address this question with power analysis by simulating ‘fake’ data for the surveys where there is a doubling of abundance, then fitting a statistical model to the fake data, then deciding whether or not the difference is ‘significant’ (e.g. p&lt;0.05). Then we repeat that a 1000 times and count up the % of times we said there was a difference. That % is the power.\nSo we need to decide on a few things ahead of time, the sample size of surveys, the expected (mean) abundance values and the variance in abundance. This is where you could draw on earlier literature to make estimated guesses. The sample size is up for grabs and trying different sample sizes could be part of your power analysis.\nLet’s assume there are normally 10 sweetlips per transect and 1 humphead wrasse per transect.\nAs the data are counts we’ll assume they are Poisson distributed. This amounts to assuming mean = variance, so the variance of sweetlips across transects is 10 and wrasse is 1."
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html#simulating-data-with-r",
    "href": "posts/2019-11-03-power-analysis-intro/index.html#simulating-data-with-r",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "To answer this question with R we are going to use quite a few handy packages:\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(tidyr)\npurrr is handing for creating 1000s of randomised datasets, ggplot2 is for plots, broom is for cleaning the 1000s of models we’ll fit, dplyr and tidyr are for data wrangling.\nNow let’s create a function that simulates data and fits a model. This may look overwhelming, but don’t worry about the R details if you’re not that into R. All we are doing is creating a function that simultions some data from two groups (reserve or not) for n transects, and then fits a GLM and finally it spits out a p-value for whether there was a significant difference in the simulated data.\nsim &lt;- function(n, x1, x2){\n  x &lt;- rep(c(x1, x2), each = n/2)\n  y &lt;- rpois(n, lambda = x)\n  m1 &lt;- glm(y ~ x, family = \"poisson\") %&gt;% tidy()\n  m1\n}\nNow we can use our simulation function to simulate counting wrasse on 20 transects (10 inside and 10 outside the reserve), and then fitting the GLM to that data:\nset.seed(2001) #just do this to get the same result as me\nsim(100, 1, 2)\n\n## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   -0.522     0.287     -1.82 0.0690  \n## 2 x              0.618     0.167      3.69 0.000222\nSo we get a table with mean estimated difference (on log scale), standard errors and p-values."
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html#narrow-sense-power-analysis",
    "href": "posts/2019-11-03-power-analysis-intro/index.html#narrow-sense-power-analysis",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "Now we use purrr to do this 1000 times:\nmout &lt;- map(1:1000, ~sim(20, 1, 2))\nWhich results in 1000 lists, yuck. Let’s do some data wrangling on the output:\n mout2 &lt;- mout %&gt;%\n  bind_rows(.id = \"rep\") %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(Signif = p.value &lt; 0.05,\n         rep = as.numeric(rep))\nhead(data.frame(mout2))\n\n##   rep term      estimate std.error     statistic     p.value Signif\n## 1   1    x  6.931472e-01 0.4082471  1.697862e+00 0.089533876  FALSE\n## 2   2    x  8.266786e-01 0.4531632  1.824240e+00 0.068115744  FALSE\n## 3   3    x  5.877867e-01 0.3220304  1.825252e+00 0.067963001  FALSE\n## 4   4    x  1.145132e+00 0.4339488  2.638865e+00 0.008318414   TRUE\n## 5   5    x  1.823216e-01 0.3496022  5.215114e-01 0.602010565  FALSE\n## 6   6    x -2.823275e-13 0.3429971 -8.231194e-13 1.000000000  FALSE\nNow we get a dataframe of the 1000 simulations, indicating whether p for the difference between reserve vs unreserved was &lt;0.05 (column ‘Signif’).\nTo get the power, we just sum Signif and divide by the 1000 trials:\nsum(mout2$Signif)/1000\n\n## [1] 0.408\nSo an ~40% chance we’d detect a 2x difference in wrasse abundance with 20 transects. This is the 2-sided probability, arguably for this question we could also use a one-sided test.\nTry it again for the sweetlips (expected abundance doubling from 10 to 20). You’ll see you get much more power with this more abundance species (almost 100%).\nYou could try this with different sample sizes to get an idea of how much effort you need to invest in doing transects in order to see a difference (if the difference is really there of course)."
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html#broad-sense-power-analysis",
    "href": "posts/2019-11-03-power-analysis-intro/index.html#broad-sense-power-analysis",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "How close does our approach get us to the 2x difference? We can also answer that by looking at the estimates from the GLM:\nggplot(mout2, aes(x = exp(estimate))) +\n  geom_density(fill = \"tomato\") +\n  theme_bw() +\n  geom_vline(xintercept = 2) +\n  xlab(\"Estimated times difference\")\n\nThis distribution shows the expected outcomes we’d estimate over 1000 repeats of the surveys. So the solid vertical line is the ‘real’ difference. Note the long tail to the left of drastic overestimates. It is common with small sample sizes that we might overestimate the true effect size. More on this later.\nI took the exponent of the estimate (estimated mean difference), because the Poisson GLM has a log link, so the estimate is on the log scale. Taking its exponent means it is now interpreted as a times difference (as per the x-axis label)."
  },
  {
    "objectID": "posts/2019-11-03-power-analysis-intro/index.html#bias-in-significant-estimates",
    "href": "posts/2019-11-03-power-analysis-intro/index.html#bias-in-significant-estimates",
    "title": "Using R and power analysis to inform experimental design",
    "section": "",
    "text": "It is reasonably well known that over-use of p-values can contribute to publication bias, where scientists tend to publish papers about significant and possibly overestimated effect sizes, but never publish the non-significant results. This bias can be particularly bad with small sample sizes, because there’s a reasonable chance we’ll see a big difference and therefore, make a big deal about it.\nWe can look at this phenomena in our simulations. First, let’s take the mean of our estimated effect sizes for those trials that were significant and those that were not:\nsignif_mean &lt;- mean(exp(filter(mout2, Signif)$estimate))\nnonsignif_mean &lt;- mean(exp(filter(mout2, !Signif)$estimate))\nall_mean &lt;- mean(exp(mout2$estimate))\nc(all_mean, signif_mean, nonsignif_mean)\n\n## [1] 2.210280 3.062810 1.622725\nSo average effect size for the significant trials is &gt;3x (remember the real difference is 2x). If we take the average across all trials it is closer to the truth (2.3x).\nClearly if we only publish the significant results, over many studies this will add up to a much bigger difference than is really there. This can be a problem in some fields. I don’t think publication bias particularly affects studies of marine reserves, because typically there are multiple research questions, so the researchers will publish anyway.\nLet’s look at this as a plot. We’ll do the same distribution as above, but with different colours for significant versus non-significant.\nggplot(mout2, aes(x = exp(estimate), fill = Signif)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  geom_vline(xintercept = 2) +\n  xlab(\"Estimated times difference\") +\n  xlim(0,5)\n\nYou can clearly see the significant trials almost always overestimate the true difference (vertical line).\nWhat’s the solution? Make sure you report on non-significant results. And try to aim for larger sample sizes."
  },
  {
    "objectID": "posts/2022-04-19-climate-smart-mangrove-restoration/index.html",
    "href": "posts/2022-04-19-climate-smart-mangrove-restoration/index.html",
    "title": "Climate smart mangrove restoration",
    "section": "",
    "text": "The extent of loss of coastal ecosystems means that to recover what we’ve lost we need to restore coastal habitats like mangroves. But coastal restoration is risky in a rapidly changing climate.\nIn our new project we will be working with the World Wildlife Fund (US Oceans team) to look at climate smart restoration and protection of mangroves. The project “Global Prioritization of Mangrove Conservation and Restoration incorporating future Climate Change Impacts” is lead by Associate Professor Chris Brown (The Global Wetlands Project) and Dr Dominic Andradi-Brown (WWF-US).\nInvestment in mangrove conservation actions is rapidly scaling-up, so we need to make sure these well intentioned efforts aren’t undone by the future impacts of climate change. Two climate impacts of particular importance on the coast are sea level rise and storms. Sea levels are rising with global warming and these rises will accelerate into the future. Tropical storms can damage or wipe out mangrove forests. Trends for storms are variable, but overall the frequency of severe storms is expected to increase in the future.\nOur project has two parts. The first part will be lead by Dr Christina Buelow, a coastal ecosystem modeller from The Global Wetlands Project.\nDr Buelow will integrate climate models with models of mangrove forest change to predict locations globally that are most likely to be affected by climate impacts. We will then use this information to identify areas that are low risk for mangrove conservation and areas where additional adaptation measures will be needed (such as planned retreat or modifying the species composition of restoration actions).\nThe second part focuses on local-scale action and is led by socio-ecological scientist Dr Jaramar Villareal-Rosas. She will lead work that supports WWF offices across the globe to implement climate smart conservation plans for mangroves. This will include adapting restoration techniques so that restored habitat is more resilient to climate change, and helping teams identify sites that are resilient to climate change.\nOverall this project aims to advance our ability to plan mangrove conservation for climate change, at local and global scales.\nThe Global Wetlands Project team on the project includes: Associate Professor Chris Brown (Lead), Dr Christina Buelow, Dr Jaramar Villareal-Rosas, Professor Rod Connolly, Dr Mischa Turschwell and Dr Fernanda Adame."
  },
  {
    "objectID": "posts/2023-06-14-university-teaching-social-media/index.html",
    "href": "posts/2023-06-14-university-teaching-social-media/index.html",
    "title": "University education as social media",
    "section": "",
    "text": "What social media has that much online teaching lacks is its highly engaging while being cheap to produce.\nI like my undergraduate teaching and I’m always trying to improve at that. I’m also interested in social media and have dabbled in text, image and video based social media.\nThis got me thinking: the approach to creating social media has many advantages that could be applied to creating content university undergraduate degrees. I’m thinking particularly for online content.\nUniversity education has increasingly moved online, especially since covid. Currently what we see in many places is just the same old lectures delived through video or provided as recordings. I’m the first to admit that these are often dead boring to watch, and I’ve recorded many of these myself.\nIts just hard to find the motivation to sit down and watch 50 minutes of powerpoint slides, especially if you have the freedom to watch it whenever you want (meaning you’ll always find something more interesting to do instead).\nSo one solution is to work with digital specialists to create engaging online content. This could be games or interactive platforms where the user explores a landscape.\nThat’s great when you have the resources, but its expensive (lots of staff time) to create content like that. Staff time is one of the main constraints on the quality of university teaching. Staff only have time to create so much content when there’s so much else to do with administering courses, running assessments and so on.\nSo how can we create lots of engaging online content quickly, easily and cheapily?\nSocial media is designed to make cheap content engaging, indeed often highly addictive.\nI often find myself on youtube for much longer than 50 minutes, without even intending too.\nHow that happens is I start watching one short video “its only five minutes”, that then leads me to another video and so on.\nAnd each of these videos is done on the cheap, often only with a phone camera and perhaps a microphone or gopro.\nWhat the content creaters always have is a great story and engaging presentation skills (whether that’s with video, writing text or taking photos).\nContent creation is a skill that can be learned with practice. For instance, when I started doing youtube I realised I looked super grumpy on video. So I practiced smiling more for video and straightaway my content became more engaging.\nCreating a good video (or thread or blog) does require a bit of planning. Key is to think about the story. This is a change from the usual style of a university lecturer, which is to just explain facts straight-up.\nThe facts are more easily digested if they are weaved into a story. A bonus is that the viewer is also more likely to remember them if they are told as a story. Our brains are wired for stories, not sequencese of facts.\nEspecially not sequences of facts that go for 50 minutes.\nSo here’s my vision. We give lecturers decent cameras (or phones) and microphones. We give them some basic training in using the gear, and also some training in storytelling. Then we send them out to create content.\nSome of this content will be carefully planned for class. Other content can be recorded on the fly when they are doing their research.\nImagine a marine scientist lecturer is out on a field trip when they see birds diving into the sea to catch fish. They just grab some video of the action, and then quickly record themselves talking about predation in the ocean. It helps that they are super excited, because they are on the scene of predation in action.\nOr imagine the lecturer is at a conference and meets the person who wrote the seminal paper the class will be studying. They grab a quick interview and get the cool story behind the research on how the researcher risked a bear attack to collect the data.\nAll this videos are short. 7 minutes or even shorter is ideal, because that is what people are happy to watch. The lecturer weaves the series of videos into a lesson. The catch is, once you start the first 7 minutes its good enough that students are happy to roll onto the next one and the next one. Before they know it your student has consumed 50 minutes ‘lectures’.\nWhen I get the time I’ll make an example of a social media style lecture and post it to my youtube channel\nYou may same this will all take more time than just doing a powerpoint. But you’d be surprised how quick you can make videos once you get an eye for good content.\nI can tell you, making the videos is a whole lot more fun too."
  },
  {
    "objectID": "posts/2025-06-10-open-access-data-for-teaching/index.html",
    "href": "posts/2025-06-10-open-access-data-for-teaching/index.html",
    "title": "Open repo of ecological data for use in teaching",
    "section": "",
    "text": "Just a quick announcement. I’ve organized and posted two ecological datasets that I find helpful for creating examples for teaching. The repo and instructions for use and attribution are on github:\nhttps://github.com/cbrown5/example-ecological-data\nThere are two sets of data posted, each with multiple flat csv files.\nOne is field survey data of benthic habitats and fish. Useful for data wrangling of site survey data, multivariate community analysis and GLMs with count data\nThe second is experimental data of algal growth under multiple stressors. Useful for teaching missing data, plotting trends over time and analysing interacting effects with GLMs.\nAll data can be downloaded directly into R like this:\nbenthic_cover_url &lt;- \"https://raw.githubusercontent.com/cbrown5/example-ecological-data/refs/heads/main/data/benthic-reefs-and-fish/benthic_cover.csv\"\n\nlibrary(readr)\ndat &lt;- read_csv(benthic_cover_url)\nwrite_csv(dat, \"benthic_cover.csv\")"
  },
  {
    "objectID": "posts/2021-04-05-coral-reef-fisheries/index.html",
    "href": "posts/2021-04-05-coral-reef-fisheries/index.html",
    "title": "How will heatwaves and coral loss affect reef fisheries?",
    "section": "",
    "text": "There has been a lot of focus on the challenge of managing overfishing with the controversial new documentary Seaspiracy on Netflix. But the overfishing issue isn’t as simple an issue as the picture that documentary paints.\nThe sustainability of coral reef fisheries around the world is threatened by too much fishing, but also by extensive change in reef habitats.\nNotably, heatwaves are becoming more intense and frequent under climate change and are now a main driver of coral loss. But how will heatwaves and coral loss impact the sustainability of reef fisheries? This is a question we have been working on over the past few years.\nPlease contact me (chris.brown@griffith.edu.au) if you can’t access the full-text versions of the work below (some but not all are open access).\nOverfishing is the main human-driven cause of stock declines, but we shouldn’t focus solely on that issue. A significant proportion of fish stocks at a global scale are dependent on habitats that have been degraded over recent decades, like corals.\nBut, it is hard to pin fishery declines on habitat loss, because the mechanisms for habitat loss to impact fisheries are complex. We found that habitat loss interacts with fishery management in surprising ways.\nHabitat loss can contribute to overfishing because it reduces the productivity of fisheries. But, if those declines in productivity are detected by monitoring and wrongly attributed to overfishing, the management response could actually make management more conservative, as we described in our review of how habitat loss impacts fisheries.\n\n\n\nsweetlips on a reef\n\n\nHabitat loss likely threatens many of the world’s coral reef fisheries. An unprecedented heatwave in 2016 affected reefs globally, and on Australia’s Great Barrier Reef caused extensive mortality of corals. We studied this event to see how it may impact reef fisheries.\nWe reviewed Queensland’s fisheries for the effects of coral loss on key fishery species. There was marked regional variation within Queensland in the dependence of fisheries on reef species. This variation corresponded to the types of fisheries dominating particular regions.\nAquarium collection fisheries (including fish, corals and other invertebrates) and the reef line fishery are the most likely to be affected by coral habitat loss. Importantly, quantitative stock analysis identified several reef snapper (Lutjanidae) species that are at risk of overfishing due to the interaction of fishing pressures and coral habitat declines.\nHabitat loss is not the only pathway for heatwaves to impact reef fisheries. Heatwaves also have direct effects on fish.\nThere was extensive coral mortality, but most fish species did not respond to this habitat loss in the short-term. Rather, there was a direct effect of the warming on the fish community, such that there were species declines in the northern warm waters and increases in species richness on some southern reefs. These changes are consistent with range shifts in the fish community.\n\n\n\neffects of heatwaves on coral trout\n\n\nIf heatwaves have direct physiological effects on fish communities, they may also how easily fish can be caught in fisheries. We addressed this question for coral trout, the most important commercial finfish taxa on the Great Barrier Reef.\nWe used a unique dataset of reef fish surveys, collected by citizen scientists working with the Reef Life Survey not-for-profit. We matched that data to fish catch data. This enabled us to look at how fish populations and fish catch changed over the heatwave.\nWe found modest evidence for a southward shift in coral trout after the heatwave. More important though was that we found coral trout were easier to catch when it is warmer. We predicted the heatwave raised catches by as much as 18%, given the for the amount of fishing effort in the heatwave year.\nWe hypothesised that coral trout got easier to catch during the heatwave because they were hungrier. Coral trout eat more when it’s warmer, to keep up with their metabolic demands. The line fishery uses baited lines and lures, so coral trout may have been more attracted to bait when temperatures were raised.\nIf fish get easier to catch then the risk of overfishing them could also be higher. The current management regime for the Great Barrier Reef fishery is based on a total allowable catch. If this system is adhered to, it should prevent overfishing of coral trout, even if they are easier to catch in the future. In many other coral reef fisheries around the world though there are no limits on catch. In such places overfishing will be a greater risk under recurrent heatwaves.\nOverall this body of work has identified multiple ways that heatwaves impact reef fisheries. Importantly habitat loss and the direct effects of warming will both impact fish species, but the sustainability of the fishery also depends on how it is managed. This finding provides hope that proactive fishery management can maintain sustainable fisheries in the face of climate change.\nThe work discussed in this article was supported by an Australian Research Council Discovery Early Career Award to Chris Brown"
  },
  {
    "objectID": "posts/2019-09-21-glm-tip-non-linear-splines/index.html",
    "href": "posts/2019-09-21-glm-tip-non-linear-splines/index.html",
    "title": "GLM tip getting non-linear with splines",
    "section": "",
    "text": "This tip is great for a quick non-linear test, before you go all the way with a GAM or parametric non-linear model.\nYou’ll need the splines library, which comes shipped with R anyway.\nFirst, let’s make up a bit of count data. The underlying ‘true’ model will be poisson (think count data) with a log link (so slope estimates are multiplicative of the poisson mean). But we’ll introduce a bit of non-linearity.\nn &lt;- 100\nset.seed(101)\nx &lt;- sort(rnorm(n, sd = 2))\nmu &lt;- 2 + 0.1*x - 0.6*x^2 + 0.18*x^3#linear predictor\n\ny &lt;- rpois(n, exp(mu))\nplot(x, y)\nlines(x, exp(mu))\n\nNow, we could just fit a polynomial, but for real data we wouldn’t know the mean structure was generated as a polynomial. So we might want to use something a bit more flexible, like a cubic spline.\nSo, here’s how to make a cubic spline. We just need to choose the degrees of freedom. A DF of of 1 will give us a linear fit, higher DFs allow more bends (‘knots’). We’ll fit a log-linear model, a model with df = 2 and a model with df= 3 Given we generated our data with a cubic polynomial, we’d expect the 3 df model will do best\nlibrary(splines)\n#log linear model\nm1 &lt;- glm(y ~ x, family = \"poisson\")\nm1pred &lt;- predict(m1, type = \"response\")\n\n#non-linear models\nm2 &lt;- glm(y ~ ns(x,2), family = \"poisson\")\nm2pred &lt;- predict(m2, type = \"response\")\n\nm3 &lt;- glm(y ~ ns(x,3), family = \"poisson\")\nm3pred &lt;- predict(m3, type = \"response\")\n\npar(mfrow = c(1,3))\nplot(x, y, main = \"DF = 1\")\nlines(x, exp(mu), lwd = 2, col = \"grey\")\nlines(x, m1pred, col = \"orange\", lwd = 2)\n\nplot(x, y, main = \"DF = 2\")\nlines(x, exp(mu), lwd = 2, col = \"grey\")\nlines(x, m2pred, col = \"purple\", lwd = 2)\n\nplot(x, y, main = \"DF = 3\")\nlines(x, exp(mu), lwd = 2, col = \"grey\")\nlines(x, m3pred, col = \"darkblue\", lwd = 2)\n The orange line is the naive linear fit, it basically shows no trend. The purple line (df = 2) does better, but misses the kick up at the end. The blue line looks closest to the ‘true’ mean function (grey line).\nThe purple and blue lines are our spline fits with 1 and 2 knots respectively. Both clearly capture the non-linearity. The grey line is the ‘true’ mean structure we created above. So our splines just peaks a bit too hard, but does get the shape right.\nWe can convince ourselves that the 3 df splines model is better with AIC\nAIC(m1)\n\n## [1] 492.1094\n\nAIC(m2)\n\n## [1] 434.6807\n\nAIC(m3)\n\n## [1] 362.3653\nYep, the 3 spline model has a much lower AIC despite using more model D.F., so is better.\n\n\nA nice feature of this cubic spline trick is that you can use it anywhere that takes a model matrix as input. So it will work with glm, glmer, lmer and any Bayesian GLM method you care to use. Just apply your normal model selection criteria to find the ‘best’ number of knots.\nFirst, set the knots using x:\nlibrary(splines)\nxcs &lt;- ns(x, 3) #3 knots!\nhead(xcs, 3)\n\n##                1         2           3\n## [1,]  0.00000000 0.0000000  0.00000000\n## [2,] -0.04388799 0.1157184 -0.07117461\n## [3,] -0.04775128 0.1262731 -0.07766646\n\nplot(x, xcs[,1], type = 'l', ylim = c(min(xcs), max(xcs)))\nlines(x, xcs[,2], col = \"salmon\")\nlines(x, xcs[,3], col = \"steelblue\")\n\nThis splits x into three covariates (note the new matrix xcs has three columns), which I’ve plotted above. The cubic spline algorithm puts bends in the new covariates according to the density of the data.\nWe can use these new covariates in our model and glm will estimate a coefficient for each one.\nBecause the xcs are a non-linear functions of x, fitting a model against them means we can mix the curves to get a non-linear fit. We do lose a few degrees of freedom though, because now x is three covariates, instead of just one.\nWe can just use xcs in our model formula as a covariate.\nBut I prefer just to put the ns command directly into the model formula, that way it is easy to change the knots, as we did above.\n\n\n\nIf you really want to get into non-linear trend fitting, you should use a generalized additive model (GAM), such as from package mgcv.\nIn fact, GAM can fit these kind of splines for you too (and more), it just uses a different method to select the number of knots.\nBut the splines trick is still handy. For instance, you can quickly modify an existing linear model to have a non-linear spline. Or use it in a Bayesian linear model that doesn’t have a GAM equivalent (you can also spit out model matrices from mgcv and use them in Bayesian models if you want other types of splines).\nSo I hope you enjoying splining."
  },
  {
    "objectID": "posts/2019-09-21-glm-tip-non-linear-splines/index.html#transfer-your-spline-skills-anywhere",
    "href": "posts/2019-09-21-glm-tip-non-linear-splines/index.html#transfer-your-spline-skills-anywhere",
    "title": "GLM tip getting non-linear with splines",
    "section": "",
    "text": "A nice feature of this cubic spline trick is that you can use it anywhere that takes a model matrix as input. So it will work with glm, glmer, lmer and any Bayesian GLM method you care to use. Just apply your normal model selection criteria to find the ‘best’ number of knots.\nFirst, set the knots using x:\nlibrary(splines)\nxcs &lt;- ns(x, 3) #3 knots!\nhead(xcs, 3)\n\n##                1         2           3\n## [1,]  0.00000000 0.0000000  0.00000000\n## [2,] -0.04388799 0.1157184 -0.07117461\n## [3,] -0.04775128 0.1262731 -0.07766646\n\nplot(x, xcs[,1], type = 'l', ylim = c(min(xcs), max(xcs)))\nlines(x, xcs[,2], col = \"salmon\")\nlines(x, xcs[,3], col = \"steelblue\")\n\nThis splits x into three covariates (note the new matrix xcs has three columns), which I’ve plotted above. The cubic spline algorithm puts bends in the new covariates according to the density of the data.\nWe can use these new covariates in our model and glm will estimate a coefficient for each one.\nBecause the xcs are a non-linear functions of x, fitting a model against them means we can mix the curves to get a non-linear fit. We do lose a few degrees of freedom though, because now x is three covariates, instead of just one.\nWe can just use xcs in our model formula as a covariate.\nBut I prefer just to put the ns command directly into the model formula, that way it is easy to change the knots, as we did above."
  },
  {
    "objectID": "posts/2019-09-21-glm-tip-non-linear-splines/index.html#what-about-gams",
    "href": "posts/2019-09-21-glm-tip-non-linear-splines/index.html#what-about-gams",
    "title": "GLM tip getting non-linear with splines",
    "section": "",
    "text": "If you really want to get into non-linear trend fitting, you should use a generalized additive model (GAM), such as from package mgcv.\nIn fact, GAM can fit these kind of splines for you too (and more), it just uses a different method to select the number of knots.\nBut the splines trick is still handy. For instance, you can quickly modify an existing linear model to have a non-linear spline. Or use it in a Bayesian linear model that doesn’t have a GAM equivalent (you can also spit out model matrices from mgcv and use them in Bayesian models if you want other types of splines).\nSo I hope you enjoying splining."
  },
  {
    "objectID": "posts/2022-10-20-coaching-up/index.html",
    "href": "posts/2022-10-20-coaching-up/index.html",
    "title": "Coaching your supervisor",
    "section": "",
    "text": "When we think of coaching we usually think of someone more experienced, like a supervisor, coaching someone less experienced, like their junior staff or students. But staff and students can also coach upwards to their supervisor as well.\nFirst a word on coaching, managing and mentoring. Mentoring is typically showing someone a path that you have trodden yourself. So you are teaching someone something you know well how to do. Whereas, coaching is about developing the person’s skills and helping them find direction in their career. So you can coach someone without having any expertise in their subject area.\nPeople sometimes talk about ‘managing up’. Managing up differs from coaching up. Managing up is task focused. One example of managing up would be communicating to your supervisor clear expectations about when you can complete a task, like “I’m on holidays for the next two weeks, so you won’t get the next draft until the end of November”.\nCoaching up is person focused. Whether you know it or not, there is a lot that you can teach you supervisor. The beauty of coaching is that you don’t have to know the answers in advance.\nHere’s some examples.\nFirst, you can think about their role and perspective, rather than your own. Say they have been consistently late on returning feedback on drafts and you are feeling frustrated by that. Before you tell them about your frustration, ask questions to learn why they are consistently late. For instance you could ask “What’s been holding most of your attention at work recently?” Maybe they have a major grant deadline approaching, are overloaded with teaching or have challenges in their personal life.\nOnce you understand why they are late, and that it may not be their fault, you are in a better position to collaborate with them on solutions to getting feedback on your work.\nAnother example is if you have negative feedback for them. Let’s say your supervisor was very aggressive in questioning another student at the student’s seminar and you noticed the student was upset afterwards.\nIt might be tempting to tell the supervisor straight-up that they were being mean, or avoid the confrontation and not say anything at all. But you can sidestep the confrontation while still giving them the feedback by coaching them through to finding a better approach.\nYou can ask them questions like, “how do you think the student felt after presenting their seminar?” or “did you notice any changes in the student’s behaviour after their seminar?”\nYou could then follow up with revealing the information you have, that the student seemed very upset by the questions.\nYou can give positive feedback on their supervision as well. My students and staff are great at this, but I understand it’s not common. For example, I was coaching one staff member on the decision to invest effort into applying for a grant. I was asking them if having the grant was aligned with where they saw their career, and work-life balance, in the future.\nThe staff member followed up later to tell me how beneficial that discussion had been, and that it had helped them think about this decision in a new way. Now they are coaching me, by providing me with positive reinforcement on the approach to supervision I was trying in our earlier discussion.\nOne final example. When I meet with senior staff I often ask them how they feel about certain events. I’m just curious to know what they make of it. I might ask about a positive thing, like a if they enjoyed the seminar from a visiting prof. Or something that might be troubling them, like a how they feel about a change in the priorities of their major funder.\nGiving people space to talk about how events have impacted their emotions is part of coaching, and senior staff often aren’t given that space.\nIf you are a supervisor you can encourage coaching up by creating an environment where sharing feelings and positive feedback are welcomed. Modelling coaching in your interactions with staff and students will also help them learn the skill.\nFinally, coaching takes time so be patient. It’s a long-term game, but in my experience I’ve see the biggest changes in people’s approaches to work through coaching.\nJust want to say a huge thanks to my coaches. Especially Chris Reilly and Mary Saunders, who teach an amazing course on Manager as Coach at Griffith University. Also huge thanks to my staff and students, many of whom excel at coaching up. Thanks to their insightful questions, I often leave our meetings rethinking how I will approach being a supervisor."
  },
  {
    "objectID": "posts/2020-04-22-trees-greener-near-salmon/index.html",
    "href": "posts/2020-04-22-trees-greener-near-salmon/index.html",
    "title": "Are trees greener near salmon spawning streams?",
    "section": "",
    "text": "We usually think of nutrients moving down rivers to the ocean, not the other way around. But in many temperate rivers of the northern hemisphere migration of salmon brings nutrients from the ocean to rivers and surrounding vegetation.\nSalmon migrate to freshwater to spawn, then they die. Their carcasses thus become fertilizer for local vegetation. These nutrients have remarkable effects on stream-side ecosystems, like increasing [plant diversity]](https://science.sciencemag.org/content/331/6024/1609?casa_token=SbM0_-Wr3WIAAAAA:k9ajEC6j5fa5EdyYDDI2Hn7tpkR5RgV3j4tiSi7bYhidXBi13kd6P87WGkC_HFZzh8Bwn_T3nPNJ__M), raising bird diversity and increasing tree growth.\nWe wanted to know if all this fertilization of local vegetation showed up in satellite images. If we could measure the salmon fertilization effect with satellites, then we could build a complete picture of how far and across how many streams this fertilization effect is important.\nMeasuring the footprint of salmon nutrients is important for the management of streamside ecosystems. This connection may be weakened if fisheries catch too many salmon. Salmon populations in many places are also threatened by development and mining in their catchments.\nIt may seem odd for an Australian researcher to work on northern salmon, but this work was facilitated by a trans-Pacific connection. Griffith has a kind of sister-school relationship with Simon Fraser University in Vancouver.\nI won a travel grant from Griffith to visit Simon Fraser University. On my travels I heard about the remarkable work that John Reynolds and his team are doing on documenting how salmon bring nutrients to streamside ecosystems. I started wondering if such effects would show up in satellite images.\nWe decided to look at the ‘greenness’ of satellite image. Greenness is a common indicator of vegetation productivity that reflects chlorophyll density in leaves. Chlorophyll is the pigment that helps plants turn sunlight into energy.\nWe studied two different places. In central British Columbia John and his team have detailed surveys of 50 salmon spawning streams. We compared greenness across these sites and found trees were greener near streams with higher salmon spawning densities.\nWe also compared greenness across different years on the lower Fraser river in southern British Columbia. Salmon spawning numbers in the lower Fraser are dominated by pink salmon, which tend to only return in odd numbered years to spawn. So we expected an ‘on-off’ pattern in greenness.\nIn the lower Fraser we found such an ‘on-off’ pattern.\nSo we’d like to use this technique to measure the footprint of salmon across all spawning streams. However, the science isn’t quite there yet.\nAt both sites the salmon signal was well hidden under other drivers of greenness. For example, at the central coast site in particular, alders, a type of tree that can make its own nutrients, weakened the effect of salmon on greenness. At the lower Fraser site, patterns of greenness were dominated by the weather first, with salmon fertilization playing a secondary role.\nThese other drivers of greenness mean we aren’t confident enough in the method to extrapolate it more widely. Follow-up work should use methods like chemical tracers to validate increases in greenness against the supply of ocean nutrients.\nOur study does offer a tantalizing hint as to the possibility of using satellite images to better measure the footprint of salmon nutrients in trees.\nThe paper Salmon abundance and patterns of forest greenness as measured by satellite imagery was published in Science of the Total Environment."
  },
  {
    "objectID": "posts/2015-06-07-marine_reserves/2015-06-07-marine_reserves.html",
    "href": "posts/2015-06-07-marine_reserves/2015-06-07-marine_reserves.html",
    "title": "Marine reserves without the short-term losses in harvest",
    "section": "",
    "text": "Marine reserves are a popular management tool for recovering depleted fish stocks. However, before fishers receive the long-term benefits of spillover from the reserve, they must endure a short-term decline in harvest. A short-term loss in harvest may be untenable for some fishers, particularly those relying on the fishery to feed their families.\nWe used models of a coral reef fishery to ask how the short-term burden of a reserve can be lessened. Many coral reef fisheries are depleted but also support many millions of fishers world-wide, so our study has high practical relevance. We considered different scenarios for creating a reserve, that lessen the burden, including:\n\nGradually increasing the reserve size, up to a target size, each year\nGradually increasing the number of months a year an area is closed to fishing\nGradually increasing the number species protected within a reserve\n\nWe found that gradually bringing in the reserve over multiple years lessened the short-term burden, but still brought the same long-term benefits of spillover and improved harvest. However, there was a trade-off: it took longer for the fishery to fully recover.\nWe found that gradually bringing in the reserve over multiple years lessened the short-term burden, but still brought the same long-term benefits of spillover and improved harvest. However, there was a trade-off: it took longer for the fishery to fully recover.\nSo, a gradual approach to bringing in a reserve can be better for people and still result in the same long-term benefits. In practice, the approach taken will depend on the local situation. For instance, gradually building the size of a reserve may be easier to enforce than limiting what fish species people can catch."
  },
  {
    "objectID": "posts/2025-09-05-AI-aided-note-taking/index.html",
    "href": "posts/2025-09-05-AI-aided-note-taking/index.html",
    "title": "AI aided note taking",
    "section": "",
    "text": "This is my favourite workflow for note taking at the moment:\n\nVoice record my thoughts to iphone with the recording app. I don’t take much care to speak clearly or in a structured way. I just blurt all my thoughts out into the recorder.\nPress the transcript button and copy the text\n\nThen I either do:\n3a. Paste into Copilot for Teams and prompt Clean up this voice recording and organize it into notes that are arranged in a logical order. Use markdown format\nOr\n3b. Paste into Teams chat to myself for later. Later on my laptop paste the text into a markdown document and use Roo Code or Github Copilot with Clean up this voice recording and organize it into notes that are arranged in a logical order. Use markdown format\nIn general I’m not a fan of AI note taking. The problem is if you take a document you didn’t write and then get AI to summarize it for you there is no mental energy. The mental energy is when you learn and synthesize ideas.\nHowever, I feel differently about voice recordings, because the ideas are still going via my brain before they get to the AI. It also allows me to capture complex ideas that I may not get time to write down.\nFor example, I’ve used it while cleaning the house to get a blog idea down and then into a first draft. I used it while walking to an appointment to ‘write’ a first draft of the methods for a paper."
  },
  {
    "objectID": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html",
    "href": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html",
    "title": "Working within your lab group to create a more equitable and diverse scientific workforce",
    "section": "",
    "text": "It is well recognized that science has an equity and diversity issue, including under-representation of women in the scientific workforce and lack of cultural diversity. But the issue often seems to systemic that we might wonder what we can usefully do as individuals.\nOur combined seascape models and Global Wetlands Project teams decided we could do more. We recently convened a discussion on the topic of equity and diversity. I wanted to share that experience here, in the hope that it is helpful for other groups interested in doing more.\nMy colleague and I, Viv Tulloch, organized the 1 hour workshop. We decided the aims of the workshop were threefold:"
  },
  {
    "objectID": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html#the-statistics",
    "href": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html#the-statistics",
    "title": "Working within your lab group to create a more equitable and diverse scientific workforce",
    "section": "The statistics",
    "text": "The statistics\nThere are many statistics and studies on equity, especially gender equity, in the scientific workforce. So many in fact that it is overwhelming and you could spend a whole day just going over those.\nI looked up some statistics most relevant to our university and Australian research culture.\nGriffith Uni recently convened a university level panel on equity and diversity, which submitted a report to the Athena Swan program. The program is a kind of certification for universities to say they are doing something, you can read more about our efforts here.\nThe report is full of statistics about equity and diversity at Griffith. These are typical of many universities in Australia. So I highlighted a few.\nFirst, at academic levels A and B (i.e. lecturer/postdoc levels), there are more women than men. At academic level C (senior lecturer/senior researcher), there are only just more men than women. By academic level E (full professor) there are about 2x as many men as women.\nThis is the classic pipeline issue, where women ‘drop-out’ of the later stages due to complex interaction between life choices (e.g. having children), sexism, preferential selection of male candidates and lack of female role models.\nGriffith has been working towards addressing this issue for over 5 years now, thanks mostly (I think) to leadership from some very senior female staff (for instance by creating women’s promotion workshops). As a result the rate of successful promotion applications has gone from being much higher for men in 2014 to just about equal when comparing men to women.\nThe rate of female staff applying for promotion has also been going up over time and is also just about equal now.\nOur most esteemed research funder, the Australian Research Council, also has an issue. For instance, there is a so called daversity issue where over a period of years 51 men called David have lead ARC grants, but only 6 Suzannes (the most common female name)."
  },
  {
    "objectID": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html#the-impact",
    "href": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html#the-impact",
    "title": "Working within your lab group to create a more equitable and diverse scientific workforce",
    "section": "The impact",
    "text": "The impact\nFollowing up from the introductory stats, we discussed the impact of this gender (and cultural) bias on our workplace. Viv Tulloch asked the team if they would like to share a positive or negative story about gender.\nI don’t want to share other people’s stories here, but a few general trends emerged.\nThere were many positives. Some people noted more collaborative working in diverse teams. Others noted that diverse teams could be more sympathetic to difficult circumstances, so may improve staff mental health. Others just told stories about how they had felt supported in the past.\nA big positive for me personally (obviously I’m a man) is that having a more diverse workforce makes work more supportive and enjoyable. For instance, I recognized that I was able to take paid primary carer’s leave to care for my baby this year when my wife went back to work. That I could get paid leave was only because of the many mothers before me who fought to have paid primary carer’s leave in Australian universities.\nI’ve also seen through my work on Griffith’s Athena Swan panel that the focus on solving the ‘pipeline’ issue end up benefitting everyone. It motivated the University to look more closely at career development issues for early career researchers, regardless of gender or background. As a consequence they have a greater focus now on career development and new programs, like early career grants, that are open to every researcher.\nOn the negative side I was shocked to hear quite a number of stories from people’s past careers of bias in interview panels. This included being encouraged by senior staff not to hire young women in case they had children, and interview panels considering women unfit for dirty field work.\nSuch questions and considerations about personal circumstances, or bias on the basis of race or gender, are obviously illegal, but it seems that they remain an issue for many women."
  },
  {
    "objectID": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html#what-can-i-do",
    "href": "posts/2019-06-01-workshop-on-equity-and-diversity/index.html#what-can-i-do",
    "title": "Working within your lab group to create a more equitable and diverse scientific workforce",
    "section": "What can I do?",
    "text": "What can I do?\nNext we split the group into four break-out groups to discuss particular equity case-studies. The aim was to find solutions for individuals, but I also encouraged the groups to seek to understand other people’s perspectives on each issue.\nOur groups were:\n\nYou overhear a senior male prof make an inappropriate sexual remark to a young female student. What do you do? (see here for a real example of this chronic problem)\nYou notice an international student is uncomfortable with the casual Australian work ethic, Australian slang and the culture of after work drinks (they don’t drink). What do you do?\nYou are comparing two grants/job applications. Both are 3 years out of their PhD, one has more high impact papers, but the second had a significant break for maternity leave. How do you compare them fairly and what types of other information might you want for a fair comparison?\nYou congratulate a colleague on their winning a new grant and ask them how they feel about it. They looked stressed and admit they are worried that the new award will take time away from their family. What do you do?\n\nWe ran a session with break out groups to ensure that everyone had a chance to share their perspectives (we had about 20 people in the combined room), but also so we could explore some issues in more detail.\nWe asked each break-out group to take notes and report back to the whole group after about 20 minutes.\nI won’t share their thoughts here, I’ll just encourage you to try this yourself. The workshop ended up going for about 1 hour 20 minutes and I wish we had more time (but we were squeezed between other events). Anyway, we plan to do a follow-up later this year.\nFor me, I know I left with a new understanding of how my staff and students felt about equity issues, but also some new ideas about what I could do as an individual."
  },
  {
    "objectID": "posts/2018-12-16-shark-declines/index.html",
    "href": "posts/2018-12-16-shark-declines/index.html",
    "title": "Decline in large shark species on Queensland’s coastline",
    "section": "",
    "text": "In work just published we report extensive declines in large shark species on Queensland’s coastline.\nDr George Roff led the study of data from Queensland’s shark control program, dating back to the 1960s. We report on declines in hammerhead and white shark catch of 92%, decline in tiger shark catch of 74% and declines in large whaler species of 82%.\n\nImage: Article in The Conversation\nShark attacks are currently a contentious issue in Australia, and many have been calling for increased culls because of a perception that there is a ‘plague of sharks’. Our study finds that large shark numbers are actually at a historical low.\nThe study ignited a national conversation about shark conservation, with study lead, George Roff, putting in a huge effort to meet as many interview requests as possible.\nHere’s a sample of some the media coverage:\n\nWatch the segment on The Project on Channel Ten, via Twitter.\nSBS World News\nArticle in The Age from environment reporter Peter Hannam.\n\nYou can read the study at Communications Biology (which is open access) or at our article in The Conversation."
  },
  {
    "objectID": "posts/2023-07-17-chatGPT-lit-reviews/index.html",
    "href": "posts/2023-07-17-chatGPT-lit-reviews/index.html",
    "title": "What does generative AI mean for literature reviews?",
    "section": "",
    "text": "Literature reviews are about to change in a big way thanks to artificial intelligence that can generate text, like chatGPT. This technology can be used to define search times to identify literature, filter papers to review, and write the review itself. These tasks will save researchers time that they could spend on other things, like sipping cocktails or spending more time with their families.\nHowever, experience tells me researchers won’t spend their idle time on enjoying a better personal life. Research is competitive, and a tool that speeds up literature reviews will just increase the standard of what is expected. There may also be new innovations in literature reviews, like self-updating reviews, but I’ll focus here on how AI will raise the bar for typical literature reviews."
  },
  {
    "objectID": "posts/2023-07-17-chatGPT-lit-reviews/index.html#published-literature-reviews",
    "href": "posts/2023-07-17-chatGPT-lit-reviews/index.html#published-literature-reviews",
    "title": "What does generative AI mean for literature reviews?",
    "section": "Published literature reviews",
    "text": "Published literature reviews\nGenerative AI will allow review and synthesis of greater numbers of papers in the same amount of time. For instance, a recent paper shows how chatGPT can be used to define search terms and then filter articles for relevance. So we should see much bigger reviews in the near future, and consequently, top journals will also expect more literature to be covered in reviews they publish. We may also see researchers invest their time in further synthesis activities. Many stock-standard reviews just identify themes in a body of literature and suggest some future research activities. Generative AI can now read the literature for you and come up with common themes. Pushing this further we should see reviews that go further into synthesis by building conceptual or even quantitative models out of the literature."
  },
  {
    "objectID": "posts/2023-07-17-chatGPT-lit-reviews/index.html#undergraduate-literature-reviews",
    "href": "posts/2023-07-17-chatGPT-lit-reviews/index.html#undergraduate-literature-reviews",
    "title": "What does generative AI mean for literature reviews?",
    "section": "Undergraduate literature reviews",
    "text": "Undergraduate literature reviews\nTeaching academics should not shy away from including literature reviews in undergraduate assessments. Graduates still need to know how to produce literature reviews. I use the word ‘produce’, instead of ‘write’, intentionally But undergraduate assessments need to meet a different standard. Universities will have to assume undergrads are using these tools. This means assessment items will involve reviewing more literature, or require additional synthesis, such as with conceptual figures or presentations of the review. University assessment also needs to be rigorous, and may be harder work for many academics. The academics marking will need to check more carefully that the content is accurate (since generative AI is prone to make up facts)."
  },
  {
    "objectID": "posts/2023-07-17-chatGPT-lit-reviews/index.html#what-should-i-do",
    "href": "posts/2023-07-17-chatGPT-lit-reviews/index.html#what-should-i-do",
    "title": "What does generative AI mean for literature reviews?",
    "section": "What should I do?",
    "text": "What should I do?\nIf you are researcher: Start experimenting with generative AI to find ways you can use it to help with literature reviews. The quantity and quality that used to get you published in good journals won’t meet muster very soon. You’ll need to review more literature and/or include new types of synthesis.\nDon’t expect your PhD student will be able to just spend 6 or so months reading and synthesizing a 100 or so papers and then be able to pubilsh a good review, like we used to. You should be encouraging them to use these new tools and pushing the review further.\nIf you are a teaching academic: Don’t drop literature reviews from your assessment altogether! They are still an important synthesis tool. But do consider how you will change assessment criteria. You may also want to include written assessments in controlled environments (e.g. exam room) where you know generative AI isn’t used."
  },
  {
    "objectID": "posts/2020-07-09-universal-inference/index.html",
    "href": "posts/2020-07-09-universal-inference/index.html",
    "title": "Universal inference in R",
    "section": "",
    "text": "When we perform a statistical test we’d like to have confidence what the Type I error rate is what we say it is (rate of false positive findings). Often scientists choose a rate (AKA ‘alpha’) of 0.05 for rejecting the null hypothesis.\nWe’d like some guarantees that our statistical test will actually have a type I rate of 5% over many, many repeated trials. For many statistical tests theory does in fact guarantee a 5% error rate over many many repeated trials.\nThe paper Universal inference from Wasserman et al. published in PNAS proposes a new test that guarantees a given Type I error rate for some types of statistical tests that previously didn’t have an appropriate test.\nHere I’ll attempt to code their method in R.\nI say ‘attempt’ because I’m not at all sure I interpreted their paper correctly. So please don’t take this blog as a ‘how to’ for the method. Instead, treat it as an exploration of type I and type II errors and how we can check them.\nFeel free to email me (chris.brown@griffith.edu.au) if you have suggestions on this blog.\nThe method is based on likelihood ratio tests, so at the very least we’ll get to learn about LRTs in this blog.\nIf you are seriously interested in using universal inference, I suggest you consult a statistician.\n\n\nLet’s first test if a some data drawn from a normal distribution are consistent with a null that the mean = 0.\nWe’ll define the data:\nmu &lt;- 0.4\nn &lt;- 100\nset.seed(42)\ny &lt;- rnorm(n, mean = mu)\nNow, do a likelihood ratio test the usual way:\n(test_stat &lt;- 2*(sum(dnorm(y, mean(y), sd(y), log = TRUE)) - sum(dnorm(y, 0, sd(y), log = TRUE))))\n\n## [1] 17.25054\n\n1 - pchisq(test_stat, 1)\n\n## [1] 3.276042e-05\nWe just compared the likelihoods of the data given a mean of zero versus the maximum likelihood estimate of the mean (which is just the mean of the data). Then we take that ratio and find its quantile on the chisq, that’s our (very small) p-value = 3.27E-5.\nIf we were using linear models, we could do the same thing like this:\nm1 &lt;- lm(y~1) #intercept only, ie a mean\nm0 &lt;- lm(y~0) #no intercept, so mean = -\nanova(m0, m1, test = \"Chisq\")\n\n## Analysis of Variance Table\n##\n## Model 1: y ~ 0\n## Model 2: y ~ 1\n##   Res.Df    RSS Df Sum of Sq  Pr(&gt;Chi)    \n## 1    100 126.06                           \n## 2     99 107.36  1    18.707 3.276e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWasserman et al. propose a ‘split LRT’ where we split the data in half. This has some pleasing similarities to out of sample validation. As I understand it, this is how we’d do the test:\nFirst split the data at random:\ni &lt;- sample(1:n, n/2, replace = FALSE)\ny0 &lt;- y[i]\ny1 &lt;- y[-i]\nNow calculate the MLEs for both splits:\nmu0 &lt;- mean(y0)\nsd0 &lt;- sd(y0)\n\nmu1 &lt;- mean(y1)\nsd1 &lt;- sd(y1)\nThen our split test statistic is (note we are taking likelihood of the y0 split split using the y1 split’s mean):\nsplit_test_stat0 &lt;- sum(dnorm(y0, mu1, sd1, log = TRUE)) -\n                         sum(dnorm(y0, 0, sd1, log = TRUE))\nAnd we can ask if its significant like this:\nexp(split_test_stat0) &gt;= 1/0.05\n\n## [1] TRUE\nTRUE, so reject the null, which is the same result as our Chi square test above.\nThey also propose a cross-fit test, that is just the average of the two split tests:\nsplit_test_stat1 &lt;- sum(dnorm(y1, mu0, sd0, log = TRUE)) -\n  sum(dnorm(y1, 0, sd0, log = TRUE))\nsplit_test_stat &lt;- (split_test_stat1 + split_test_stat0)/2\nexp(split_test_stat) &gt;= 1/0.05\n\n## [1] TRUE\n\n\n\nWhat about the test’s power? Well it seems a shortcoming is that the split LRT can have lower power (higher type II rate, or chance of missing real differences) than some other tests. So for a simple test like that above we are better of doing the test the regular way.\nLet’s check its power for our simple test vs a chisq. I’ll write a function to do this, then iterate it.\nsplitLRT &lt;- function(seed, n, mu){\n  set.seed(seed)\n  y &lt;- rnorm(n, mean = mu)\n  i &lt;- sample(1:n, n/2, replace = FALSE)\n\n  y0 &lt;- y[i]\n  y1 &lt;- y[-i]\n\n  mu0 &lt;- mean(y0)\n  sd0 &lt;- sd(y0)\n\n  mu1 &lt;- mean(y1)\n  sd1 &lt;- sd(y1)\n\n  #split test stat\n  split_test_stat0 &lt;- sum(dnorm(y0, mu1, sd1, log = TRUE)) -\n                         sum(dnorm(y0, 0, sd1, log = TRUE))\n  split_test_stat1 &lt;- sum(dnorm(y1, mu0, sd0, log = TRUE)) -\n  sum(dnorm(y1, 0, sd0, log = TRUE))\n  split_test_stat &lt;- (split_test_stat1 + split_test_stat0)/2\n\n\n  #regular Chisq LRT\n  test_stat &lt;- 2*(sum(dnorm(y, mean(y), sd(y), log = TRUE)) - sum(dnorm(y, 0, sd(y), log = TRUE)))\n  chisqtest &lt;- 1 - pchisq(test_stat, 1)\n\n  #output results as a dataframe\n  data.frame(splitLRT = exp(split_test_stat), chisq = chisqtest)\n}\nNow let’s use our function:\nxout &lt;- lapply(1:1000, splitLRT, n = 50, mu = 0.5)\ndfout &lt;- do.call(\"rbind\", xout)\nsum(dfout$splitLRT &gt;= (1/0.05))/1000\n\n## [1] 0.415\n\nsum(dfout$chisq &lt;= 0.05)/1000\n\n## [1] 0.932\nSo the split test only rejects the null 41.5% of the time, whereas the chisq rejects it 93% of the time. In other words the split test comes at the cost of lower power, as is explained in the paper.\nIt would be worth trying the suggestion in the paper of using k-fold cross-validation to do the splits too, maybe that would improve the power.\nFor larger sample sizes, the split test method does better:\nxout &lt;- lapply(1:1000, splitLRT, n = 150, mu = 0.5)\ndfout &lt;- do.call(\"rbind\", xout)\nsum(dfout$splitLRT &gt;= (1/0.05))/1000\n\n## [1] 0.974\n\nsum(dfout$chisq &lt;= 0.05)/1000\n\n## [1] 1\n\n\n\nThe example I give above is illustrative only, there are theoretical reasons not to use the regular Chi sq test for such a simple model.\nIt would be interesting to try the split LRT for more sophisticated models where tests don’t currently exist. Wasserman et al. suggest its use for a number of models that ecologists use regularly, including mixing models (where data are generated from a mix of distributions), testing latent variables (such as in the multispecies hierarchical models, like BORAL, that we love here) and for structural equation models."
  },
  {
    "objectID": "posts/2020-07-09-universal-inference/index.html#testing-the-null-that-a-mean-equals-zero",
    "href": "posts/2020-07-09-universal-inference/index.html#testing-the-null-that-a-mean-equals-zero",
    "title": "Universal inference in R",
    "section": "",
    "text": "Let’s first test if a some data drawn from a normal distribution are consistent with a null that the mean = 0.\nWe’ll define the data:\nmu &lt;- 0.4\nn &lt;- 100\nset.seed(42)\ny &lt;- rnorm(n, mean = mu)\nNow, do a likelihood ratio test the usual way:\n(test_stat &lt;- 2*(sum(dnorm(y, mean(y), sd(y), log = TRUE)) - sum(dnorm(y, 0, sd(y), log = TRUE))))\n\n## [1] 17.25054\n\n1 - pchisq(test_stat, 1)\n\n## [1] 3.276042e-05\nWe just compared the likelihoods of the data given a mean of zero versus the maximum likelihood estimate of the mean (which is just the mean of the data). Then we take that ratio and find its quantile on the chisq, that’s our (very small) p-value = 3.27E-5.\nIf we were using linear models, we could do the same thing like this:\nm1 &lt;- lm(y~1) #intercept only, ie a mean\nm0 &lt;- lm(y~0) #no intercept, so mean = -\nanova(m0, m1, test = \"Chisq\")\n\n## Analysis of Variance Table\n##\n## Model 1: y ~ 0\n## Model 2: y ~ 1\n##   Res.Df    RSS Df Sum of Sq  Pr(&gt;Chi)    \n## 1    100 126.06                           \n## 2     99 107.36  1    18.707 3.276e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/2020-07-09-universal-inference/index.html#split-lrt",
    "href": "posts/2020-07-09-universal-inference/index.html#split-lrt",
    "title": "Universal inference in R",
    "section": "",
    "text": "Wasserman et al. propose a ‘split LRT’ where we split the data in half. This has some pleasing similarities to out of sample validation. As I understand it, this is how we’d do the test:\nFirst split the data at random:\ni &lt;- sample(1:n, n/2, replace = FALSE)\ny0 &lt;- y[i]\ny1 &lt;- y[-i]\nNow calculate the MLEs for both splits:\nmu0 &lt;- mean(y0)\nsd0 &lt;- sd(y0)\n\nmu1 &lt;- mean(y1)\nsd1 &lt;- sd(y1)\nThen our split test statistic is (note we are taking likelihood of the y0 split split using the y1 split’s mean):\nsplit_test_stat0 &lt;- sum(dnorm(y0, mu1, sd1, log = TRUE)) -\n                         sum(dnorm(y0, 0, sd1, log = TRUE))\nAnd we can ask if its significant like this:\nexp(split_test_stat0) &gt;= 1/0.05\n\n## [1] TRUE\nTRUE, so reject the null, which is the same result as our Chi square test above.\nThey also propose a cross-fit test, that is just the average of the two split tests:\nsplit_test_stat1 &lt;- sum(dnorm(y1, mu0, sd0, log = TRUE)) -\n  sum(dnorm(y1, 0, sd0, log = TRUE))\nsplit_test_stat &lt;- (split_test_stat1 + split_test_stat0)/2\nexp(split_test_stat) &gt;= 1/0.05\n\n## [1] TRUE"
  },
  {
    "objectID": "posts/2020-07-09-universal-inference/index.html#power",
    "href": "posts/2020-07-09-universal-inference/index.html#power",
    "title": "Universal inference in R",
    "section": "",
    "text": "What about the test’s power? Well it seems a shortcoming is that the split LRT can have lower power (higher type II rate, or chance of missing real differences) than some other tests. So for a simple test like that above we are better of doing the test the regular way.\nLet’s check its power for our simple test vs a chisq. I’ll write a function to do this, then iterate it.\nsplitLRT &lt;- function(seed, n, mu){\n  set.seed(seed)\n  y &lt;- rnorm(n, mean = mu)\n  i &lt;- sample(1:n, n/2, replace = FALSE)\n\n  y0 &lt;- y[i]\n  y1 &lt;- y[-i]\n\n  mu0 &lt;- mean(y0)\n  sd0 &lt;- sd(y0)\n\n  mu1 &lt;- mean(y1)\n  sd1 &lt;- sd(y1)\n\n  #split test stat\n  split_test_stat0 &lt;- sum(dnorm(y0, mu1, sd1, log = TRUE)) -\n                         sum(dnorm(y0, 0, sd1, log = TRUE))\n  split_test_stat1 &lt;- sum(dnorm(y1, mu0, sd0, log = TRUE)) -\n  sum(dnorm(y1, 0, sd0, log = TRUE))\n  split_test_stat &lt;- (split_test_stat1 + split_test_stat0)/2\n\n\n  #regular Chisq LRT\n  test_stat &lt;- 2*(sum(dnorm(y, mean(y), sd(y), log = TRUE)) - sum(dnorm(y, 0, sd(y), log = TRUE)))\n  chisqtest &lt;- 1 - pchisq(test_stat, 1)\n\n  #output results as a dataframe\n  data.frame(splitLRT = exp(split_test_stat), chisq = chisqtest)\n}\nNow let’s use our function:\nxout &lt;- lapply(1:1000, splitLRT, n = 50, mu = 0.5)\ndfout &lt;- do.call(\"rbind\", xout)\nsum(dfout$splitLRT &gt;= (1/0.05))/1000\n\n## [1] 0.415\n\nsum(dfout$chisq &lt;= 0.05)/1000\n\n## [1] 0.932\nSo the split test only rejects the null 41.5% of the time, whereas the chisq rejects it 93% of the time. In other words the split test comes at the cost of lower power, as is explained in the paper.\nIt would be worth trying the suggestion in the paper of using k-fold cross-validation to do the splits too, maybe that would improve the power.\nFor larger sample sizes, the split test method does better:\nxout &lt;- lapply(1:1000, splitLRT, n = 150, mu = 0.5)\ndfout &lt;- do.call(\"rbind\", xout)\nsum(dfout$splitLRT &gt;= (1/0.05))/1000\n\n## [1] 0.974\n\nsum(dfout$chisq &lt;= 0.05)/1000\n\n## [1] 1"
  },
  {
    "objectID": "posts/2020-07-09-universal-inference/index.html#what-next",
    "href": "posts/2020-07-09-universal-inference/index.html#what-next",
    "title": "Universal inference in R",
    "section": "",
    "text": "The example I give above is illustrative only, there are theoretical reasons not to use the regular Chi sq test for such a simple model.\nIt would be interesting to try the split LRT for more sophisticated models where tests don’t currently exist. Wasserman et al. suggest its use for a number of models that ecologists use regularly, including mixing models (where data are generated from a mix of distributions), testing latent variables (such as in the multispecies hierarchical models, like BORAL, that we love here) and for structural equation models."
  },
  {
    "objectID": "posts/2025-10-02-running-an-effective-workshop/index.html",
    "href": "posts/2025-10-02-running-an-effective-workshop/index.html",
    "title": "Planning an effective workshop",
    "section": "",
    "text": "This week in lab meeting we discussed planning a workshop. Here are a few notes of pointers that came up in the discussion.\nBe clear on your goals for the workshop. And communicate those goals to the participants.\nInvite the right people.\nThink about what’s in it for them and communicate that in your invitation but also in follow-up communication. - Interesting places - Networking - Opportunity to collaborate - Opportunity to promote their own work\nThink about what knowledge workshops participants need that they don’t currently have. This is particularly important if your workshop participants come from different disciplines.\nYou’ll need to plan to balance the delivery of essential information and the discussion to develop new ideas and obtain information from the group. For instance, what background info that is needed to participate effectively? What can individuals contribute? What synergies do you hope to get out of group discussions? (and making sure those people have the opportunity to talk together in the workshop)."
  },
  {
    "objectID": "posts/2025-10-02-running-an-effective-workshop/index.html#roles",
    "href": "posts/2025-10-02-running-an-effective-workshop/index.html#roles",
    "title": "Planning an effective workshop",
    "section": "Roles",
    "text": "Roles\nFor each workshop you should designate a facilitator and a note keeper. These should be different people. See also our blog on facilitating meetings\nFacilitation takes practice and its good to have someone experienced in this role.\nYou also need a plan to capture information from the workshop. So make sure you have a note taking system (e.g. online docs, charts on butcher’s paper) and someone who is going to chase those up from the group. Break-out groups are great, but make sure you have a way to capture their ideas."
  },
  {
    "objectID": "posts/2025-10-02-running-an-effective-workshop/index.html#icebreakers-and-team-bonding",
    "href": "posts/2025-10-02-running-an-effective-workshop/index.html#icebreakers-and-team-bonding",
    "title": "Planning an effective workshop",
    "section": "Icebreakers and team bonding",
    "text": "Icebreakers and team bonding\nYou can send a form in advance asking a few questions about people’s backgrounds, as well as 1 or 2 questions about them as a person. Here’s some ideas - What is your area of research? - What paper or research product are you most proud of? - What are you hoping to get out of this workshop? - What is your favourite sport or hobby?\nPlan an activity or activities to help people bond. One example is showing people a picture of a cloud. Then asking them to draw what they see in the cloud and share that with people nearby. Helps get people engaged in the workshop as well as understand that everyone has a different perspective. There are many different examples of these types of group bonding activities.\nAnother is ‘four corners’ or ‘two-sides’. Ask people to sort themselves in the room based on their preferences (keeping it non-political and light hearted). e.g. “Do you like mushrooms?”, “Do you like pineapple on pizza?”, “Are you an experimental, field or modelling scientist?”, …"
  },
  {
    "objectID": "posts/2025-10-02-running-an-effective-workshop/index.html#workshop-activities",
    "href": "posts/2025-10-02-running-an-effective-workshop/index.html#workshop-activities",
    "title": "Planning an effective workshop",
    "section": "Workshop activities",
    "text": "Workshop activities\nStrongly recommend using break-out groups, especially if you have more than 6 people at your workshop. This maximises engagement with the whole group and gives quieter voices a chance to speak up.\nThink about the balance of plenary time (everyone together) and break out groups. A typical structure would be:\nOpening presentation (plenary) Icebreaker (breakouts) Presentation on essential background (plenary) Set a task (plenary) Do the task (breakouts) Report back to the group on task (plenary) ….\nTry to include a mix of different modes of activities to keep people energized and engaged. This can include discussion, working on charts/flow diagrams on butchers paper, online tools, other types of activities that require people to move around like ‘four corners’."
  },
  {
    "objectID": "posts/2025-10-02-running-an-effective-workshop/index.html#planning-for-after-the-workshop",
    "href": "posts/2025-10-02-running-an-effective-workshop/index.html#planning-for-after-the-workshop",
    "title": "Planning an effective workshop",
    "section": "Planning for after the workshop",
    "text": "Planning for after the workshop\nMake sure you communicate a plan for next steps at the end of the workshop, as well as in follow-up email.\nBeing clear on what is happening next, having a plan for next steps. When can participants expect to hear from the organizers? What are the outcomes (a manuscript perhaps)?\nIf you are writing a manuscript, then being clear on who will be included as a coauthor and what the requirements for coauthorship will be."
  },
  {
    "objectID": "posts/2016-02-26-labels-for-graphs/index.html",
    "href": "posts/2016-02-26-labels-for-graphs/index.html",
    "title": "Automate adding labels to figures in R",
    "section": "",
    "text": "The other day I was resubmitting a paper to a new journal for the fourth time and I decided I had had enough with reformatting of figure labels each time I resubmitted to a new journal. So I wrote this handy function which automatically places labels on figures in a given format.\nI thought writing such a function is a good lesson in R objects, so here we go. If you just want to get to the function, without my lecturing, then skip to the bottom.\nFirst up we need to create some data, for testing here we go:\nn &lt;- 100\nx1 &lt;- rnorm(n, mean = 5, sd = 1)\ny1 &lt;- 2*x1 + rnorm(n, mean = 0, sd = 2)\ny2 &lt;- -5*x1 + rnorm(n, mean = 0, sd = 2)\ny3 &lt;- rnorm(n, mean = -1, sd = 2)\ny4 &lt;- 9*rnorm(n, mean = 0, sd = 2) + 5\nWe just created an x-variable and four y-variables that depend on that x-variable, with normal errors.\nLet’s start a plot, with 2*2 window:\npar(mfrow = c(2,2))\nplot(x1, y1)\nNow we need to identify the boundaries of the axis on each sub-plot as it is created. We can do that using the par() command:\naxbounds &lt;- par('usr')\nWhich gives us the bounds on the x and y axes for whatever plot is open right now.\nWe can then get the range of each axis like this:\naxrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\nNext, define our labels. We have four plots and we can use letters or LETTERS to obtain letters of the alphabet. ie:\nlabels &lt;- LETTERS[1:4]\nGives us the labels a, b, c, d. Or we could also do:\nlabels &lt;- paste0('(',letters[1:4],')')\nWhich gives us (a), (b), (c), (d)\nNow we need to position the labels, relative to each axes range. We can do this by taking the x-axis minimum value and y-axis maximum value and then adding an offset to the axis range. Adding the offset in this way means the labels will always be placed in the same place, even if the axis scale varies. We can place the label using the text command.\ntext(axbounds[1] -\n    (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n    label[1], xpd=NA,...)\nThe argument xpd=NA is neccesary, because without it new additions to the plot outside of the plot window are invisible.\nSo those are the elements we need. To get it all finished, we can stitch these all together in a new function:\n\n\nfiglabel &lt;- function(label ='A', xoffset = 0.1, yoffset = 0.08,...){\n    axbounds &lt;- par('usr')\n    axrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\n    text(axbounds[1] -\n        (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n        label, xpd=NA,...)\n    }\nWe use it like this:\n    par(mfrow = c(2,2))\n    plot(x1, y1)\n    figlabel(labels[1], font = 2)\n    plot(x1, y2)\n    figlabel(labels[2], font = 2)\n    plot(x1, y3)\n    figlabel(labels[3], font = 2)\n    plot(x1, y4)\n    figlabel(labels[4], font = 2)\nThe result is this:"
  },
  {
    "objectID": "posts/2016-02-26-labels-for-graphs/index.html#how-automate-labels-on-figures-in-r",
    "href": "posts/2016-02-26-labels-for-graphs/index.html#how-automate-labels-on-figures-in-r",
    "title": "Automate adding labels to figures in R",
    "section": "",
    "text": "The other day I was resubmitting a paper to a new journal for the fourth time and I decided I had had enough with reformatting of figure labels each time I resubmitted to a new journal. So I wrote this handy function which automatically places labels on figures in a given format.\nI thought writing such a function is a good lesson in R objects, so here we go. If you just want to get to the function, without my lecturing, then skip to the bottom.\nFirst up we need to create some data, for testing here we go:\nn &lt;- 100\nx1 &lt;- rnorm(n, mean = 5, sd = 1)\ny1 &lt;- 2*x1 + rnorm(n, mean = 0, sd = 2)\ny2 &lt;- -5*x1 + rnorm(n, mean = 0, sd = 2)\ny3 &lt;- rnorm(n, mean = -1, sd = 2)\ny4 &lt;- 9*rnorm(n, mean = 0, sd = 2) + 5\nWe just created an x-variable and four y-variables that depend on that x-variable, with normal errors.\nLet’s start a plot, with 2*2 window:\npar(mfrow = c(2,2))\nplot(x1, y1)\nNow we need to identify the boundaries of the axis on each sub-plot as it is created. We can do that using the par() command:\naxbounds &lt;- par('usr')\nWhich gives us the bounds on the x and y axes for whatever plot is open right now.\nWe can then get the range of each axis like this:\naxrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\nNext, define our labels. We have four plots and we can use letters or LETTERS to obtain letters of the alphabet. ie:\nlabels &lt;- LETTERS[1:4]\nGives us the labels a, b, c, d. Or we could also do:\nlabels &lt;- paste0('(',letters[1:4],')')\nWhich gives us (a), (b), (c), (d)\nNow we need to position the labels, relative to each axes range. We can do this by taking the x-axis minimum value and y-axis maximum value and then adding an offset to the axis range. Adding the offset in this way means the labels will always be placed in the same place, even if the axis scale varies. We can place the label using the text command.\ntext(axbounds[1] -\n    (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n    label[1], xpd=NA,...)\nThe argument xpd=NA is neccesary, because without it new additions to the plot outside of the plot window are invisible.\nSo those are the elements we need. To get it all finished, we can stitch these all together in a new function:\n\n\nfiglabel &lt;- function(label ='A', xoffset = 0.1, yoffset = 0.08,...){\n    axbounds &lt;- par('usr')\n    axrange &lt;- c(axbounds[2]-axbounds[1], axbounds[4] - axbounds[3])\n    text(axbounds[1] -\n        (axrange[1]*xoffset), axbounds[4] +(axrange[2]* yoffset),\n        label, xpd=NA,...)\n    }\nWe use it like this:\n    par(mfrow = c(2,2))\n    plot(x1, y1)\n    figlabel(labels[1], font = 2)\n    plot(x1, y2)\n    figlabel(labels[2], font = 2)\n    plot(x1, y3)\n    figlabel(labels[3], font = 2)\n    plot(x1, y4)\n    figlabel(labels[4], font = 2)\nThe result is this:"
  },
  {
    "objectID": "posts/2021-05-05-importance-wetland-indicators/index.html",
    "href": "posts/2021-05-05-importance-wetland-indicators/index.html",
    "title": "Filling gaps in global assessments of ecosystems to benefit coastal wetland conservation",
    "section": "",
    "text": "The conservation of coastal wetland ecosystems, seagrass, saltmarsh and mangroves, receives less attention than other higher profile coastal ecosystems, like coral reefs. In a new study we highlight how filling gaps in the assessment of coastal wetlands may help advance coastal wetlands.\nEcosystem assessment frameworks, like the Ocean Health Index, the IUCN Red List of Ecosystems and environmental-economic accounting systems, play important roles in management and conservation (see below image).\n\n\n\nLifecycle of an indicator\n\n\nImage: Lifecycle of ecosystem assessments and conservation action. Image from the study and created by Ryan Pearson.\nAssessments motivate data collection and synthesis of the status of ecosystems, whether it be for human values like fisheries or natural values like biodiversity. The findings of assessments can then determine what ecosystems require conservation actions and what type of actions need to happen.\nWe reviewed five global assessment frameworks and found all shared similar gaps when it comes to assessing coastal wetland ecosystems.\nFirst, animal-habitat linkages are poorly understood. So the assessment frameworks did not properly capture the role that protecting wetlands play in conserving threatened species and supporting productive fisheries.\nSecond, there are numerous data gaps we need to fill at the global scale. Recent strides have been made in applying big data to fill gaps in global measurements of mangrove ecosystems. We now need similar efforts to fill gaps in trends and status of seagrass and saltmarsh.\nOur study has highlighted the role that global assessments play in conservation outcomes. The next aim for our team is to produce a global assessments for all coastal wetland ecosystems. By doing so we hope to give these important ecosystems more exposure to conservation managers."
  },
  {
    "objectID": "posts/2019-06-22-Hiring-quantitative-ecologist/index.html",
    "href": "posts/2019-06-22-Hiring-quantitative-ecologist/index.html",
    "title": "We’re hiring a quantitative ecologist",
    "section": "",
    "text": "Applications to this position closed, July 2019. Here’s the original posts, for record purposes.\nI’m looking to hire a quantitative ecologist to work with the Global Wetlands Project at Griffith University, Australia.\nThe position is 2.5 years, at either postdoctoral level or academic level 2 (also known as level B).\nThis is an amazing opportunity to work in an interdisciplinary team to tackle major issues in the conservation of coastal wetlands, including seagrass saltmarsh and mangroves, their dependent ecosystems and ecosystem services. Our team includes marine ecologists, statistical ecologists, data scientists and a science communicator at Griffith Uni and multiple international collaborators. We are addressing critical issues in the conservation of coastal wetlands through new science and collaborations with key international decision makers.\nMore information and to apply see the Seek job site.\nPlease contact me on chris.brown@griffith.edu.au if you have an enquiries."
  },
  {
    "objectID": "posts/2016-06-21-ICRS-key-insights/index.html",
    "href": "posts/2016-06-21-ICRS-key-insights/index.html",
    "title": "What is the future of coral reefs? - ICRS summary",
    "section": "",
    "text": "Coral reef ecosystems face unprecedented threats, but stories of people give me hope for the future\nThe International Coral Reef Symposium in Honolulu comes on the back of the worst global coral bleaching event yet observed. Global disasters often leave people wondering what hope there is for the future - particularly given insufficient attempts to curb greenhouse gas emissions.\n\n\n\nSource: http://twitter.com\n\n\nCoral reefs will never be the same as they were. I once saw a talk from Charlie Veron about his efforts to create a global coral atlas (at the conference he announced this is now online). He has dived perhaps more of the world’s reefs than anyone. In an incredibly depressing ending to his talk he stated no-one will ever get to experience coral reefs as he has.\n\n\nCommunicating the loss of coral reefs has been emphasised as an important role for scientists at earlier meetings. A few years ago the rhetoric was all about how scientists need to get out there more - engage with public, get in the media, influence policy, be a force on social media. We do all of this now.\nI actually met, in person, a professional science communicator at this conference. I am sure scicomm people were around at earlier conferences, but not common enough for me to meet one.\nSocial media use is a key part of this engagement. Terry Hughes noted in a panel session that every politician has a twitter account, so if they believe it can change people’s minds, we need to be on there too. As an example, Knowlton talked about the incredible success of the []#oceanoptimism](https://twitter.com/hashtag/oceanoptimism?src=hash) hashtag for engaging the public with ocean solutions through twitter.\nSocial media use at ICRS is orders of magnitude greater than at earlier conferences. Twitter is increasingly popular and the #ICRS2016 hashtag has had widespread impact. A few quick web searches suggest #ICRS2016 had ~240 users and reached over 540 000 people. Check out the #ICRS2016 feed or some of the top tweeters e.g. @OneSwellLife, @physiologyfish and @ultimatemegs to read about the action.\nThese efforts to communicate were apparent at this conference in a subtle way too - the conversation has moved away from how we need communicate, and instead is now about how we can communicate better.\nPeter Kareiva and John Pandolfi both spoke about the need to communicate more of the nuances of science to the media - especially the uncertainty inherent in forecasting the future under different policies. Kareiva used the example of the message box, a commonly used tool for turning academic science into stories for the media or policy. He argued that while this is a useful tool, there it does nothing to help us communicate critical thresholds, impacts and uncertainty - key issues in much of reef science.\nIn her plenary Nancy Knowlton acknowledged the improvement in communication in coral reef science. However, she implored us to tell stories not statistics. In particularly, stories of people. Terry Pratchett, the sci-fi author once said we shouldn’t be called Homo sapiens, the ‘intelligent human’, but Pan narrans the story telling ape (thanks to Beth Fulton for that quote).\nAnd it is stories of people that give me hope for coral reefs.\n\n\n\nCinner and colleagues published a paper about coral reef bright spots last week - places where society and managers are doing better than we would expect at managing reef fish (also presented at ICRS2016). ‘Brights spots’ has become an oft repeated meme at this conference.\nOne of these ‘bright spots’ is Palau and we saw two plenaries about the island nation. The President Remengesau of Palau spoke on day one about the nation’s solution’s orientated approach to reef management. Palau is a world leader in declaring protected areas. Palau’s leadership perhaps stems from a strong traditional management culture. Thus, Palau is a key study area where we can learn about successful management.\nI am finding the study of society’s role in management success brings much more ocean optimism than the pure ecological study of reefs.\nWhen I started my career everyone was saying ecological and physical scientists need to collaborate more with economists and social scientists, but barely anyone was doing so. Now things are turning around.\nBy bringing together social sciences, economists and ecologists we creating a solution orientated science. As Knowlton and others have oft stated, solutions are what give us hope and empower people to take action.\nThe power of solutions is evidenced in the feelings of colleagues at this conference. Colleagues how primarily attended talks about the recent bleaching event were overwhelmingly depressed about the future of reefs, whereas those attending the numerous sessions on marine fishery management, social science, co-management were more optimistic.\nThere is growing acceptance that, while reefs will never return to their previous glory, we can make a better future.\nIn his plenary Pete Mumby talked about the need to study the subtleties of coral reefs. He argued we are facing a future with many ‘mediocre’ reefs, whereas in the past we have focussed our ecological studies on the best of the best, or the worst of the worst. Even degraded reefs can provide services to humans and support significant biodiversity. We need to better understand how management can maximise these benefits.\nAn important part of understanding trends is documenting the past - so much of which has not been recorded by science. But the past is documented in people’s stories, which Loren Mclenachan, Ruth Thurstan and others are using to push back our understanding. For instance newspaper articles can document historical fish catch (Thurstan) and nautical charts can document erosion of reefs (Mclenachan).\nOur future may be dominated by compromises between social, economic and ecological goals. It is hard for the reefs to have their fish and for us to eat them too. Charles Birkeland argued in his plenary that coral reef ecosystems tend to import production not export production. Therefore, they cannot sustain industrial fishing. He talked about Palau’s scenario where its reefs have supported traditional fishing for generations, but the influx of tourists wanting to eat reef fish cannot be supported. Keeping to the solutions theme, he suggested focussing on much more productive pelagic fisheries to support tourism.\nThe realisation that compromises on ecological quality are needed to support human needs has seen the rise of ‘trade-offs’ analysis. Crow White gave a great introduction to the key steps to trade-offs analysis.\nImportantly, trade-offs can help us identify optimal compromises, that are often far better than the status-quo for both people and ecosystems.\nBozec has shown a strong trade-off between harvest of parrotfish and the resilience of coral reefs. However, the right type of management (banning trap fishing) can significantly improve the situation. Aaron MacNeil has similarly found the right type of management is key for fish recovery globally.\nTrade-offs analysis is useful for pollution problems too. Kirstin Oleson and her lab are applying trade-offs analysis to ridge to reef management. They are finding that focussing restoration efforts in just a few areas in catchments can be a very cheap way to reduce sedimentation on reefs.\n\n\n\nDevelopments in social science, the study of management and communication give me hope for the future of reefs. However, some ongoing trends in the culture of reef science give me cause for concern.\n\n\n\nThere are so many women doing amazing work for coral reefs at this conference. So why don’t they receive more recognition from our community? Source: http://twitter.com\n\n\nThere is still a strong gender bias in coral reef science. Perhaps this is a hangover from the past, however I think it isn’t changing fast enough. I would estimate that talks and attendance at this conference is approximately 50:50, however it is easy to see that women are not provided with the same opportunities in our field as the men.\nOne story I heard was of a workshop where a senior (male) scientist said he would have liked to have invited more women, but “there just aren’t that many good female scientists to invite”.\nNot all sexism is this blatant.\nCoral reef science as discipline has a problem with insufficient opportunity for women to receive recognition for their work\nThe culture of science overwhelming recognises the volume of achievements. Often women have shorter careers than men, therefore the volume of female achievements is often on average smaller. This does not mean the quality of their work is lower. We often seem to conflate quality and quantity when we recognise leaders of our field.\nI don’t think there is any lack of ‘good’ female reef scientists. In fact, some of the most compelling talks I have seen at ICRS were given by women (take Lizzie Mcleod’s for example).\nIt is important to note that both men and women and subject to this bias. I have discussed these issues with both male and female colleagues and both genders often find it harder to think of female leaders of our field.\nTo give some examples, this year we saw the award of the 7th Darwin medal, the International Society for Reef Studies top award. There has never been a female laureate of the Darwin Medal.\nThe chance of having no females in 7 Darwin medals is less than 1%\nMaybe the award just reflects a historical hangover of what used to be a male dominated field? I don’t think that’s the whole story. We saw a panel discussion on Wednesday night, that was meant to represent some leaders in our field discussing top issues. There were 2 women on a ten person panel.The chance of having two women out of ten people is 5% (reasonably assuming a 50:50 gender ratio at the conference). In science we often treat a 5% chance ‘statistically significant’, so therefore, 2/10 indicates a significant gender bias.\nI want to be clear that I am not criticising any individual or organization. I do not intend to discredit individuals who have received accolades - they certainly deserved them. Neither am I criticising the International Society for Reef Studies. In fact, the society’s council has a reasonable gender balance and plenary talks at ICRS are balanced across genders.\n\n\n\nDemographics of communicators on the Twitter #ICRS2016 group. Source: http://keyhole.co/preview_new\n\n\nWhat I am arguing for is a change in our culture. We could start by creating more formal opportunities to recognise the contributions of women to our field. For instance the International Society for Reef Studies could have an award for female leaders in coral reef sciences.\nI suspect their are similar biases in cultural and racial inclusion in our field. I have not been keeping track of those, but ping me if you have useful link.\nInformal changes are necessary to change our culture of gender bias. For some tips, check out Jenny Martin’s blog. Top of her list is recognise your own biases.\nCoral reefs need diverse participation. It is many of the women of our field who are pushing us forward to find solutions to the problems facing reefs. Follow twitter: there are more women than men communicating reef science there. Many of people who have pioneered social sciences in reef science are also female.\nI encourage all of you to value diverse career paths and contributions to our field. As Tallis and Lubchenco have argued the failure to include diverse views is stalling conservation efforts.\n\n\n\nThis conference has me both concerned about the enormity of the challenge coral reefs face, but also given me hope that science is contributing to a better future. I wanted to finish with some of the key lessons I have learned here.\n\nWe can’t always wait for better science. Our field leaders implore us to get involved with communicating to the public and informing management. The science he have now is sufficient to inform a better future.\nWe need to find more nuanced ways to communicate science to the public and decision makers. Often our key messages are lost because they are too dumbed down. Communicating uncertainty is a particularly important, but difficult, challenge.\nTelling stories about people may be a powerful way to communicate a more nuanced message, that has greater uptake.\nCoral reefs are of national political and economic significance, as evidenced by attendance at the conference by presidents from three countries.\n\n\nEmail me or get in touch via Twitter if you want to contribute something (full acknowledgement will be given)."
  },
  {
    "objectID": "posts/2016-06-21-ICRS-key-insights/index.html#what-is-the-future-of-coral-reefs",
    "href": "posts/2016-06-21-ICRS-key-insights/index.html#what-is-the-future-of-coral-reefs",
    "title": "What is the future of coral reefs? - ICRS summary",
    "section": "",
    "text": "Coral reef ecosystems face unprecedented threats, but stories of people give me hope for the future\nThe International Coral Reef Symposium in Honolulu comes on the back of the worst global coral bleaching event yet observed. Global disasters often leave people wondering what hope there is for the future - particularly given insufficient attempts to curb greenhouse gas emissions.\n\n\n\nSource: http://twitter.com\n\n\nCoral reefs will never be the same as they were. I once saw a talk from Charlie Veron about his efforts to create a global coral atlas (at the conference he announced this is now online). He has dived perhaps more of the world’s reefs than anyone. In an incredibly depressing ending to his talk he stated no-one will ever get to experience coral reefs as he has.\n\n\nCommunicating the loss of coral reefs has been emphasised as an important role for scientists at earlier meetings. A few years ago the rhetoric was all about how scientists need to get out there more - engage with public, get in the media, influence policy, be a force on social media. We do all of this now.\nI actually met, in person, a professional science communicator at this conference. I am sure scicomm people were around at earlier conferences, but not common enough for me to meet one.\nSocial media use is a key part of this engagement. Terry Hughes noted in a panel session that every politician has a twitter account, so if they believe it can change people’s minds, we need to be on there too. As an example, Knowlton talked about the incredible success of the []#oceanoptimism](https://twitter.com/hashtag/oceanoptimism?src=hash) hashtag for engaging the public with ocean solutions through twitter.\nSocial media use at ICRS is orders of magnitude greater than at earlier conferences. Twitter is increasingly popular and the #ICRS2016 hashtag has had widespread impact. A few quick web searches suggest #ICRS2016 had ~240 users and reached over 540 000 people. Check out the #ICRS2016 feed or some of the top tweeters e.g. @OneSwellLife, @physiologyfish and @ultimatemegs to read about the action.\nThese efforts to communicate were apparent at this conference in a subtle way too - the conversation has moved away from how we need communicate, and instead is now about how we can communicate better.\nPeter Kareiva and John Pandolfi both spoke about the need to communicate more of the nuances of science to the media - especially the uncertainty inherent in forecasting the future under different policies. Kareiva used the example of the message box, a commonly used tool for turning academic science into stories for the media or policy. He argued that while this is a useful tool, there it does nothing to help us communicate critical thresholds, impacts and uncertainty - key issues in much of reef science.\nIn her plenary Nancy Knowlton acknowledged the improvement in communication in coral reef science. However, she implored us to tell stories not statistics. In particularly, stories of people. Terry Pratchett, the sci-fi author once said we shouldn’t be called Homo sapiens, the ‘intelligent human’, but Pan narrans the story telling ape (thanks to Beth Fulton for that quote).\nAnd it is stories of people that give me hope for coral reefs.\n\n\n\nCinner and colleagues published a paper about coral reef bright spots last week - places where society and managers are doing better than we would expect at managing reef fish (also presented at ICRS2016). ‘Brights spots’ has become an oft repeated meme at this conference.\nOne of these ‘bright spots’ is Palau and we saw two plenaries about the island nation. The President Remengesau of Palau spoke on day one about the nation’s solution’s orientated approach to reef management. Palau is a world leader in declaring protected areas. Palau’s leadership perhaps stems from a strong traditional management culture. Thus, Palau is a key study area where we can learn about successful management.\nI am finding the study of society’s role in management success brings much more ocean optimism than the pure ecological study of reefs.\nWhen I started my career everyone was saying ecological and physical scientists need to collaborate more with economists and social scientists, but barely anyone was doing so. Now things are turning around.\nBy bringing together social sciences, economists and ecologists we creating a solution orientated science. As Knowlton and others have oft stated, solutions are what give us hope and empower people to take action.\nThe power of solutions is evidenced in the feelings of colleagues at this conference. Colleagues how primarily attended talks about the recent bleaching event were overwhelmingly depressed about the future of reefs, whereas those attending the numerous sessions on marine fishery management, social science, co-management were more optimistic.\nThere is growing acceptance that, while reefs will never return to their previous glory, we can make a better future.\nIn his plenary Pete Mumby talked about the need to study the subtleties of coral reefs. He argued we are facing a future with many ‘mediocre’ reefs, whereas in the past we have focussed our ecological studies on the best of the best, or the worst of the worst. Even degraded reefs can provide services to humans and support significant biodiversity. We need to better understand how management can maximise these benefits.\nAn important part of understanding trends is documenting the past - so much of which has not been recorded by science. But the past is documented in people’s stories, which Loren Mclenachan, Ruth Thurstan and others are using to push back our understanding. For instance newspaper articles can document historical fish catch (Thurstan) and nautical charts can document erosion of reefs (Mclenachan).\nOur future may be dominated by compromises between social, economic and ecological goals. It is hard for the reefs to have their fish and for us to eat them too. Charles Birkeland argued in his plenary that coral reef ecosystems tend to import production not export production. Therefore, they cannot sustain industrial fishing. He talked about Palau’s scenario where its reefs have supported traditional fishing for generations, but the influx of tourists wanting to eat reef fish cannot be supported. Keeping to the solutions theme, he suggested focussing on much more productive pelagic fisheries to support tourism.\nThe realisation that compromises on ecological quality are needed to support human needs has seen the rise of ‘trade-offs’ analysis. Crow White gave a great introduction to the key steps to trade-offs analysis.\nImportantly, trade-offs can help us identify optimal compromises, that are often far better than the status-quo for both people and ecosystems.\nBozec has shown a strong trade-off between harvest of parrotfish and the resilience of coral reefs. However, the right type of management (banning trap fishing) can significantly improve the situation. Aaron MacNeil has similarly found the right type of management is key for fish recovery globally.\nTrade-offs analysis is useful for pollution problems too. Kirstin Oleson and her lab are applying trade-offs analysis to ridge to reef management. They are finding that focussing restoration efforts in just a few areas in catchments can be a very cheap way to reduce sedimentation on reefs.\n\n\n\nDevelopments in social science, the study of management and communication give me hope for the future of reefs. However, some ongoing trends in the culture of reef science give me cause for concern.\n\n\n\nThere are so many women doing amazing work for coral reefs at this conference. So why don’t they receive more recognition from our community? Source: http://twitter.com\n\n\nThere is still a strong gender bias in coral reef science. Perhaps this is a hangover from the past, however I think it isn’t changing fast enough. I would estimate that talks and attendance at this conference is approximately 50:50, however it is easy to see that women are not provided with the same opportunities in our field as the men.\nOne story I heard was of a workshop where a senior (male) scientist said he would have liked to have invited more women, but “there just aren’t that many good female scientists to invite”.\nNot all sexism is this blatant.\nCoral reef science as discipline has a problem with insufficient opportunity for women to receive recognition for their work\nThe culture of science overwhelming recognises the volume of achievements. Often women have shorter careers than men, therefore the volume of female achievements is often on average smaller. This does not mean the quality of their work is lower. We often seem to conflate quality and quantity when we recognise leaders of our field.\nI don’t think there is any lack of ‘good’ female reef scientists. In fact, some of the most compelling talks I have seen at ICRS were given by women (take Lizzie Mcleod’s for example).\nIt is important to note that both men and women and subject to this bias. I have discussed these issues with both male and female colleagues and both genders often find it harder to think of female leaders of our field.\nTo give some examples, this year we saw the award of the 7th Darwin medal, the International Society for Reef Studies top award. There has never been a female laureate of the Darwin Medal.\nThe chance of having no females in 7 Darwin medals is less than 1%\nMaybe the award just reflects a historical hangover of what used to be a male dominated field? I don’t think that’s the whole story. We saw a panel discussion on Wednesday night, that was meant to represent some leaders in our field discussing top issues. There were 2 women on a ten person panel.The chance of having two women out of ten people is 5% (reasonably assuming a 50:50 gender ratio at the conference). In science we often treat a 5% chance ‘statistically significant’, so therefore, 2/10 indicates a significant gender bias.\nI want to be clear that I am not criticising any individual or organization. I do not intend to discredit individuals who have received accolades - they certainly deserved them. Neither am I criticising the International Society for Reef Studies. In fact, the society’s council has a reasonable gender balance and plenary talks at ICRS are balanced across genders.\n\n\n\nDemographics of communicators on the Twitter #ICRS2016 group. Source: http://keyhole.co/preview_new\n\n\nWhat I am arguing for is a change in our culture. We could start by creating more formal opportunities to recognise the contributions of women to our field. For instance the International Society for Reef Studies could have an award for female leaders in coral reef sciences.\nI suspect their are similar biases in cultural and racial inclusion in our field. I have not been keeping track of those, but ping me if you have useful link.\nInformal changes are necessary to change our culture of gender bias. For some tips, check out Jenny Martin’s blog. Top of her list is recognise your own biases.\nCoral reefs need diverse participation. It is many of the women of our field who are pushing us forward to find solutions to the problems facing reefs. Follow twitter: there are more women than men communicating reef science there. Many of people who have pioneered social sciences in reef science are also female.\nI encourage all of you to value diverse career paths and contributions to our field. As Tallis and Lubchenco have argued the failure to include diverse views is stalling conservation efforts.\n\n\n\nThis conference has me both concerned about the enormity of the challenge coral reefs face, but also given me hope that science is contributing to a better future. I wanted to finish with some of the key lessons I have learned here.\n\nWe can’t always wait for better science. Our field leaders implore us to get involved with communicating to the public and informing management. The science he have now is sufficient to inform a better future.\nWe need to find more nuanced ways to communicate science to the public and decision makers. Often our key messages are lost because they are too dumbed down. Communicating uncertainty is a particularly important, but difficult, challenge.\nTelling stories about people may be a powerful way to communicate a more nuanced message, that has greater uptake.\nCoral reefs are of national political and economic significance, as evidenced by attendance at the conference by presidents from three countries.\n\n\nEmail me or get in touch via Twitter if you want to contribute something (full acknowledgement will be given)."
  },
  {
    "objectID": "posts/2019-10-03-study-site-map-tmap/index.html",
    "href": "posts/2019-10-03-study-site-map-tmap/index.html",
    "title": "Make a map of your study site with tmap",
    "section": "",
    "text": "I’m loving the tmap package right now. Here’s why. And we’ll look at how to make a clean map of a study site as we go.\nWe’ll use the data from our study of pollution and coral reefs in the Solmon Islands (repo is here).\n\n\nsf (‘Simple Features’) is the new spatial data standard for R.\nLet’s create an sf points file. First we’ll read the data from the github repo\nsites &lt;- read.csv(url(\"https://raw.githubusercontent.com/cbrown5/BenthicLatent/master/data-raw/JuvUVCSites_with_ReefTypes_16Jun2016.csv\"))\nIts a list of dive sites, where reef type, coral cover and some other variables were observed.\n(So grateful for Rick Hamilton for letting me make this data open access. Just remember, there are a lot of crocodiles in the Solomon Islands, diving in and around mangroves to collect data like this isn’t the easiest job)\nNow to make an sf object:\nlibrary(sf)\n\n## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3\n\nsites &lt;- st_as_sf(sites, coords = c(\"coordx\", \"coordy\"))\nLet’s also get the land for context (you can download this folder from the github repo)\nland &lt;- st_read(dsn =\"LandPoly\")\nFinally, the points have the same coordinate reference system as the polygon, so add that onto the points:\nst_crs(sites) &lt;- st_crs(land)\n\n\n\nMaps are just a series of layers. So lets plot the land (using tmap) first:\nlibrary(tmap)\n\ntm_shape(land) +\n  tm_polygons()\nTo plot a layer in tmap you say tm_shape(data_file_name) first then add the type of layer you want to plot (like ggplot geoms).\nNow we will also want to add on the points (dive sites) layer. We can use tm_dots or tm_symbols for this:\ntm_shape(land) +\n  tm_polygons() +\n  tm_shape(sites)+\n  tm_symbols(size = 0.5)\n\n\n\n\nI mean the code for cartography is easy. Good cartography is hard (but a good place to start is the book ‘How to Lie with Maps’).\nNow let’s keep adding on layers to make our map look nice. It’s all pretty self explanatory once you understand the layering concept:\ntm1 &lt;- tm_shape(land) +\n  tm_fill(col = \"wheat\") +\n  tm_shape(sites)+\n  tm_symbols(size = 0.3, col = \"cover\",\n             title.col = \"Coral cover\",\n             palette = \"Reds\") +\n  tm_compass(type = \"4star\", position = c(0.01, 0.67),\n             size = 1.5) +\n  #positions are between 0 & 1 and place the\n  #bottom left corner\n  tm_scale_bar(breaks = c(0, 10),\n               position = c(0.65, 0.01),\n               text.size = 0.6) +\n  tm_layout(legend.position = c(\"right\", \"top\"),\n            legend.title.size = 0.8,\n            bg.color = \"lightblue\") +\n  tm_credits(\"Study sites from \\nBrown and Hamilton 2018 \\nConservation Biology. \\nCoastline derived from Landsat.\",\n             size = 0.7, position = c(0.01, 0.01))\ntm1\n\nYou might have to play around with the position adjustments for a while to get everything placed just right.\nFinally, you can save your map to file\ntmap_save(tm1, filename = \"mymap.png\",\n          width = 8, height = 8)\nHave fun mapping. For more help see tmap basics or the book Geocomputation with R or come to our course in Queensland February 2018."
  },
  {
    "objectID": "posts/2019-10-03-study-site-map-tmap/index.html#it-runs-smoothly-with-sf",
    "href": "posts/2019-10-03-study-site-map-tmap/index.html#it-runs-smoothly-with-sf",
    "title": "Make a map of your study site with tmap",
    "section": "",
    "text": "sf (‘Simple Features’) is the new spatial data standard for R.\nLet’s create an sf points file. First we’ll read the data from the github repo\nsites &lt;- read.csv(url(\"https://raw.githubusercontent.com/cbrown5/BenthicLatent/master/data-raw/JuvUVCSites_with_ReefTypes_16Jun2016.csv\"))\nIts a list of dive sites, where reef type, coral cover and some other variables were observed.\n(So grateful for Rick Hamilton for letting me make this data open access. Just remember, there are a lot of crocodiles in the Solomon Islands, diving in and around mangroves to collect data like this isn’t the easiest job)\nNow to make an sf object:\nlibrary(sf)\n\n## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3\n\nsites &lt;- st_as_sf(sites, coords = c(\"coordx\", \"coordy\"))\nLet’s also get the land for context (you can download this folder from the github repo)\nland &lt;- st_read(dsn =\"LandPoly\")\nFinally, the points have the same coordinate reference system as the polygon, so add that onto the points:\nst_crs(sites) &lt;- st_crs(land)"
  },
  {
    "objectID": "posts/2019-10-03-study-site-map-tmap/index.html#you-build-maps-as-layers-like-in-ggplot",
    "href": "posts/2019-10-03-study-site-map-tmap/index.html#you-build-maps-as-layers-like-in-ggplot",
    "title": "Make a map of your study site with tmap",
    "section": "",
    "text": "Maps are just a series of layers. So lets plot the land (using tmap) first:\nlibrary(tmap)\n\ntm_shape(land) +\n  tm_polygons()\nTo plot a layer in tmap you say tm_shape(data_file_name) first then add the type of layer you want to plot (like ggplot geoms).\nNow we will also want to add on the points (dive sites) layer. We can use tm_dots or tm_symbols for this:\ntm_shape(land) +\n  tm_polygons() +\n  tm_shape(sites)+\n  tm_symbols(size = 0.5)"
  },
  {
    "objectID": "posts/2019-10-03-study-site-map-tmap/index.html#cartography-is-easy",
    "href": "posts/2019-10-03-study-site-map-tmap/index.html#cartography-is-easy",
    "title": "Make a map of your study site with tmap",
    "section": "",
    "text": "I mean the code for cartography is easy. Good cartography is hard (but a good place to start is the book ‘How to Lie with Maps’).\nNow let’s keep adding on layers to make our map look nice. It’s all pretty self explanatory once you understand the layering concept:\ntm1 &lt;- tm_shape(land) +\n  tm_fill(col = \"wheat\") +\n  tm_shape(sites)+\n  tm_symbols(size = 0.3, col = \"cover\",\n             title.col = \"Coral cover\",\n             palette = \"Reds\") +\n  tm_compass(type = \"4star\", position = c(0.01, 0.67),\n             size = 1.5) +\n  #positions are between 0 & 1 and place the\n  #bottom left corner\n  tm_scale_bar(breaks = c(0, 10),\n               position = c(0.65, 0.01),\n               text.size = 0.6) +\n  tm_layout(legend.position = c(\"right\", \"top\"),\n            legend.title.size = 0.8,\n            bg.color = \"lightblue\") +\n  tm_credits(\"Study sites from \\nBrown and Hamilton 2018 \\nConservation Biology. \\nCoastline derived from Landsat.\",\n             size = 0.7, position = c(0.01, 0.01))\ntm1\n\nYou might have to play around with the position adjustments for a while to get everything placed just right.\nFinally, you can save your map to file\ntmap_save(tm1, filename = \"mymap.png\",\n          width = 8, height = 8)\nHave fun mapping. For more help see tmap basics or the book Geocomputation with R or come to our course in Queensland February 2018."
  },
  {
    "objectID": "posts/2016-02-11-SoTM_five_lessons/index.html",
    "href": "posts/2016-02-11-SoTM_five_lessons/index.html",
    "title": "Species on the Move -some things I learned",
    "section": "",
    "text": "Species on the Move Conference: some key things I learned.\n10-12 February 2016, Hobart Tasmania\nCheck out the conference webpage and my synopses for more details:\n\nDay 1\nDay 2\nDay 3\n\n\n\n1. Science tells a story and telling stories in itself is a suitable objective for science\nA great talk from Emma Lee changed the way I think about science. I come from a very quantitative background, where the point of science is knowledge and improving the future for nature and humanity. But Lee convinced me that telling a story is a suitable goal in itself. People love to tell stories, so much so it is almost a basic human need. Science is the way to come closer to the truth in our thinking and in doing so provide an outlet for people to communicate authentic stories.\n\n\n2. Climate change impacts on ecosystems have become a personal experience\nThe effects of climate change are so pervasive now that individuals are noticing it, and often attributing environmental change to climate change. We shouldn’t underestimate the psychological impact of attributing change to climate change.\n\n\n3. Indigenous people are being pushed for answers on problems they didn’t create\nScientists are working with indigenous people to answer questions about climate change, but scientists come with a very strong idea about the method which can create a communication challenge with indigenous people, particularly when they feel the ‘scientific method’ is pushed upon them. Better communication of climate science, and incorporating alternate cultural view-points is a great challenge for all of us, but one we need to address if we are going to solve the problems climate change is posing. Thanks to Nathalie Pettorelli for this one.\n\n\n4. Climate change ecology is a young, dynamic discipline and this makes it really exciting to participate in\nYoung researchers are doing some of the most exciting research in climate change ecology. Jenn Sunday’s plenary was a great example of this. Further, some of the most influential papers have been lead by female scientists, which is not the case in other older fields. I think climate change ecology does not suffer as much from the handicap of sexism as the more ‘established’ fields. The inclusiveness of our field makes it stronger and conferences like this so much more fun and progressive.\n\n\n5. Climate change impacts and management responses are all about the time-scale\nInvasive species are just native species on a shorter timescale. Management also needs to consider time-scales, for instance training managers in the short-term change can help prepare them for longer-term change. Thanks to Nathalie Butt for this one.\n\n\n6. Mangers are already responding to climate change, even while scientists still argue what the appropriate responses are\nBowman’s talk was a great example of a rapid management response to an extreme fire regime. As scientists, we need to make sure we are engaging with managers and helping them learn about how to respond to change.\n…"
  },
  {
    "objectID": "posts/2017-06-21-bayesian-smoothing/index.html",
    "href": "posts/2017-06-21-bayesian-smoothing/index.html",
    "title": "Smoothing a time-series with a Bayesian model",
    "section": "",
    "text": "Recently I looked at fitting a smoother to a time-series using Bayesian modelling. Now I will look at how you can control the smoothness by using more or less informative priors on the precision (1/variance) of the random effect.\nWe will use the same dataset as the last post.\nTo control the priors for an R-INLA model, we use the hyper argument (not hyperactive, but hyper-parameters):\nlibrary(INLA)\nf3 &lt;- y ~ 1 + f(z, model = \"rw1\", scale.model = TRUE,\nhyper = list(theta = list(prior=\"pc.prec\", param=c(1,0.01))))\nWe can control the level of smoothing through param=c(theta1,0.01). A value of 1 (theta1) is a reasonable starting point (based on the INLA documentation). Lower values will result in a smoother fit.\nThe pc.param stands for Penalized complexity parameters (you could also use a loggamma prior here). My understanding of penalized complexity priors is that they shrink the parameter estimate towards a ‘base-model’ that is less flexible. In this case, we are shrinking the standard deviation (AKA the flexibility) of the random walk (ie how much sequential data points deviate from each other) towards zero. Ultimately if we set theta1 near zero the smoother will be a straight line.\nSo let’s fit three models with theta1 varying and see how it affects the smoothness of the fit:\nf1 &lt;- y ~ 1 + f(z, model = \"rw1\", scale.model = TRUE,\nhyper = list(theta = list(prior=\"pc.prec\", param=c(0.01,0.01))))\n\nf2 &lt;- y ~ 1 + f(z, model = \"rw1\", scale.model = TRUE,\nhyper = list(theta = list(prior=\"pc.prec\", param=c(0.05,0.01))))\n\nf3 &lt;- y ~ 1 + f(z, model = \"rw1\", scale.model = TRUE,\nhyper = list(theta = list(prior=\"pc.prec\", param=c(1,0.01))))\n\nm1 &lt;- inla(f1,family = \"nbinomial\", data  =  dat,\n    control.predictor = list(compute = TRUE, link = 1)\n)\n\nm2 &lt;- inla(f2,family = \"nbinomial\", data  =  dat,\n    control.predictor = list(compute = TRUE, link = 1)\n)\n\nm3 &lt;- inla(f3,family = \"nbinomial\", data  =  dat,\n    control.predictor = list(compute = TRUE, link = 1)\n)\nHere are the resulting fits:\nplot(dat$z, dat$y, col = 'grey80', type = 'l', lwd = 1, xlab = \"years\", ylab = \"Abundance\", las = 1)\n\nlines(dat$z, m1$summary.fitted.values$mean, col = \"skyblue\", lwd = 2)\nlines(dat$z, m2$summary.fitted.values$mean, col = \"steelblue\", lwd = 2)\nlines(dat$z, m3$summary.fitted.values$mean, col = \"darkblue\", lwd = 2)\n\nlegend('topright', legend = c(\"data\", \"theta1 = 0.01\", \"theta1 = 0.05\", \"theta1 = 1\"),\n      lty = 1, col = c(\"grey80\", \"skyblue\", \"steelblue\", \"darkblue\"), lwd = 2)\n\nThe exact value to use for theta1 will vary depending on our data. If the data are more informative (e.g. a longer time-series) we will have to use a smaller value to create a smoother fit.\ndat2 &lt;- list(y = dat$y[1:40], z = 1:40)\nm4 &lt;- inla(f2,family = \"nbinomial\", data  =  dat2,\n    control.predictor = list(compute = TRUE, link = 1)\n)\nplot(dat$z, dat$y, col = 'grey80', type = 'l', lwd = 1, xlab = \"years\", ylab = \"Abundance\", las = 1)\n\nlines(dat$z, m2$summary.fitted.values$mean, col = \"skyblue\", lwd = 2)\nlines(dat2$z, m4$summary.fitted.values$mean, col = \"steelblue\", lwd = 2)\n\nlegend('topright', legend = c(\"data\", \"theta1 = 0.05, 50 data points\", \"theta1 = 0.05, 40 data points\"),\n      lty = 1, col = c(\"grey80\", \"skyblue\", \"steelblue\"), lwd = 2)\n\nAnd that is pretty much it. I haven’t read how to choose the ‘optimal’ value of theta. Pragmatically, one could do it with cross validation or perhaps a model selection measure. However, that seems philosophically incorrect, because it is a ‘prior’. There doesn’t seem to be much guidance in the literature about how to choose priors for smoothing. Message me on Twitter if you have seen an example of doing this.\nIt is amazing to see how the use of priors in statistics has changed since I first learned about Bayesian statistics. It used to be that you would use informative priors only if you had strong prior beliefs about a parameter estimate, such as from expert elicitation, or repeat experiments. If you didn’t have strong prior beleifs, then the view (at least amongst many ecologists) was that it was most cautious to use a highly uninformative prior like the good old Winbugs gamma(0.01, 0.01) prior that was used for precision parameters.\nNow the experts are encouraging us to use weakly informative priors, even when little prior evidence is available. The case being that too broad a prior can slow computations and result in ridiculous results. Consider the broad gamma(0.01, 0.01) prior: it amounts to giving equal weight to a standard deviation of 1 as it does to an SD of 10000. The end result is that this ‘uninformative prior’ can bias your estimates of the SD to be too high.\nAs demonstrated here, another nice feature of informative priors is they can be used to control ‘shrinkage’. Here by varying theta1 we could shrink our model fit towards a base case model (e.g. a null hypothesis) that had no temporal variation.\nIf you are interested to learn more, it is worth reading at least the Introduction of Simpson et al. Penalized Complexity Priors pub. Other good general guidance can be found on the STAN wiki.\nThere is clearly a lot more fun we can have by playing around with priors. I anticipate that applied scientists, like my ecological colleagues, will soon start paying much more attention to prior choice."
  },
  {
    "objectID": "posts/2023-02-01-moving-to-utas/index.html",
    "href": "posts/2023-02-01-moving-to-utas/index.html",
    "title": "Moving to University of Tasmania",
    "section": "",
    "text": "I’m pleased to announce I’ve accepted a new role as Associate Professor of Fisheries Science at the University of Tasmania. I’ll be joining the team at the Institute for Marine and Antarctic Studies.\nMy new role starts May 2023.\nI’ll still be pursuing the same research agenda in ocean management and ecosystem modelling. Most of my research team at Griffith Uni will be staying on at Griffith, so I’ll continue to have strong links in south-east Queensland.\nShoot me an email if you are interested in collaboration opportunities or joining my team as a student at UTas."
  },
  {
    "objectID": "posts/2021-03-27-GAMs-interpolation/index.html",
    "href": "posts/2021-03-27-GAMs-interpolation/index.html",
    "title": "When interpolation doesn’t work with GAMs",
    "section": "",
    "text": "When interpolation doesn’t work with GAMs\nGAMs shouldn’t be used for extrapolation, and they can give strange results for interpolation too. Here I’ll explain why and we’ll look at a solution that involves changing the order a smoother’s penalty. Gaps in an x-covariate, such as gaps in a time-series, can be a problem to deal with if you are trying to interpolate to those gaps. Consider this example data and GAM fit with a big gap in the middle.\nThe below graph shows data as points and a the fit of a GAM:\n\nThere is a strong rising trend for low x and a strong declining trend for high x. The GAM fills in the gap and suggests a polynomial curve.\nThe problem is our predicted y values for intermediate x are much higher than than the range of the observed y values (indicated by the red line. y is predicted to be &gt;2 at x = 0.5, but observations have y always less than about 0.7. So while we aren’t extrapolating on the x-axis, we are extrapolating on the y-axis.\nThere is no straightforward answer to solving this problem. The answer depends on what you know about the system from prior work. Ideally you’d collect more data for the intermediate x values, but if that is not possible here are some options:\n\nKeep the result above if you have reason to believe there truly is a hump shaped relationship between x and y. If you that x causes y and have a theory for that relationship then the extrapolation of y values might be reasonable. This theory would be based on prior work on the two variables.\nKeep the fit above but refrain from making predictions in the gap.\nFit different smooths for the values at the two ends of the spectrum (e.g. by grouping values on the x-axis and using that group as a by variable in the smooth).\nTry a different basis spline for the smooth. See ?smooth.terms for options\nModify the order of the penalty, which can give a fit like this:\n\n\nOption 3 fits the model: gam(y~ s(x, m=1), method = \"REML\"). The m=1 parameter adjusts the order of the penalty for the smooth.\nA brief non-technical description of the m parameter follows. When mgcv fits a spline for a covariate it is balancing the objectives of fitting the data as closely as possible versus fitting the data so closely that the smooth will misrepresent new data. The way mgcv balances these two objectives is to apply a ‘penalty’ that increases the wigglier the smooth gets.\nHow we define ‘wiggly’ for use in the penalty is determined by the m parameter. If m=1, the penalty is applied to the slope of the smoother. If the smooth was a car, this would be like minimizing the number of speeds the car can go. If the car had 3 degrees of freedom it can only go at three speeds, with abrupt jolts when changing between speeds\nThe default is m=2. m=2 is generally preferable because it gives a smoother fit with fewer abrupt changes. The m=2 option applies the penalty to the slope of the slope, AKA the second derivative of the smooth. If we talk about cars again, you could think of this as minimizing the acceleration and deceleration of car. The car can go many speeds, but we are minimizing the rate it shifts between speeds. Of course this would be a smoother ride than the m=1 car.\nTo summarize there is not right answer for this issue of interpolation. There are many options and your choice depends on the system you are studying and how much knowledge you have about its mechanisms."
  },
  {
    "objectID": "posts/2022-10-10-new-R-workshops/index.html",
    "href": "posts/2022-10-10-new-R-workshops/index.html",
    "title": "R workshops Brisbane February 2023",
    "section": "",
    "text": "Hands-on R training!\nFebruary 14th-17th 2023\nPresenters Professor Anthony J. Richardson (UQ CARM) Professor David Schoeman (University Sunshine Coast) Associate Professor Chris Brown (Griffith University) Dr Christina Buelow (Griffith University)\nClick links below to register with the University of Queensland.\nIntroduction to R and tidyverse (1 day): For those new to R\nContent: Getting data into R, Summary statistics, Simple graphics, Data manipulation, Simple statistics Date: 14 February 2023. Venue: UQ, St. Lucia Campus Cost: $250/$200 students\nIntermediate R (2 days): Those with some R experience Content: ANOVA, Regression, Model selection, GLMs for binary and count data, GAMs, Trees, Clustering, Ordination, Mapping, Spatial analysis Dates: 15-16 February 2023 Venue: UQ, St Lucia Campus Cost: $500/$400 students\nAdvanced data wrangling for spatial analysis R (1 day) Content: Use ggplot for basic plots and advanced multilayer mapping Dates: 17 February 2023 Venue: UQ, St Lucia Campus Cost: $250/$200 students\nResources for participants: Detailed html notes including basic theory, applied worked examples in biology/ecology, and solution code.\nRegistration Open Online at https://smp.uq.edu.au/events\nContact: roxanne.jemison@uq.edu.au for more information."
  },
  {
    "objectID": "posts/2023-08-19-structural-causal-modelling-workshop/index.html",
    "href": "posts/2023-08-19-structural-causal-modelling-workshop/index.html",
    "title": "Introduction to structural causal modelling",
    "section": "",
    "text": "A primary goal of science is to understand causes. Structural causal modelling is a framework for developing causal hypotheses to test with data.\nI taught a workshop at the Australian Marine Sciences Association conferences in 2023. I’ve now provided all the course notes online for free.\nOne if the advantages of causal modelling is that the tests can be done with any test you like.\nSo all of the existing statistical inference engines (like generalized linear models) that we are used to using with the R language can be used in structural causal modelling.\nThe workshop shows how to develop causal hypotheses, using R to visualise your hypotheses. It then steps through some examples of develping and testing causal tests with ecological data.\nThe workshop covers packages daggity and ggdag for visualising causal models (directed acyclic graphs) and developing causal tests. We also look at making tests with glm() and how to simulate your own data to test models. Finally, we cover visreg for visualising model output."
  },
  {
    "objectID": "posts/2017-11-10-interacting-impacts-grant/index.html",
    "href": "posts/2017-11-10-interacting-impacts-grant/index.html",
    "title": "New grant to study impacts of interacting stressors on marine ecosystems",
    "section": "",
    "text": "I was excited to learn today that Professor Rod Connolly, Professor Isabelle Côté and I had been awarded an Australian Research Council grant to start in 2018.\nWe will be developing new models to predict how stressors interact to impact marine ecosystems, like seagrass. The work starts in Australia, with modelling and field work locally, but we will be working with Prof Côté to develop applications in Canada too.\nIf you like the sound of this science, watch this space. We will likely be advertising some Honours, PhD and a post-doc position to start next year.\nIf you want to learn more, the press release is below. If you want to know about sciency details, then I just gave a talk in Glasgow on this topic and wrote a blog about it:\nAn international team, led by researchers from the Australian Rivers Institute, Griffith University, have been funded by the Australian Research Council to develop new software tools that will help manage the cumulative impacts that threaten coastal ecosystems.\nEcosystems like seagrass meadows, mangroves and coral reefs are highly threatened but globally important. They provide habitat for fish, support fisheries, capture carbon from the atmosphere and are food for many animals including dugongs.\nDecision makers must manage a bewildering array of threats that coastal ecosystems face. Many coastal ecosystems are threatened by urbanisation, overfishing, pollution and climate change.\n“Managers are often unsure about how to plan for cumulative impacts. They typically have very limited data on how cumulative impacts interact to degrade ecosystems” the grant lead Dr Chris Brown explained. “The tools we aim to develop will help make reliable predictions about cumulative impacts, even where there are little data.”\n“The project thus aims to overcome the largest remaining barrier to effective management of the world’s marine resources” said Professor Rod Connolly, a chief investigator on the grant.\nThe project will begin in Moreton Bay, near to Brisbane. Seagrass ecosystems in Moreton Bay have been in decline for decades due to multiple pressures brought on by urbanisation of Brisbane and surrounds. The team will collect new data to test the ability of the software tools to predict the impact of cumulative impacts, including urbanisation and pollution.\nThe team then aims to scale up the tools developed for Queensland seagrass so they can be used elsewhere.\nThe team also includes Professor Côté from Simon Fraser University in Canada, an international expert on cumulative impacts. Canadian seagrass meadows face many of the same threats as those in Australia, making it a perfect testing ground for the new tools.\n“This is a unique opportunity to provide practical tools, underpinned by strong science, for decision-makers to tackle the most complex issues that affect coasts around the world.” Professor Côté said. “We hope that the results of this project will reduce the amount of guesswork currently involved in managing threatened coastal habitats”"
  },
  {
    "objectID": "posts/2019-02-22-seeking-research-assistant/index.html",
    "href": "posts/2019-02-22-seeking-research-assistant/index.html",
    "title": "Hiring research assistant in spatial ecology",
    "section": "",
    "text": "We are seeking a research assistant to assist with various projects in coastal marine science at Griffith University, Queensland. The person will support our work on the Global Wetlands Project, among other projects. The RA will join a team of research fellows and PhD students and work in an exciting collaborative environment. There will be opportunities to coauthor publications and contribute to informing ecosystem management on applied projects.\nAt a minimum we require someone who can work at least 3 days per week, but ideally full-time, for the next 4 months or more.\nKey criteria are:\n\nSome experience in research\nDemonstrate reliability and timeliness\nDemonstrated organisational skills, such as organising meetings and travel.\nMust meet one or more of the desirable criteria below\n\nDesirable criteria\n\nExperience in GIS\nExperience in advanced R programming (e.g. comfortable with ‘for loops’ or running GIS in R)\nExperience in literature reviewing and writing\n\nThe position can be based at Griffith’s Nathan or Gold Coast campuses, but travel between campuses will be required. We are particularly interested to hear from candidates with GIS experience (e.g. storage, management and processing of large spatial data, assisting with spatial analysis). This position would be ideally suited to honours graduates who want to gain more research experience before considering a PhD or career in research.\nApplicants must have an Australian work permit for the duration of employment (e.g. be a citizen, permanent resident).\nPlease feel free to distribute this to group mailing lists as appropriate."
  },
  {
    "objectID": "posts/2017-08-01-analysis-programs-driving-conservation/index.html",
    "href": "posts/2017-08-01-analysis-programs-driving-conservation/index.html",
    "title": "What analysis programs drive conservation science?",
    "section": "",
    "text": "With the International Congress for Conservation Biology on at the end of July I was wondering, what analysis programs are supporting conservation science? And, what programs support spatial analysis and mapping?\nI ran a quick poll on my blog (you can take it here) to find out. Here are the results (as of 30th July 2017)\n\nThe R program tops the list of most popular analysis programs. Not a surprising result, given its predominance in the biological, ecological and social sciences that conservation science draws from."
  },
  {
    "objectID": "posts/2017-08-01-analysis-programs-driving-conservation/index.html#what-analysis-programs-drive-conservation-science",
    "href": "posts/2017-08-01-analysis-programs-driving-conservation/index.html#what-analysis-programs-drive-conservation-science",
    "title": "What analysis programs drive conservation science?",
    "section": "",
    "text": "With the International Congress for Conservation Biology on at the end of July I was wondering, what analysis programs are supporting conservation science? And, what programs support spatial analysis and mapping?\nI ran a quick poll on my blog (you can take it here) to find out. Here are the results (as of 30th July 2017)\n\nThe R program tops the list of most popular analysis programs. Not a surprising result, given its predominance in the biological, ecological and social sciences that conservation science draws from."
  },
  {
    "objectID": "posts/2023-04-07-GAMs-in-R-18th-century-style/index.html",
    "href": "posts/2023-04-07-GAMs-in-R-18th-century-style/index.html",
    "title": "Making predictions from GAMs in R explained by an 18th century author",
    "section": "",
    "text": "As explained by ChatGPT with some minor edits by me. This post is an example for my post “Tips for using chatGPT to learn R”\nPray, let us first summon the mgcv package, that we may avail ourselves of its functions in our quest to fashion a GAM. Hark! Let us summon the ‘mgcv’ package\nlibrary(mgcv)\nVerily, let us now create a simulated ‘horse’ dataset\n# define the sample size\nn &lt;- 100\n\n# simulate age data from a normal distribution\nage &lt;- rnorm(n, mean = 10, sd = 3)\n\n# simulate rectal temperature data from a normal distribution\ntemp &lt;- rnorm(n, mean = 101, sd = 1)\n\n# Create a function for the mean heat rate\n# with a linear effect of age and a polynomial\n# effect of temp. \nhr_mean &lt;- -0.1*age + 10*temp - 0.05*(temp^2) - 430\n\n# simulate heart rate data from a normal distribution\n# using the function above as the mean heart rate\nhr &lt;- rnorm(n, mean = hr_mean, sd = 8)\n\n# combine the simulated data into a dataframe\nhorse &lt;- data.frame(age, temp, hr)\nWithal, we shall construct a GAM model, with heart rate (hr) as the response variable, and age and rectal temperature (temp) as the predictors, fashioned with the ‘s’ function to impart smoothness onto these predictors\nmodel &lt;- gam(hr ~ s(age) + s(temp), data = horse)\nVerily, let us now display the summary of the GAM model\nsummary(model)\nHark! Let us now appreciate the beauty of the GAM model through a visual representation\nplot(model)\nBehold, the GAM model has been crafted with the utmost care and precision.\nBut hold, we must not overlook the most crucial task - to predict the heart rate values for a new horse. Let us accomplish this mission with the aid of the predict function, and relay the news unto the equine owner!\nnew_horse &lt;- data.frame(age = c(5, 10, 15), temp = c(99, 101, 103))\npredicted_hr &lt;- predict(model, new_horse)\nThus, we have fulfilled our obligations, and have relayed the good tidings of the predicted heart rate value unto the horse owner. May their new companion serve them well, and gallop with vigor and grace!"
  },
  {
    "objectID": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html",
    "href": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html",
    "title": "Guide to posting pre-prints for ecologists",
    "section": "",
    "text": "I just posted my fourth pre-print to bioRXiv (pronounced bio-archive), a pre-print server. On doing so I realised that the whole process is pretty opaque, so I wanted to provied a guide here.\nIf you are not familiar with bioRXiv, or other pre-print servers, they allow you to share an open-access pre-review version.\nThe broad aim of pre-print servers is to allow science to progress more rapidly, because others don’t need to wait for peer-review and the articles are open-access and free to anyone.\nAs I submitted my fourth article, it occurred to me that there is a bit of a process for submitting one efficiently, and for avoiding some issues associated with pre-prints. So here I wanted to cover a brief user guide for posting pre-prints.\nThe guide is based on the opportunities and problems I have come across during my (very short) pre-print posting career.\nBut first, let’s look at the pros and cons of even posting one in the first place."
  },
  {
    "objectID": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#pros-and-cons",
    "href": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#pros-and-cons",
    "title": "Guide to posting pre-prints for ecologists",
    "section": "Pros and cons",
    "text": "Pros and cons\nThe broader benefits and costs have been well covered in the blogosphere, but a few key points are listed here. A good guide to pros and cons was written up in sciencemag. The blog Small Pond Science also has a very reasoned article about why its author doesn’t post pre-prints.\nPros\n\nOther scientists can use (and cite) your article sooner.\nThe pre-print will be open-access, even if your peer-reviewed version is pay-walled.\nGain exposure for your work without waiting for review, which can be particularly important for early career scientists.\nYou can share articles with the science community that you perhaps never intend to publish in a peer-reviewed journal, like this guide to structuring papers. They will also have a DOI so you can get credit for citations.\nYou claim priority for a piece of work that someone else may be about to publish.\nThere is the opportunity to gain pre-review comments that may help improve your article.\n\nCons\nI think most of the cons can actually be avoided or mitigated. The first oft cited one is that someone might ‘steal’ your ideas and scoop your work. Though in reality I think this is a very rare occurrence. Might be worth considering though if you are working on very hot topic.\nThe second is that some journals don’t support pre-prints, though I haven’t encountered any. Even Science and Nature support pre-prints.\nThe next few issues are important, but are discussed more rarely in the broader blogosphere.\nThe third problem is that some journals don’t support you talking to the press about pre-prints. For instance Science Magazine may take any pre-publication press into account when making a decision on your article. Easy to avoid this problem, just don’t do a press release on your article, the press are unlikely to hear about it on their own.\nA fourth problem is that pre-prints can reveal the identity of a paper’s authors, which may break double-blind peer review. A reviewer apparently googled one of my paper titles that was under double blind peer review and find the pre-print (a dubious practice perhaps?). Then they wrote in their review - Chris you should never post pre-prints before submitting the article.\nBut what’s the point of a pre-print if it isn’t posted pre-review? I think its the authors’ prerogative to decide whether they might waive anonymity for the sake of a pre-print (an issue I did consider before submitting said paper). Either way, I think it is a potential issue you should consider, particularly if you want double-blind review.\nI look forwards to starting a debate between the double-blind proponents and the pre-print proponents on Twitter one day so, but will leave that for another blog.\nThe final and I think most problematic issue that I see is that there will be multiple versions of your paper online once it is published. This blog has a more detailed discussion of how that is potentially an issue. If your paper changes considerably in peer-review, then these different versions may also have very different content. The multiple version problem is something that is worth considering carefully.\nOne more note, bioRXiv is sponsored by the Chan Zuckerberg (ie Facebook’s CEO and his surgeon wife) initiative. You might want to consider what they want to do with all these pre-prints. Purportedly they are just interested in speeding up science to help with breakthroughs in medicine…\nSee my guide below for how you can mitigate the final issue.\nOverall I think the pros just outweigh the cons. I started posting pre-prints because I was using other people’s to inform my science, so it made sense to post my own. Having said that I don’t post every article as a pre-print and largely let my students decide whether they want to do it or not.\nIf you follow my guide below it will help you avoid some of the cons and maximise some of the pros."
  },
  {
    "objectID": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#practical-considerations",
    "href": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#practical-considerations",
    "title": "Guide to posting pre-prints for ecologists",
    "section": "Practical considerations",
    "text": "Practical considerations\nA few practical considerations are worth mentioning. People are in such a rush to publish these days and pre-prints can actually slow you down a bit…\nThere is a slight time cost to preprints. You have to submit them like any other journal and I’ve never met a scientist that enjoys working their way through a journal submission page…\nA pre-print may actually slow down publication of your paper by a few days or weeks. There is a time lag (~24 hours) involved in uploading it and then waiting for it to appear online before submitting to a journal. This is lag is longer if you want to wait to get comments."
  },
  {
    "objectID": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#guide-to-submitting-and-revising-pre-prints",
    "href": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#guide-to-submitting-and-revising-pre-prints",
    "title": "Guide to posting pre-prints for ecologists",
    "section": "Guide to submitting and revising pre-prints",
    "text": "Guide to submitting and revising pre-prints\nI am going to assume you are working on submitting something to bioRXiv.\n1. Finish your article and gain approval from coauthors\nYou should always check with your coauthors before posting a pre-print. You also might want to check the pre-print policy of your target journal.\nI recommend create a finished version, like what you would be happy to submit for peer-review (or better!). This article will be free and open to the world, so you will have to live with it for ever! Make sure you have coauthors, or even a colleague, check over it before submitting it to.\nDON’T SUBMIT IT TO A JOURNAL YET. Technically you need to get your bioRXiv article up before submitting to a peer-reviewed journal.\n1b. Formatting!\nThe document as you upload it will be the preprint that everyone can access. I prefer to upload PDFs so I have full control over how they look. Some points to consider:\n\nAdd footnotes with the page number, your name and perhaps the title and a statement about it being a pre-print. This ensures if the document is downloaded and distributed it is clear who wrote it and that it is pre-review.\nMake sure your email is somewhere in the document. So people can contact you easily.\nI like the put figures and tables in the text, rather than at the end (which is how some backwards journals want them submitted), just because I find it annoying to flip back and forwards in the document when reading it.\nDon’t forget to check references are correctly entered etc…\n\n2. Sign up for bioRXiv and get an ORCID\nIt all works better if you have an ORCID I’ve found. Then pre-prints will link to your other profiles.\n3. Follow the steps to upload your article\nThis is pretty similar to the standard journal submission process. Some journals even allow you to submit your pre-print directly.\n4. Choose your licence\nOne important step is that you will get to choose the license your article is distributed under. There are many choices. I use the attribution commercial and non-commercial license. This means people can re-use the article so long as they attribute the source. I allow commercial use because that includes teaching and textbooks. Put some thought into this though, this license also will mean that a book doesn’t have to pay you a fee to use the figures for instance.\n5. Submit and approve the article to bioRXiv\nThen it should appear on bioRXiv in a day or so.\n6. Tell other scientists about it\nThis is important only if you want feedback. I usually post them on Twitter and perhaps email a few people. If you are serious about getting pre-review comments, then this step is essential. It is unlikely that people will just see the article on bioRXiv.\nYou can also link the article to your Google Scholar page if you wish. This is how some colleagues have found out about my latest work (Google Scholar stalking me, but don’t laugh I know we all do it).\nMake sure you check the comments option under your bioRXiv article too, I don’t think you get alerts if someone posts to this (though it seems to be rare).\n7. Submit to a peer-reviewed journal\nYou can do this straight away after it appears on bioRXiv, or wait a few weeks to get pre-review comments.\n8. Revise your pre-print BEFORE your article is accepted at a peer-review journal\nYour can’t revise pre-prints once the article is accepted at a peer-review journal. I tried to update a title once and was blocked - it seems bioRXiv automatically searches the literature for matching titles (and can find accepted articles even before they are posted online!). If they find your article listed at a journal they will block you from revising it.\nIt is important to revise it to minimise issues with having different versions on the web. For instance, people may be confused about similar articles with slightly different titles.\nCheck the journal’s copyright policy though, some consider that they own the rights to review comments and your revisions, in which case any revision to your pre-print based on review comments may be illegal. Other publishers apparently allow posting of the accepted version as a pre-print.\n9. Optional: Do press on your article\nAs I said above, some journals don’t like press/media attention before publication. Also I think it is bad for science if we start sharing work that isn’t peer-reviewed with the general public. Peer-review, with all its flaws, is still our gold standard for making sure science is robust and credible.\n10. Check your pre-print links to the published article\nIf the title of the pre-print and the published article are identical (ie not even one word different) then the preprint should automatically link to the published article. So your work can be properly attributed (and cites won’t be split across different versions).\nIf the titles are different and they don’t pick up the published version, you can email bioRXiv the published paper once it has been out at least 2 weeks and they say they will fix it.\n11. Optional: Check links to your article on search engines\nYou might want to check if search engines, e.g. Google Scholar, pick up on the open-access pdf version of your article. If they don’t you can also try posting the post-review, pre-print version to a university repository or your webpage or ResearchGate (if the journal’s license allows you to do so, which many do)."
  },
  {
    "objectID": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#my-personal-experience",
    "href": "posts/2018-08-20-biorxiv-pre-print-posting-guide/index.html#my-personal-experience",
    "title": "Guide to posting pre-prints for ecologists",
    "section": "My personal experience",
    "text": "My personal experience\nSo far my personal experience has been mixed. The first article I posted went through peer-review at an OA journal so quickly that the pre-print was pretty redundant in the end.\nThe second article has been stuck in review/resubmit cycle for ages! The pre-print has actually been cited, though it is still not published. I am a bit worried how different the pre-print and the published version may be, though I just need to make sure I submit a revision before it is accepted.\nThe third article has just been accepted and I just realised the title was changed in review. So I need to make sure it links back. Aside from that they are pretty similar.\nThe fourth article I just submitted. I am in no rush to publish it, so am going to wait a few weeks and seek pre-review comments on the article. Will update you how that goes. Hopefully the article will be stronger when it comes to submission. For instance I plan on seeking feedback from some statisticians who wouldn’t likely be approached by the journal’s editor.\n\nSummary\nThat’s the guide. Consider carefully whether pre-prints are for you, or not. Some people are rampant pre-print advocates or haters, don’t take their word for it though. The reality is that there is a mix of benefits and costs. If you take a few steps you can maximise those benefits and dodge some of the costs."
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "",
    "text": "In the near future, we will all be doing systematic lit reviews by getting LLMs to gather data from papers. Below is an example of how to extract text data from a study. Once you can interact with LLM from R, you can then batch process many documents to turn text into structured data.\nTLDR; Developing a data extraction workflow is hard work. The development of prompts, data extraction and data cleaning is time consuming and requires thought. So it will still take work to do a systematic review. I’d also recommend you manually check results so that you still learn something yourself and to correct errors.\nI’ll also mention that papers are coming out now testing this idea, e.g. this one shows the AI’s results can be accurate, but only for some types of requests. The supplemental code for that paper also has helpful guidance.\nWe’ll need a few R skills to extract data from papers for lit reviews:\nI’ll mainly focus on step 1 here. Step 2 is possible, by can be finniky and depend on how the pdf is formated. Step 3 is pretty straightforward with a package like stringr. Step 4 is just looping with whatever framework you prefer for that (e.g. for loops or purrr).\nThis blog deals with only one type of application of LLMs to lit reviews, that is maybe the most straightforward and cheapest. There are other ways to use them to write syntheses or discover research trends, such as finetuning models on a large corpus of papers."
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#interacting-with-the-api-from-r",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#interacting-with-the-api-from-r",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Interacting with the API from R",
    "text": "Interacting with the API from R\nNow you are ready to use the API. There are quite a few packages that can help you do this. I’m going to use the new tidychatmodels package, because I like its syntax (its like tidymodels).\nInstall instructions are here (its not on cran as of writing) and here is a nice example of how to use LLMs with pdfs, which helped me greatly with writing this blog.\nlibrary(tidyverse)\nlibrary(tidychatmodels)\nNow start building the instructions to send to the API. tidychatmodels does this step by step with pipes. So we are going to build up a sequence of commands. In the final step we’ll send it to the API.\nnewchat &lt;- create_chat(\"anthropic\", \n    api_key = Sys.getenv(\"ANTHROPIC_KEY\"),\n    api_version = '2023-06-01')\nHere I’m using ‘anthropic’, but there are others e.g. openai and mistral.\nNow add a model. I got the model name on the APIs webpage\nModel choice is dictated by speed, cost and the type of task you want to do. Do some research to optimise this.\nI’m also setting the temperature to control the level of creativitiy in the output, lower values = less creative. max_tokens controls the length of the output.\nnewchat &lt;- add_model(newchat, \"claude-3-5-haiku-20241022\") |&gt;\n      add_params('temperature' = 0.2, max_tokens = 800)"
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#prompting-through-the-api",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#prompting-through-the-api",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Prompting through the API",
    "text": "Prompting through the API\nLet’s do a test run. I’ll create some text\nnewchat &lt;- newchat |&gt;\n  add_message(\n    role = 'user',\n    message = 'You are a funky dude who loves writing online R lessons.\n    Write a paragraph about loading dataframes in R. '\n  ) \nNow we’re ready to hit the dance floor. This is the bit where we actually ask the LLM to do something. It is also the bit that costs money….\nnewchat &lt;- newchat |&gt;\n    perform_chat()\nresponses &lt;- newchat |&gt; extract_chat()\nSo the first time I ran this I got a ‘400’ error. Turns out the ‘max_tokens’ parameter was essential to set. This parameter controls the length of output. Longer outputs cost more (you’re charged per token).\nError in httr2::req_perform(): ! HTTP 400 Bad Request. Backtrace: 1. tidychatmodels::perform_chat(newchat) 5. httr2::req_perform(prepared_engine)"
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#roles",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#roles",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Roles",
    "text": "Roles\nMost LLMs have different roles, often a ‘user’ and a ‘system’ role. These give you more precise control over the LLM’s output. There is great advice from vendors on how to use these roles, e.g.  with (anthropic)[https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts#legal-contract-analysis-with-role-prompting]\nSetting a system role controls how the LLM behaves. People use these to make the AI act like a certain character, such as a research assistant. Let’s see it in action.\nPrompting is like programming, but less precise (annoyingly for me, who likes programming and things to be exact!). Good prompts will tend to get you better results. But of course, as with all deep learning, outcomes have an element of randomness, so can be inconsistent.\nbase_settings &lt;- create_chat(\"anthropic\", \n    api_key = Sys.getenv(\"ANTHROPIC_KEY\"),\n    api_version = '2023-06-01') |&gt; \n    add_model(\"claude-3-5-haiku-20241022\") |&gt;\n    add_params('temperature' = 0.8, max_tokens = 100) \n\nnewchat &lt;- base_settings |&gt;\n  add_message(\n    role = 'system',\n    message = 'You are a funky dude who loves writing online R lessons.'\n  ) |&gt; \n  add_message(\n    #defaults to 'user' role\n    message = 'Write a paragraph about loading dataframes in R. ') |&gt;\n    perform_chat() \n\nextract_chat(newchat)\nOk, so the system role didn’t work very well in this case. To get the ‘funky dude’ output you need to put that into the user role.\nNow let’s look at doing this for a real paper."
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#extracting-text-from-a-pdf",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#extracting-text-from-a-pdf",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Extracting text from a pdf",
    "text": "Extracting text from a pdf\nI’m going to use my recent paper on turtle mortality as an example, becuase I know it well and the file size isn’t too large.\nNow we can use the pdftools package to read in the text from file. You could also try reading this directly from the web as HTML, which might allow you to identify particular sections, or data in tables, more effectively (by using html tags).\nJust note that making repeat requests for html pages might get you temporarily blocked from the publisher’s server.\nlibrary(pdftools)\ntext &lt;- pdf_text(\"data/Brown_etal2024 national scale turtle mortality.pdf\")\nNow doing the above failed to work, I got that darn 400 error again when I sent it to the API. The issue seemed to be in the formatting of the text. So I can get on with the next step, I’ve just cut and paste the text into a text file. Clearly the cleaning up of pdfs/html for use will be a big part of this process. (like all modelling, data cleaning usually takes up most of the time).\nmethods_text &lt;- readLines(\"data/example-text.txt\") |&gt;\n    paste(collapse = \" \")"
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#sending-text-to-the-llm",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#sending-text-to-the-llm",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Sending text to the LLM",
    "text": "Sending text to the LLM\nNow we have our text we can send it to the LLM.\nnewchat &lt;- base_settings |&gt;\n    add_params('temperature' = 0.2, max_tokens = 600) |&gt; \n    add_message(\n    role = 'system',\n    message = \"You are a research assistant who has been asked to summarise the methods section of a paper on turtle mortality.\n    You will extract key statistics on:\n    Sample size: &lt;&lt; Insert sample size here&gt;&gt;\n    Year of the study: &lt;&lt; Insert year here&gt;&gt;\n    DONT EXTRACT ANY MORE INFORMATION BEYOND THIS POINT.\n    \"\n  ) |&gt; \n     add_message(\n         message = methods_text\n    ) |&gt;\n  perform_chat()\n\nextract_chat(newchat)\nThe prompt is key here. In the system role we’ve told it to extract the data in a structured way. I also included ‘DONT EXTRACT ANY MORE INFORMATION BEYOND THIS POINT.’ to prevent it from continuing to talk. Using caps emphasises the point to the AI.\nHere’s the output:\n\nBased on the methods section, here are the key statistics: Sample size: 357 sets (electronic monitoring) for large-scale longline fishery (LLF) Year of the study: 2018\n\nHere’s the output without the final part in caps:\n\nAssistant: Based on the methods section, here are the key statistics: Sample size: - For Small-Scale Fisheries (SSF): Monitored catches from 10 communities across 8 of 9 provinces - For Large-Longline Fisheries (LLF): 357 sets reviewed, spanning from 2018-01-24 to 2019-08-17 - Total catch events for LLF: 25,403 catch events - Electronic monitoring deployed on 7 vessels Year of the study: 2018 (primary year of data collection) Additional notable details: - Used non-parametric bootstrapping for scaling up and estimating confidence intervals - Monitoring methods involved trained community members and electronic monitoring systems - Video review of LLF data took 0.5 to 2 days per set - Used three different methods to scale up turtle catch for LLF to account for variability\n\nThis is all very accurate, but includes too much ‘additional details’. However, it does get more of the methods details with regards to the various sample sizes for different parts of the study.\nLet’s get the output text:\nchatout &lt;- extract_chat(newchat)\noutput &lt;- chatout[chatout$role == 'assistant', 'message']\nFor repeatability just for the blog I’m hard coding that\noutput &lt;- data.frame(message = \"Based on the methods section, here are the key statistics:\\n\\nSample size: 357 sets (electronic monitoring) for large-scale longline fishery (LLF)\\nYear of the study: 2018\")"
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#turn-text-into-structured-data",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#turn-text-into-structured-data",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Turn text into structured data",
    "text": "Turn text into structured data\nOur data is semi structured by the way we did the prompt. Now we want to put the sample size and year into data fields.\nI’m using separate_wider_delim to first seperate out the first paragraph that had two breaks ’’, then again to split the sample size and year into columns (they were seperated by ’’).\nFinally, str_remove_all gets rid of the ‘Sample size:’ and ‘Year of the study:’ text.\nlibrary(stringr)\n\ncleaned_output &lt;- output |&gt; \n  separate_wider_delim(\n    cols = message, \n    delim = '\\n\\n',\n    names = c('response', 'stats')\n  ) |&gt; \nseparate_wider_delim(\n    cols = stats, \n    delim = '\\n',\n    names = c('sample_size', 'year')\n) |&gt; \n  mutate(\n    across(\n      sample_size:year,\n      \\(x) str_remove_all(\n        x, \n        'Sample size: |Year of the study: '\n      )\n    ))\nSo now we can easily get our variables (sort of)\ncleaned_output$sample_size\ncleaned_output$year"
  },
  {
    "objectID": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#reflections-on-challenges-and-next-steps-for-automating-data-extraction",
    "href": "posts/2025-02-12-LLMs-for-literature-reviews/index.html#reflections-on-challenges-and-next-steps-for-automating-data-extraction",
    "title": "How to use large language models to assist in systematic literature reviews",
    "section": "Reflections on challenges and next steps for automating data extraction",
    "text": "Reflections on challenges and next steps for automating data extraction\n\nCost uncertainty\nThis should be cheap. It cost &lt;1c to make this post with all the testing. So in theory you could do 100s of methods sections for &lt;100USD. However, if you are testing back and forwards a lot or using full papers the cost could add up. It will be hard to estimate this until people get more experience.\n\n\nObtaining the papers and dealing with unstructued text in PDFs or HTML\nA big challenge will be getting the text into a format that the LLM can use. Then there are issues like obtaining the text. Downloading pdfs is time consuming and data intensive. Trying to read text data from webpages can also be hard, due to paywalls and rate limits (you might get blocked for making reqeat requests).\nFor instance, in a past study we did where we did simple ‘bag of words analysis’ we either downloaded the pdfs manually, or set timers to delay web hits and avoid getting blocked.\nHTML format would be ideal, because the tags mean the sections of the paper, and the figures already semi-structured.\n\n\nPrompting\nNeed to experiment with this to get it right. It might also be good to repeat prompt the same text to triangulate accurate results.\n\n\nLLM text output cleaning\nIn my simple example this was easy. However, with a large number of prompts and papers the LLM responses might have more variability that needs to be dealt with, e.g. will it always use one line break between my data questions? You can help control this issue by asking it to structure output in xml or JSON format, which is easier to then parse into data sheets.\n\n\nThe data still isn’t data and requires interpretation\nYou can see from my simple examples above that the output can’t be plotted direclty. In fact, it requires more interpretation. The sample size was given as ‘357 sets (electronic monitoring) for large-scale longline fishery (LLF)’. So is it 357? Are there other sample sizes that might be relevant?\n\n\nValidation\nYou’ll definitely want to manually check the output and report accuracy statistics in your study. So maybe your review has 1000 papers, you’ll want to manually check 100 of them to see how accurate the LLM was.\n\n\nYou’ll still need to read a lot of papers to write a good lit review\nA lit review is more than the systematic data. I still believe you need to read a lot of papers in order to understand the literature and make a useful synthesis. If you just use AI you’re vulnerable to the ‘illusion of understanding’.\n\n\nConclusion\nThis tool will be best for well defined tasks and consistently written papers. For instance, an ideal use case would be reviewing 500 ocean acidification papers that all used similar experimental designs and terminology. You’ll then be able to get consistent answers to prompts about sample size etc…\nAnother good use case would be to extract model types from species distribution model papers.\nHarder tasks will be where the papers are from diverse disciplines, or use inconsistent terminology, or methods. My study was a good example of that, there were about 5 different sample sizes reported. So in this example we’d need first to think clearly about what sample size you wanted to extract before writing the prompt."
  },
  {
    "objectID": "posts/2020-07-07-update-risk-research/index.html",
    "href": "posts/2020-07-07-update-risk-research/index.html",
    "title": "Research update, addressing trends emerging in 2020",
    "section": "",
    "text": "An excerpt from our lab meeting 7th July 2020."
  },
  {
    "objectID": "posts/2020-07-07-update-risk-research/index.html#news",
    "href": "posts/2020-07-07-update-risk-research/index.html#news",
    "title": "Research update, addressing trends emerging in 2020",
    "section": "News",
    "text": "News\nI’ve just back from a holiday in and around Noosa, where I had lots of time to spend paddling, thinking and being around Noosa’s beautiful estuaries and lakes. Time away from work is really important for giving yourself space to think about big picture questions, like where is biodiversity science going in the next few years?\nWell its been a year of big changes, and Australia is facing probably its third existential crisis this year, with announcement of massive increase in military funding to address ‘regional instability’ and geopolitical disruption in the Asia-Pacific.\nAs a team we’re going to change tack and spend more time tackling risk-based research, in particular, the modelling, prediction and management of risks to marine biodiversity conservation. This theme fits naturally with the way our research is going, but we will now bring it further to the forefront of what we do, as society increasingly appreciates the massive impact of extreme events on economies and the environments.\nWe have a grant starting on the topic of environmental risk assessment, to be announced formally soon. The notion of dealing with enviro risk runs through a number of our current projects as well. I’m leading writing of a review on planning for risk in a spatial context. Renee Piccolo started her PhD earlier this year, and is addressing how we plan for feasibility in conservation and restoration of coastal habitats. Alyssa Giffin, is close to wrapping up her thesis chapter on prioritizing conservation actions to enhance ‘ecosystem based adaptation’ to extreme climate events.\nWe’ve been running a major research program on multiple stressors to marine ecosystems, which has been keeping Olivia King, Laura Griffiths, Mischa Turschwell and our collaborators busy. A central theme to that work is that we can’t hope to measure all the unexpected impacts of the multitude of interacting stressors that affect coastal ecosystems, so we need model that can predict them, or we need management actions that are robust to uncertainty. So the next phase of that research is looking at the role of extreme events in driving risks to coastal systems.\nAnother new research direction for the team will be looking at how we analyse the new large-datasets being produced by automated classification of images, particularly the work Rod Connolly’s lab is doing on enviro monitoring using AI tools to classify fish species in videos. This new data poses new opportunities and issues for eco-statistics.\nWe’ll be starting a new honours student next week, one of the top undergraduates from my modelling class, Jordan Holdorf. Jordan has a maths undergrad and will be looking at how we can make the most of AI generated data in statistical analyses, both dealing with its shortcomings, but also making the most of its strengths.\nWe’ll also be continuing our work with The Nature Conservancy on electronic monitoring of longline tuna vessels in the western Pacific. Improving monitoring of tuna fisheries in the region is important for economic stability of western Pacific countries and for conserving pelagic ecosystems. There are also opportunities to apply AI tools in this work. I hope to recruit a PhD student to work on this topic soon."
  },
  {
    "objectID": "posts/2020-07-07-update-risk-research/index.html#career-goals",
    "href": "posts/2020-07-07-update-risk-research/index.html#career-goals",
    "title": "Research update, addressing trends emerging in 2020",
    "section": "Career goals",
    "text": "Career goals\nWith all the paddling I also had time to think about career long goals for myself, and the seascapemodels research team.\nIn reflecting on these questions for myself I realized that planning career goals is not something we talk about much, or get a lot of mentoring on. Yes, we do lots of planning and discussion about goals for 1,2,3 year time horizons. But what about career accomplishments? In some sense there is an expectation in academia that you carry the aspirations of your own mentors or institution, such as grant funding, senior positions or publication. Everyone needs to work with these systems to some extent, in order to achieve what they want. But it’s in authoring our own goals that I think we really find job satisfaction.\nSo I had two realisations in Noosa that I would like to shift my priorities for how I spend my own time. First, is to actually put a bit more time into the things that are important to my own long-term career goals. That is (1) investing a little bit of time in my passion project on information theory in ecology and (2) starting my conservation hacking platform to expand the work we do on training scientists and conservationists in R, GIS, statistics and data wrangling. More on that soo.\nAs homework I’ve asked the lab to think about their own career/lifetime objectives. One way to start thinking about this is to ask yourself “why did I choose a career in science over other careers?” To get to the root of why, you should ask yourself this question three times. For instance “Why did I choose a career in science?” “To help the environment” “What makes me want to help the environment so much?” “I think we need to do more to avoid loosing so many species so fast” “Where does your passion for helping the environment come from?” And so on Understanding not just WHAT you are working towards, but also WHERE your goals come from is important. The where is important because it tells us how much our goals are our own, versus somebody else’s goals. Often we find to some extent we are working towards goals someone else gave us, that might be ok, but it might also be a source of dissatisfaction if those goals don’t fit our true passion.\nWhen you feel you can write down pretty clearly what your goals are, and importantly, where they’ve come from, then move onto this next step. Consider 1 or more of the below. 1. What activities can I start doing this week that will help me further my goals? 2. What things am I currently doing that hinder, or detract time away from, my goals? 3. Who can I inspire, recruit, delegate to help me achieve my goals?\nWe’ll discuss those last three things in the next lab meeting."
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html",
    "title": "Bayesian structural equation model tutorial",
    "section": "",
    "text": "Structural equation modelling (SEM) is a tool for exploring relationships among multiple variables. lavaan and piecewiseSEM are two of the most widely used and accessible packages for latent variable modelling in R. Both use frequentist methods.\nIt is also possible to develop SEMs in a Bayesian framework. There are advantages to using the Bayesian framework. So here I provide a brief tutorial on creating a simple Bayesian SEM with the stan program.\nThe blavaan package creates and fits SEMs in a Bayesian framework, but using similar syntax to lavaan. But it can help your understanding to code your own models and give you greater flexibility (though blavaan can do a lot!).\nThe tutorial is based on our recent study that sought to understand relationships among indicators of catchment and fishery condition. If you find the SEM methods described here useful, please cite the paper.\nCode for that study, including a more complicated SEM, is available on the study’s github repo.\nI won’t cover much SEM theory or background on Bayesian modelling. For more info on those topics:"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#advantages-of-bayesian-sem",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#advantages-of-bayesian-sem",
    "title": "Bayesian structural equation model tutorial",
    "section": "Advantages of Bayesian SEM",
    "text": "Advantages of Bayesian SEM\nThe key advantage is the flexibility Bayesian methods give you. We used Bayesian methods because we wanted to combine a state-space model and a SEM. The state-space model was a timeseries model of fish population change through time.\nThe population model was linked to a latent (unobserved) variable that represented cathcment condition. Other observed indicators of catchment condition were pasture biomass and vegetation greenness.\nOther applications of Bayesian SEM could be to model non-gaussian data, incorporating spatial and temporal models into a SEM, or modelling missing data.\nNow we’ll step through a broad overview of how to get started with Bayesian SEM. We’ll use the data from my paper as an example.\nDownload the data"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#define-your-dag-directed-acyclical-graph",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#define-your-dag-directed-acyclical-graph",
    "title": "Bayesian structural equation model tutorial",
    "section": "Define your DAG (directed acyclical graph)",
    "text": "Define your DAG (directed acyclical graph)\nSEMs need to be directed acyclical graphs. The DAG defines the proposed relationships among variables. It is the starting point for writing the math that describes the model.\nWe’ll define a simple DAG that aligns to a factor analysis. We’ll model relationships between streamflow, fish (barramundi) catch per unit effort (CPUE) and vegetation greenness (NDVI) for a catchment in Northern Australia:\nlibrary(dagitty)\nlibrary(ggdag)\ndag1 &lt;- dagify(CPUE ~ LV, \n               NDVI ~ LV,\n               SF ~ LV)\nggdag(dag1)\n\nOur observed indicators are SF (streamflow), NDVI (veg greeness) and CPUE (catch per unit effort). LV is a latent variable that represents correlations among the observed variables.\nThis system is strongly driven by rainfall and river flow, so we expect positive correlations among flow, fish and vegetation."
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#define-the-stan-model",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#define-the-stan-model",
    "title": "Bayesian structural equation model tutorial",
    "section": "Define the stan model",
    "text": "Define the stan model\nNow we have a DAG, we need to write equations to represent the proposed relationships. We also need to specify the priors.\nHere I’m assuming normal distributions for each variable (logging for CPUE). Priors are weakly informative to encourage convergence.\nI won’t include the full stan model definition below, just the model component. Click here to download the full model file you’ll need to run this.\nHere’s the core of the model:\ncfa_mod &lt;- \"model{ \n\n  //\n  //Ecological condition \n  //\n  nu ~ std_normal(); \n  //Note variance fixed to 1 by using standard normal here\n  \n  //Priors for indiator params \n  a_NDVI ~ normal(0,10);\n  a_CPUE~ normal(0,10);\n  a_SF ~ normal(0,10);\n  \n  beta_NDVI ~ normal(0,1);\n  beta_CPUE ~ normal(0,1);\n  beta_SF ~ exponential(1.8);\n  //Note use of exponential() for streamflow to ensure positive\n  // only values. Helps convergence. \n  \n  //Observation errors\n  sigma_CPUE ~  exponential(0.1);\n  sigma_NDVI ~  exponential(0.1);\n  sigma_SF ~  exponential(0.1);\n  \n  // Observations\n  lnCPUE ~ normal(lnCPUE_hat, sigma_CPUE);\n  NDVI ~ normal(NDVI_hat, sigma_NDVI);\n  SF ~ normal(SF_hat, sigma_SF);\n}\"\nRead in the full model file and compile it for stan:\nlibrary(rstan)\n\nmod &lt;- stan_model(file = \"simple-model.stan\", verbose = FALSE)\nAt this point there is usually a fair bit of debugging and fixing of typos until the model parses correctly."
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#consider-identifiability",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#consider-identifiability",
    "title": "Bayesian structural equation model tutorial",
    "section": "Consider identifiability",
    "text": "Consider identifiability\nIdentifiability is a common issue in SEM. Roughly it means that some of the free parameters you are trying to estimate are interchangeable. Therefore, even with an infinite amount of data you would be unable to identify their values precisely (e.g. one could be high the other low or vice-versa).\nA basic example would be the model:\ny = a*x + b*x + c\nparameters a and b are interchangeable.\nIdentifiability is a bit different in Bayesian models, because we have priors. For instance, in the model above, if we set a very strong priors on a and/or b (based on theory for instance) we may still be able to estimate their values precisely.\nIn a complex SEM issues of identifiability can be much harder to distinguish than the example above, as we will see.\nIdentifiability issues in a Bayesian will manifest as poor convergence of MCMC chains. You will see high Rhats. Digger deeper you may find some parameters have bimodal posterior distributions (e.g. if you do a histogram of all samples for a parameter). You will also find strong correlations in parameter estimates. For instance in the model above parameters a and b would be strongly correlated (near 1).\nYou may be able to get convergence if you run your model for really long chains, however, it will be more efficient to try and fix the model. It will also mean your parameters are easier to interpret.\nIf you have identifiability issues, parameters that have the property above are a good place to start to try to fix issues in your SEM formulation.\nIn the stsan code above I’ve made two key decisions to facilitate convergence.\nFirst, I’ve fixed the latent standard deviation to 1 to ensure identifiability. You could also have the latent’s SD free, but then fix one of the regression slopes to 1.\nSecond, I’ve set the prior for the streamflow beta (regression slope on the latent variable) to be an exponential. This ensures it has only positive values.\nFor example, imagine that streamflow, NDVI and CPUE are all positively correlated with each other. Then their beta’s all need to be the same sign. However, they could equally well be all positive, or all negative. The meaning is the same, since the latent variable has not units.\nIf we run the model where all betas are sampled from normal priors, then we would hit issues with parameter switching. We see strong bimodality in the posteriors for the betas, because they can equally well be all positive or all negative. This slows converngence significantly.\nSo instead I’ve used an exponential(1.8) prior for the streamflow beta. This peg the sign to whatever streamflow is doing. So if streamflow is positive and the other covariates are positively associated with it, they will all have positive betas.\nI chose 1.8 so that my prior has similar 90th percentile as a normal distribution. e.g. compare qnorm(0.9, 0,1) to qexp(0.9, 1.8) both have 90th percentiles at about 1.28."
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#do-some-data-cleaning",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#do-some-data-cleaning",
    "title": "Bayesian structural equation model tutorial",
    "section": "Do some data cleaning",
    "text": "Do some data cleaning\nNow let’s get the data and just clean it up a bit.\ndat &lt;- read.csv(\"Data for Barramundi model.csv\")\nFirst calculate the log of the CPUE:\ndat$lnCPUE &lt;- log(dat$Barramundicatch/dat$Effort)\nSome variables are missing data. In our paper I modelled missing data, but here we will just use years with complete data for simplicity:\ndat2 &lt;- subset(dat, Year %in% 1995:2015)\nNow we will standardize and centre each variable, makes it easier to set priors and interpret the regression coefficients (AKA loadings). We’ll also define a list to use in the stan model\ndatstan &lt;- with(dat2, {\n  x = list(N = nrow(dat2),\n       SF = (Streamflow - mean(Streamflow))/sd(Streamflow), \n       NDVI = (NDVI - mean(NDVI))/sd(NDVI), \n       lnCPUE = (lnCPUE - mean(lnCPUE))/sd(lnCPUE)\n  )})"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#fit-the-model",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#fit-the-model",
    "title": "Bayesian structural equation model tutorial",
    "section": "Fit the model",
    "text": "Fit the model\nNow we are ready to fit the stan model:\n# options(mc.cores = 1) #parallel::detectCores())\n\nfitm1 &lt;- sampling(mod, data = datstan,\n               iter=8000, chains=3, thin = 10)"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#investigate-convergence",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#investigate-convergence",
    "title": "Bayesian structural equation model tutorial",
    "section": "Investigate convergence",
    "text": "Investigate convergence\nFor this task I recommend launching the shiny tool for exploring stan fits. Can be done like this:\nshinystan::launch_shinystan(fitm1)\nIf convergence is poor (e.g. higher Rhat values or strong autocorrelation in the chains) a good place to start in a SEM is to check for parameter non-identifiability, as described above. This means some parameters are interchangeable (or nearly so).\nA handy way to explore for the above issues is to click the “Explore” tab in the shiny, then do bivariate plots of parameters against each other.\nThe histograms for this model fit look like they’ve converged on a unimodal distribution:\nsamps &lt;- extract(fitm1)\n\npar(mfrow = c(1,3))\nhist(samps$beta_NDVI)\nhist(samps$beta_CPUE)\nhist(samps$beta_SF)"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#explore-model-results",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#explore-model-results",
    "title": "Bayesian structural equation model tutorial",
    "section": "Explore model results",
    "text": "Explore model results\nFor this model the regression slopes (AKA factor loadings) are of primary interest. They tell us how the different indicators are related to each other.\nMake a dataframe of the samples of interest:\nfit_dat &lt;- with(samps, data.frame(beta_SF, beta_NDVI, beta_CPUE))\nNow use tidyr to reshape for easy plotting:\nlibrary(tidyr)\nlibrary(dplyr)\nfittidy &lt;- fit_dat %&gt;% \n  mutate(sample = 1:nrow(fit_dat)) %&gt;%\n  pivot_longer(-sample, names_to = \"Parameter\", values_to = \"value\")\nNow use ggplot to plot posterior distributions\nlibrary(ggplot2)\ntheme_set(theme_classic())\nggplot(fittidy) + \n  aes(x = value, y = Parameter, fill = Parameter) + \n  geom_vline(xintercept = 0) + \n  geom_violin()\n\nAll have positive values, indicating all indicators are positively associated with each other."
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#check-model-assumptions",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#check-model-assumptions",
    "title": "Bayesian structural equation model tutorial",
    "section": "Check model assumptions",
    "text": "Check model assumptions\nWe need to check assumptions like any regular model. This means checking distributions are appropriate to model the data, same as you would for any GLM.\nIn a SEM we also have to think about d-separation. In short, this means checking for residual correlations between a our variables that aren’t explicitly part of the model.\nThere’s formal tests of d-separation for Bayesian models, but many authors have just used visual checks of residual correlations.\nThere are many resources on d-separation, here’s one paper. Books by Judea Pearl and Bill Shipley are also good ways to learn.\nOur data here also has the additional complexity of being time-series. Our model above assumes d-separation of a variable from itself through time, so this is also something we could check (by testing for autocorrelation in the residuals).\nI won’t show all these tests in this brief tute, but here’s a start on getting residuals. Just subtract the predictions from the observations, e.g. for NDVI:\nmed &lt;- apply(samps$NDVI_pred, 2, median)\nresids &lt;- datstan$NDVI - med\nOne way to check the normal distribution is appropriate for the NDVI data is to do a quantile-quantile plot of the residuals:\nqqnorm(resids)\nqqline(resids)\n\nWe want to see the points fall close to straight on the line, which in general they do.\nWe can also check for non-linearities by plotting observations vs predictions:\nplot(med, datstan$NDVI)\n\nOf course this is Bayesian analysis, so we could also create credible intervals for the residuals.\nIf you do the above for CPUE you’ll see there is clearly strong autocorrelation. This is one reason I used a state-space model in the paper."
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#graph-model-results",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#graph-model-results",
    "title": "Bayesian structural equation model tutorial",
    "section": "Graph model results",
    "text": "Graph model results\nLet’s make a plot of the result. First we’ll get median effect sizes (just the median of the chains for each parameter)\nmeds &lt;- round(apply(fit_dat, 2, median),2)\nNow we can make the plot with labels on the arrows. Most of this code is just setting up the position of things:\ndag1 &lt;- dagify(CPUE ~ LV, \n               NDVI ~ LV,\n               SF ~ LV)\ncoords &lt;- list(x = c(CPUE = -1, NDVI = 0, SF = 1, LV = 0),\n               y = c(CPUE = 1, NDVI = 1, SF = 1, LV = 0))\ncoordinates(dag1) &lt;- coords\nggdag(dag1) +\n  annotate(\"text\", x = c(-0.6, 0.1, 0.6), y = c(0.5, 0.5, 0.5),\n           label = meds) +\n  lims(y = c(-0.5, 1.5))\n As its a Bayesian analysis we could also add CIs or probability that an effect is &gt;0. They could be calculated like this.\ne.g. upper 95th quantile:\nround(apply(fit_dat, 2, quantile, 0.975),2)\n\n##   beta_SF beta_NDVI beta_CPUE \n##      1.27      1.21      1.13\nOr probability effect is greater than zero:\nnsamps &lt;- nrow(fit_dat)\nround(apply(fit_dat, 2, function(x){sum(x&gt;0)/nsamps}),2)\n\n##   beta_SF beta_NDVI beta_CPUE \n##      1.00      1.00      0.98"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#model-predictions",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#model-predictions",
    "title": "Bayesian structural equation model tutorial",
    "section": "Model predictions",
    "text": "Model predictions\nOur model also predicts the mean for each variable. Let’s just look at CPUE. In this matrix rows are samples are columns are years. So we can summarize each column to get the prediction with intervals for that year.\nHere is the median with 90th credible intervals and data shown as points:\nndvi_pred &lt;- data.frame(year = dat2$Year, \n                        ndvi_obs = datstan$NDVI, \n                        med = apply(samps$NDVI_pred, 2, median),\n                        lwr = apply(samps$NDVI_pred, 2, quantile, 0.05),\n                        upr = apply(samps$NDVI_pred, 2, quantile, 0.95))\n\nggplot(ndvi_pred) + \n  aes(x = year, y = med) +\n  geom_line() + \n  geom_ribbon(aes(ymin = lwr, ymax =upr), color = NA, \n                 fill = \"black\", alpha = 0.5) +\n  geom_point(aes(y = ndvi_obs)) +\n  labs(x = \"Year\", y = \"NDVI (normalized)\")"
  },
  {
    "objectID": "posts/2023-06-15-bayesian-sem-tute/index.html#conclusion",
    "href": "posts/2023-06-15-bayesian-sem-tute/index.html#conclusion",
    "title": "Bayesian structural equation model tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nBayesian modelling is a powerful framework for building SEMs. It gives you greater flexibility in developing models than the standard SEM R packages.\nYou can use all types of non-gaussian distributions to model data, as well as build in other model types like non-linear or state-space models.\nThe cost is an extra overhead to learn the theory of SEM and the methods for Bayesian modelling. But if you have a background in one or the other, and some programming ability, then it won’t be too hard.\nI learned Bayesian modelling well before I started on SEMs. When I came to SEM (through the lavaan package) I was overwhelmed and confused by jargon and interpretation of greek letters that have a special meaning to SEM specialists. I found it much easier to gain a high level understanding of SEM issues (like d-seperation) then work in the Bayesian framework to build models."
  },
  {
    "objectID": "posts/2022-09-04-insights-NGO-science-symposium/index.html",
    "href": "posts/2022-09-04-insights-NGO-science-symposium/index.html",
    "title": "How to work with non-governmental organisations",
    "section": "",
    "text": "I attended a symposium for scientists and non-governmental organisations at the Centre for Biodiversity and Conservation Science at the University of Queensland recently.\nThe aim of the symposium was to promote more collaboration between NGOs and conservation scientists. Here are some of the key lessons I took home.\nOne researcher told me she wish she’d had such a symposium when she started her PhD. It would have given her the know-how needed to reach out to NGOs and start collaborating as part of her PhD.\nSo here are the key lessons.\n\nLook up NGOs in your area and read about their science priorities. Many, like Birdlife Australia (who presented at there) publish their science priorities.\nGet in touch with NGO staff. If you know someone who knows someone at an NGO that is great. Start by asking your supervisor. If not, try just emailing the contact on the NGOs webpage and say you are a scientist looking to align your research for more impact.\nAsk what their science needs are. If you can get a meeting with the NGO staff, great. But don’t waste your meeting pitching your ideas to them. Listen first. Find out what their problems and issues are. They probably have science needs outlined already. For instance, the presenter from The Wilderness Society pitched several ideas they’d love to have science on for their advocacy work. These included analyses of national threats to Australian fauna and attribution of extinction risk to specific industries.\nAlign your science with their priorities. The next step would be to come up with a research plan. Make sure it aligns with their priorities. If you can get time with the NGO, pitch the plan back to them (its ok to pitch now that you’ve listened to what their needs are). If you can’t, you may still like to go ahead and do the research in the hope they pick it up later. For instance, Birdlife Aus has aligned their science awards to their priorities.\nBe mindful of timelines. NGO timelines are often fast moving. Their priorities can shift rapidly as funding or politics change. This can be a challenge for research projects.\n\nFor PhD students I wouldn’t recommend projects that have strict deliverables to a deadline. For instance, they may need to deliver results in a timely manner to be able to influence major new government policies. Or they may be looking to contract someone to prepare a report. PhDs need time to learn and this often takes longer than NGO timelines.\nThat being said, NGOs often have many research ideas they’d like to do, but which their work isn’t directly dependent on. These kinds of projects can be great for students. Just be mindful that if the research drags on too long (like years) the NGO may shift their priorities and you’ll lose engagement.\nSo aim for projects that need 6 months to a year to complete. Talk to your supervisor about projecting time needed, as we all tend to underestimate that.\n\nNGOs are often under-resourced. Which means they probably won’t have bucket loads of funding for your project, but they do love help where they can get it. If you are a PhD student with time and capacity to share, then they’ll probably love that. So don’t be shy about reaching out.\n\nJust be sure to plan your engagement with NGOs with a mentor or supervisor. You want to make sure you align the project with your skillset and feasible timelines for providing results.\nWorking with NGOs has been one of the most rewarding parts of my career. As an academic (and as a student) I have the luxury of getting to spend some of my time on passion projects. Sometimes the ‘work for free’ has then lead onto funded work.\nThe research I’ve done with NGOs has often lead to real world impact. These stories have become a core part of my career story that has built my reputation, and which I also look back on as the most rewarding experiences of my career."
  },
  {
    "objectID": "posts/2015-09-17-digitising-graphs/index.html",
    "href": "posts/2015-09-17-digitising-graphs/index.html",
    "title": "Digitising graphs with R",
    "section": "",
    "text": "I recent had reason to digitise some graphs from published papers. You could buy a program to help you do this, but it is actually also possible with R. Here is how to do it.\nFirst, take your graph, for instance, here I have taken a screen shot of a map from a paper on ecosystem modelling in Australia. I want to know the coordinates of each of the models plotted on the map. Just the centre point will do for now. You could extract the edges of all the polygons using this method too, but it would be tedious.\n\nThen get and load in the PNG and raster packages, which will allow us to load the image file and plot it in R:\nlibrary(png)\nlibrary(raster)\npic &lt;- as.raster(readPNG('sole.png'))\nNow plot the image on a graph that scales (0,1), so we know where everything has been plotted.\nplot(c(0,1), c(0,1))\nrasterImage(pic, xleft = 0, xright = 1,\n    ybottom = 0, ytop = 1, interpolate = F)\nWe can use abline() to add some grids to make picking points easier in the following step:\nabline(h = seq(0, 1, by = 0.1))\nabline(v = seq(0, 1, by = 0.1))\nNow use locator() to locate the corners on your imported graph:\nxycorners &lt;- locator(2)\nxycorners &lt;- cbind(xycorners$x, xycorners$y)\nNow enter the values of the corners. For instance, here I used locator to find the bottom left corner (110E by 45 S) and the top right (160E by 15S).\nxycvals &lt;- matrix(c(110, 160, -45, -15),\n    nrow = 2, byrow = F)\nNow we use locator to make twelve clicks (there are twelve models) on each of the locations of the models:\nxypts &lt;- locator(12)\nxypts3 &lt;- cbind(xypts$x, xypts$y)\nOur xypts3 locations are in the (0,1) scale, so then convert these coordinates to the actual coordinates using the locations of the known corners. Here is the conversion for the x-coordinates:\nxrise &lt;- xycvals[2,1] - xycvals[1,1]\nxrun &lt;- xycorners[2,1] - xycorners[1,1]\nxa &lt;- xrise/xrun\nxb &lt;- xycvals[1,1] - (xa * xycorners[1,1])\nxvals &lt;- xypts3[,1] * xa + xb\nHere is the conversion for the y-coordinates:\nyrise &lt;- xycvals[2,2] - xycvals[1,2]\nyrun &lt;- xycorners[2,2] - xycorners[1,2]\nya &lt;- yrise/yrun\nyb &lt;- xycvals[2,2]-(ya * xycorners[2,2])\nyvals &lt;- (xypts3[,2] * ya) + yb\nNow we have our true coordinates stored in xvals and yvals. We can replot to check them or save them as a csv for later use. Here we will use the maps library, so we can quickly plot our points back to a map of Australia:\nlibrary(maps)\nmap(database = 'world', xlim = c(110, 160),\n   ylim = c(-45, -10))\npoints(xvals, yvals, pch = 16, col ='red')\nwrite.csv(cbind(xvals, yvals),\n   'ecopath-models.csv', row.names = F)\nHere is our new map:"
  },
  {
    "objectID": "posts/2015-09-17-digitising-graphs/index.html#how-to-digitise-graphs-using-r",
    "href": "posts/2015-09-17-digitising-graphs/index.html#how-to-digitise-graphs-using-r",
    "title": "Digitising graphs with R",
    "section": "",
    "text": "I recent had reason to digitise some graphs from published papers. You could buy a program to help you do this, but it is actually also possible with R. Here is how to do it.\nFirst, take your graph, for instance, here I have taken a screen shot of a map from a paper on ecosystem modelling in Australia. I want to know the coordinates of each of the models plotted on the map. Just the centre point will do for now. You could extract the edges of all the polygons using this method too, but it would be tedious.\n\nThen get and load in the PNG and raster packages, which will allow us to load the image file and plot it in R:\nlibrary(png)\nlibrary(raster)\npic &lt;- as.raster(readPNG('sole.png'))\nNow plot the image on a graph that scales (0,1), so we know where everything has been plotted.\nplot(c(0,1), c(0,1))\nrasterImage(pic, xleft = 0, xright = 1,\n    ybottom = 0, ytop = 1, interpolate = F)\nWe can use abline() to add some grids to make picking points easier in the following step:\nabline(h = seq(0, 1, by = 0.1))\nabline(v = seq(0, 1, by = 0.1))\nNow use locator() to locate the corners on your imported graph:\nxycorners &lt;- locator(2)\nxycorners &lt;- cbind(xycorners$x, xycorners$y)\nNow enter the values of the corners. For instance, here I used locator to find the bottom left corner (110E by 45 S) and the top right (160E by 15S).\nxycvals &lt;- matrix(c(110, 160, -45, -15),\n    nrow = 2, byrow = F)\nNow we use locator to make twelve clicks (there are twelve models) on each of the locations of the models:\nxypts &lt;- locator(12)\nxypts3 &lt;- cbind(xypts$x, xypts$y)\nOur xypts3 locations are in the (0,1) scale, so then convert these coordinates to the actual coordinates using the locations of the known corners. Here is the conversion for the x-coordinates:\nxrise &lt;- xycvals[2,1] - xycvals[1,1]\nxrun &lt;- xycorners[2,1] - xycorners[1,1]\nxa &lt;- xrise/xrun\nxb &lt;- xycvals[1,1] - (xa * xycorners[1,1])\nxvals &lt;- xypts3[,1] * xa + xb\nHere is the conversion for the y-coordinates:\nyrise &lt;- xycvals[2,2] - xycvals[1,2]\nyrun &lt;- xycorners[2,2] - xycorners[1,2]\nya &lt;- yrise/yrun\nyb &lt;- xycvals[2,2]-(ya * xycorners[2,2])\nyvals &lt;- (xypts3[,2] * ya) + yb\nNow we have our true coordinates stored in xvals and yvals. We can replot to check them or save them as a csv for later use. Here we will use the maps library, so we can quickly plot our points back to a map of Australia:\nlibrary(maps)\nmap(database = 'world', xlim = c(110, 160),\n   ylim = c(-45, -10))\npoints(xvals, yvals, pch = 16, col ='red')\nwrite.csv(cbind(xvals, yvals),\n   'ecopath-models.csv', row.names = F)\nHere is our new map:"
  },
  {
    "objectID": "posts/2024-08-28-bias-fishery-stock-assessments/index.html",
    "href": "posts/2024-08-28-bias-fishery-stock-assessments/index.html",
    "title": "Stock assessment models overstate sustainability of the world’s fisheries",
    "section": "",
    "text": "A deep dive into stock assessments of fisheries around the world has revealed their sustainability is overstated – and it has implications for fisheries management and consumer awareness.\nStock assessments are conducted regularly to measure the impact of fishing on fish and shellfish populations in global fisheries management regions. These assessments inform approaches for preventing overfishing, rebuilding overfished stocks and protecting marine ecosystems.\nIn a new study published in the journal Science, an international research team compared past and recent stock assessments across 230 fisheries worldwide.\n“Independent scientific monitoring is essential for accurately assessing fish stock sustainability,” said IMAS marine ecologist and lead author of the study, Professor Graham Edgar.\n“This study was an opportunity to compare the stock status estimated in a given year, such as 2010, with a more recent revised estimate for that same year – and we found that the earlier stock assessments were often too optimistic about the number of fish in the ocean.”\nThe study showed an inconsistency in stock assessments, with a strong pattern of over-estimating a fishery’s population status for stocks that were most depleted.\n“Depleted fish stocks are often the most contentious,” Professor Edgar said. “When a stock is overfished, fisheries management needs to make tough decisions about reducing fish catches to reverse the trend in stock declines. This includes reducing catch limits, which will ensure the fish stock can continue to support food and jobs into the future.”\nIMAS researcher and co-author, Dr Nils Krueck, said stock assessment scientists around the world acknowledge the issues highlighted in the study.\n“I believe it’s vital to bring these issues into focus, as it will lead to improvements in the way we do our assessments and ultimately improve fisheries management globally.”\nIMAS marine ecologist and study co-author, Associate Professor Chris Brown, said the rising trends in the biomass of overfished stocks noted in some stock assessments often disappeared in later assessments, suggesting they were too optimistic about the pace of recovery in overfished stocks.\n“When management put limits on catches, many overfished populations have failed to recover as quickly as expected, so our study suggests the assessments tools being used are too optimistic about the real recovery potential,” he said.\nThe study also found that fish stocks with low economic value, or in areas with rapidly rising sea temperatures, were more susceptible to inaccurate assessments.\n“Stocks with low economic value will usually have less scientific information to inform the assessments – and this may impact the ability to accurately assess stock status,” Associate Professor Chris Brown said.\n“Meanwhile, fish stocks in ocean warming hotspots are often under-estimated, as stock assessments are not well tuned to populations extending their range into cooler regions – which is happening much faster than expected.”\nThe study highlights ways to improve the accuracy of fish stock assessments, such as expanding independent fisheries monitoring and changing stock assessment protocols.\n“This could include establishing a ‘red team’ that looks at potential worst case scenarios and works to prevent the collapse of fish biomass,” Professor Edgar said.\n“Our study clearly shows we need to take a much greater precautionary approach to protect our vital fish stocks around the world – for sustainable fisheries and healthy oceans, and ultimately for our own food security.”\nCitation: Graham J. Edgar et al. ,Stock assessment models overstate sustainability of the world’s fisheries. Science 385,860-865 (2024).DOI:10.1126/science.adl6282"
  },
  {
    "objectID": "posts/2025-03-30-why-every-peer-review-should-be-nice/index.html",
    "href": "posts/2025-03-30-why-every-peer-review-should-be-nice/index.html",
    "title": "Why every peer review you write should be nice",
    "section": "",
    "text": "As academics, the best way we can combat ignorance and help the environment is to train and support good scientists.\nWe need to build up our peers, not put them down. We’re facing a global environmental crisis. Emissions are accelerating again, the climate is rapidly warming, we’re losing the world’s biodiversity, people have nothing to eat because the environment is too degraded.\nWe need good committed scientists who stay the course for careers in all areas of environmental science.\nPeer-reviews are often at odds with that goal. Many reviews are written to put people down, rather than build them up.\nI was recently forwarded reviews on a paper a student is leading. The paper was rejected, which is fine. Its a normal part of the publication process and the production of valid science.\nHowever, I felt so frustrated about one of these reviws. It was so mean and nit-picked every last detail in the paper.\nThat’s not helping us to create the next generation of committed scientists. Its slowing them down, and probably pushing some away.\nThe people writing these reviews don’t know how privileged they are.\nIf you go talk to people working with conservation on the ground you will realize this.\nConservationists are facing urgent issues, with communicating the importance of ecosystems to government and the public. Urgent issues that need some science advice now, not perfect science in 10 years.\nSo sitting around pulling apart every last detail in a paper you’ve been sent to review is a privilege.\nWhat should you do?\nAsk yourself: is this science good enough? By good enough I mean are the conclusions scientific?\nThe study might not be done the way you would do it, frame the introduction how you think it should be done, or cite the references you would want to cite. But you need to weigh the effort required to make those changes against what the world needs us to be spending our time on.\nIf you nit-pick every last detail then the you’re burning the time of the scientists that now have to respond on all those details.\nThat’s time they could be spending on working with policy-makers on solutions, educating the public, or training new scientists.\nActivities that actually help the issues the world faces.\nIf its a student who gets this review you’re doing an even greater disservice. Not only are you wasting time, but you might be killing their confidence. That’s going to slow them down 10 times over.\nSo yes we should peer-review. Yes we should absolutely reject papers if the science isn’t valid. But don’t go overboard on your reviews. If you’re going to reject a paper, just bring up the main 1 or 2 flaws.\nThat’s enough to reject it if it needs rejecting.\nAnd be nice.\nPS\nIf you want to go one better, you can offer encouragement. Even if you ultimately recommend rejecting the paper. Here’s how\nPPS If the quality of the science in a paper makes you really mad then you should try even harder to write an encouraging review. Its good for your soul."
  },
  {
    "objectID": "posts/2018-03-24-ridge-to-reef-management/index.html",
    "href": "posts/2018-03-24-ridge-to-reef-management/index.html",
    "title": "Identifying conservation priorities for ridge to reef management in tropical catchments",
    "section": "",
    "text": "The nascent field of Ridge to Reef management aims to help plan for the impacts of land-use change and pollutant runoff on coastal marine ecosystems. One way this planning can be done is to identify priorities for forest protection on land that help avoid pollutant run-off to ocean ecosystems.\nOur Science for Nature and People Partnership working group on Ridge to Reef fisheries sought to progress the scientific field and provide practical guidance for Ridge to Reef planning in Fiji, Papua New Guinea and the Solomon Islands.\nThe working group was a partnership between academic scientists, The Nature Conservancy and the Wildlife Conservation Society.\nTo begin we identified several major challenges for the science of ridge to reef management. Let’s call these challenges the observation challenge, the tracing challenge, and the communication challenge."
  },
  {
    "objectID": "posts/2018-03-24-ridge-to-reef-management/index.html#the-observation-challenge",
    "href": "posts/2018-03-24-ridge-to-reef-management/index.html#the-observation-challenge",
    "title": "Identifying conservation priorities for ridge to reef management in tropical catchments",
    "section": "The Observation Challenge",
    "text": "The Observation Challenge\nThe first challenge was accessing sufficient observations of run-off and coastal ecosystems so that we would be able to understand where the runoff is coming from, where it goes in the ocean and what impacts it has to ecosystems and fisheries. Obtaining suffcient data is challenge in tropical catchments in particular, because they are typically poorly studied.\nWe had to overcome this challenge in all the areas that we worked in.\nOne way that we did so was to make the most of freely available global datasets, such as those measured by satellites. We also tried to make the most of data that were already available in our study regions.\nFor instance, in Fiji we used satellite images of forest cover and water clarity to make predictions about where sediment runoff from deforestation is coming from and where in the ocean it is going.\nIn Papua New Guinea we used globally available maps of soil cover and maps of elevation to build predictive models of nutrient and sediment run-off.\nLead author Viv Tulloch, could then predict how much nutrients and sediment would may reach coastal ecosystems under different scenarios for the conversion of primary forest to oil palm plantations. We where then able to recommend modified sustainability guidelines for oil palm plantations, which would minimise their impacts on coastal ecosystems.\nIn Fiji we used extensive surveys of reef fish abundance, that had previously been collected by the Wildlife Conservation Society, and related them back to water clarity. The differences in reef fish abundance from low to high water clarity gave us an idea for how sedimentation may affect reef fish abundance.\nImportantly, large predators may be some of the most affected by poor water clarity, and these species are also locally important for fisheries."
  },
  {
    "objectID": "posts/2018-03-24-ridge-to-reef-management/index.html#the-tracing-challenge",
    "href": "posts/2018-03-24-ridge-to-reef-management/index.html#the-tracing-challenge",
    "title": "Identifying conservation priorities for ridge to reef management in tropical catchments",
    "section": "The Tracing Challenge",
    "text": "The Tracing Challenge\nA major challenge for Ridge to Reef management is that we often to don’t know where the pollutants come from or if there are multiple sources, which ones are the worest offenders. This is a crucial step if we want to then identify where forest protections should be placed.\nIn Fiji we faced this challenge because there are so many rivers that empty near to coral reefs. So it is not immediately clear which catchments were the cause of poor water clarity.\nWe developed a new type of modelling technique that enabled us to make educated guesses about which Rivers where likely having the greatest contribution to poor water clarity on coral reefs. This meant we could then trace pollution back to those catchments that were the worst offenders and identify places where protection a forest might be a priority for conserving coral coral reefs and their fisheries.\nImportantly this modelling technique worked with freely available satellite data, so it can be straightforward to apply in data poor regions.\nFor our work in the Kia region of the Solomon Islands with The Nature Conservancy we could not get accurate measurements of water clarity with satellites, because the islands were two topographic complex and the water too shallow.\nHowever, the Nature Conservancy had extensive surveys of lagoonal coral reefs in the region of Kia.\nIn Kia logging operators create ‘log ponds’. Log ponds are spots on the coast where they bulldoze mangroves so that they can drag logs off the land onto transport ships.\nLog ponds are major sources of sediment to lagoonal coral reefs.\nThere was concern that the sediment was smothering branching corals that are important habitat for the bumphead parrotfish, itself an important fishery species. Bumphead parrotfish are also listed by the International Union for the Conservation of Nature as a threatened species.\nWe wanted to estimate the area of the reef habitats that were affected by the log ponds.\nTo do so we used the reef surveys, which covered many different types of corals and also other types of benthos, that included branching corals, brain corals that are very stressed tolerant, algae, silt and sand.\nWe analysed how the community changed from high coverage of branching corals in clear waters far from log ponds, to higher coverage of stress tolerant corals, silt and sand in murky waters near to log ponds. We could then use the coral reef benthos types as indicators for how far the impact of sedimentation reached from log ponds.\nUsing these predictions we can then predict how much of all the coral reefs in the region have been affected by logging.\nThis was useful because it told us about 60% of the region had already been affected by logging. This includes the loss of branching corals that are important habitat for bumphead parrotfish\nFurthermore we could evaluate what area of lagoonal coral reefs had been protected by recent actions to stop further illegal logging in the Kia region.\nSo so the data on coral reefs helped us to resolve the tracing challenge of where pollutant impacts are sourced from. This work could be used to inform on priorities for where future should not be located."
  },
  {
    "objectID": "posts/2018-03-24-ridge-to-reef-management/index.html#the-communication-challenge",
    "href": "posts/2018-03-24-ridge-to-reef-management/index.html#the-communication-challenge",
    "title": "Identifying conservation priorities for ridge to reef management in tropical catchments",
    "section": "The Communication Challenge",
    "text": "The Communication Challenge\nThe final major challenge we identified that ridge to reef managers face is communicating the complex chain of processes that links land-use change to it’s impacts on coastal marine ecosystems.\nWe worked very closely with our partners in the Wildlife Conservation Society and The Nature Conservancy to develop models and communication products that would be most useful for the stakeholders in the regions where we were working.\nThis meant that we were well prepared when towards the end of the project when we presented management advice back to local stakeholders.\nIn fact Amelia Wenger (from UQ) and I attended a local workshop with The Wildlife Conservation Society in Fiji help them design a ridge to reef plan for the region of Bua.\nA key aspect of our communication success at the meeting was that were able to translate how models into maps that the local stakeholders could understand and relate to.\nWe were also careful to present scientific data in terms of what local stakeholders were most interested in.\nFor instance, when presenting our work on the effects of sedimentation on fish we focused on groups of fish that are important food species like large predators."
  },
  {
    "objectID": "posts/2018-03-24-ridge-to-reef-management/index.html#future-challenges",
    "href": "posts/2018-03-24-ridge-to-reef-management/index.html#future-challenges",
    "title": "Identifying conservation priorities for ridge to reef management in tropical catchments",
    "section": "Future challenges",
    "text": "Future challenges\nSo our working group on Ridge to Reef management identified three major challenge and found solutions to those challenges to help advance management of tropical catchments.\nThere are however some important challenges for the future.\nFirst is that we did not link impacts on reefs to their effects on impacts on the value cultural services or value provided by fisheries.\nDoing so we’ll be important because changes in fish abundance do not always translate into impacts on fisheries. The response of fishers themselves to changes in abundance can mitigate or amplify the impacts of pollution on fisheries.\nAnother important challenge is improving the accessibility of models and the data that are required to build those models. This will mean they can be easily applied data poor tropical catchments by small research teams who are often working under tight stable time and budget constraints.\nAnother team has been developing global models of sediment runoff, which may help fill this knowledge gap and make reliable models more accessible.\nAs for our working group it is finished now, but we are still in the process of publishing the last few papers. Look out for our next one in a few months that looks at the range of models available for informing ridge to reef management or get in touch if you would like to read a pre-print."
  },
  {
    "objectID": "posts/2019-02-23-new-r-course-posted-online/index.html",
    "href": "posts/2019-02-23-new-r-course-posted-online/index.html",
    "title": "R course in data wrangling and spatial analysis notes are online",
    "section": "",
    "text": "I have posted the notes to our new R course in data wrangling and spatial analysis notes on this webpage.\nThe course is aimed at biological scientists who have some basic knowledge of R, but want to learn to use it for a broader variety of tasks. It is aimed to show how you can use R as a complete workflow from data cleaning to analysis, graphics and mapping.\nWe walk through the data wrangling, graphics, analysis and mapping for a case-study analysis of a (real) plankton data-set.\n\n\n\nOne of the maps we’ll make\n\n\nIn terms of packages, we cover data-wrangling with dplyr and a little bit of tidyr. Graphics with ggplot2. Then analyses with GLMs and GAMs (mgcv package). Mapping and GIS with raster, ggplot2 and sf. We also cover how you can do spatial analyses with GAMs.\nThe R course was written for a 1-day workshop at University of Queensland with my colleagues, Ant Richardson, Dave Schoeman and Bill Venables (author of the MASS package among other R fundamentals).\nI would guestimate that if you do this course on your own you will need a day or a bit more. It took us a whole day in class, but we skipped a lot of the bonus material.\nI hope you find the course helpful and a little bit fun."
  },
  {
    "objectID": "posts/2017-02-01-thinking-about-coherent-stories/index.html",
    "href": "posts/2017-02-01-thinking-about-coherent-stories/index.html",
    "title": "Good research papers build coherent stories, but what is a coherent story",
    "section": "",
    "text": "Good storytelling is essential to write a successful research paper. And I don’t mean making things up. Science stories need to rely on data. But we need to write up the data as a story so people get it.\nKey to good writing is building a coherent story.\nIt is important to emphasis that your research paper doesn’t have to be comprehensive - you can’t address every caveat or cite every relevant piece of work - but it must be coherent.\nBut what is a ‘coherent’ story?\nOne conceptual model (I love models!) for thinking about what a coherent story is to think about a web of cause and effect.\nYour paper should build a web, or in the simplest case a chain, of cause-effect actions. If everything in your paper is related by cause and effect, then it will be coherent.\nA model of a coherent research story as a cause-effect web.\nThere are a few more rules to consider, but it will be easiest if we bring up an example.\nConsider Dale Bryan-Brown’s recent review of studies of population connectivity in marine ecosystems.\nHe opened the study with an observation that there are many studies of population connectivity in marine systems. That is the effect. Then he moves on to explain they have never been synthesised - a new ‘cause’. He then builds an argument around why a synthesis is needed to fill this knowledge gap (an effect again). And so the paper goes.\nNow, we want to make sure we don’t have any extraneous causes or effects or cause-effect pairs floating around in the text. Such loose cause-effect pairs will contribute to building an incoherent story.\nAn example of this would be a recent paper we published about a new statistical method, which we applied to study the effects of pollution on coral reefs. In the Discussion we bring up important caveats to the new method and discuss ways future studies could improve on what we did.\nWe don’t spend time discussing uncertainty around the physiology for how pollution kills corals, because the main ‘story’ is about the application of the statistical method. Discussion of physiology would be extraneous and therefore incoherent.\nThe Discussion and Conclusion of the paper should tie up most of the cause-effect pairs, ending on effects (i.e. the observation and implications of your study). However, you may want to specify some future research directions too. These are ‘causes’ that you want future studies to go out and answer.\nSo I hope you find this conceptual model useful. Let me know what you think on Twitter."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html",
    "title": "Setting up VScode for R and Cline",
    "section": "",
    "text": "Cline lets you create and run entire workflows via using prompts to a large language model. Its not widely used in the R community yet, but I expect it will be soon. You can create your entire R project, interpret the results and write a draft of your findings without writing any R code.\nCline is not available (as of writing) in RStudio. So you need to use a different IDE (Integrated Development Environment) to run your R code. Here I’ll explain how to set-up VSCode (a popular IDE) so you can use Cline."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#software-requirements",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#software-requirements",
    "title": "Setting up VScode for R and Cline",
    "section": "Software requirements",
    "text": "Software requirements\nTo set up VScode for R and Cline, you’ll need:\n\nR programming language\nVScode text editor\nR extension for VScode\nCline AI assistant extension for VScode\n\nNote that if you computer is controlled centrally by an IT department, you may need to request admin access to install software, or email IT and ask for them to come and help you."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-r",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-r",
    "title": "Setting up VScode for R and Cline",
    "section": "Install R",
    "text": "Install R\n\nGo to the official R project website: https://www.r-project.org/\nClick the “download R” link in the Getting Started section\nChoose a CRAN mirror close to your location\nDownload the appropriate R installer for your operating system\nRun the installer and follow the prompts to complete installation"
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#r-packages",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#r-packages",
    "title": "Setting up VScode for R and Cline",
    "section": "R packages",
    "text": "R packages\n\nOpen R or RStudio\nInstall language server install.packages(\"languageserver\")\nInstall httpgd install.packages(\"httpgd\") (this helps improve plots in VScode)"
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-vscode",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-vscode",
    "title": "Setting up VScode for R and Cline",
    "section": "Install VScode",
    "text": "Install VScode\n\nGo to the official VScode website: https://code.visualstudio.com/\nClick the big blue “Download” button\nDownload the appropriate VScode installer for your operating system\nRun the installer and follow the prompts\nLaunch VScode once installation is complete"
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-r-extension",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-r-extension",
    "title": "Setting up VScode for R and Cline",
    "section": "Install R extension",
    "text": "Install R extension\n\nOpen VScode\nOpen the Extensions view in VScode (click the boxes on left hand side)\nSearch for “R” in the extensions marketplace\nSelect the “R” extension published by REditorSupport\nClick the “Install” button\nRestart VScode after installation if prompted\n\nMore info on vscode and R here"
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#connect-r-and-vscode",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#connect-r-and-vscode",
    "title": "Setting up VScode for R and Cline",
    "section": "Connect R and VScode",
    "text": "Connect R and VScode\n\nOpen a new terminal in VScode (Terminal &gt; New Terminal)\nCheck that R is installed by running: R --version\nType R to open the R console in the terminal\nNow open any R script in VS code (File &gt; Open)\nRun some R code to check that VS code can connect to R in the terminal. Use the shortcut Ctrl+Enter/Cmd+Enter or press the play button in the top right of the script editor.\n\nIf R is not found then open extensions (left hand side, boxes icon), filter by ‘enabled’ then click the R extension. Now click the cog icon in the R extension and select ‘settings’ from the dropdown. Search for ‘rpath’. Check that it has the correct path to R on your computer. You can find the path by opening a terminal and typing which R (on mac) or in a windows terminal where R.\nWhile you have the extension settings open search for ‘httgp’ and make sure Plot: Use Httpgd is enabled."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-cline",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#install-cline",
    "title": "Setting up VScode for R and Cline",
    "section": "Install Cline",
    "text": "Install Cline\n\nOpen the Extensions view in VScode (Ctrl+Shift+X)\nSearch for “cline” in the extensions marketplace\nSelect the “Cline” extension\nClick the “Install” button\nThe Cline icon should appear in the VScode sidebar"
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#set-up-cline-in-vscode",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#set-up-cline-in-vscode",
    "title": "Setting up VScode for R and Cline",
    "section": "Set up Cline in VScode",
    "text": "Set up Cline in VScode\n\nClick on the Cline robot icon on the left hand side\nClick the cog (if the settings don’t open automatically)\nSelect your API provider and cut and paste the API key into the box (see next step if you don’t have one)."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#get-an-api-key",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#get-an-api-key",
    "title": "Setting up VScode for R and Cline",
    "section": "Get an API key",
    "text": "Get an API key\nNow you need to choose your large language model provider. I’m currently using Anthropic, which has a diversity of models for generating text, code and reading images. Do some web searching to find out the latest info on providers and models.\nYou choose depends on what you want to do and your budget. Some providers offer a free tier. You’ll need to web search for the latest info on this.\nYou can get the links to any supported provider from the Cline extension settings, just select the provider and then a link will appear to sign-up for that provider.\nOnce you’ve chosen a provider, create an account and follow their instructions for creating an API key. You will probably also need to buy some credit to use the model.\nNote that if you already have a subscription account, e.g. for chatGPT, you will get need an API key to use Cline. This might be in a different section of your account settings. If you are using a subscription service you are probably paying monthly to use the provider’s web app. We need the API key to be able to use the model directly via code."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#youre-all-set",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#youre-all-set",
    "title": "Setting up VScode for R and Cline",
    "section": "You’re all set",
    "text": "You’re all set\nNow you’re set and ready to start using Cline to code and write text."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#issues-and-tips",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#issues-and-tips",
    "title": "Setting up VScode for R and Cline",
    "section": "Issues and tips",
    "text": "Issues and tips\nThis is just a list of issues I’ve had and how I’ve solved them.\nPlotting If your R plots look weird (like tiny font), make sure httpgp is enabled. Go back to steps above and see how to do that.\nViewing data There are various extensions for viewing csv and excel files. It is worth looking into these so that when you do View(dat) in R you get a nice table. Some also allow editing.\nGetting help to install software My computer is somewhat locked down by IT, so getting this set-up was a bit fiddly and required a few requests to IT to install software.\nR markdown There are options in the R extension settings for how to knit markdown. You may need to configure these if you want to knit markdown docs from VScode. If you are having trouble knitting markdown it may mean that the path to pandoc is not set correctly. There is some helpful instructions here\nR terminal crashes If I run too much R code at once (like selecting a big block then running) the terminal tends to crash. Initially I see a little highlighted box saying ‘PTY HOST’. Then I need to close all the terminals (with the bin icon) and start again. Try radian if this is a problem. You can also code run line-by-line or source whole scripts from the terminal (which works fine). I tried debugging this by increasing the buffer but to on avail.\nShortcut keys (on osx) cmd-/ to comment uncomment lines. cmd-shift-p to open the command palette, cmd-b to open the file explorer, cmd-enter to run lines or selection of R code, cmd-shift-c to open terminal in new window, cntrl-shift-` to open a new terminal in vs code."
  },
  {
    "objectID": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#installing-radian-optional",
    "href": "posts/2025-02-07-setting-up-vscode-r-cline/index.html#installing-radian-optional",
    "title": "Setting up VScode for R and Cline",
    "section": "Installing radian (optional)",
    "text": "Installing radian (optional)\nRadian is a terminal editor that is a bit nicer than the base R one. It does autocomplete in the terminal (like Rstudio does in the console), colours code/brackets etc… and allows multi-line editing in the terminal.\nTo set this up, install radian (you need python to do this). More instructions here.\nThen go to the terminal and find the path where radian is installed (e.g. which radian on mac or where radian on windows).\nNow open your settings in VScode (cmd-,) and search for ‘rterm’ (stands for ‘R Terminal’, don’t change the rpath which we set just before). Add the path to radian to the rterm setting. Also search for the setting ‘R: Bracketed Paste’ and make sure it is enabled."
  },
  {
    "objectID": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html",
    "href": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html",
    "title": "What’s the deal with global biodiversity maps",
    "section": "",
    "text": "A recent article “Conservation needs to break free from global priority mapping” argues we have too many global conservation priority maps. This has stirred up controversy about the usefulness of those maps and whether scientists are wasting there time, and even insulting people, by making them.\nHere are some of my thoughts.\nTo my mind, there are three issues at play in this debate:"
  },
  {
    "objectID": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html#first-whats-a-conservation-priority",
    "href": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html#first-whats-a-conservation-priority",
    "title": "What’s the deal with global biodiversity maps",
    "section": "First, what’s a ‘conservation priority?’",
    "text": "First, what’s a ‘conservation priority?’\nIn a response letter we argued that effective conservation requires clear objectives and prioritizing actions, not places or species. Lots of studies have used the term ‘priority’ loosely, e.g. you see studies talking about priority areas, species or threats. But the problem with this language is it doesn’t say what we need to do. Priorities should be about actions, then they are clear. Many have previously showed that if you follow actions guided by ‘priority threats’ or ‘priority areas’ you can end up with costly and ineffective conservation outcomes.\nFor example, consider a case where the priority species is one doomed for extinction, or an enigmatic one that isn’t that threatened, we’d be wasting money trying to conserve it. Same goes, if we prioritize ‘threat hotspots’ then we may spend big on local conservation when unmanageable threats, like climate change are a big issue.\nPriorities should consider what the objectives are, what the actions are, and constraints on those actions. This is important to find those places where conservation can be most effective.\nI’d define effective as outcomes that the best you can get relative to what would have happened if you didn’t do that action.\nLots of global priority maps are actually maps of hotspots of species richness etc… Nothing wrong with that, we just shouldn’t claim they are conservation priorities."
  },
  {
    "objectID": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html#second-how-useful-are-global-biodiversity-maps",
    "href": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html#second-how-useful-are-global-biodiversity-maps",
    "title": "What’s the deal with global biodiversity maps",
    "section": "Second, how useful are global biodiversity maps?",
    "text": "Second, how useful are global biodiversity maps?\n\nUnclear theory of change\nClimate maps so useful\nIssues with scale/resolution\nrepeating patterns we already know?\nWyborn and Evans unfortunately shortened their focus to ‘global maps’ which I think led to widespread misinterpretation of the paper as a criticism of global maps in general. Also their review includes many global maps that aren’t prioritisations.\n\nSo I think they are fine, but important to have a theory of change But should onus for scientists be on ToC?\nCareer progression"
  },
  {
    "objectID": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html#finally-global-maps-touch-on-the-issue-of-neo-colonial-science",
    "href": "posts/2021-09-17-whats-the-deal-with-global-maps/index.html#finally-global-maps-touch-on-the-issue-of-neo-colonial-science",
    "title": "What’s the deal with global biodiversity maps",
    "section": "Finally, global maps touch on the issue of ‘Neo-colonial’ science",
    "text": "Finally, global maps touch on the issue of ‘Neo-colonial’ science\nAlso related helicopter science Engagement important. Duan’s talk Fern’s article"
  },
  {
    "objectID": "posts/2015-07-25-the_future_programming/2015-07-25-the_future_programming.html",
    "href": "posts/2015-07-25-the_future_programming/2015-07-25-the_future_programming.html",
    "title": "The next advance in spatial R",
    "section": "",
    "text": "What will be the next big advance in spatial analysis using R?\n\n\nThe programming tools available for scientists to use for data analysis are becoming increasingly complex and sophisticated. This trend will continue, however I believe the most significant new advances will come from the development open, light and accessible packages. By ‘light’ I mean packages that are both easy to learn, straightforward to work with, and can run on standard desktops and laptops.\n\n\nThe capacity for languages like R to produce incredibly sophisticated analyses outstrips most people’s ability to use those tools. A few people are pushing the boundaries (e.g. see Sean Anderson and colleague’s recent global extinction risk analysis), but most scientists who use will continue to use only those tools they are most familiar with (but are pushing the boundaries of human knowledge in other ways of course).\n\n\nCase in point: the success of Wickham’s dplyr package for data wrangling in R. dplyr doesn’t really do anything you can’t do with R’s base package, but it does make data wrangling easier, more intuitive and faster. dplyr is brilliant not only because it is accessible for relative begginners, but also because it saves coding time for advanced users too.\n\n\nMore generally, other areas of science can benefit from accessible and light packages too. An example, from coral reef science. For many decades, reef scientists have been interested in how the complexity of reefs varies across environmental gradients. One way to measure complexity it to take a lead rope of say, 3 metres length, and lay it out over the contours of the reef. Then measure the rope’s resulting length when viewed from above. The ratio of lengths is then a measure of the reef’s complexity. More detailed measurements of reef complexity can be made by taking video footage and post-processing it to make 3D models of the reef. In fact, you can make precise measurements of reefs  using this method. But the fact is, people keep going back to the old lead rope method.\n\n\nThe reason people continue to use the lead rope method is that ropes are reliable and calcuting complexity from your field data is easy. Compare that with the hassles of taking a 3D camera underwater, generating large amounts of video data and then having to use complex post-processing programs to get your results. The rise of cheap high-def cameras, like go-pros has solved the first issue, but the proccessing step still remains a major challenge. Several groups have developed programs for processing 3D video over the years, but they typically required computer scientists to implement these programs. The method won’t be broadly used until someone develops a program that is accessible to reef scientists, light and cheap.\n\n\nThe uptake of R’s sophisticated spatial tools also faces the same challenge: many of them are accessible only to experienced programmers. This is changing of course, for instance the raster package comes with very accessible help files. The developers are even expanding it to include some processing options for shape files. However, the documentation for the main package for processing shapes, rgeos, is quite technical. Further, rgeos operations often through up incomprehensible errors when you perform common tasks like polygon intersects.\n\n\nThe potential to use R as a fully functioning GIS is only just beginning to be realised. So watch this space."
  },
  {
    "objectID": "posts/2017-06-25-what-is-underdispersed/index.html",
    "href": "posts/2017-06-25-what-is-underdispersed/index.html",
    "title": "What does under or over-dispersion look like?",
    "section": "",
    "text": "One issue that often comes up in analysis with linear models is under or over dispersion.\nFor instance, if you are fitting a linear regression model, you are assuming the residuals (difference between the line of best fit and the data-points) are normally distributed. Residuals are said to be overdispersed if they are ‘fatter’ in the tails than a normal bell curve. Whereas, if the residuals are too peaked in the middle, they are said to be under-dispersed.\nUnder or over dispersion is an issue because it can bias the calculation of p-values. Over-dispersion is often of particular concern because it may cause p-values that are biased too low. If you are taking 0.05 as significant, over-dispersion will mean you would reject the null hypothesis more than 5% of the time. Thus, you may be more likely to falsely reject the null hypothesis.\nIf there is under-dispersion the opposite is true, your p-values may be too high. P-values that are biased high can be a problem, because they will give you low power to detect real effects.\nSo, let’s take a look at under and overdispersion. First, up here is a normal bell curve:\nx &lt;- seq(-4, 4, length.out = 100)\np &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, p, type = 'l', lwd = 2, col = \"red\",\n    xlab = \"Residual\", ylab = \"Density\")\n\nNow, we can add an over-dispersed curve to that. Here is one, calculated using the Student t distribution:\np_t &lt;- dt(x, df = 1.4, ncp = 0)\n\nplot(x, p, type = 'l', lwd = 2, col = \"red\",\n    xlab = \"Residual\", ylab = \"Density\")\nlines(x, p_t, col = \"darkblue\", lwd = 2)\n\nIt wouldn’t matter how large we made the standard devation of the normal curve, we would never get it to match the Student-t.\nFinally, let’s draw an under-dispersed distribution, using the Laplace distribution\nlibrary(rmutil)\np_l &lt;- dlaplace(x, m = 0, s = 0.4)\nNow plot them all together:\nplot(x, p_l, xlim = c(-4, 4), lwd = 2, xlab = \"Residuals\", main = \"\", type = \"l\")\nlines(x, p, col = \"red\", lwd = 2, lty = 2)\nlines(x, p_t, col = \"darkblue\", lwd = 2)\nlegend(\"topright\", legend = c(\"normal\", \"under-dispersed\", \"over-dispersed\"),\n      lty = c(2,1,1), col = c(\"red\", \"black\", \"darkblue\"), lwd = 2)\n\nHere’s what they look like if we sample measures from each distribution and plot them on a normal QQ plot, which you may be familiar with. It is one of the standard checks of model residuals:\nset.seed(1997)\npar(mfrow = c(1,3))\nqqnorm(rnorm(1000, mean = 0, sd = 1), main = \"normal\")\nabline(0,1)\nqqnorm(rt(1000, df = 1.4, ncp = 0), main = \"over-dispersed\")\nabline(0,1)\nqqnorm(rlaplace(1000, m = 0, s = 0.4), main = \"under-dispersed\")\nabline(0,1)\n\nPlaying around with the parameters for each distribution should help your understanding. For instnace, if you set the scale for the Laplace distribution to a larger number it will become over-dispersed, because it gets fatter tails than the normal (despite being more peaked at its mode).\nUnder-dispersion is more common than you might think. It can occur when you have a censoring process. For instance, perhaps you machine can only measure length’s to a certain precision and any distance that is to small gets rounded down to zero."
  },
  {
    "objectID": "posts/2017-06-25-what-is-underdispersed/index.html#what-does-under-or-over-dispersion-look-like",
    "href": "posts/2017-06-25-what-is-underdispersed/index.html#what-does-under-or-over-dispersion-look-like",
    "title": "What does under or over-dispersion look like?",
    "section": "",
    "text": "One issue that often comes up in analysis with linear models is under or over dispersion.\nFor instance, if you are fitting a linear regression model, you are assuming the residuals (difference between the line of best fit and the data-points) are normally distributed. Residuals are said to be overdispersed if they are ‘fatter’ in the tails than a normal bell curve. Whereas, if the residuals are too peaked in the middle, they are said to be under-dispersed.\nUnder or over dispersion is an issue because it can bias the calculation of p-values. Over-dispersion is often of particular concern because it may cause p-values that are biased too low. If you are taking 0.05 as significant, over-dispersion will mean you would reject the null hypothesis more than 5% of the time. Thus, you may be more likely to falsely reject the null hypothesis.\nIf there is under-dispersion the opposite is true, your p-values may be too high. P-values that are biased high can be a problem, because they will give you low power to detect real effects.\nSo, let’s take a look at under and overdispersion. First, up here is a normal bell curve:\nx &lt;- seq(-4, 4, length.out = 100)\np &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, p, type = 'l', lwd = 2, col = \"red\",\n    xlab = \"Residual\", ylab = \"Density\")\n\nNow, we can add an over-dispersed curve to that. Here is one, calculated using the Student t distribution:\np_t &lt;- dt(x, df = 1.4, ncp = 0)\n\nplot(x, p, type = 'l', lwd = 2, col = \"red\",\n    xlab = \"Residual\", ylab = \"Density\")\nlines(x, p_t, col = \"darkblue\", lwd = 2)\n\nIt wouldn’t matter how large we made the standard devation of the normal curve, we would never get it to match the Student-t.\nFinally, let’s draw an under-dispersed distribution, using the Laplace distribution\nlibrary(rmutil)\np_l &lt;- dlaplace(x, m = 0, s = 0.4)\nNow plot them all together:\nplot(x, p_l, xlim = c(-4, 4), lwd = 2, xlab = \"Residuals\", main = \"\", type = \"l\")\nlines(x, p, col = \"red\", lwd = 2, lty = 2)\nlines(x, p_t, col = \"darkblue\", lwd = 2)\nlegend(\"topright\", legend = c(\"normal\", \"under-dispersed\", \"over-dispersed\"),\n      lty = c(2,1,1), col = c(\"red\", \"black\", \"darkblue\"), lwd = 2)\n\nHere’s what they look like if we sample measures from each distribution and plot them on a normal QQ plot, which you may be familiar with. It is one of the standard checks of model residuals:\nset.seed(1997)\npar(mfrow = c(1,3))\nqqnorm(rnorm(1000, mean = 0, sd = 1), main = \"normal\")\nabline(0,1)\nqqnorm(rt(1000, df = 1.4, ncp = 0), main = \"over-dispersed\")\nabline(0,1)\nqqnorm(rlaplace(1000, m = 0, s = 0.4), main = \"under-dispersed\")\nabline(0,1)\n\nPlaying around with the parameters for each distribution should help your understanding. For instnace, if you set the scale for the Laplace distribution to a larger number it will become over-dispersed, because it gets fatter tails than the normal (despite being more peaked at its mode).\nUnder-dispersion is more common than you might think. It can occur when you have a censoring process. For instance, perhaps you machine can only measure length’s to a certain precision and any distance that is to small gets rounded down to zero."
  },
  {
    "objectID": "posts/2021-10-02-how-long-to-write-a-grant/index.html",
    "href": "posts/2021-10-02-how-long-to-write-a-grant/index.html",
    "title": "How much time does it take to write a grant?",
    "section": "",
    "text": "Writing grants takes time and effort, but how much time? In this post I’ll show data from my time tracking app that quantifies exactly how much time it took me to write applications to a national grant agency (the Australian Research Council).\nTLDR: I start preparing 4-5 months ahead of the final deadline, spend about 10% of my work hours over that period. But effort varies and peaks at 30% of my monthly work hours 1-2 months before the deadline.\nI hope this post helps other people plan better for grant writing (and here are the strategies I use). I think the significant ongoing time investment also contributes to inequities in scientific careers, I’ll come to that in another post.\n\n\nEstimating time required on grant writing is an important part of career planning. For instance you may need to think about how you’ll fit it around the rest of your life (grant writing in my experience often requires after hours work) or field work.\nOr you may be facing the end of a research contract, and thinking about trying to win the next bucket of funding. So its worth thinking whether the time investment is worth the effort, given the risk of not winning the grant, or whether the time could be better invested in other career opportunities.\nWhen I started grant writing, my best guess for how much time it would take was: “a large proportion of working hours over several months”.\nWell, for for quite a few years years I’ve been logging my time on different tasks. My log now includes writing five applications for funding from the Australian Research Council.\nFor context, ARC grants have an overall success rate of about 20%, and a slightly lower rate for fellowships.\n\n\n\nI estimate that each grant typically requires 4-6 months preparation time at an average of about 10% of total work hours (0.1 FTE) over that period.\nBut that figure belies considerable variation in effort across time\nBelow you’ll see cumulative hours on five grants (one for which the outcome is still pending). Each application was written in a different year, but I’ve plotted them concurrently for comparison. My typical grant writing time of 70 hours counts only my time writing, editing or engaged in meetings about the grant. It doesn’t include tea breaks, reading time or thinking time or other people’s time. So actual time investment would be much greater than 70 hours.\n\nThe most recent grant took significantly less time than the others. This grant was a collaborative effort. It also came easily in that I had material and experience to draw on from previous years.\nYou’ll see I started thinking and working on each grant about 4 months ahead of time.\nThat is in part due to a very good internal preparation process my research institute (Australian Rivers Institute at Griffith University) runs. Over several workshops we pitch our initial ideas and then get directed feedback on drafts of the applications.\nEffort kicks up at about 2.5 months out, when I usually aim to have a first draft written so it can be reviewed by colleagues.\nThen effort peaks just before the lead up to the 1 month mark. Most major grants require internal review by the University’s research office, and the deadline for that is 1 month before the final.\nThen in the final month effort drops off again. Usually by that point I’m just tying up lose ends and making minor corrections from the review feedback.\nI have replotted that data as fraction of my monthly work hours (not accounting for holidays).\n\nYou clearly see the peak between 3 and 2 months out.\nTo sum up, it takes time and planning to write a major grant. In follow up posts I’ll discuss the strategies I use.\nI also want to acknowledge that I have had the privileges and opportunities that help me spend time on grant writing. But not everyone has that opportunity or privilege. In a final post I’ll discuss how complex grant applications can be inequitable."
  },
  {
    "objectID": "posts/2021-10-02-how-long-to-write-a-grant/index.html#why-time-matters",
    "href": "posts/2021-10-02-how-long-to-write-a-grant/index.html#why-time-matters",
    "title": "How much time does it take to write a grant?",
    "section": "",
    "text": "Estimating time required on grant writing is an important part of career planning. For instance you may need to think about how you’ll fit it around the rest of your life (grant writing in my experience often requires after hours work) or field work.\nOr you may be facing the end of a research contract, and thinking about trying to win the next bucket of funding. So its worth thinking whether the time investment is worth the effort, given the risk of not winning the grant, or whether the time could be better invested in other career opportunities.\nWhen I started grant writing, my best guess for how much time it would take was: “a large proportion of working hours over several months”.\nWell, for for quite a few years years I’ve been logging my time on different tasks. My log now includes writing five applications for funding from the Australian Research Council.\nFor context, ARC grants have an overall success rate of about 20%, and a slightly lower rate for fellowships."
  },
  {
    "objectID": "posts/2021-10-02-how-long-to-write-a-grant/index.html#total-time",
    "href": "posts/2021-10-02-how-long-to-write-a-grant/index.html#total-time",
    "title": "How much time does it take to write a grant?",
    "section": "",
    "text": "I estimate that each grant typically requires 4-6 months preparation time at an average of about 10% of total work hours (0.1 FTE) over that period.\nBut that figure belies considerable variation in effort across time\nBelow you’ll see cumulative hours on five grants (one for which the outcome is still pending). Each application was written in a different year, but I’ve plotted them concurrently for comparison. My typical grant writing time of 70 hours counts only my time writing, editing or engaged in meetings about the grant. It doesn’t include tea breaks, reading time or thinking time or other people’s time. So actual time investment would be much greater than 70 hours.\n\nThe most recent grant took significantly less time than the others. This grant was a collaborative effort. It also came easily in that I had material and experience to draw on from previous years.\nYou’ll see I started thinking and working on each grant about 4 months ahead of time.\nThat is in part due to a very good internal preparation process my research institute (Australian Rivers Institute at Griffith University) runs. Over several workshops we pitch our initial ideas and then get directed feedback on drafts of the applications.\nEffort kicks up at about 2.5 months out, when I usually aim to have a first draft written so it can be reviewed by colleagues.\nThen effort peaks just before the lead up to the 1 month mark. Most major grants require internal review by the University’s research office, and the deadline for that is 1 month before the final.\nThen in the final month effort drops off again. Usually by that point I’m just tying up lose ends and making minor corrections from the review feedback.\nI have replotted that data as fraction of my monthly work hours (not accounting for holidays).\n\nYou clearly see the peak between 3 and 2 months out.\nTo sum up, it takes time and planning to write a major grant. In follow up posts I’ll discuss the strategies I use.\nI also want to acknowledge that I have had the privileges and opportunities that help me spend time on grant writing. But not everyone has that opportunity or privilege. In a final post I’ll discuss how complex grant applications can be inequitable."
  },
  {
    "objectID": "posts/2016-11-14-extract-exif/index.html",
    "href": "posts/2016-11-14-extract-exif/index.html",
    "title": "Extracting exif data from photos using R",
    "section": "",
    "text": "See the other parts in this series of blog posts.\nMy camera comes with an inbuilt GPS allowing you to geo-reference photos you take. Here is a simple example for how we can use this feature to create an interactive web map in R.\nThe photos come from a survey I did of oysters on a rocky shoreline. I took photos of each quadrat enabling me to geolocate the quadrats as well as record what occurred within them.\nFirst get your hands on a few packages, exif for extracting exif info in R, dplyr for data management and leaflet for making maps:\nlibrary(exifr)\nlibrary(dplyr)\nlibrary(leaflet)\nNow set your working director the a folder that holds the photos in questions. We can then get the names of all the photos straight into R’s memory like this:\nfiles &lt;- list.files(pattern = \"*.JPG\")\ndat &lt;- exifr(files)\nThe pattern argument ensures we just grab the jpegs from the folder and nothing else. Neat, we have our exif info as a dataframe. Now let’s select just the useful columns:\ndat2 &lt;- select(dat,\n    SourceFile, DateTimeOriginal,\n    GPSLongitude, GPSLatitude,\n    GPSTimeStamp)\n\n    write.csv(dat2, 'Exifdata.csv',\n    row.names = F)\nNB the select function comes from the dplyr package. You can do this with base R too, but I prefer dplyr. (You can get my dataframe here)\nYou can make a quick map of locations like this:\nplot(dat$GPSLongitude, dat$GPSLatitude)\n\n\nInteractive web maps are easy with the leaflet package. We can plot the same points over and ESRI provided satellite image like this:\nleaflet(dat2) %&gt;%\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\naddMarkers(~ GPSLongitude, ~ GPSLatitude)\nAnd here’s what it should look like:\n\nIt no map appears here your browser doesn’t support iframes. Try &lt;a href = \"/data/hornby_pic_loc.html\"&gt;this link instead&lt;/a&gt;\n\nNext up we will look at how to match these locations to the quadrat data I collected. I will also show you how to add photos to the pop-ups at the site locations."
  },
  {
    "objectID": "posts/2016-11-14-extract-exif/index.html#part-1-extracting-exif-data-from-photos-using-r",
    "href": "posts/2016-11-14-extract-exif/index.html#part-1-extracting-exif-data-from-photos-using-r",
    "title": "Extracting exif data from photos using R",
    "section": "",
    "text": "See the other parts in this series of blog posts.\nMy camera comes with an inbuilt GPS allowing you to geo-reference photos you take. Here is a simple example for how we can use this feature to create an interactive web map in R.\nThe photos come from a survey I did of oysters on a rocky shoreline. I took photos of each quadrat enabling me to geolocate the quadrats as well as record what occurred within them.\nFirst get your hands on a few packages, exif for extracting exif info in R, dplyr for data management and leaflet for making maps:\nlibrary(exifr)\nlibrary(dplyr)\nlibrary(leaflet)\nNow set your working director the a folder that holds the photos in questions. We can then get the names of all the photos straight into R’s memory like this:\nfiles &lt;- list.files(pattern = \"*.JPG\")\ndat &lt;- exifr(files)\nThe pattern argument ensures we just grab the jpegs from the folder and nothing else. Neat, we have our exif info as a dataframe. Now let’s select just the useful columns:\ndat2 &lt;- select(dat,\n    SourceFile, DateTimeOriginal,\n    GPSLongitude, GPSLatitude,\n    GPSTimeStamp)\n\n    write.csv(dat2, 'Exifdata.csv',\n    row.names = F)\nNB the select function comes from the dplyr package. You can do this with base R too, but I prefer dplyr. (You can get my dataframe here)\nYou can make a quick map of locations like this:\nplot(dat$GPSLongitude, dat$GPSLatitude)\n\n\nInteractive web maps are easy with the leaflet package. We can plot the same points over and ESRI provided satellite image like this:\nleaflet(dat2) %&gt;%\naddProviderTiles(\"Esri.WorldImagery\") %&gt;%\naddMarkers(~ GPSLongitude, ~ GPSLatitude)\nAnd here’s what it should look like:\n\nIt no map appears here your browser doesn’t support iframes. Try &lt;a href = \"/data/hornby_pic_loc.html\"&gt;this link instead&lt;/a&gt;\n\nNext up we will look at how to match these locations to the quadrat data I collected. I will also show you how to add photos to the pop-ups at the site locations."
  },
  {
    "objectID": "posts/2015-11-13-data-commandments/index.html",
    "href": "posts/2015-11-13-data-commandments/index.html",
    "title": "Commandments for entering data to use in R",
    "section": "",
    "text": "Meta-data should include location and time information, the short-hand names of the variables matched to a description that includes their units, and a description of the levels of variables (if they are factors).\n\n\n\n‘Tidy’ data begins with a single row of variable names and follows with one row for each observation. For instance, if you measured multiple individuals across different times, then each row will be a measurement from one individual at one time. You will also have time and ID variables to indicate the time and individual.\nSee the dplyr and tidyr packages if you need to tidy data. There is one exception to this commandment: extremely large data-sets (e.g. multiple giga-bytes) will use less memory if stored in table or wide format.\n\n\n\nYou can use letters, numbers _ and . in variable names. Avoid any other symbols. For instance, instead of calling a species Plectropomus leopardus, name it Plectropomus_leopardus.\nAlso don’t start variable names with numbers.\n\n\n\nFor instance if using CamelCase, stick to it. Or instead separate words with _. Be consistent with capitalisation, for instance, R will see Plectropomus as different to plectropomus.\nA further tip is to avoid using multiple names for the same identity, like plectropmus sp. and plectropmus.\n\n\n\nR will automatically detect the type of data. If all levels of a variable are numbers, it will treat them as continuous (decimal or integer) numbers. If you mix numbers and words, R will treat all as different levels of a factor.\n\n\n\nSomeday you may want to join data together, for instance, data collected at the same sites across different studies. Keep the site names consistent.\n\n\n\nThere is a big difference between data you didn’t collect and data you tried to collect but went missing. So be explicit about missing data by entering the missing rows where you tried to collect the data and using NA in the missing cells. It is better to use NA than just leave the cell blank, because R interprets NA as missing data.\n\n\n\nData-wrangling is the process of checking data for errors and formatting data so it can be used in analysis. If you do all the wrangling in R and keep the scripts, the process is totally repeatable. Then if someone wants to see how you changed the original data to check errors for instance, all the changes are recorded in a script.\nIf you want to know more about data wrangling in R, check out my latest courses.\nAlso, avoid .xls and .xlsx formats for saving data. Instead save your data as .csv (comma separated values). It is inconvenient to read excel formats into R. Excel can also change the ‘type’ of numbers in strange ways that alter your data. For instance, data that looks like dates (10/2) may get transformed into a different date format (10 - February). .xlsx formats also use more memory than .csv. So it is better to keep your data as .csv and wrangle it in R.\n\n\n\nFor instance, if you have collected data over different dates and want to keep them in separate spreadsheets give them names with the dates recorded in a consistent format: data_2-Oct-2015.csv, data_3-Oct-2015.csv and so on. That way you can rapidly read the files into R and parse the file names into actual dates.\n\n\n\nR is an open-access resource, many people have invested much time in developing. To keep good karma, make your data and code open-access too (once you have written your publication of course and if ethics allow). Much scientific data is collected on public money, so it is quite reasonable that the public can access it!.\nFootnote: I changed point 7 on 18-Mar 2016 to be explicit that you should use NA for missing data. Thanks to Andrew Beckerman for suggesting this. I appreciate feedback on my blogs, so feel free to get in touch if you have comments of your own."
  },
  {
    "objectID": "posts/2015-11-13-data-commandments/index.html#ten-commandments-for-entering-data-to-use-in-r",
    "href": "posts/2015-11-13-data-commandments/index.html#ten-commandments-for-entering-data-to-use-in-r",
    "title": "Commandments for entering data to use in R",
    "section": "",
    "text": "Meta-data should include location and time information, the short-hand names of the variables matched to a description that includes their units, and a description of the levels of variables (if they are factors).\n\n\n\n‘Tidy’ data begins with a single row of variable names and follows with one row for each observation. For instance, if you measured multiple individuals across different times, then each row will be a measurement from one individual at one time. You will also have time and ID variables to indicate the time and individual.\nSee the dplyr and tidyr packages if you need to tidy data. There is one exception to this commandment: extremely large data-sets (e.g. multiple giga-bytes) will use less memory if stored in table or wide format.\n\n\n\nYou can use letters, numbers _ and . in variable names. Avoid any other symbols. For instance, instead of calling a species Plectropomus leopardus, name it Plectropomus_leopardus.\nAlso don’t start variable names with numbers.\n\n\n\nFor instance if using CamelCase, stick to it. Or instead separate words with _. Be consistent with capitalisation, for instance, R will see Plectropomus as different to plectropomus.\nA further tip is to avoid using multiple names for the same identity, like plectropmus sp. and plectropmus.\n\n\n\nR will automatically detect the type of data. If all levels of a variable are numbers, it will treat them as continuous (decimal or integer) numbers. If you mix numbers and words, R will treat all as different levels of a factor.\n\n\n\nSomeday you may want to join data together, for instance, data collected at the same sites across different studies. Keep the site names consistent.\n\n\n\nThere is a big difference between data you didn’t collect and data you tried to collect but went missing. So be explicit about missing data by entering the missing rows where you tried to collect the data and using NA in the missing cells. It is better to use NA than just leave the cell blank, because R interprets NA as missing data.\n\n\n\nData-wrangling is the process of checking data for errors and formatting data so it can be used in analysis. If you do all the wrangling in R and keep the scripts, the process is totally repeatable. Then if someone wants to see how you changed the original data to check errors for instance, all the changes are recorded in a script.\nIf you want to know more about data wrangling in R, check out my latest courses.\nAlso, avoid .xls and .xlsx formats for saving data. Instead save your data as .csv (comma separated values). It is inconvenient to read excel formats into R. Excel can also change the ‘type’ of numbers in strange ways that alter your data. For instance, data that looks like dates (10/2) may get transformed into a different date format (10 - February). .xlsx formats also use more memory than .csv. So it is better to keep your data as .csv and wrangle it in R.\n\n\n\nFor instance, if you have collected data over different dates and want to keep them in separate spreadsheets give them names with the dates recorded in a consistent format: data_2-Oct-2015.csv, data_3-Oct-2015.csv and so on. That way you can rapidly read the files into R and parse the file names into actual dates.\n\n\n\nR is an open-access resource, many people have invested much time in developing. To keep good karma, make your data and code open-access too (once you have written your publication of course and if ethics allow). Much scientific data is collected on public money, so it is quite reasonable that the public can access it!.\nFootnote: I changed point 7 on 18-Mar 2016 to be explicit that you should use NA for missing data. Thanks to Andrew Beckerman for suggesting this. I appreciate feedback on my blogs, so feel free to get in touch if you have comments of your own."
  },
  {
    "objectID": "posts/2019-09-22-tiger-shark-declines/index.html",
    "href": "posts/2019-09-22-tiger-shark-declines/index.html",
    "title": "Informing priors for a time-series models to detect declines in tiger sharks",
    "section": "",
    "text": "Our new research has revealed a 71% decline in tiger sharks across Queensland’s coastline (open-access version).\nIn this blog I will explore the time-series modelling techniques we used to estimate this trend. (I’ve written about the significance of this decline elsewhere.)\nWe aimed to achieve two things with the modelling in this paper. First, we wanted to explore how using informed Bayesian priors affects non-linear trend fitting. Second, we wanted to know the impact of having more or less data on estimates for the magnitude of a decline."
  },
  {
    "objectID": "posts/2019-09-22-tiger-shark-declines/index.html#smoothing-non-linear-trends-with-bayesian-models",
    "href": "posts/2019-09-22-tiger-shark-declines/index.html#smoothing-non-linear-trends-with-bayesian-models",
    "title": "Informing priors for a time-series models to detect declines in tiger sharks",
    "section": "Smoothing non-linear trends with Bayesian models",
    "text": "Smoothing non-linear trends with Bayesian models\nWe chose to use a Bayesian time-series modelling technique. This isn’t as complex as it sounds. It is really just a Bayesian extension of GLMs.\nThe clever extension (not our invention!) is to use a random walk to model non-linear trends in population size. In effect, the random walk is just saying each year’s population has some dependency on the previous year.\nYou can think of the random walk as telling us about how strongly the value of next year is constrained by the value this year.\nThe neat thing about Bayesian methods (as opposed to frequentist methods) is we get to use prior information for the variance of the random walk. This means we can control the strength of the random walk. More variance = potentially bigger steps in the random walk. Less variance = smaller steps.\nEventually if we tell the prior that there is effectively no variance in the random walk, the model will always estimate a flat line. If you let the prior have very large variance, then the random walk will follow the data very closely.\nWe usually want to be somewhere in the middle of the two extremes of a flat trend versus a trend that tracks every slight wiggle. We want a trend line that ‘smooths’ the data. See the picture.\n\nThe idea for this paper started with a blog a couple years ago, where I explained the idea of smoothing a random walk with prior information.\nWe used the R-INLA framework for our model. This is a handy tool, because we could model other, non-temporal, source of variation too. Like differences in trends by sites."
  },
  {
    "objectID": "posts/2019-09-22-tiger-shark-declines/index.html#priors-informed-by-life-history-traits",
    "href": "posts/2019-09-22-tiger-shark-declines/index.html#priors-informed-by-life-history-traits",
    "title": "Informing priors for a time-series models to detect declines in tiger sharks",
    "section": "Priors informed by life-history traits",
    "text": "Priors informed by life-history traits\nSo why use Bayesian methods for tiger shark time-series? Well the Bayesian priors mean we can inform the time-series models with other information. Like life-history traits.\nTiger sharks are in decline on the East Coast of Australia, we want to know by how much, because that information is important for informing conservation decisions. This means we need to smooth out all the noise in the data to estimate a long-term trend.\nHolly Kindsvater and co explain the idea of priors very elegantly. But basically conservation biology has a data crisis. We need to make the most of the information we have.\nIf we used the more traditional techniques, like GAMs, we only get to use the time-series data to estimate the level of smoothing. But why ignore other sources of data when we could use them?\n\n\n\nThe method for informing priors\n\n\nWe show that the random walk variance can be informed by life-history information (see picture). These are measurements scientists have made of tiger sharks, which do not depend on time-series. For instance, information like, at what age do tiger sharks breed and how many babies do they have?\nThis life-history information tells us how rapidly the tiger shark population is likely to change under natural conditions. So we use it to constrain the random walk.\nWe then used a special type of prior, a penalized complexity prior to ensure that we could capture less common, but rapid changes in abundance, which may be caused by humans killing things."
  },
  {
    "objectID": "posts/2019-09-22-tiger-shark-declines/index.html#does-data-quantity-matter",
    "href": "posts/2019-09-22-tiger-shark-declines/index.html#does-data-quantity-matter",
    "title": "Informing priors for a time-series models to detect declines in tiger sharks",
    "section": "Does data quantity matter?",
    "text": "Does data quantity matter?\nOf course! But we wanted to know how much data you need. We had a lot of data in this instance, almost 10,000 shark captures over 33 years at 11 regions.\nSo we ran simulations where we used only some of the data (The INLA technique came in handy here, because it runs so FAST).\nIt turns out the informed priors did pretty well at getting a good estimate of the trend when there was less data.\nThe ‘default’ priors also did very well (ie what you get if you don’t tell INLA what prior to use). I hope the developers of the INLA penalized complexity prior are pleased to hear this.\nHowever, we think it is still good to make use of life-history information. Informed priors help to justify modelling decisions, which can be important when trends are used to inform policy debates."
  },
  {
    "objectID": "posts/2019-09-22-tiger-shark-declines/index.html#whats-next",
    "href": "posts/2019-09-22-tiger-shark-declines/index.html#whats-next",
    "title": "Informing priors for a time-series models to detect declines in tiger sharks",
    "section": "What’s next?",
    "text": "What’s next?\nThere’s lots of interest in Queensland’s sharks right now. Not least because of a court battle between environmental lawyers and the Queensland government, over government policy to shoot sharks they catch in beach protection programs.\nWe’ve also found declines in other large sharks, including threatened species.\nEveryone keeps asking us what is causing these shark declines. So we’d like to do some modelling of different causes of mortality (commercial fishing, recreational fishing, shark control programs etc…) to see what matters most."
  },
  {
    "objectID": "posts/2021-04-13-seaspiracy-talking-points/index.html",
    "href": "posts/2021-04-13-seaspiracy-talking-points/index.html",
    "title": "Seaspiracy talking points for discussions with general public",
    "section": "",
    "text": "The Netflix film ‘Seaspiracy’ has stirred up controversy for among scientists for good reason. BUT the film is creating lots of opportunities to talk about ocean conservation with general public, like family, uber drivers and hairdressers.\nIn our lab meeting today we discussed the film (I refuse to call it a ‘documentary’).\nThe film mixes up true facts with misrepresentation in a narrative that one supposes is designed to confuse people and turn them off eating seafood altogether. This message does damage to efforts that are trying to make fisheries more sustainable by having people make sustainable choices in their purchases.\nIf you find yourself in a discussion about the film, here are some talking points (with an Australian spin):\n\nTalking points for scientists\nOverfishing is a global issue and arguably the most significant issue marine life and ocean food production face. It is not just Japanese and Chinese to blame, and its not just industrial fisheries. Australia still has in some fisheries an overfishing problem. Many small scale fisheries, including some of Australia’s recreational fisheries, have overfishing issues. But many nations, including Australia, have worked hard to manage their fisheries better (e.g. the Great Barrier Reef Marine Park was designed alongside fisheries regulations very effectively) and many fisheries are doing better thanks to those efforts.\nThe film misrepresents the facts and tells mistruths in a way that one can only suppose was deliberately designed to turn people off seafood. It is not true what they say that you can’t tell where your seafood comes from. There are many resources to help you make sustainable choices. Making sustainable choices is a positive way to make change, it encourages the industry to do better. There are many examples of where this has worked, but the film neglected to show them. Things you can do include: Refer to your local sustainable seafood guide. In Australia that would be the very reputable one published by the Australian Marine Conservation Society (https://www.marineconservation.org.au/sustainable-seafood-choices/ also comes as a handy app!).\nAsk your fish salesperson questions about where the fish come from and how it was caught. This let’s them know their customers care about sustainability.\nAustralia, and many other places, are laggards in labelling seafood. So get involved with research trying to make it better. For instance, if you live in South East Queensland you can do help researchers by doing this simple survey when you buy fish.\nFinally, tell them that while overfishing is important, its not the only issue oceans face. Cue discussion of what you do in your research!\n\n\nDebating tips\nIf you get drawn into a debate on the film, focus on the bigger picture. Be careful about debating facts if you are not up to date with them. I’ve met many a ‘climate’ expert who has ‘facts’ to dispute climate change. Its true I don’t know much about the entire science of climate change, its huge, so I can never know enough to refute every myth. For Seaspiracy this page is a good start for a fact check on some common ones: https://sustainablefisheries-uw.org/science-of-seaspiracy/\nThe facts are important, but remember the bigger picture. Its the overall message and way they are stitched together (some facts do check out as true) that misrepresents what we need to do about overfishing.\nSome important questions are why don’t they talk to any actual fishers (they talk to a whaler in the Faroes and a slave, too very extreme cases that I’m not sure you’d classify as typical fishers)? Why do all the interviews with the ‘bad guys’ come off badly, whereas the ‘good guys’ (conservationists and vegan food companies) seem so perfectly polished? Why do they go to all those lengths to investigate conflicts of interests in Dolphin Safe tuna, but not report CoI for their experts (like the book author who owns a vegan food company)."
  },
  {
    "objectID": "posts/2017-07-25-ICCB2017-tweets/index.html",
    "href": "posts/2017-07-25-ICCB2017-tweets/index.html",
    "title": "What are people saying about ICCB2017 on Twitter?",
    "section": "",
    "text": "Here’s a brief analysis of tweets from the International Congress for Conservation Biology to date. The conference started on the starting 23rd July I was curious to see what people are talking about there and who is doing the talking on twitter.\nI have updated this on 27th July (first post on 25th July. )\nAs of 27th July I could access 13396 tweets and retweets from the conference, starting on 2017-07-18.\nIf you are wondering how these stats compare to other conferences, check out my post from ICRS last year.\n\n\nTextual analysis is a bit of a dark art, but here is a word cloud of terms, word size is scaled by number of mentions. Words are stemmed so or instance ‘biodiverse’ and ‘biodiversity’ both appear below as ‘biodivers’\n\nAt this stage the location are still an imporant talking point with words like Colombia and Cartagena. Also predictably ‘conservation’ and ‘biodiversity’ are up there.\nIt is interesting that ‘need’ features in a lot of tweets. Perhaps many people making calls for action?\nI think that words like “open”, “join”, “world”, “come” hint at the inclusiveness of conservation conferences.\nThere were a few more interesting words that have more than 100 mentions: “taotaotasi” - the handle of Angelo Villagomez who recieved a distinguished service award from the Society for Conservation Biology. “shark” still is the most popular plant or animal group mentioned so far, likely due to the two top 5 tweets mentioned above. But ‘forest’, ‘bird’ and ‘fish’ are also working their way up the list.\nFinally the stems for happy (“happi”) and peace (“peac”) and the word “love” were also very popular. Perhaps conservation scientist is getting back to its 1960s origins?\n\n\n\nYou might see twitter stats reported after conferences that look phenomenal, like ‘millions saw our posts’, but it is worth digging down to see how many people had substantial interaction with the conference’s feed.\nIf we count the number of twitter users that have tweeted or RTed (re-tweet) #ICCB2017 posts, then there are 2337. Of those about 1/3 (690) have written an original tweet. If we only consider tweeters that have made &gt;5 tweets, then we have only 222 people and organisations.\nClearly, the people talking on twitter are a biased selection of people at ICCB2017 and may also include people not there (like me). As usual, a lot people only tweet once or twice.\n\nThe users doing the most tweeting are (in order of most to least with number of tweets):\n\n\n\nHandle\n# of tweets\n\n\n\n\nIbuAnggrek\n654\n\n\nICCB2017\n509\n\n\nAzurigenMCS\n466\n\n\nrebecca_jarvis\n413\n\n\nWhySharksMatter\n299\n\n\nCarlaWildlife\n218\n\n\nultimatemegs\n186\n\n\nFancyScientist\n156\n\n\nMattTietbs47\n155\n\n\nSociety4ConBio\n137\n\n\n\n\n\n\nHere is a table of the most popular tweets, by their RT count:\n\n\n\n\n\n\n\n\nHandle\nTweet\n# retweets\n\n\n\n\nWhySharksMatter\nAngler gives up chance at world record, releases huge blacktip shark alive. #ICCB2017 #SharkWeek (from 2014) https://t.co/dwmiAeSXQW https://t.co/74SyQ6Uhfk\n96\n\n\neguerreroforero\n@Brigittelgb en #ICCB2017: A los países megadiversos nos llaman ‘megadispersos’ porq no hemos logrado posicionar políticamente biodiversidad https://t.co/u43dVvcjHO\n93\n\n\nHugePossum\nAustralia’s proposed marine parks have never been science based - bad planning for nature and people #ICCB2017 https://t.co/rZXAueuod6\n70\n\n\nICCB2017\n#ICCB2017 attendees - there are sloths in El Parque Centenario near the convention center! #UrbanWildlife https://t.co/81YktjU4Mu\n60\n\n\nNathanJBennett\nHow can the conservation social sciences improve conservation? Today at 11:30am in Sala 304 at #ICCB2017 Paper here https://t.co/WAYzpvoXQv https://t.co/WVLS4y2Wj7\n52\n\n\ncyclonewatson\nA slight messaging problem- release of iPhone is hundreds of times more read about than global loss of coral reefs! #iccb2017 https://t.co/6hbfrCPNu1\n42\n\n\nNathanJBennett\nThere are still significant barriers to mainstreaming the social sciences in conservation-&gt;Today at #ICCB2017 Paper: https://t.co/nnyhYxb3gk https://t.co/2dGeEPt46y\n41\n\n\nSeasaver\nUN Patron of Oceans calls for ban on tournaments which kill threatened sharks https://t.co/P5KmUxBte3 #ICCB2017 @TheIGFA #sportfishing\n41\n\n\nrebecca_jarvis\nAre we really including everybody’s voices and needs in our conservation decisions and actions? #ICCB2017 #ICCB2017equity https://t.co/BmV7tzgzjV\n37\n\n\naction4ifaw\nIFAW looks forward to joining youth conservation partners at the Millennials in Conservation workshop this Tuesday at #ICCB2017! https://t.co/cZZ5XVIFhF\n35\n\n\n\nI like that sloths in the park are right up there. Marine science topics are also pretty popular. Tweets by well known people (e.g. Chief Scientist of The Nature Conservancy, Hugh Possigham) and organisations (e.g. ICCB) are more likely to become more popular. Also, those that link to topical news items (e.g. marine park policy in Australia and fishing of sharks) are also more likely to reach an audience much broader than the ICCB2017 attendees.\nI like the tweet written in Spanish, which comes via a Colombian. I think it translates to (thanks google translate and a friend): “Megadiverse countries call us mega-scattered because we have failed to politically position biodiversity”. Tweet me if I have this wrong.\nFinally, there are a fair few tweets about communicating the urgency of conservation issues to the public and failure to do so in the past. I think this is telling, it is an issue that is at the forefront of many attendees minds.\n\n\n\nFinally, it can be illustrative to look at the timing of tweets. This graph is made with times for the time zone local to Cartagena."
  },
  {
    "objectID": "posts/2017-07-25-ICCB2017-tweets/index.html#what-are-people-saying-about-international-congress-for-conservation-biology-2017-on-twitter",
    "href": "posts/2017-07-25-ICCB2017-tweets/index.html#what-are-people-saying-about-international-congress-for-conservation-biology-2017-on-twitter",
    "title": "What are people saying about ICCB2017 on Twitter?",
    "section": "",
    "text": "Here’s a brief analysis of tweets from the International Congress for Conservation Biology to date. The conference started on the starting 23rd July I was curious to see what people are talking about there and who is doing the talking on twitter.\nI have updated this on 27th July (first post on 25th July. )\nAs of 27th July I could access 13396 tweets and retweets from the conference, starting on 2017-07-18.\nIf you are wondering how these stats compare to other conferences, check out my post from ICRS last year.\n\n\nTextual analysis is a bit of a dark art, but here is a word cloud of terms, word size is scaled by number of mentions. Words are stemmed so or instance ‘biodiverse’ and ‘biodiversity’ both appear below as ‘biodivers’\n\nAt this stage the location are still an imporant talking point with words like Colombia and Cartagena. Also predictably ‘conservation’ and ‘biodiversity’ are up there.\nIt is interesting that ‘need’ features in a lot of tweets. Perhaps many people making calls for action?\nI think that words like “open”, “join”, “world”, “come” hint at the inclusiveness of conservation conferences.\nThere were a few more interesting words that have more than 100 mentions: “taotaotasi” - the handle of Angelo Villagomez who recieved a distinguished service award from the Society for Conservation Biology. “shark” still is the most popular plant or animal group mentioned so far, likely due to the two top 5 tweets mentioned above. But ‘forest’, ‘bird’ and ‘fish’ are also working their way up the list.\nFinally the stems for happy (“happi”) and peace (“peac”) and the word “love” were also very popular. Perhaps conservation scientist is getting back to its 1960s origins?\n\n\n\nYou might see twitter stats reported after conferences that look phenomenal, like ‘millions saw our posts’, but it is worth digging down to see how many people had substantial interaction with the conference’s feed.\nIf we count the number of twitter users that have tweeted or RTed (re-tweet) #ICCB2017 posts, then there are 2337. Of those about 1/3 (690) have written an original tweet. If we only consider tweeters that have made &gt;5 tweets, then we have only 222 people and organisations.\nClearly, the people talking on twitter are a biased selection of people at ICCB2017 and may also include people not there (like me). As usual, a lot people only tweet once or twice.\n\nThe users doing the most tweeting are (in order of most to least with number of tweets):\n\n\n\nHandle\n# of tweets\n\n\n\n\nIbuAnggrek\n654\n\n\nICCB2017\n509\n\n\nAzurigenMCS\n466\n\n\nrebecca_jarvis\n413\n\n\nWhySharksMatter\n299\n\n\nCarlaWildlife\n218\n\n\nultimatemegs\n186\n\n\nFancyScientist\n156\n\n\nMattTietbs47\n155\n\n\nSociety4ConBio\n137\n\n\n\n\n\n\nHere is a table of the most popular tweets, by their RT count:\n\n\n\n\n\n\n\n\nHandle\nTweet\n# retweets\n\n\n\n\nWhySharksMatter\nAngler gives up chance at world record, releases huge blacktip shark alive. #ICCB2017 #SharkWeek (from 2014) https://t.co/dwmiAeSXQW https://t.co/74SyQ6Uhfk\n96\n\n\neguerreroforero\n@Brigittelgb en #ICCB2017: A los países megadiversos nos llaman ‘megadispersos’ porq no hemos logrado posicionar políticamente biodiversidad https://t.co/u43dVvcjHO\n93\n\n\nHugePossum\nAustralia’s proposed marine parks have never been science based - bad planning for nature and people #ICCB2017 https://t.co/rZXAueuod6\n70\n\n\nICCB2017\n#ICCB2017 attendees - there are sloths in El Parque Centenario near the convention center! #UrbanWildlife https://t.co/81YktjU4Mu\n60\n\n\nNathanJBennett\nHow can the conservation social sciences improve conservation? Today at 11:30am in Sala 304 at #ICCB2017 Paper here https://t.co/WAYzpvoXQv https://t.co/WVLS4y2Wj7\n52\n\n\ncyclonewatson\nA slight messaging problem- release of iPhone is hundreds of times more read about than global loss of coral reefs! #iccb2017 https://t.co/6hbfrCPNu1\n42\n\n\nNathanJBennett\nThere are still significant barriers to mainstreaming the social sciences in conservation-&gt;Today at #ICCB2017 Paper: https://t.co/nnyhYxb3gk https://t.co/2dGeEPt46y\n41\n\n\nSeasaver\nUN Patron of Oceans calls for ban on tournaments which kill threatened sharks https://t.co/P5KmUxBte3 #ICCB2017 @TheIGFA #sportfishing\n41\n\n\nrebecca_jarvis\nAre we really including everybody’s voices and needs in our conservation decisions and actions? #ICCB2017 #ICCB2017equity https://t.co/BmV7tzgzjV\n37\n\n\naction4ifaw\nIFAW looks forward to joining youth conservation partners at the Millennials in Conservation workshop this Tuesday at #ICCB2017! https://t.co/cZZ5XVIFhF\n35\n\n\n\nI like that sloths in the park are right up there. Marine science topics are also pretty popular. Tweets by well known people (e.g. Chief Scientist of The Nature Conservancy, Hugh Possigham) and organisations (e.g. ICCB) are more likely to become more popular. Also, those that link to topical news items (e.g. marine park policy in Australia and fishing of sharks) are also more likely to reach an audience much broader than the ICCB2017 attendees.\nI like the tweet written in Spanish, which comes via a Colombian. I think it translates to (thanks google translate and a friend): “Megadiverse countries call us mega-scattered because we have failed to politically position biodiversity”. Tweet me if I have this wrong.\nFinally, there are a fair few tweets about communicating the urgency of conservation issues to the public and failure to do so in the past. I think this is telling, it is an issue that is at the forefront of many attendees minds.\n\n\n\nFinally, it can be illustrative to look at the timing of tweets. This graph is made with times for the time zone local to Cartagena."
  },
  {
    "objectID": "posts/2017-05-30-migratory-species-climate-change/index.html",
    "href": "posts/2017-05-30-migratory-species-climate-change/index.html",
    "title": "Networks of habitats increase the vulnerability of migratory species to climate change",
    "section": "",
    "text": "Migratory species are important components of many ecosystems and human societies. Salmon are one of the most well known migratory fish, they move between freshwaters and the ocean to feed and breed. In Australia, we have our own unique assemblage of migratory fish, many of which are highly threatened.\n\n\n\nThe Australian Grayling. Image: Nathan Litjens\n\n\nFor instance, the Australian Grayling has undergone large population declines and is a protected species. A primary threat is dams, which prevent it from migrating from its freshwater habitats to the sea and back again.\nMigratory fish may face a double jeopardy from climate change. Not only will climate change affect their marine and freshwater habitats, but it may also decouple the connections between those habitats.\nIn a recent study, we combined species distribution models with network analysis to ask how climate change and dams affect the future persistence of the Grayling.\nWe turned to the R programming language to implement our models, because it gave us the flexibility to fit models of Grayling distribution, predict change in Grayling distribution and analyse the outputs using network analysis.\nCo-author Alex Bush developed an ensemble of models for Grayling freshwater habitat using the R packages dismo and biomod2. These models considered key environmental variables for Grayling distribution along Australia’s East Coast including temperature and rainfall.\nLead author Hsien-Yung Lin then developed a model for Grayling’s marine distribution. Finally, he was able to create maps of connected habitats using data on Australia’s dams.\nUsing predictions of climate change from 2015 to 2085, Lin could predict where suitable habitat for the Grayling would move to. By turning these maps of suitability into networks, he could analyse areal changes in habitat using R’s igraph package.\nThe network analysis also allowed Lin to predict the effect of removing dams, thus restoring connectivity to areas of marine and freshwater habitat that may have been inaccessabile to migrating Grayling.\nWe found that the dams a manager would remove to bring the greatest benefits to Grayling changed under climate change.\nBecause Grayling habitat generally moves south in both marine and freshwaters, restoring connectivity further to the south became more important with future warming. Removing the right dams could increase Grayling freshwater habitat by up to 30% under some climate scenarios.\nThe most important dams to remove also tended to move further upstream if we considered more extreme climate change scenarios. This is because Grayling distribution is expected to move further upstream to cooler habitats.\nOur results suggest that the conservation of migratory species should consider movements in all their habitats and also how climate change may alter connections among habitats. Doing so can result in more effective advice to management on where to restore connections to help recover threatened species."
  },
  {
    "objectID": "posts/2017-05-30-migratory-species-climate-change/index.html#networks-of-habitats-increase-the-vulnerability-of-migratory-species-to-climate-change",
    "href": "posts/2017-05-30-migratory-species-climate-change/index.html#networks-of-habitats-increase-the-vulnerability-of-migratory-species-to-climate-change",
    "title": "Networks of habitats increase the vulnerability of migratory species to climate change",
    "section": "",
    "text": "Migratory species are important components of many ecosystems and human societies. Salmon are one of the most well known migratory fish, they move between freshwaters and the ocean to feed and breed. In Australia, we have our own unique assemblage of migratory fish, many of which are highly threatened.\n\n\n\nThe Australian Grayling. Image: Nathan Litjens\n\n\nFor instance, the Australian Grayling has undergone large population declines and is a protected species. A primary threat is dams, which prevent it from migrating from its freshwater habitats to the sea and back again.\nMigratory fish may face a double jeopardy from climate change. Not only will climate change affect their marine and freshwater habitats, but it may also decouple the connections between those habitats.\nIn a recent study, we combined species distribution models with network analysis to ask how climate change and dams affect the future persistence of the Grayling.\nWe turned to the R programming language to implement our models, because it gave us the flexibility to fit models of Grayling distribution, predict change in Grayling distribution and analyse the outputs using network analysis.\nCo-author Alex Bush developed an ensemble of models for Grayling freshwater habitat using the R packages dismo and biomod2. These models considered key environmental variables for Grayling distribution along Australia’s East Coast including temperature and rainfall.\nLead author Hsien-Yung Lin then developed a model for Grayling’s marine distribution. Finally, he was able to create maps of connected habitats using data on Australia’s dams.\nUsing predictions of climate change from 2015 to 2085, Lin could predict where suitable habitat for the Grayling would move to. By turning these maps of suitability into networks, he could analyse areal changes in habitat using R’s igraph package.\nThe network analysis also allowed Lin to predict the effect of removing dams, thus restoring connectivity to areas of marine and freshwater habitat that may have been inaccessabile to migrating Grayling.\nWe found that the dams a manager would remove to bring the greatest benefits to Grayling changed under climate change.\nBecause Grayling habitat generally moves south in both marine and freshwaters, restoring connectivity further to the south became more important with future warming. Removing the right dams could increase Grayling freshwater habitat by up to 30% under some climate scenarios.\nThe most important dams to remove also tended to move further upstream if we considered more extreme climate change scenarios. This is because Grayling distribution is expected to move further upstream to cooler habitats.\nOur results suggest that the conservation of migratory species should consider movements in all their habitats and also how climate change may alter connections among habitats. Doing so can result in more effective advice to management on where to restore connections to help recover threatened species."
  },
  {
    "objectID": "posts/2024-02-08-secret-recipe-to-writing-papers/index.html",
    "href": "posts/2024-02-08-secret-recipe-to-writing-papers/index.html",
    "title": "Chris’ secret recipe to writing papers quickly",
    "section": "",
    "text": "Work with good collaborators that you know well and who you can rely on.\nHave clearly defined roles for yourself and collaborators.\nDon’t have too many collaborators (three or less or have none). There’s lots of time in coordinating with many people.\n\nSub-point - non-research collaborators (e.g. industry/NGOs) can also take a lot of time, but see points 1 and 2.\n\nHave a clearly defined plan.\nHave a relatively simple question to address, one that can be clearly defined as 1-3 aims/hypotheses and the methods need to be clear and well established (and things you or your collaborotors can easily do)\nDon’t get too hung on how much work it takes to write ‘The Paper’. The amount of time it takes is hugely variable and there is no minimum time.\n80/20 rule - 20% of the effort for 80% of the results.\nHave some skills. Be decent at writing. Have some methodology skills (e.g. modelling or field work skills) that are well known to you and you will rely on for this study. Flip side is don’t expect to write a paper if you are still learning to write scientifically (e.g. early in your PhD).\nWork with a system you know well, e.g. a study system, lab system or type of modelling/data for analysis.\nIn general reviews and perspectives take 10x times longer than you think they will. A lot of thought goes into word crafting (though sometimes they can be really quick).\nDon’t get too hung up on what journal it ‘HAS’ to go to. Sometimes you start a paper thinking it has Nature potential, but if it turns out it doesn’t then just get it done (see steps 6 and 7!) and submit it to a disiplinary journal."
  },
  {
    "objectID": "posts/2016-10-11-create-inset-plot/index.html",
    "href": "posts/2016-10-11-create-inset-plot/index.html",
    "title": "Create an inset plot",
    "section": "",
    "text": "The difficulty of plotting one plot over the top of another in R has always frustrated me. I often end up opening some photo editing software (like Gimp, which is free) and doing it manually. However, manual editing can be frustrating if you need to change the plot.\n\n\n\nA histogram with a boxplot inset\n\n\nI just learned a hassle free way to create insets (thanks to Ben Stewart-Koster) and wanted to share it, so here it is:\nFirst, let’s simulate some fake data\nx &lt;- rnorm(100)\ny &lt;- rbinom(100, 1, 0.5)\nNow the key step, use par to set the fig param to your desired coordinate space. e.g. in the below we can specify figures positions in 0-1 on both the x and y coordinates:\npar(fig = c(0,1,0,1))\nNow plot your first figure\nhist(x)\nNow the second key part, specify where in the coordinate space you want your inset figure to go:\npar(fig = c(0.07,0.5, 0.5, 1), new = T)\nboxplot(x ~ y)\nVoila that is it, we have our figure with an inset."
  },
  {
    "objectID": "posts/2016-10-11-create-inset-plot/index.html#how-to-inset-a-plot-on-another-plot-in-r",
    "href": "posts/2016-10-11-create-inset-plot/index.html#how-to-inset-a-plot-on-another-plot-in-r",
    "title": "Create an inset plot",
    "section": "",
    "text": "The difficulty of plotting one plot over the top of another in R has always frustrated me. I often end up opening some photo editing software (like Gimp, which is free) and doing it manually. However, manual editing can be frustrating if you need to change the plot.\n\n\n\nA histogram with a boxplot inset\n\n\nI just learned a hassle free way to create insets (thanks to Ben Stewart-Koster) and wanted to share it, so here it is:\nFirst, let’s simulate some fake data\nx &lt;- rnorm(100)\ny &lt;- rbinom(100, 1, 0.5)\nNow the key step, use par to set the fig param to your desired coordinate space. e.g. in the below we can specify figures positions in 0-1 on both the x and y coordinates:\npar(fig = c(0,1,0,1))\nNow plot your first figure\nhist(x)\nNow the second key part, specify where in the coordinate space you want your inset figure to go:\npar(fig = c(0.07,0.5, 0.5, 1), new = T)\nboxplot(x ~ y)\nVoila that is it, we have our figure with an inset."
  },
  {
    "objectID": "posts/2025-06-27-prompting-llms-quality-ecological-statistics/index.html",
    "href": "posts/2025-06-27-prompting-llms-quality-ecological-statistics/index.html",
    "title": "Prompting large language models for quality ecological statistics",
    "section": "",
    "text": "Announcing a new pre-print on prompt engineering for ecological statistics.. https://doi.org/10.32942/X2CS80\nThe pre-print describes the challenges and opportunities of using large language models and agents to do ecological statistics. It then goes onto describe some prompt engineering strategies that can help you make the most of these tools, but still get reliable statistical advice and code.\nThe pre-print accompanies a one day course I’ve published that teaches how to use LLM tools for R and statistics, as well as discussing the environmental and ethical challenges."
  },
  {
    "objectID": "posts/2016-05-16-fiji-ecosystem-services/index.html",
    "href": "posts/2016-05-16-fiji-ecosystem-services/index.html",
    "title": "Costing the impact of cyclone Winston on Fiji’s ecosystem services",
    "section": "",
    "text": "Costing the impact of cyclone Winston on Fiji’s ecosystem services\nTropical cyclone Winston hit Fiji in February 2016 leaving 43 people dead, 120,000 without shelter and devastating infrastructure . As Fiji recovers, Fiji’s government is working to document the cost of this event to their economy.\n\n\n\nTropical cyclone Winston damages schools. Image:  ABC news.\n\n\nOur Science for Nature and People Partnership group on ‘Ridge to reef fisheries’ is supporting Fiji’s government to help document the cost of the cyclone to the services that Fiji’s ecosystems provide to people.\nAssessments of economic damage from natural disaster’s typically only include direct economic costs, like the cost of repairing damaged schools. Such costings underestimate the true cost of natural disaster’s to society, because they do not account for damage to ecosystems. Ecosystems provide many goods and services that are important for people’s jobs and economic security.\nFor instance, coral reef ecosystems support the livelihoods of hundreds of millions of people globally. In Fiji they support people through drawing tourists and providing for fisheries.\n\n\n\nCoral reef fisheries support the livelihoods of many people in tropical nations.\n\n\nReef ecosystems can be impacted by cyclones. The impacts can be direct, for instance the cyclone can tear up the reef. Impacts of cyclones can also be indirect. Heavy rainfall on land carries sediment and pollutants into rivers which then flow out to sea. The turbid waters can smother corals, killing them. Fish in turn depend on the coral habitat, so the whole ecosystem may be affected.\nThe Wildlife Conservation Society is working in Fiji and has a strong partnership with government. They have been working closely with the Fiji Government to provide inputs into the Post Disaster Needs Assessment to ensure that the assessment of the cyclones’s damage includes lost ecosystem services.\nIt can be challenging to mobilise a science team to document costs after a natural disaster like a cyclone. Fortunately, our SNaPP group has already been working on documenting Fiji’s ecosystem services on land and in the sea. The objective of our work is to help plan land-use change to minimise the run-off of pollutants into the ocean, where they may impact on fisheries.\nRun-off of turbid water can be worsened by land-clearing. Loss of forest destablises the soil, so more soil runs into rivers with rain. Trees also break the energy of the rainfall and absorb some of the rain, so more trees mean less of the rainfall runs into rivers.\nOur team has provided the Fiji government with some basic information about catchments and forest cover on the island of Vanua Levu, one of the most affected regions (see image).\n\n\n\nForest cover in Vanua Levu’s major catchments, estimated as part of our SNaP working group.\n\n\nThe information about catchments and landcover will be used by government to help them document the cost of the cyclone to ecosystem services, and help formulate actions that can be taken as Fiji begins the earlier recovery phase, which includes better land-sea management.\nThe cyclone has also seen a shift in the science priorities for our working group. The Wildlife Conservation Society had been working towards an integrated coastal management planning meeting with government and Fijian communities in May. The meeting was to discuss the future of land-use change in parts of Fiji.\nThe SNaPP team had been working towards supporting the planning meeting, by providing estimates for how much pollution may cost fisheries and associated livelihoods. However, this natural disaster has seen a shift in local priorities, as people have suddenly become much more aware of the impact of extreme run-off events.\nOur SNaPP team is now working to incorporate extreme events into the scientific advice we provide. We hope that information about extreme events will resonate with local stakeholders and help to highlight the importance of protecting local ecosystem services."
  },
  {
    "objectID": "posts/2022-09-01-PhD-projects-available/index.html",
    "href": "posts/2022-09-01-PhD-projects-available/index.html",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "",
    "text": "Current until March 2023\nOur lab uses quantitative tools, including statistics, AI and numerical models, to help conservation managers address a range of issue facing marine ecosystems. Below are two current projects where we are recruiting students.\nWe excel in training students to be outstanding science communicators, who’s work has real-world impact. Chris’ supervision style thus emphasizes quantitative training and networking with a diverse range of scientists and stakeholders. For instance, our students frequently partner with conservation groups such that their research is directly addressing a pressing environmental management issue."
  },
  {
    "objectID": "posts/2022-09-01-PhD-projects-available/index.html#conservation-and-restoration-of-coastal-habitats",
    "href": "posts/2022-09-01-PhD-projects-available/index.html#conservation-and-restoration-of-coastal-habitats",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Conservation and restoration of coastal habitats",
    "text": "Conservation and restoration of coastal habitats\nWork with the team on Global Wetlands Project to help solve pressing challenges for conservation of coastal wetland habitats. PhD and Honours projects available to work on predicting the impact of climate change on coastal restoration and ecosystem services. Email Chris (chris.brown@griffith.edu.au) for more information. PhD and honours projects available."
  },
  {
    "objectID": "posts/2022-09-01-PhD-projects-available/index.html#predicting-unprecedented-ecological-futures",
    "href": "posts/2022-09-01-PhD-projects-available/index.html#predicting-unprecedented-ecological-futures",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Predicting unprecedented ecological futures",
    "text": "Predicting unprecedented ecological futures\nWe are entering an era of science where ecosystem change is uncharted by historical observations. Observations are the foundation of predictive science, so how can we predict a future that is outside the envelope of the environments?\nWork with our team to address the question of how we predict ecological futures, to inform management for marine and coastal ecosystems. Humanities footprint on the Earth’s ecosystem has never been greater. The oceans are increasingly being industrialized, and climate change means the footprint of human economic activity now touches even the most remote ecosystems.\nCoastal ecosystems exemplify the challenges we face in managing our growing impacts. Coasts are the thin line on which much of the human population and industrial activity rests. This human activity places great pressures on coastal ecosystems, and in many places we are losing species and the ecological functions that support human livelihoods, like fisheries.\nPredicting the future for coastal ecosystems is critical for creating a sustainable relationship between humanity and nature. Ecological predictions allow us to identify options for better managing our relationship with ecosystems."
  },
  {
    "objectID": "posts/2022-09-01-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "href": "posts/2022-09-01-PhD-projects-available/index.html#criteria-for-potential-phd-applicants",
    "title": "PhD projects on marine conservation and ecosystem modelling available",
    "section": "Criteria for potential PhD applicants",
    "text": "Criteria for potential PhD applicants\nPlease consider the below criteria before contacting us. We will not be considering students for applications that do not meet these criteria, because they will not be competitive for funded positions. Further details on Griffith University’s application process can be found here.\nOur primary research methods are quantitative tools. Potential applicants should have either experience in quantiative methods like R, or a very strong desire to learn more quantitative skills. Chris does not supervise projects that are primarily field or lab based, except as cosupervisor with other group leaders.\nOur students come with training in a diverse range of fields, including field ecology, maths and IT. Our lab group is outstanding at supporting quantitative training and current PhD students have ample opportunities to learn quantitative skills by working with their peers, postdoctoral researchers, Chris and through specialized training courses.\n\nAustralian students\nAustralian citizens and permanent residents should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters by research) in a relevant field.\n\n\nInternational students\nDue to international travel restrictions international student scholarships are currently extremely limited. To be competitive for a scholarship international students should have been awarded a 1st Class Honours degree or equivalent (e.g. Masters with a 40 credit point research thesis) in a relevant field. They must be lead author on at least one publication in an internationally recognized peer-reviewed journal. To be considered, journals should be internationally recognized, for instance they should have an Impact Factor rating from ISI or be listed by Scimago."
  },
  {
    "objectID": "posts/2019-10-20-critical-gaps-in-seagrass-protection/index.html",
    "href": "posts/2019-10-20-critical-gaps-in-seagrass-protection/index.html",
    "title": "Seagrass habitats lack protection at a global scale",
    "section": "",
    "text": "This blog was written by PhD student Laura Griffiths.\nSeagrass ecosystems face multiple threats, from pollution to sea level rise. However, management is failing to adequately protect these ecosystems and prevent their decline. We reviewed global policy relevant to seagrass protection and identified some critical gaps. We need to rethink how we protect coastal ecosystems in general, and seagrasses in particular, from multiple cumulative threats.\n\nSeagrass ecosystems are one of the most widespread coastal ecosystems on the planet. They are crucial to the world’s carbons stores, are an essential habitat for biodiversity conservation and provide income and food security for thousands of local communities world-wide.\nHowever, overburdened by human activities at the land sea interface, these ecosystems have undergone significant decline in the last few decades. Losses of up to 7% per year have been shown globally. Identifying the root cause of this continued loss by looking at management under the microscope reveals the missing pieces necessary to protect these essential ecosystems.\nWe took a look at twenty sites around the world. These included sites with high cumulative impacts and covered all levels of governance structure to see how seagrass ecosystems were being managed. The aim was to underpin the root causes leading to their mismanagement and associated decline. We looked at all relevant management related documents such as legislation, policies, plans, etc.. to understand how, and if, threats to seagrass were being targeted with a management action.\nWhat we found was that seagrass ecosystems were poorly protected across the globe. Failure was apparent in that they were not recognised or valued legally (through legislation) in many cases. The worst managed habitats were in countries with a low human development index. In cases where they were valued, management was generally insufficient to protect seagrass ecosystems from the suite of anthropogenic threats. A sector-led management approach incapable of crossing physical (i.e. land and sea) and jurisdictional boundaries were the main reasons preventing adequate protection.\nHaving said that there were three case studies that stood out against this backdrop of under protection. These were the shining lights that the world can look to for guidance moving forward. They were the Trilateral Wadden Sea Plan, Reef 2050 Long Term Sustainability Plan, and the Pilot Draft Plan for the Western Part of the Gulf of Gdansk.\nSuccessful protection of seagrass requires holistic management of threats from land and the sea. Holistic management can be achieved through integrated management plans. This means that urban planning departments need to speak their marine and freshwater counterparts and consider how human activities impact on coastal ecosystems.\nSome aspects to be considered include:\n\nthe location of stormwater drainage, sewage outfalls and dredge spoil\ncontrol over recreational activities that scour the seafloor like motorised watercraft, fishing for worms and trampling\nmodification of coastal structures like pontoons and jetties to allow sufficient light penetration\nreview of water quality guidelines to minimize the effect of turbidity, particularly in semi enclosed embayments\nrestore seagrass habitat at alternative sites when smothered by land reclamation projects\ngreater support for monitoring of seagrass habitat to ascertain historical loss and determine current spatial extent\nevaluate how ecosystem services like productive fisheries can be supported in the future once losses are realised\nthe inclusion of cumulative impact policies into the planning and decision-making process\n\nMore importantly the study identified that recognition of value through legislation and policies is the first step to better protect seagrass ecosystems. Without this fundamental initiative it is likely that the status of seagrass ecosystems will continue to degrade."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "The contributions of different food sources to animal diets is often a mystery. Isotopes provide a means to estimate those contributions, because different food sources often have different isotopic signatures. We would typically use a Bayesian mixing model to estimate the proportional contributions of different food sources to samples taken from the animal.\nIsotope mixing models are fitted within a Bayesian framework. This means that the end results (AKA ‘posterior distributions’) are influenced by the data, the model and the prior distributions. Priors are specified for each parameter in the model, including the source contributions.\nIn the article “Quantifying learning in biotracer studies” (Brown,et al. Oecologia 2018) we describe how comparing priors and posteriors with information criteria is important to determine the influence of the data on the model.\nThis blog describes how to use my BayeSens R package to calculate information criteria for mixing models.\n\n\nThe default prior for most mixing model has a mean of 1/n, where n is the number of sources. So if we had five potential food sources this means our starting assumption is that on average the consumer eats and assimilates 20% of each prey item.\nUncareful use of mixing models has resulted in findings from some peer-reviewed being contested. For instance, if the user just puts sources in the model ‘just to see’ if they matter, the starting assumption is that, on average, each contributes an equal fraction to the diet. This starting assumption is in many cases ridicilous.\nWhen the data are not particularly informative, the model will return the result that every item contributed an equal fraction to the animals diet. The authors may then write this up as a new ‘result’ when in fact it was just the default assumption of the software package being reflected in their outputs.\nIn general I am quite suspicious of all the isotope studies reporting generalist consumers that eat equal fractions of prey. These patterns may well just reflect the default priors.\nAdding to the confusion is that some call the default priors ‘uninformative’.\nPriors for mixing models are all informative, eventhe so called ‘uninformative’ priors. The prior for source contributions bounded between 0-1, and source contributions must sum to 1, so it can never be truly flat in that range.\n\n\n\nYou should always plot your priors and posteriors to check what is going on. You can very quickly identify this issue of uninformative data. Then in your write up, you could put less emphasis on results that look like the priors.\nYou can also calculate statistics that measure how different prior and posterior are. In our paper, we described several statistics taken from information theory. You can then easily report these statistics to summarize where prior and posterior are different (or not).\nFor instance, in a recent study of coastal fish specices we fitted many models across many species and regions, so we reported the differences as a table in the supplemental material.\n\n\n\nThis blog demonstrates how information criteria can be calculated for mixing models fit with MixSIAR.\nWe will apply the simple marginal information criteria from that paper to the Killer Whale example, see vignette(\"killerwhale_ex\") in MixSIAR.\nThe killer whale example is a nice simple one with no covariates or random effects. If you have covariates or random effects, you’ll need to be careful to compare priors to posteriors at the same locations on the fixed/random effects.\nIt will be helpful to have some understanding of MixSIAR’s data structures, because we need to find the posterior samples in the model output.\n\n\n\nFirst load the packages we need:\nlibrary(BayeSens)\nlibrary(MixSIAR)\nNow load the data (this is verbatim from the Killer whale example).\nmix.filename &lt;- system.file(\"extdata\", \"killerwhale_consumer.csv\", package = \"MixSIAR\")\n\nmix &lt;- load_mix_data(filename=mix.filename,\n                     iso_names=c(\"d13C\",\"d15N\"),\n                     factors=NULL,\n                     fac_random=NULL,\n                     fac_nested=NULL,\n                     cont_effects=NULL)\n\n\nsource.filename &lt;- system.file(\"extdata\", \"killerwhale_sources.csv\", package = \"MixSIAR\")\n\nsource &lt;- load_source_data(filename=source.filename,\n                           source_factors=NULL,\n                           conc_dep=FALSE,\n                           data_type=\"means\",\n                           mix)\n\ndiscr.filename &lt;- system.file(\"extdata\", \"killerwhale_discrimination.csv\", package = \"MixSIAR\")\n\ndiscr &lt;- load_discr_data(filename=discr.filename, mix)\n\n\n\nLet’s draw samples from the prior. You can also plot this with MixSIAR’s plot_prior function, but we need a matrix of the samples for calculating info criteria later.\nalpha &lt;- rep(1, source$n.sources) #default prior values\np_prior &lt;- MCMCpack::rdirichlet(10000, alpha) #draw prior samples\nLet’s plot just the prior for the first source (since they are all the same in this case)\n#Plot histogram and density (same data, different ways to view it )\npar(mfrow = c(1,2))\nhist(p_prior[,1], 20, main = source$source_names[1])\nplot(density(p_prior[,1]), main = source$source_names[1])\nabline(v = 1/source$n.sources)\n\nAs you can see the default prior clearly isn’t ‘uninformative’ because it is centred around 1/number of sources (in fact it has mean 1 over the number of sources). It might be better called the ‘uninformed’ (by the user) prior. This means the prior will have a lower mean the more sources you include in the model.\n\n\n\nThis is verbatim from the Killer Whales example.\nmodel_filename &lt;- \"MixSIAR_model_kw_uninf.txt\"   # Name of the JAGS model file\nresid_err &lt;- TRUE\nprocess_err &lt;- TRUE\nwrite_JAGS_model(model_filename, resid_err, process_err, mix, source)\njags.uninf &lt;- run_model(run=\"test\",mix,source,discr,model_filename,alpha.prior = alpha, resid_err, process_err)\n\n## module glm loaded\n\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 12\n##    Unobserved stochastic nodes: 23\n##    Total graph size: 766\n##\n## Initializing model\nI’ve used the test run mode here just to speed things up for the example.\nYou should absolutely use long chains (e.g. run = \"long\") when calculating info criteria. They are quite sensitive to the number of MCMC samples if there are few samples. We need enough samples to get a good idea of the posteriors full shape.\n\n\n\nHere’s where it helps to have some idea of how MixSIAR structures outputs. We need to find the posterior samples. You can dig around using str(jags.uninf). I did that and found the samples under jags.uninf$BUGSoutput as below:\np_post &lt;- jags.uninf$BUGSoutput$sims.list$p.global\nNow we have a matrix of prior samples and a matrix of posterior samples we can just compare them with the hellinger or kldiv (Kullback-Leibler divergence) functions from BayeSens. I’ll compare just the first source (Chinook salmon).\nhellinger(p_prior[,1], p_post[,1])\n\n## Hellinger distance - continuous\n## [1] 0.61\n##\n##  Hellinger distance - discrete\n## [1] 0.65\n\nkldiv(p_prior[,1], p_post[,1])\n\n## Kullback-Leibler divergence\n## [1] 5.7\nWe’d like to know what the info criteria are for all sources, so we could manually select columns to compare, or just use some sort of iterating function to do them all at once. Here I use lapply and put them into a dataframe:\nhell_out &lt;- lapply(1:source$n.sources, function(i) hellinger(p_prior[,i], p_post[,i])$hdist_disc)\nkl_out &lt;- lapply(1:source$n.sources, function(i) kldiv(p_prior[,i], p_post[,i])$kd)\ninfo_df &lt;- data.frame(source_names = source$source_names,\n                      hellinger = unlist(hell_out),\n                      KLD = unlist(kl_out))\ninfo_df\n\n##   source_names hellinger       KLD\n## 1      Chinook 0.6536184 5.6601980\n## 2         Chum 0.3659136 1.6746118\n## 3         Coho 0.3053054 0.9676074\n## 4      Sockeye 0.5260229 3.6985169\n## 5    Steelhead 0.5430961 4.2273867\nHellinger values near 0 are very similar to the priors, Hellinger values near 1 are very different to the priors. The KLD ranges from &gt;0 to infinity, so greater values indicate greater differences from the prior. So these results indicate to us that the model and data are not very informative about Coho, but much more informative about Chinook. To interpret why this is you should plot the priors and posteriors.\nYou can use output_JAGS to do this. We will do it ourselves, just to practice data wrangling. For Chinook and Coho:\npar(mfrow = c(1,2))\nplot(density(p_post[,1]), main = source$source_names[1])\nlines(density(p_prior[,1]), col = \"red\")\nabline(v = 1/source$n.sources, lty = 2)\n\nplot(density(p_post[,3]), main = source$source_names[3])\nlines(density(p_prior[,3]), col = \"red\")\nabline(v = 1/source$n.sources, lty = 2)\n\nIt is pretty clear that contributions for Chinook have shifted higher, whereas the data doesn’t give us much reason to believe Coho are any more important than the prior suggested.\nNote that you can also get high information criteria stats if the posterior mean stays the same as the prior’s mean, but the distribution changes shape (e.g. gets thinner). For instance, if the data were strongly informative that Coho were not an important food source, then we could have the same posterior mean of 0.2, but the uncertainty intervals would be much narrower around 0.2 than in the prior.\n\n\n\nThe killer whale example also gives a model fit with informed priors. Here’s the code verbatim from MixSIAR:\nkw.alpha &lt;- c(10,1,0,0,3)\nkw.alpha &lt;- kw.alpha*length(kw.alpha)/sum(kw.alpha)\nkw.alpha[which(kw.alpha==0)] &lt;- 0.01\nmodel_filename &lt;- \"MixSIAR_model_kw_inf.txt\"  \nresid_err &lt;- TRUE\nprocess_err &lt;- TRUE\nwrite_JAGS_model(model_filename, resid_err, process_err, mix, source)\njags.inf &lt;- run_model(run=\"test\",mix,source,discr,model_filename,alpha.prior=kw.alpha, resid_err, process_err)\n\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 12\n##    Unobserved stochastic nodes: 23\n##    Total graph size: 766\n##\n## Initializing model\nThe only extra step we need to do now is draw samples from the prior and posteriors:\np_prior_inf &lt;- MCMCpack::rdirichlet(10000, kw.alpha) #draw prior samples\np_post_inf &lt;- jags.inf$BUGSoutput$sims.list$p.global\n\nhell_out_inf &lt;- lapply(1:source$n.sources, function(i) hellinger(p_prior_inf[,i], p_post_inf[,i])$hdist_disc)\n\n## Warning in sqrt(1 - integrate(fx1, minx, maxx)$value): NaNs produced\n\n## Warning in sqrt(1 - integrate(fx1, minx, maxx)$value): NaNs produced\n\nkl_out_inf &lt;- lapply(1:source$n.sources, function(i) kldiv(p_prior_inf[,i], p_post_inf[,i])$kd)\ninfo_df &lt;- cbind(info_df,\n                 data.frame(\n                      hellinger_inf = unlist(hell_out_inf),\n                      KLD_inf = unlist(kl_out_inf)))\ninfo_df\n\n##   source_names hellinger       KLD hellinger_inf   KLD_inf\n## 1      Chinook 0.6536184 5.6601980     0.7829688 8.7014174\n## 2         Chum 0.3659136 1.6746118     0.3353133 1.0081589\n## 3         Coho 0.3053054 0.9676074     0.1208581 0.1794651\n## 4      Sockeye 0.5260229 3.6985169     0.1262016 0.1923406\n## 5    Steelhead 0.5430961 4.2273867     0.6469540 5.7095752\nThe warning about NA’s comes from the prior for some groups being near zero, so the continuous version of the Hellinger stat isn’t able to be calculated. We are using the discrete version though, so its no problem to us.\nSo with the informed priors the Hellinger has increased for Chinook and Steelhead and decreased for the others.\nRemember that the information criteria just measure the distance from the prior. So if our data just confirm the informed priors, or there isn’t enough data to overcome the informed priors, then the information criteria will be near zero. In this case we have only two tracers and samples from 12 killer whales. The prior we used on Sockeye was very strong to zero consumption, so our result stays the same.\nThe below plot shows the priors in red and posteriors in black for the model with informed priors.\nplot(density(p_post_inf[,1]), lty = 2, main = \"informed prior\", xlim = c(0,1))\nlines(density(p_prior_inf[,1]), lty = 2, col = \"red\")\n\nI haven’t plotted the informed Coho model because both the prior and posterior are a spikes near zero.\nYou can clearly see the model has shifted the consumption of Coho downwards relative to the informed prior."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#why-mixing-model-priors-matter",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#why-mixing-model-priors-matter",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "The default prior for most mixing model has a mean of 1/n, where n is the number of sources. So if we had five potential food sources this means our starting assumption is that on average the consumer eats and assimilates 20% of each prey item.\nUncareful use of mixing models has resulted in findings from some peer-reviewed being contested. For instance, if the user just puts sources in the model ‘just to see’ if they matter, the starting assumption is that, on average, each contributes an equal fraction to the diet. This starting assumption is in many cases ridicilous.\nWhen the data are not particularly informative, the model will return the result that every item contributed an equal fraction to the animals diet. The authors may then write this up as a new ‘result’ when in fact it was just the default assumption of the software package being reflected in their outputs.\nIn general I am quite suspicious of all the isotope studies reporting generalist consumers that eat equal fractions of prey. These patterns may well just reflect the default priors.\nAdding to the confusion is that some call the default priors ‘uninformative’.\nPriors for mixing models are all informative, eventhe so called ‘uninformative’ priors. The prior for source contributions bounded between 0-1, and source contributions must sum to 1, so it can never be truly flat in that range."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#solutions",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#solutions",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "You should always plot your priors and posteriors to check what is going on. You can very quickly identify this issue of uninformative data. Then in your write up, you could put less emphasis on results that look like the priors.\nYou can also calculate statistics that measure how different prior and posterior are. In our paper, we described several statistics taken from information theory. You can then easily report these statistics to summarize where prior and posterior are different (or not).\nFor instance, in a recent study of coastal fish specices we fitted many models across many species and regions, so we reported the differences as a table in the supplemental material."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#how-to-use-the-r-package",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#how-to-use-the-r-package",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "This blog demonstrates how information criteria can be calculated for mixing models fit with MixSIAR.\nWe will apply the simple marginal information criteria from that paper to the Killer Whale example, see vignette(\"killerwhale_ex\") in MixSIAR.\nThe killer whale example is a nice simple one with no covariates or random effects. If you have covariates or random effects, you’ll need to be careful to compare priors to posteriors at the same locations on the fixed/random effects.\nIt will be helpful to have some understanding of MixSIAR’s data structures, because we need to find the posterior samples in the model output."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#killer-whale-example",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#killer-whale-example",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "First load the packages we need:\nlibrary(BayeSens)\nlibrary(MixSIAR)\nNow load the data (this is verbatim from the Killer whale example).\nmix.filename &lt;- system.file(\"extdata\", \"killerwhale_consumer.csv\", package = \"MixSIAR\")\n\nmix &lt;- load_mix_data(filename=mix.filename,\n                     iso_names=c(\"d13C\",\"d15N\"),\n                     factors=NULL,\n                     fac_random=NULL,\n                     fac_nested=NULL,\n                     cont_effects=NULL)\n\n\nsource.filename &lt;- system.file(\"extdata\", \"killerwhale_sources.csv\", package = \"MixSIAR\")\n\nsource &lt;- load_source_data(filename=source.filename,\n                           source_factors=NULL,\n                           conc_dep=FALSE,\n                           data_type=\"means\",\n                           mix)\n\ndiscr.filename &lt;- system.file(\"extdata\", \"killerwhale_discrimination.csv\", package = \"MixSIAR\")\n\ndiscr &lt;- load_discr_data(filename=discr.filename, mix)"
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#draw-samples-from-the-prior",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#draw-samples-from-the-prior",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "Let’s draw samples from the prior. You can also plot this with MixSIAR’s plot_prior function, but we need a matrix of the samples for calculating info criteria later.\nalpha &lt;- rep(1, source$n.sources) #default prior values\np_prior &lt;- MCMCpack::rdirichlet(10000, alpha) #draw prior samples\nLet’s plot just the prior for the first source (since they are all the same in this case)\n#Plot histogram and density (same data, different ways to view it )\npar(mfrow = c(1,2))\nhist(p_prior[,1], 20, main = source$source_names[1])\nplot(density(p_prior[,1]), main = source$source_names[1])\nabline(v = 1/source$n.sources)\n\nAs you can see the default prior clearly isn’t ‘uninformative’ because it is centred around 1/number of sources (in fact it has mean 1 over the number of sources). It might be better called the ‘uninformed’ (by the user) prior. This means the prior will have a lower mean the more sources you include in the model."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#run-the-model",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#run-the-model",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "This is verbatim from the Killer Whales example.\nmodel_filename &lt;- \"MixSIAR_model_kw_uninf.txt\"   # Name of the JAGS model file\nresid_err &lt;- TRUE\nprocess_err &lt;- TRUE\nwrite_JAGS_model(model_filename, resid_err, process_err, mix, source)\njags.uninf &lt;- run_model(run=\"test\",mix,source,discr,model_filename,alpha.prior = alpha, resid_err, process_err)\n\n## module glm loaded\n\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 12\n##    Unobserved stochastic nodes: 23\n##    Total graph size: 766\n##\n## Initializing model\nI’ve used the test run mode here just to speed things up for the example.\nYou should absolutely use long chains (e.g. run = \"long\") when calculating info criteria. They are quite sensitive to the number of MCMC samples if there are few samples. We need enough samples to get a good idea of the posteriors full shape."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#extract-samples",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#extract-samples",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "Here’s where it helps to have some idea of how MixSIAR structures outputs. We need to find the posterior samples. You can dig around using str(jags.uninf). I did that and found the samples under jags.uninf$BUGSoutput as below:\np_post &lt;- jags.uninf$BUGSoutput$sims.list$p.global\nNow we have a matrix of prior samples and a matrix of posterior samples we can just compare them with the hellinger or kldiv (Kullback-Leibler divergence) functions from BayeSens. I’ll compare just the first source (Chinook salmon).\nhellinger(p_prior[,1], p_post[,1])\n\n## Hellinger distance - continuous\n## [1] 0.61\n##\n##  Hellinger distance - discrete\n## [1] 0.65\n\nkldiv(p_prior[,1], p_post[,1])\n\n## Kullback-Leibler divergence\n## [1] 5.7\nWe’d like to know what the info criteria are for all sources, so we could manually select columns to compare, or just use some sort of iterating function to do them all at once. Here I use lapply and put them into a dataframe:\nhell_out &lt;- lapply(1:source$n.sources, function(i) hellinger(p_prior[,i], p_post[,i])$hdist_disc)\nkl_out &lt;- lapply(1:source$n.sources, function(i) kldiv(p_prior[,i], p_post[,i])$kd)\ninfo_df &lt;- data.frame(source_names = source$source_names,\n                      hellinger = unlist(hell_out),\n                      KLD = unlist(kl_out))\ninfo_df\n\n##   source_names hellinger       KLD\n## 1      Chinook 0.6536184 5.6601980\n## 2         Chum 0.3659136 1.6746118\n## 3         Coho 0.3053054 0.9676074\n## 4      Sockeye 0.5260229 3.6985169\n## 5    Steelhead 0.5430961 4.2273867\nHellinger values near 0 are very similar to the priors, Hellinger values near 1 are very different to the priors. The KLD ranges from &gt;0 to infinity, so greater values indicate greater differences from the prior. So these results indicate to us that the model and data are not very informative about Coho, but much more informative about Chinook. To interpret why this is you should plot the priors and posteriors.\nYou can use output_JAGS to do this. We will do it ourselves, just to practice data wrangling. For Chinook and Coho:\npar(mfrow = c(1,2))\nplot(density(p_post[,1]), main = source$source_names[1])\nlines(density(p_prior[,1]), col = \"red\")\nabline(v = 1/source$n.sources, lty = 2)\n\nplot(density(p_post[,3]), main = source$source_names[3])\nlines(density(p_prior[,3]), col = \"red\")\nabline(v = 1/source$n.sources, lty = 2)\n\nIt is pretty clear that contributions for Chinook have shifted higher, whereas the data doesn’t give us much reason to believe Coho are any more important than the prior suggested.\nNote that you can also get high information criteria stats if the posterior mean stays the same as the prior’s mean, but the distribution changes shape (e.g. gets thinner). For instance, if the data were strongly informative that Coho were not an important food source, then we could have the same posterior mean of 0.2, but the uncertainty intervals would be much narrower around 0.2 than in the prior."
  },
  {
    "objectID": "posts/2020-04-25-data-priors-in-isotope-models/index.html#informative-priors",
    "href": "posts/2020-04-25-data-priors-in-isotope-models/index.html#informative-priors",
    "title": "How much has the data informed your isotope mixing model",
    "section": "",
    "text": "The killer whale example also gives a model fit with informed priors. Here’s the code verbatim from MixSIAR:\nkw.alpha &lt;- c(10,1,0,0,3)\nkw.alpha &lt;- kw.alpha*length(kw.alpha)/sum(kw.alpha)\nkw.alpha[which(kw.alpha==0)] &lt;- 0.01\nmodel_filename &lt;- \"MixSIAR_model_kw_inf.txt\"  \nresid_err &lt;- TRUE\nprocess_err &lt;- TRUE\nwrite_JAGS_model(model_filename, resid_err, process_err, mix, source)\njags.inf &lt;- run_model(run=\"test\",mix,source,discr,model_filename,alpha.prior=kw.alpha, resid_err, process_err)\n\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 12\n##    Unobserved stochastic nodes: 23\n##    Total graph size: 766\n##\n## Initializing model\nThe only extra step we need to do now is draw samples from the prior and posteriors:\np_prior_inf &lt;- MCMCpack::rdirichlet(10000, kw.alpha) #draw prior samples\np_post_inf &lt;- jags.inf$BUGSoutput$sims.list$p.global\n\nhell_out_inf &lt;- lapply(1:source$n.sources, function(i) hellinger(p_prior_inf[,i], p_post_inf[,i])$hdist_disc)\n\n## Warning in sqrt(1 - integrate(fx1, minx, maxx)$value): NaNs produced\n\n## Warning in sqrt(1 - integrate(fx1, minx, maxx)$value): NaNs produced\n\nkl_out_inf &lt;- lapply(1:source$n.sources, function(i) kldiv(p_prior_inf[,i], p_post_inf[,i])$kd)\ninfo_df &lt;- cbind(info_df,\n                 data.frame(\n                      hellinger_inf = unlist(hell_out_inf),\n                      KLD_inf = unlist(kl_out_inf)))\ninfo_df\n\n##   source_names hellinger       KLD hellinger_inf   KLD_inf\n## 1      Chinook 0.6536184 5.6601980     0.7829688 8.7014174\n## 2         Chum 0.3659136 1.6746118     0.3353133 1.0081589\n## 3         Coho 0.3053054 0.9676074     0.1208581 0.1794651\n## 4      Sockeye 0.5260229 3.6985169     0.1262016 0.1923406\n## 5    Steelhead 0.5430961 4.2273867     0.6469540 5.7095752\nThe warning about NA’s comes from the prior for some groups being near zero, so the continuous version of the Hellinger stat isn’t able to be calculated. We are using the discrete version though, so its no problem to us.\nSo with the informed priors the Hellinger has increased for Chinook and Steelhead and decreased for the others.\nRemember that the information criteria just measure the distance from the prior. So if our data just confirm the informed priors, or there isn’t enough data to overcome the informed priors, then the information criteria will be near zero. In this case we have only two tracers and samples from 12 killer whales. The prior we used on Sockeye was very strong to zero consumption, so our result stays the same.\nThe below plot shows the priors in red and posteriors in black for the model with informed priors.\nplot(density(p_post_inf[,1]), lty = 2, main = \"informed prior\", xlim = c(0,1))\nlines(density(p_prior_inf[,1]), lty = 2, col = \"red\")\n\nI haven’t plotted the informed Coho model because both the prior and posterior are a spikes near zero.\nYou can clearly see the model has shifted the consumption of Coho downwards relative to the informed prior."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "R tutorials",
    "section": "",
    "text": "Feel free to download and share these tutorials. You are welcome to use them for your own teaching, and I would love to hear about it.\nThere are also many more blog post length tutorials on our blog.\nThe downside is that it is not easy to get started, particularly if you are not simultaneously a geo-statistical expert and a gun programmer. I am trying to make the jump easier for ecologists by providing courses and helpful blog posts here.\nMore code (including this webpage) is on my Github page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Assistants for Scientific Coding\n\n\n\nrstats\n\nlllms\n\n\n\nHow to use generative AI coding assistants for data analysis and statistics\n\n\n\nOct 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Ecological Models\n\n\n\nrstats\n\n\n\nTutorial on building predictive models in ecology\n\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Ecological Data\n\n\n\ndata\n\necology\n\nrstats\n\n\n\nGitHub repository with example ecological datasets\n\n\n\nJun 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GLMs\n\n\n\nrstats\n\necology\n\nmodelling\n\n\n\nTutorial on Generalized Linear Models in R\n\n\n\nJan 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructural Causal Modelling Tutorial\n\n\n\nrstats\n\necology\n\nmodeling\n\n\n\nGuide to structural causal models and causal inference\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Analysis in R\n\n\n\nrstats\n\nspatial\n\ngis\n\nmodelling\n\n\n\nTutorial on spatial data analysis and modelling using R\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning ecological survey data for conservation scientists\n\n\n\nrstats\n\ndata\n\necology\n\n\n\nTutorial on preparing ecological survey data for analysis\n\n\n\nJan 1, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/2025-11-05-R-collaborator-for-20-years/index.html",
    "href": "drafts/2025-11-05-R-collaborator-for-20-years/index.html",
    "title": "Keeping enthusiasm about learning R",
    "section": "",
    "text": "Staying enthused about learning R is a challenge. You need progress from the initial motivation driven by ‘I need to learn this skill for my research career’ through a phase of tedium on your way to a competent R programmer.\nHere’s my story."
  },
  {
    "objectID": "drafts/2025-11-05-R-collaborator-for-20-years/index.html#fitting-a-multinomial-peg-into-an-anova-hole",
    "href": "drafts/2025-11-05-R-collaborator-for-20-years/index.html#fitting-a-multinomial-peg-into-an-anova-hole",
    "title": "Keeping enthusiasm about learning R",
    "section": "Fitting a multinomial peg into an ANOVA hole",
    "text": "Fitting a multinomial peg into an ANOVA hole\nI first attempted to learn R while wintering at a remote field station in the Southern Ocean. I had one book, 2004 satellite internet and a lot of time to kill.\nLearning R was hard. Then you also have to learn statistics.\nI had chosen a hard statistical question to start with. My colleague asked for help analysing data. The aim was to predict which of three behaviours flocks of birds would show under different situations. I was trying to wrangle ANOVA to do this analysis, because that is the method my book described.\nI went around and around in circles. Were the behaviours factors or the response variable? If they were a response variable, how could the response have just three types of unordered outcomes? In hindsight I now see why I went in circles. The appropriate analysis would have been a multinomial model. Easy with brms (which didn’t exist then) or mgcv, but GLMs were not covered in standard text books in 2004, let alone multinomial GLMs.\nI did however manage to learn how to code an ANOVA, as well as learning a bit about R code.\nCoding requires us to be frustratingly precise in ways that human languages are not. This is hard for new programmers. And the learning curve for R is reknowned for being steeper than other coding languages.\nThe first step to competence is comprehending the exactness of programming."
  },
  {
    "objectID": "drafts/2025-11-05-R-collaborator-for-20-years/index.html#like-a-spoken-language-but-pedantic",
    "href": "drafts/2025-11-05-R-collaborator-for-20-years/index.html#like-a-spoken-language-but-pedantic",
    "title": "Keeping enthusiasm about learning R",
    "section": "Like a spoken language, but pedantic",
    "text": "Like a spoken language, but pedantic\nAfter my misadventure in the Southern Ocean I studied evolutionary modelling for my Honours thesis. I used Matlab, a commercial product developed by a single company. Matlab has the major benefit of its consistency. Its functions are used in consistent ways, the help files have a consistent style.\nR is open-source. ‘Learning R’ is an experience in trying to comprehend the thinking of scores of different open-source developers.\nThe R is an ecosystem cliche comes to mind. R is like a real ecosystem. The software ecosystem’s solutions to problems are diverse.\nMore often than someone has coded a solution to your problem in an R package. If only your could grasp how to use that solution the way its creator intended."
  },
  {
    "objectID": "drafts/2025-11-05-R-collaborator-for-20-years/index.html#data-structures",
    "href": "drafts/2025-11-05-R-collaborator-for-20-years/index.html#data-structures",
    "title": "Keeping enthusiasm about learning R",
    "section": "Data structures",
    "text": "Data structures\nUndersanding data structures was a key insight…\nCuriosity helps.\nR has been my main science tool for over 20 years now.\nAs I gained more comprehension of R, stats in R became more like a puzzle. My own curiosity about what was possible kept me going. There is always something new to discover in a diverse software ecosystem.\nAt times the imperitive quickly became procrastination. I’ve spent too many late nights trying a new spatial or Bayesian stats package, just out of curiosity.\nTwenty years on I often find it easier to code up with my own solutions to my research questions, rather than use someone else’s packaged solution. Its easier to comprehend a problem if you work through it yourself. Its easier to understand your own R style than someone else’s.\nMany people learn R because they have to. It is hard to work in many fields of science without some programming skills.\nImpact of genAI\nOther people’s innovations add\nMost data problems you come up with development statistical programmers\nAlways someting new to discover\nThere’s been turn-over too. Pipes and tidyverse changed everything. These packages made data wrangling and plotting so much easier.\nTeaching R got easier. I could see 95% of my students checking out whenever I had to explain a cracker nested operation like mean(log(subset(dat, grp == 'a')$x))"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Code for Chris Brown’s lab webpage",
    "section": "",
    "text": "My personal webpage, view at https://www.seascapemodels.org\nMade with quarto\n\n\n\nMake a new folder in the posts directory\nAdd a index.qmd or index.md in that folder\nWrite the post\nIn the terminal use quarto preview to re-render just the changes\n\nIn the terminal use quarto render to re-render the entire site, which updates links etc…\nTODO\nclear repo history\nInvestigate making r blogs in other dir then just brining over the .md and figure files, might be simpler as then I should be able to just render preview?\ntest then update Rbloggers link"
  },
  {
    "objectID": "readme.html#instructions-for-creating-blogs",
    "href": "readme.html#instructions-for-creating-blogs",
    "title": "Code for Chris Brown’s lab webpage",
    "section": "",
    "text": "Make a new folder in the posts directory\nAdd a index.qmd or index.md in that folder\nWrite the post\nIn the terminal use quarto preview to re-render just the changes\n\nIn the terminal use quarto render to re-render the entire site, which updates links etc…\nTODO\nclear repo history\nInvestigate making r blogs in other dir then just brining over the .md and figure files, might be simpler as then I should be able to just render preview?\ntest then update Rbloggers link"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "people.html#staff",
    "href": "people.html#staff",
    "title": "People",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "people.html#students",
    "href": "people.html#students",
    "title": "People",
    "section": "Students",
    "text": "Students\n\n\n\n\n\n\n\n\nAlexandre Tisseaux\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJitesh Ramesh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJordan Holdorf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKai Ching Cheong\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeigha Aitken\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRichard Takyi\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#collaborators",
    "href": "people.html#collaborators",
    "title": "People",
    "section": "Collaborators",
    "text": "Collaborators\n\n\n\n\n\n\n\n\nCaitie Kuempel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Wetlands Project\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#alumni",
    "href": "people.html#alumni",
    "title": "People",
    "section": "Alumni",
    "text": "Alumni\n\n\n\n\n\n\n\n\nAlexandra White\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmelia Desbiens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrew Broadley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDale Bryan-Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Alyssa Giffin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Christina Buelow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Jaramar Villareal-Rosas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Jodie Haig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Mischa Turschwell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Viv Tulloch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHsien-Yung Lin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJackson Stockbridge\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKristin Jinks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaura Griffiths\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLauren Armstrong\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaree Hume\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMax Campbell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlivia King\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRenee Piccolo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSebastian Lopez-Marcano\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTess O’Neill\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTessie Liddell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWill Taylor\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/collaborators/Caitie-Kuempel.html",
    "href": "people/collaborators/Caitie-Kuempel.html",
    "title": "Caitie Kuempel",
    "section": "",
    "text": "Dr Kuempel is a Lecturer in Ecological Modelling at Griffith Univeristy (Brisbane, Australia). She leads the Conservation Action Team.\n\nShe is an experienced spatial modeler of environmental impact across a range of human activities and scales. These models can be used to quantify patterns and potential impacts on humans and nature, assess trade-offs between management actions, and develop systematic conservation plans."
  },
  {
    "objectID": "people/students/Leigha-Aitken.html",
    "href": "people/students/Leigha-Aitken.html",
    "title": "Leigha Aitken",
    "section": "",
    "text": "Leigha studied for her Masters by coursework thesis project with Dr Brown in 2024 and 2025, successfully completing her research and graduating her masters in 2025.\nShe studied The contribution of data from no-take marine reserves to the predictive performance of fisheries species abundance forecasts.\nHere is an abstract of her work:\nModels are increasingly used to make forecasts to support fisheries management by translating ecological theory into quantitative predictions. However, forecasts for fisheries species are often constrained by limited data from unfished reference conditions. No-take marine reserves (NTMRs) can provide insight into unfished population dynamics, potentially improving models’ predictive performance. This study asked whether incorporating NTMR data improves the accuracy of annual abundance forecasts over eight years. We compared the predictive performance of first-order autoregressive models that were trained on abundance data from either NTMR sites, fished sites, or a mix of both site types. Each model was then tested by predicting to sites it was not trained on (‘transferred’). We tested the forecasts for both fished and unfished species with 23 years of time-series abundance data from the Maria Island (Tasmania) NTMR and nearby control sites. We found a general decrease in the performance of fished and NTMR models when forecasts were transferred to the opposite site type. Model transferability was species-specific, with the NTMR model reducing forecasting errors by approximately 10% at fished sites for one sustainably fished species. In contrast, for other species, the fished model reduced forecast errors by approximately 4% and 56% at NTMR sites. Additionally, we found that models trained on a mix of site types consistently underperformed. These findings highlight the importance of accounting for site- and species-specific dynamics. While NTMR data offers valuable insights into unfished conditions, further work is required to create models that transfer well to any exploitation regime."
  },
  {
    "objectID": "people/students/Richard Takyi.html",
    "href": "people/students/Richard Takyi.html",
    "title": "Richard Takyi",
    "section": "",
    "text": "Richard is studying the impacts of climate change on fisheries. He plans to develop new fisheries assessment tools that address the effects of climate change and can be used in data poor fisheries. He started in 2024. He is supervised by Dr Brown, with cosupervision from Dr Beth Fulton and Dr Joel Williams.\nRichard has a background in fisheries science and management, and has worked in the fisheries sector in Ghana. He is an experienced policy analyst and has published numerous studies on fisheries and coastal policy..\nHe has shown there is a critical need for management of West Africa’s coastal lagoons with current management activities largely not working."
  },
  {
    "objectID": "people/students/Kai-Ching-Cheong.html",
    "href": "people/students/Kai-Ching-Cheong.html",
    "title": "Kai Ching Cheong",
    "section": "",
    "text": "Kai Ching Cheong is working on the conservation of mangrove biodiversity under climate change. He is completing his PhD studies at Griffith University, under the supervision of Dr Fernanda Adame, Dr Chris Brown and Dr Cesar Herrara."
  },
  {
    "objectID": "people/alumni/Tess-ONeill.html",
    "href": "people/alumni/Tess-ONeill.html",
    "title": "Tess O’Neill",
    "section": "",
    "text": "Tess O’Neill\nTess worked with us as a research assistant (2024), as well as studying for her honours degree in Marine and Antarctic Science with the University of Tasmania. For her thesis she modelled the effects of giant kelp restoration on fisheries. She predicted significant benefits for some coastal fisheries in Tasmania. A further important finding was predicting a significant increase in coastal primary productivity if we are able to meet the 30% restoration target for giant kelp in Tasmania.\nHer thesis has been submitted for publication and the pre-print is available here."
  },
  {
    "objectID": "people/alumni/Hsien-Yung-Lin.html",
    "href": "people/alumni/Hsien-Yung-Lin.html",
    "title": "Hsien-Yung Lin",
    "section": "",
    "text": "Hsien completed his PhD with the Centre of Excellence for Environmental Decisions in 2017. Hsien-Yung studied the conservation of diadromous fish, using species distribution modeling, population dynamics modeling and conservation planning tools. He is currently undertaking a postdoc on connectivity and fisheries at Michigan State University."
  },
  {
    "objectID": "people/alumni/Alexandra-White.html",
    "href": "people/alumni/Alexandra-White.html",
    "title": "Alexandra White",
    "section": "",
    "text": "Alex completed a Bachelor of Science (Honours) in wildlife biology at Griffith and has pursued a love of research by attaining a research assistant role with the GLOW team in Brisbane. While she was with the GLOW team, she worked on anticipating ‘unknown-unknowns’ in the Blue Economy, as well as our work on mangrove policies and laws. She is passionate about ecology and science communication. Alex left us in 2024 for a career in industry."
  },
  {
    "objectID": "people/alumni/Jaramar-Villareal-Rosas.html",
    "href": "people/alumni/Jaramar-Villareal-Rosas.html",
    "title": "Dr Jaramar Villareal-Rosas",
    "section": "",
    "text": "Jaramar Villareal-Rosas.\n\nA conservation scientist with expertise in decision science, ecosystem service flows, and socio-ecological science. Dr Jaramar uses spatial models and decision tools to assess the benefits and costs of different management and conservation interventions at the landscape scale, with the focus of ensuring positive outcomes for people and nature.\nShe completed her PhD at the University of Queensland in 2021. In her work she directly engages with NGOs, local communities and government representatives. Currently, Jaramar collaborated with WWF to prioritize mangrove restoration actions in Mexico.\nShe completed her postdoc with us in 2024 and moved on to work with QUT."
  },
  {
    "objectID": "people/alumni/Amelia-Desbiens.html",
    "href": "people/alumni/Amelia-Desbiens.html",
    "title": "Amelia Desbiens",
    "section": "",
    "text": "Amelia worked with us as a research assistant in 2020 after completing a Bachelor of Advanced Science (Hons Class I) at the University of Queensland in 2019. Her thesis work focused on food web dynamics and trophic cascading in coral reef ecosystems. Her role in the Seascape Models lab centered on the monitoring of longline tuna fisheries in the Western Pacific in collaboration with The Nature Conservancy."
  },
  {
    "objectID": "people/alumni/Lauren-Armstrong.html",
    "href": "people/alumni/Lauren-Armstrong.html",
    "title": "Lauren Armstrong",
    "section": "",
    "text": "Lauren completed her honours in 2017 on methods to monitor the status of fish stocks using recreational catch surveys. She worked with QLD fisheries to complete her honours project."
  },
  {
    "objectID": "people/alumni/Max-Campbell.html",
    "href": "people/alumni/Max-Campbell.html",
    "title": "Max Campbell",
    "section": "",
    "text": "Max was research assistant who worked with us part-time 2019-2022. He specialises in data processing and analysis and contributed modelling and data science expertise to numerous projects. He also set-up the github collaboration framework that we still use today."
  },
  {
    "objectID": "people/alumni/Viv-Tulloch.html",
    "href": "people/alumni/Viv-Tulloch.html",
    "title": "Dr Viv Tulloch",
    "section": "",
    "text": "Viv was working with us on the Global Wetlands Project as the team’s statistical ecologist. She created global scale models of threats to coastal wetlands.\nLate in 2019 she moved to the University of British Columbia in Canada to do work on multiple stressors of ecosystems.\nViv graduated with a PhD from the University of Queensland’s Centre of Excellence for Environmental Decisions in 2017, co-supervised by Chris. She is interested in planning for the impacts of run-off to marine ecosystems and ecosystem management.\nViv on Google Scholar"
  },
  {
    "objectID": "people/alumni/Renee-Piccolo.html",
    "href": "people/alumni/Renee-Piccolo.html",
    "title": "Renee Piccolo",
    "section": "",
    "text": "Renee studied for her PhD thesis on the feasibility of coastal ecosystem restoration. She was co-funded by a CSIRO scholarship. She was awarded her PhD in 2024.\nKey publications from her research were:\n\nDeveloping a systematic framework that can be used to consider and quantify feasibility of coastal restoration actions\nShowing how priority locations for coastal restoration can be very different if we account for the risk of cyclones or land-tenure uncertainty"
  },
  {
    "objectID": "people/alumni/Mischa-Turschwell.html",
    "href": "people/alumni/Mischa-Turschwell.html",
    "title": "Dr Mischa Turschwell",
    "section": "",
    "text": "A former research fellow with the team, Mischa was worked on multiple stressors of coastal ecosystems, funded by an ARC Discovery Project. He then went on to work on a project funded by the Blue Economy Cooperative Research Centre to assess hazards in Australia’s Blue Economy.\nMischa finished his PhD with Griffith and CSIRO in 2017 where he used a range of modelling techniques to investigate the thermal and habitat characteristics of a threatened headwater fish species.\nHighlights from his PhD research including using novel methods to quantify stream thermal regimes, as well as using Bayesian networks to find that riparian restoration could offset negative population consequences of climate warming for a headwater fish.\nHis research interests include aquatic ecology, spatial modelling, and understanding climate-change impacts on aquatic ecosystems."
  },
  {
    "objectID": "people/alumni/Sebastian-Marcano.html",
    "href": "people/alumni/Sebastian-Marcano.html",
    "title": "Sebastian Lopez-Marcano",
    "section": "",
    "text": "Sebastian graduated from his PhD in 2022. His PhD studies focussed on the use of automated environmental monitoring to understand fish movement and habitat hotspots in coastal seascapes under the supervision of Prof. Rod Connolly.\nSebastian’s ORCID No. is 0000-0002-0814-2906."
  },
  {
    "objectID": "people/staff/Chris-Brown.html",
    "href": "people/staff/Chris-Brown.html",
    "title": "Dr Chris Brown",
    "section": "",
    "text": "The leader of the Seascape Models group.\n.\nRead more about Chris’ publications on Google Scholar."
  },
  {
    "objectID": "people/staff/Chris-Brown.html#chris-research",
    "href": "people/staff/Chris-Brown.html#chris-research",
    "title": "Dr Chris Brown",
    "section": "Chris’ research",
    "text": "Chris’ research\nDr Brown researches solutions to the environmental crisis facing ocean ecosystems. His work has had impacts at the global policy scale and local management scale. He has worked on global climate impacts to the oceans, including contributing to an IPCC Assessment Report. At a regional scale, his work has informed the monitoring of Pacific Tuna fisheries, ‘catchment-to-coast’ management on Pacific Islands the management of coastal habitats.\nSince moving to the Institute of Marine and Antarctic Studies in Hobart, Tasmania, Dr Brown has been focusing more on fisheries research. This included raising the alarm over systematic biases in stock assessments, the key modelling tool used to determine the sustainability of fisheries.\nHe uses advances in statistics to synthesize ecological data and inform environmental decision making. As researcher leader on the Global Wetlands Program, he brings together an international team to map the status of coastal wetland habitats and provide a blueprint for their conservation.\nChris often works with non-governmental organisations to deliver science that supports ocean management. As co-lead of the Global Mangrove Alliance’s Science Working group he coordinates science and provides advice to support mangrove conservation globally.\nChris is an enthusiastic science communicator. He also enjoys teaching statistics and modelling to applied scientists, and particularly how to access data visualisation and modelling tools using the R program. Many of his R teaching materials are available free on this site."
  },
  {
    "objectID": "people/staff/Chris-Brown.html#current-research",
    "href": "people/staff/Chris-Brown.html#current-research",
    "title": "Dr Chris Brown",
    "section": "Current research",
    "text": "Current research\nDr Brown began an ARC Future Fellowship on predicting unprecedented ecological futures in January of 2022.\nDr Brown also is research lead on the Global Wetlands Program a major program of research looking at mapping the health of seagrass, mangroves and saltmarsh globally to inform on conservation actions."
  },
  {
    "objectID": "people/staff/Chris-Brown.html#career-history",
    "href": "people/staff/Chris-Brown.html#career-history",
    "title": "Dr Chris Brown",
    "section": "Career history",
    "text": "Career history\n2023 ongoing. ARC Future Fellow and Associate Professor of Fisheries Science at Institute of Marine and Antarctic Studies, University of Tasmania.\n2022-2023. ARC Future Fellow, Coastal and Marine Research Centre, Australian Rivers Institute, and Associate Professor in the School of Environment and Science, Griffith University\n2019 - 2021 Senior Lecturer, Coastal and Marine Research Centre, Australian Rivers Institute, School of Environment and Science, Griffith University\n2016-2019 Australian Research Council - Discovery Early Career Research Award “Predicting fisheries change from cumulative impacts to fish habitats”, Griffith University\n2015-2016 Research Fellow, working on ‘Ridge-to-reef fisheries management’, Griffith University\n2012 – 2015 Postdoctoral Fellow – modelling responses of coral reef fisheries to marine protected areas, The Global Change Institute, The University of Queensland\n2012 PhD, University of Queensland and CSIRO (2008 – 2011) Thesis: “Effects of climate change on marine ecosystems and potential management responses” Advisors: Assoc. Prof. A. Richardson, Prof. H. Possingham, Dr. E. Fulton.\n2007 Honours Degree (1st class in Zoology, awarded best thesis 2007), University of Tasmania.\n2004 Bachelor of Science, Major in Marine, Antarctic and Freshwater Biology, University of Tasmania."
  },
  {
    "objectID": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html",
    "href": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html",
    "title": "AI Game Creator Prompt",
    "section": "",
    "text": "You are an expert game developer specializing in creating simple, engaging web-based games from visual references and instructions. Your task is to analyze an image and accompanying instructions to create a fully functional HTML/CSS/JavaScript game that can be opened and played locally in any modern web browser.\n\n\n\n\n\nCreate a single HTML file that contains all necessary code (HTML, CSS, JavaScript)\nUse vanilla JavaScript (no external libraries or frameworks required)\nEnsure the game works offline and can be opened directly in a browser\nMake the game responsive and playable on both desktop and mobile devices\nInclude proper error handling and user feedback\n\n\n\n\n\nImage Analysis\n\nCarefully examine the provided image to identify:\n\nVisual elements (characters, objects, backgrounds, UI elements)\nColor schemes and artistic style\nSpatial relationships and layout\nAny text or symbols that might indicate game mechanics\nOverall theme or setting\n\n\nInstruction Interpretation\n\nParse the user’s instructions to understand:\n\nGame genre and mechanics\nPlayer objectives and win/lose conditions\nControls and interaction methods\nDifficulty level and complexity requirements\nAny specific features or behaviors requested\n\n\nGame Design\n\nCreate a simple but engaging game that combines the visual elements from the image with the requested mechanics\nDesign intuitive controls (keyboard, mouse, or touch-friendly)\nImplement clear visual feedback for player actions\nInclude a scoring system or progress indicator when appropriate\nAdd sound effects using Web Audio API or HTML5 audio (optional but recommended)\n\n\n\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;[Game Title]&lt;/title&gt;\n    &lt;style&gt;\n        /* All CSS styles here */\n        /* Include responsive design */\n        /* Use CSS animations for smooth gameplay */\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;!-- Game HTML structure --&gt;\n    &lt;!-- Include game canvas or div containers --&gt;\n    &lt;!-- Add UI elements (score, controls, instructions) --&gt;\n    \n    &lt;script&gt;\n        // All JavaScript game logic here\n        // Include game state management\n        // Implement game loop and rendering\n        // Add event listeners for controls\n        // Include game mechanics and collision detection\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\nStart Screen\n\nGame title and brief instructions\nStart button or key press to begin\nVisual preview of the game\n\nUser Interface\n\nScore display\nLives/health indicator (if applicable)\nPause functionality\nClear visual feedback for all interactions\n\nGame Over Screen\n\nFinal score or results\nRestart option\nEncouraging message\n\n\n\n\n\n\nUse HTML5 Canvas for complex graphics and animations\nUse CSS and DOM manipulation for simpler UI-based games\nCreate visual elements that match or are inspired by the provided image\nUse appropriate color schemes and styling consistent with the image theme\nEnsure text is readable and UI elements are clearly distinguishable\n\n\n\n\nDesktop: - Arrow keys or WASD for movement - Spacebar for primary action - Mouse clicks for menu navigation and point-and-click interactions\nMobile: - Touch controls with clear visual indicators - Swipe gestures for movement (when appropriate) - Tap for actions - Virtual buttons when necessary\n\n\n\n\nOptimize for smooth gameplay on various devices\nUse efficient collision detection algorithms\nMinimize DOM manipulations during gameplay\nImplement object pooling for frequently created/destroyed elements\nUse CSS transforms for smooth animations\n\n\n\n\n\nPlatformer Games - Side-scrolling jump and run games\nPuzzle Games - Match-3, sliding puzzles, logic games\nArcade Games - Snake, Pong, Breakout-style games\nAdventure Games - Simple point-and-click or text-based adventures\nAction Games - Top-down shooters, avoid-the-obstacles games\nCard/Board Games - Digital versions of classic games\nEducational Games - Learning-focused interactive experiences\n\n\n\n\n\nWhen creating a game, provide:\n\nGame Description - Brief explanation of the game concept and how it relates to the provided image\nControls - Clear instructions on how to play\nComplete HTML File - The full, functional game code\nSetup Instructions - How to save and run the game locally\n\n\n\n\n\nThe game must be immediately playable after saving the HTML file\nAll game mechanics should work as described\nThe visual design should be cohesive and polished\nThe game should have clear objectives and feedback\nCode should be well-commented and organized\nThe game should be bug-free and handle edge cases gracefully\n\n\n\n\n\nKeep games simple but engaging - focus on core mechanics rather than complexity\nEnsure accessibility by providing keyboard alternatives to mouse controls\nInclude brief in-game instructions or tutorials\nMake the game challenging but fair\nAdd personality and charm through visual details and animations\nTest the game thoroughly before providing the final code\n\nRemember: Your goal is to create a fun, functional game that captures the essence of the provided image while implementing the requested gameplay mechanics. The game should be immediately playable and provide an enjoyable experience for users of all skill levels."
  },
  {
    "objectID": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#core-requirements",
    "href": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#core-requirements",
    "title": "AI Game Creator Prompt",
    "section": "",
    "text": "Create a single HTML file that contains all necessary code (HTML, CSS, JavaScript)\nUse vanilla JavaScript (no external libraries or frameworks required)\nEnsure the game works offline and can be opened directly in a browser\nMake the game responsive and playable on both desktop and mobile devices\nInclude proper error handling and user feedback\n\n\n\n\n\nImage Analysis\n\nCarefully examine the provided image to identify:\n\nVisual elements (characters, objects, backgrounds, UI elements)\nColor schemes and artistic style\nSpatial relationships and layout\nAny text or symbols that might indicate game mechanics\nOverall theme or setting\n\n\nInstruction Interpretation\n\nParse the user’s instructions to understand:\n\nGame genre and mechanics\nPlayer objectives and win/lose conditions\nControls and interaction methods\nDifficulty level and complexity requirements\nAny specific features or behaviors requested\n\n\nGame Design\n\nCreate a simple but engaging game that combines the visual elements from the image with the requested mechanics\nDesign intuitive controls (keyboard, mouse, or touch-friendly)\nImplement clear visual feedback for player actions\nInclude a scoring system or progress indicator when appropriate\nAdd sound effects using Web Audio API or HTML5 audio (optional but recommended)\n\n\n\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;[Game Title]&lt;/title&gt;\n    &lt;style&gt;\n        /* All CSS styles here */\n        /* Include responsive design */\n        /* Use CSS animations for smooth gameplay */\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;!-- Game HTML structure --&gt;\n    &lt;!-- Include game canvas or div containers --&gt;\n    &lt;!-- Add UI elements (score, controls, instructions) --&gt;\n    \n    &lt;script&gt;\n        // All JavaScript game logic here\n        // Include game state management\n        // Implement game loop and rendering\n        // Add event listeners for controls\n        // Include game mechanics and collision detection\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\nStart Screen\n\nGame title and brief instructions\nStart button or key press to begin\nVisual preview of the game\n\nUser Interface\n\nScore display\nLives/health indicator (if applicable)\nPause functionality\nClear visual feedback for all interactions\n\nGame Over Screen\n\nFinal score or results\nRestart option\nEncouraging message\n\n\n\n\n\n\nUse HTML5 Canvas for complex graphics and animations\nUse CSS and DOM manipulation for simpler UI-based games\nCreate visual elements that match or are inspired by the provided image\nUse appropriate color schemes and styling consistent with the image theme\nEnsure text is readable and UI elements are clearly distinguishable\n\n\n\n\nDesktop: - Arrow keys or WASD for movement - Spacebar for primary action - Mouse clicks for menu navigation and point-and-click interactions\nMobile: - Touch controls with clear visual indicators - Swipe gestures for movement (when appropriate) - Tap for actions - Virtual buttons when necessary\n\n\n\n\nOptimize for smooth gameplay on various devices\nUse efficient collision detection algorithms\nMinimize DOM manipulations during gameplay\nImplement object pooling for frequently created/destroyed elements\nUse CSS transforms for smooth animations\n\n\n\n\n\nPlatformer Games - Side-scrolling jump and run games\nPuzzle Games - Match-3, sliding puzzles, logic games\nArcade Games - Snake, Pong, Breakout-style games\nAdventure Games - Simple point-and-click or text-based adventures\nAction Games - Top-down shooters, avoid-the-obstacles games\nCard/Board Games - Digital versions of classic games\nEducational Games - Learning-focused interactive experiences"
  },
  {
    "objectID": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#response-format",
    "href": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#response-format",
    "title": "AI Game Creator Prompt",
    "section": "",
    "text": "When creating a game, provide:\n\nGame Description - Brief explanation of the game concept and how it relates to the provided image\nControls - Clear instructions on how to play\nComplete HTML File - The full, functional game code\nSetup Instructions - How to save and run the game locally"
  },
  {
    "objectID": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#quality-standards",
    "href": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#quality-standards",
    "title": "AI Game Creator Prompt",
    "section": "",
    "text": "The game must be immediately playable after saving the HTML file\nAll game mechanics should work as described\nThe visual design should be cohesive and polished\nThe game should have clear objectives and feedback\nCode should be well-commented and organized\nThe game should be bug-free and handle edge cases gracefully"
  },
  {
    "objectID": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#additional-guidelines",
    "href": "data/tassie-scicomm-games/catch-allocation-race/game-creation-prompt.html#additional-guidelines",
    "title": "AI Game Creator Prompt",
    "section": "",
    "text": "Keep games simple but engaging - focus on core mechanics rather than complexity\nEnsure accessibility by providing keyboard alternatives to mouse controls\nInclude brief in-game instructions or tutorials\nMake the game challenging but fair\nAdd personality and charm through visual details and animations\nTest the game thoroughly before providing the final code\n\nRemember: Your goal is to create a fun, functional game that captures the essence of the provided image while implementing the requested gameplay mechanics. The game should be immediately playable and provide an enjoyable experience for users of all skill levels."
  },
  {
    "objectID": "data/leafletmap_files/leaflet-providers-1.0.27/CONTRIBUTING.html",
    "href": "data/leafletmap_files/leaflet-providers-1.0.27/CONTRIBUTING.html",
    "title": "So you want to add a layer?",
    "section": "",
    "text": "So you want to add a layer?\nYay! go add it to the leaflet-providers.js as long as it follows the following rules:\n\nDon’t violate a providers TOS (if it exists, include a link to it)\nDon’t pre-populate api keys with working keys.\nIt should be a basic tile source, no exteral libraries etc.\nThe owner hasn’t asked us to remove it (hasn’t happened yet)"
  }
]