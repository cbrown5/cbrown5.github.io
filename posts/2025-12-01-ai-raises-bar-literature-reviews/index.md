---
date: '12/01/2025'
title: AI has raised the bar for literature reviews
categories: [research-skills, genAI]
published: TRUE
---

I've been reviewing papers recently that provide lists of applications - particularly common in reviews about AI use in ecology and fisheries. The problem with these lists? They don't provide a compelling read. 

More importantly, any chatbot can generate these lists now. ChatGPT, Claude, and Copilot all have web search integrated. They can produce literature reviews on demand with real references. Ask "how can AI be used in ecology?" and you'll get a comprehensive list in seconds.

So here's the uncomfortable question: if readers can generate that content themselves with AI, why would they bother reading your journal article? And if you're producing content that AI can replicate, why does science need you?

This raises the standard for what needs to go into literature reviews. They need to be informed by deep human thought. AI can be a tool, but you need original insights that go beyond what everyday chatbots can produce.

One example that gets this right is [a recent paper reviewing multimodal large language models in ecology](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14556). Yes, they list different applications, but there's greater depth. They provide ecology-specific examples that take work to develop and find. More importantly, they synthesize these potential uses and discuss future challenges and directions with tangible actions.

Going forward, AI has raised the bar. Don't write or produce anything that AI can easily generate. There's no point - anyone can grab that content themselves. Focus on what only you can bring: deep domain expertise, synthesis, and original thought.
